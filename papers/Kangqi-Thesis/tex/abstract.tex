%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode
%%==================================================
%% abstract.tex for SJTU Master Thesis
%%==================================================

\begin{abstract}

自然语言是人类进行信息交流和知识保存的重要工具，
同时也是人机交互过程中最主要的形式。
因此，让机器实现对自然语言的理解，
是现阶段人工智能的重要发展方向，同时也是学术界的热门研究课题。
自然界中存在的不同事物，以及事物之间的联系已是海量级别，
随着互联网中以维基百科、IMDB等数据库为首的结构化信息的大量积累，
用于组织和维护开放领域中海量知识的大规模结构化知识库应运而生。
它们以标准化的符号存储了千万以上的实体、以及十亿以上实体之间具有的关系，
成为了语义表达的有效载体，同时也引出了一系列基于知识库的自然语言理解研究。
因此，本文针对描述客观事实的自然语言文本，利用知识库实现多个维度的语义理解。

%注意这个地方，研究问题可以解决一些任务，但是特定任务总有多种解决方法，其中不是每个方法都隶属于
%我们要研究的问题。
根据语义所体现的不同层次，本文从实体、关系和句子这三个层面研究自然语言理解问题。
实体是语义中不可再分的元素，多个实体由关系互相连接构成基本事实，
而句子往往包含着多个关系，具有更加复杂的整体语义。
具体而言：
实体层面的理解体现为直接匹配，将文本中代表实体的短语链接至知识库中的特定实体；
关系层面的理解体现为结构匹配，将自然语言关系转换为由知识库关系（谓词）所构建的特定语义结构；
句子层面的理解则对单一关系的结构匹配进行深入扩展，对于问句而言还体现为推理匹配，
即根据语义结构，从知识库中寻找问句的正确答案。
对于这些粒度的自然语言理解问题，需要使用不同的方法进行语义建模。

对于实体理解问题，其核心为计算实体短语的上下文信息与候选知识库实体间的匹配程度。
经典的实体链接任务具有以下特点：候选实体数量庞大，实体短语普遍存在的一词多义性，
以及候选实体之间存在相互依赖关系。
本文中，我们关注对表格文本进行跨语言的实体链接任务，
除了上述特点以外，表格文本所具有的半结构性，
以及文本和知识库由不同的语言所描述，
这给此任务带来了新的挑战。
%一方面，已有的翻译工具可以帮助将跨语言的链接任务转换为单语言，
%但链接模型效果容易受限于翻译质量。
%另一方面，表格中相同行列的实体之间具有更加特殊的联系，
为此，我们提出了基于神经网络和跨语言词向量的链接模型，其优势在于：
降低翻译过程带来的信息损失，学习表格行列方向的上下文和一致性特征，
并通过联合训练框架提升整体链接质量。
在跨语言和单语言两个场景上的实验表明，
我们的模型有效捕捉表格中实体之间的特殊联系，同时在跨语言场景中具有稳定而良好的效果。

对于关系理解问题，其核心为用知识库中的结构描述自然语言中，一个二元关系的语义。
该问题主要具有以下两个特点：首先自然语言关系同样存在多义性，
其次关系和知识库中的谓词存在语义间隔，难以实现简单的一一对应。
%由于自然语言与知识库的构建方式存在语义间隔，自然语言谓语有可能难以直接对应知识库中的单个谓语。
基于这两个特点，我们对自然语言关系进行了两个粒度的语义建模。
粗粒度的建模聚焦于关系的多义性，我们通过对知识库构建更加丰富的类型层次结构，
挖掘一个二元关系的主语和宾语所具有的不同类型搭配，
实验结果表明我们的模型效果优于传统的选择偏好模型。
%
细粒度的建模旨在利用知识库实现对关系语义的精确表达，
我们致力于使用人类能理解的图结构描述关系语义，
提出了基于规则推导的模式图推理模型，以挖掘关系可能的复杂结构表示，
并将其运用于知识库补全任务。
实验结果显示，我们的模式图推理模型不仅具有高度可解释性，
而且效果优于其它规则推导模型和新兴的知识库向量模型。

对于问句理解问题，我们着眼于基于知识库的自动问答任务，
即在知识库中寻找代表答案的实体集合。
由于问句包含了未知答案与其它实体的一个甚至多个关系，
其语义变得更加复杂的同时，带来了如下挑战：
如何描述问句的复杂语义，以及如何有效度量问句和语义结构之间的相似度。
基于深度学习的语义匹配模型得到了广泛的研究，但这些模型所适用的语义结构存在限制，
对复杂问题的回答存在瓶颈。
为此，我们提出了针对复杂问题的的深度学习语义匹配模型。
该模型沿用关系理解中的图结构表示，首先生成问句可能对应的候选查询图，
然后利用深度神经网络学习这些查询结构的整体语义表示，
以此捕捉问句中不同语义成分的有机结合。
实验结果表明，基于复杂查询图的深度学习模型
在多个复杂问题和简单问题数据集上都具有良好的性能。

综上所述，本文从实体、关系、问句三个粒度出发，
研究自然语言和知识库之间的语义理解与匹配问题。
在实体理解中，我们提出了基于神经网络、跨语言词向量以及联合训练的链接模型，
并用于解决跨语言场景中对表格文本进行的实体链接问题。
对关系和问句的语义理解，我们始终贯彻语义建模的可解释性，
使用主宾语类型搭配描述关系具有的多义性，
以及使用基于知识库的图结构描述关系或问句的精确语义。
对于自动问答任务，我们提出的深度学习模型实现了对复杂图结构的整体建模，
得以充分体现其特征学习能力，更有效地度量问句与复杂结构的语义匹配程度。
最后，希望本文的一系列工作能够对该领域今后的学术研究有所帮助。


\keywords{知识库，自然语言理解，实体链接，知识库补全，自动问答，深度学习}
\end{abstract}

\begin{englishabstract}

Natural language is an important tool for human information exchange and knowledge preservation,
and also the most important form used in human-computer interaction.
To make the machine better understand natural language (NL) becomes 
the main direction of artificial intelligence at this stage, and also the hot
research topic in academic fields.
There exists massive things and relations between things in the world.
With the growing number of structural knowledge in the World Wide Web,
such as Wikipedia and IMDB, 
researchers had developed large-scale structured databases to store, organize
and maintain those massive facts in open domains, and we call them Knowledge Bases (KB).
Knowledge bases make use of standardized symbols to
store more than tens of millions of entities
and more than one billion facts that exist between entities.
Therefore, the KB becomes an effective carrier of semantic representation,
leading to a series of KB-based semantic understanding research.
In this dissertation, we use the KB to realize the semantic understanding
of multiple dimensions for natural language texts that describe objective facts.

Considering different granularities of semantics, 
we conduct our research of the semantic understanding problem
from three levels: entity, relation and sentence level.
Entities are semantically indivisible elements,
and multiple entities are connected by a relation, describing a single fact.
While a sentence may contain several relations,
thus holds a more complex semantics.
For the understanding at the entity level,
we directly mapping phrases in the text to specific entities in the KB.
For the relation level, we attempt to represent natural language predicates by
using specific structures made up of predicates in the KB.
The understanding at the sentence level goes deeper than relation level,
especially for questions, where we aim at automatically retrieving answers by the inference over the KB.
Different methods are needed for the semantic modeling at different levels.

For entity understanding, the kernel part is to calculate 
the degree of matching between the phrase and the KB entity.
The classical entity linking task has the following characteristics: 
large number of candidate entities, ambiguity of phrases,
and the interdependence of entities of different phrases.
Moreover, we focus on the cross-lingual entity linking task for the text from web tables.
In addition to the above features,
how to leverage the semi-structure of table texts,
and how to bridge the linguistic gap between texts and KBs,
become the new challenges of the task.
Thus, we propose a linking model based on neural networks and cross-lingual word vectors,
which has the advantages of reducing the information loss caused by the translation process,
learning the context and coherence features of the table row and column direction,
and improving the overall link quality through the joint training framework.
Experiments on both monolingual and cross-lingual scenarios show that our model
effectively captures the special connections between entities in the table, 
and keeps a stable and good result.

For relational understanding, the core is to describe the semantics of a 
NL relation with structures in the KB.
It has two characteristics: First, the NL predicate has ambiguity; 
second and different from entity understanding,
there exists semantic gaps between NL and KB predicates,
hence it is difficult to achieve a simple one-to-one mapping.
Based on these two points, we attempt to model the semantics of a relation via two granularities.
The coarse-grained modeling focuses on the ambiguity of NL predicates.
By constructing a richer hierarchical structure for types in the KB, 
we mine the different type combinations of the subject and object that a NL predicate holds.
Experimental results show that our model outperforms the traditional selectional preference model.
The fine-grained modeling aims to precisely express the semantics of relations using the KB.
We use the human-understandable graph structure to describe the NL predicate,
and propose a rule induction based inference model, 
which is able to express the complex semantics of NL predicates via schema graphs.
We apply the structural representations to the knowledge base completion task,
and the experimental results show that our schema graph inference model
is not only highly interpretable,
but also outperforms other rule induction model and the emerging knowledge base embedding model.

For question understanding, we focus on the task of knowledge base question answering,
that is to retrieve the answer entity set of the question from the KB.
There exists one or more relations between the unknown answer
and the other entities in the question, 
which brings a more complex semantics as well as the following challenges:
how to describe such complex semantics, and how to effectively
measure the similarity of the question and candidate semantic structures.
The semantic matching model based on deep learning has been widely studied,
but usually these target structures are limited, hence
answering complex questions becomes a bottleneck of previous work.
To this end, we propose the deep semantic matching model for answering complex questions,
which follows the idea of graph based semantic representation used in relation understanding.
We first generate candidate query graphs of the question,
then encode such complex query structure into a uniform vector representation
via deep neural networks, thus successfully capture the interactions between
individual semantic components within a complex question.
Experiments on multiple QA datasets show that our approach
consistently outperforms existing methods on complex questions
while staying competitive on simple questions.

In summary, starting from the three granularities of entity, relation and question,
this paper studies the problem of semantic understanding and matching
between natural languages and knowledge bases.
For entity understanding, we propose the link model based on 
deep neural networks, cross-language word vectors and joint learning scheme,
for solving the entity linking task of tabular texts in cross-lingual scenarios.
For the understanding of both relations and questions, 
to keep the interpretability of semantic modeling, 
we use the type combination of subjects and objects to describe the ambiguity of the relation,
and use the graph structure based on the KB to describe
the exact semantics of the relation or the question.
For the task of question answering, our proposed deep learning model
aims at the encoding of the entire query structure,
which makes better use of the ability of feature learning,
and more effectively measures the matching level 
between questions and complex semantic structures.
Finally, hoping our work in this paper can help future academic researches in this field.

\englishkeywords{knowledge bases, natural language understanding, 
entity linking, knowledge base completion, question answering, deep learning}
\end{englishabstract}

