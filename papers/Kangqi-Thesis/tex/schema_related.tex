%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode

\subsection{相关工作}
\label{sec:schema-related}

%\KZ{First discuss AAAI 2012 and EMNLP 2015, their pros and cons and how
%we stack up with them. Then discuss other less similar work. Finally
%applications that can benefit from this work, eg. QA, etc.}
% introduce AAAI2012, analyze pros and cons
%is based on such a procedure called ontology mapping. Given a user-specified relation along with its labeled instances, ontology mapping actually generates several complex SQL expressions over types and relations on the KB's schema. This procedure is quite difficult since the space of possible SQL views can be extremely large. In order to reduce the search space and select the best views, the authors first generates several constraints (hard rules) described in Markov Logic. This step actually is the a procedure to generate simple candidates schemas. Then, the probability of mappings is described using Markov logic Network after adding different rules into the network. Through weight training and relaxing the optimization problem to a linear problem, those candidate schemas with a high probability form to the mapping result. This work is able to show a set of best schemas for a target relation. However, the authors add several hand-crafted soft rules to Markov Logic Network which limits the dimension of feature space. Besides, the complex SQL views can actually be transformed to simple schemas (paths) which can not be able to handle complex relations in natural language form.
% introduce emnlp 2015 SFE


%Previous work~\cite{zhang2012ontological,gardner2015efficient,gardner2014incorporating,lao2011random} has attempted to map a relation to
%background KB skeletons.
%The goal of these works is to complete the imperfectly extracted KB
%\textit{NELL} \cite{carlson2010toward} by predicting all concept $b$
%which potentially have the relation $R(a, b)$ given a concept $a$.


%However, all of the above work only
%considers the simple path representations.
%In contrast, our approach adopts complex schema
%with constraints, which can describe more sophisticated NL relations.
%Moreover, when solving KB completion problem, we use only schemas
%of NL relations as features, whereas previous work use many other
%features.

随着大规模结构化知识库的提出与广泛使用，知识库补全任务成为了近年来的热门研究课题。
该任务旨在对知识库中已有的谓词进行建模，
通过预测潜在的($e_1$, $p$, $e_2$)三元组，实现扩充知识库的最终目的。
%Lao et al. \cite{lao2011random} proposed Path Ranking Algorithm (PRA),
%which used a random walk path finding algorithm to infer new relation instances by
%mapping the target KB relation into a path of several basic relations.
%The state-of-art system \cite{gardner2015efficient}
%examined the disadvantage of PRA and proposed a technique called subgraph feature extraction (SFE).
%It first runs local search to characterize the subgraph around each input entity in KB.
%Then SFE runs a set of feature extractors over these subgraphs to retrieve
%structural and semantic features for each candidate node.
%SFE outperforms other KB completion methods as it used more advanced features.
% embedding approaches
到目前位置，在该课题上的研究方法主要分为两类：
基于知识库表示学习和基于规则推导。
%By far most literature fall into two categories: {\em embedding} based and {\em rule} based.

知识库表示学习受到词向量技术\cite{mikolov2013exploiting,pennington2014glove}的启发，
将知识库中的实体类比为单词，
每个实体具有一个向量表示，对应连续语义空间上的一个点。
作为连接不同实体的桥梁，知识库中的每个谓词都对应着各自的向量或矩阵表示。
通过定义不同的向量或矩阵之间的运算方式，这类方法可以计算每个三元组的置信度，
以此实现对实体及谓词的表示学习。

RESCAL模型\cite{nickel2012factorizing}是一个基础的知识库向量模型，
它基于实体向量和谓词矩阵表示的双线性运算。
HOLE模型\cite{nickel2015holographic}是RESCAL模型的改进，
使用向量循环平移的技巧计算实体间的组合语义向量，大幅度降低了谓词的表示维度。
在众多知识库表示学习的方法中，有一组方法称为隐距离模型，
它们对三元组置信度的计算方式主要基于连续空间中的距离度量：
将主宾语向量经过某种方式的映射（翻译）之后，距离越小，置信度越高。
最典型的研究工作为TransE，其核心思路在于尽可能使每个三元组(h,r,t)
对应的向量计算满足$\textbf{h} + \textbf{r} \simeq \textbf{t}$，
即利用谓词向量将连续空间中的主语进行平移，使其尽量与宾语重合。
为了能更好地表示多对多的关系，相关文献\parencite{wang2014knowledge,lin2015learning}   %TransH, TransR
对TransE模型进行了改良。
%TransG,pTransE,sTransE,甚至更多自己还没听过的
%TODO:如果需要填充，这个可以在外面的Related Work里面多说一些。
% KALE, TEKE, HOLE
Wang等人提出了TEKE模型\cite{wang2016text}，它对已有的翻译模型进行改良，
充分利用结构化文本的知识，寻找三元组中单词级别的共现，
并利用共现上下文微调实体和谓词的向量表示。


% too detailed
%In TransE, $\textbf{h} + \textbf{r} \simeq \textbf{t}$ is expected whenever triplet $(h, r, t)$ exists, and a loss function of $\sum_{(h,r,t)} \sum_{(h^{'},r,t^{'})} [\gamma + d(\textbf{h} + \textbf{r}, \textbf{t}) - d(\textbf{h'} + \textbf{r}, \textbf{t'})]$ is minimized during training, where $\gamma$ is the margin hyperparameter, $d(\cdot)$ is certain dissimilarity measure, $(h, r, t)$ is the true triplet and $(h^{'}, r, t^{'})$ are the corrupted triplets. TransH addressed TranE's problem in modeling 1-to-N, N-to-1 and N-to-N relations. It allows an entity to have different distributed representation when involved in different relations. TransH models each relation as a vector on a hyperplane, and when calculating dissimilarity, $\textbf{h}$ and $\textbf{t}$ are first projected onto that relation's hyperplane, then compute $d(\textbf{h}_{\bot} + \textbf{r} - \textbf{t}_{\bot})$. TransR~\cite{lin2015learning} proposed to represent entities and relations in distinct spaces, with translation performed in the relation space. For each relation $r$, TransR sets a projection matrix $\textbf{M}_r$ to project entity vector into relation space, i.e. $\textbf{h}_r = \textbf{hM}_r$, $\textbf{t}_r = \textbf{tM}_r$, and dissimilarity is computed as $||\textbf{h}_r + \textbf{r} - \textbf{t}_r||^2_2$.
%Other embedding methods include SME~\cite{bordes2012joint}, RESCAL~\cite{nickel2012factorizing}.

%TODO:这段不够严谨，需要靠外部Related进行补充，怎么着得讲清楚
基于规则推导的方法旨在用逻辑规则的形式表达谓词的语义。
例如$\text{parent}(x, y) \land \text{parent}(y, z) \rightarrow \text{grandparent}(x, z)$
是一个常识性的规则，我们可以通过规则的左侧部分，在知识库中寻找出更多的祖孙间的关系。
Jiang等人的工作\parencite{jiang2012learning}基于马尔科夫逻辑，通过挖掘的规则
对自动构建的知识库进行信息过滤。
其它一些方法使用概率软逻辑或关联规则挖掘完成类似的任务\cite{pujara2013large,volker2011statistical}。
%Pujara et al.~\cite{pujara2013large} proposed to use probabilistic soft logic (PSL) for this job.
%V\"olker et al.~\cite{volker2011statistical} proposed a statistical approach to induct schemas based on association rule mining.
Gal\'arraga等人提出的AMIE\cite{galarraga2013amie}以及AMIE+\cite{galarraga2015fast}系统
则直接根据知识库的三元组寻找置信度较高的一阶逻辑规则。
%TODO:妈呀我在说什么？
%TODO:上面这些研究感觉不属于KB的范畴，需要对它们进行一个概括，就这么放在这里肯定是不好的
%TODO:拟放进外面Related的东西：一个古老的论文（不一定是AMIE），PRA/SFE
最新的一些研究着眼于在知识库中寻找路径形式的规则，%为什么，得给一个理由吧
通过挖掘大量可能的路径，作为表示语义的特征。
Lao等人提出了PRA模型\cite{lao2011random}，
通过在谓词路径上的随机游走策略，衡量其连接一对实体的好坏程度，
目标关系的语义等同于不同路径特征的带权组合。
Gardner等人对PRA模型进行改进，提出了SFE模型\cite{gardner2015efficient}，
除了捕捉连接主宾语的路径以外，还从主宾语各自的知识库子图中挖掘独立的特征，
同时谓词路径的定义更加宽泛，允许在其中使用通配符表示任意谓词。
此外，Wang等人提出了CPRA模型\cite{wang2016knowledge}，
这是对PRA模型的另一种改进，
通过挖掘目标关系中的相关性，使得相似关系之间的路径挖掘结果可以互相影响。
然而，通过开放式信息抽取获得的三元组数量相对有限，
不同的关系之间几乎不存在重叠的实体对，在这种场景下，
CPRA模型效果等价于原始的PRA模型。

%  Other works treat rules as paths through entities in the KB.
%  %Path based methods infer connections between entities from existing paths in the KB.
%  Lao et al.~\cite{lao2011random} proposed Path Ranking Algorithm (PRA),
%  which used a random walk path finding algorithm to map the target KB relation into a sequence of several basic relations.
%  Subgraph feature extraction \cite{gardner2015efficient}, known as SFE, explores more than paths in the KB 
%  by exploring structural features around entities, and allows using a wildcard to indicate any possible edges.
%  %. It first runs local search to characterize the subgraph around each input entity in KB, then SFE runs a set of feature extractors over these subgraphs to retrieve structural and semantic features for each candidate node.
%  Wang et al.~\cite{wang2016knowledge} improved PRA with Coupled Path Ranking Algorithm (CPRA),
%  where similar relations are clustered and jointly learned.
%  However, in the experiment setting of this paper, relations in OpenIE dataset usually do not overlap and CPRA degenerates to PRA.

一些相关的研究尝试在知识库向量学习的基础之上加入一定的逻辑规则。
Guo等人提出了KALE模型\cite{guo2016jointly}，其主要思想是将规则转换为多个三元组之间的与或非逻辑操作，
因此基于翻译模型计算的三元组置信度得以在逻辑规则级别产生交互。
TRESCAL模型\cite{chang2014typed}在经典的RESCAL模型中加入了知识库的类型限制。
%TODO:Rockt\"aschel et al.~\cite{rocktaschel2015injecting} proposed to embed first-order logic into low-dimensional vector spaces,
而Wang等人的工作\cite{wang2015knowledge}使用整数线性规划技术，将知识库向量表示和规则挖掘进行统一，

%Some works in KBC combine above approaches by incorporating rules into embedding models.
%Logic rules are combined with embedding in KALE \cite{guo2016jointly},
%where the idea is to represent and model triples and rules in a unified framework.
%TRESCAL~\cite{chang2014typed} encodes type constraints into RESCAL~\cite{nickel2012factorizing}.
%Rockt\"aschel et al.~\cite{rocktaschel2015injecting} proposed to embed first-order logic into low-dimensional vector spaces,
%and Wang et al.~\cite{wang2015knowledge} integrated KB embedding and rules with integer linear programming (ILP),
%with the objective function derived from the embedding model and constraints translated from rules.

狭义的知识库补全任务只考虑知识库中的谓词，
我们的工作将知识库补全的场景进行了扩展。
考虑到为了降低知识库结构与自然语言描述的差距，
知识库补全任务也可以针对自然语言中的二元关系。
开放式信息抽取与这样的任务相契合，
既提供了全新谓词，又有一定量的三元组用于补全学习。
一些已有的工作也关注了自然语言关系到知识库的映射。
Zou等人的工作\cite{zou2014natural}使用了非监督学习的方式，
利用TF-IDF特征寻找关系到谓词路径的匹配。
Zhang等人的工作\cite{zhang2012ontological}
利用马尔科夫逻辑网络\cite{richardson2006markov}，
学习自然语言关系对应于不同候选谓词路径的概率。
这些方法对关系的表示局限于路径的形式，
无法准确地描述一个形式简单但具有组合语义的关系。
我们的工作旨在理解具有复杂语义的关系，挖掘其包含的隐含限制条件，
并通过具有 ``{路径+分支}'' 结构的模式图进行语义建模。


%  In traditional KBC tasks, the target relation is an existing predicate in the KB,
%  while we extend the definition of KBC into a broader scenario, since one may wish to add
%  a new predicate (derived from natural language) into existing KB, and the Open IE system
%  can help provide seed relation instance for further enrichment.
%  In terms of mapping NL relation into KB, Zou et al.~\cite{zou2014natural} proposed an unsupervised TfIdf-based algorithm to figure out the mapping confidence of predicate paths to one relation.
%  %The algorithm adopted the idea of tf-idf, which combines entity pairs covered by the path
%  %in a relation (as term frequency)
%  %and the number of distinct relations that one path could support (as inverted document frequency).
%  Zhang et al.~\cite{zhang2012ontological} also focused on learning path predicates using a Markov Logic Network~\cite{richardson2006markov}.
%  %which consists of soft rules
%  %on both positive and negative entity pair coverage, along with length of paths.
%  While the above KBC systems focus on path representation, our work aims at understanding
%  semantically complex relations and adopts complex schema with constraints.






%Natural language relations always have more complex meaning
%than KB predicates and using our system,
%a target human raised relation can be mapped into an
%explicit readable schema graph.

%To solve the paraphrasing problem between natural language relations and KB predicates, we aim to represent a human raised relation with several explicit schemas. One major difference between our technique and others is that during the procedure of generating candidate schemas for target relations, we do not limit the schemas to be simple only. We fully utilize the information of KB, adding extra constraints to the simple schemas and resulting in more complex schemas. Specifically, we perform a breadth-first search to construct the skeleton of a specific relation schema which is similar as the path finding procedure in the previous works \cite{gardner2015efficient,gardner2014incorporating,lao2011random,zhang2012ontological}. Beyond relation path, we use a depth-first search to further add more information attributes to the relation path generated in the first step and transform it into a more specific and complex graph form, under the guidance of \textit{Minimum Description Length} (MDL) \cite{fisher2008dirt,grunwald2007minimum} principle. MDL principle is used as a trade-off since it measures the cost of transmitting both schemas and entity pairs.

% query synthesis

% question and answering via paraphrasing
%% remove QA
%Our work also intersects with ontology question answering.%~\cite{yahya2012natural,krishnamurthy2012weakly,fader2013paraphrase}.
%One branch of QA techniques are semantic parsing based, it translates questions directly into structural query graphs through pre-defined grammars, such as CCG~
%\cite{kwiatkowski2010inducing,cai2013large,kwiatkowski2013scaling,reddy2014large}
%and $\lambda$-DCS~\cite{liang2011learning,berant2013semantic,berant2014semantic}
%, then perform query on SPARQL engine.
%Our schema shares the similar backbone structure with query graph, but the key difference between
%semantic parsing and our work is how to generate the query graph:
%In semantic parsing methods, the query graph is constructed recursively based on combining syntactic components,
%therefore complex semantic can only be generated if the question is \textbf{syntactically} complex;
%therefore complex semantics is limited to \textit{syntactically} complex questions;
%in contrast, we leverage grouped training data to discover semantic representation
%for syntactically simple but \textit{semantically} complex phrases.

%Another branch of QA techniques are information retrieval based~
%\cite{yao2014information,bordes2014question,yih2015semantic},
%which first retrieves a broad set of candidate answers or query graphs by traversing around
%question focus entity over the knowledge base, and then design syntactic and semantic features
%to capture the latent associations between question surface and the answer.
%Closest to our schema generation method is Yih et al.~\cite{yih2015semantic} who also
%uses depth-first search method to extract candidate query graphs.
%However, their constraints relied on handcrafted rules with explicit syntactic components,
%and are available only in certain domains (such as gender, marriage and event time).
%While our method is able to summarize more flexible constraints from existing relation instances.

%Besides, our work is similar to query synthesis in relational database~\cite{niehren2013query,das2010synthesizing,cheung2012inferring,cheung2013optimizing}.Given a set of input table and an output table,query synthesis automatically produces a relational query that produces the output when applied to the input.This problem is similar to ours as the query is analogous to the schema while the database is similar to the KB, but the database query is more close to a decision tree, because the task requires producing the exact output table. Typically, Zhang and Sun \cite{zhang2013automatically} address query synthesis using a three-steps technique. First they create an incomplete query skeleton which captures the basic structure of the result query, then complete the skeleton by adding some concrete and accurate rules and generate a list of candidates, and finally ranks the candidates, which simply prefers queries with simpler structures. Though first two steps share the same intuition with our schema generation procedure, the techniques cannot be directly applied to NL domain, due to the different functionalities between relational query and schema.

%In question answering by paraphrasing \cite{harabagiu2006methods,berant2013semantic,fader2013paraphrase,kwiatkowski2013scaling,berant2014semantic},
%as a representative, Berant and Liang~\cite{berant2014semantic} attack
%semantic parsing by mapping natural language utterances into logical forms
%to be executed on a KB using a paraphrase model and furthermore
%improved QA performance. We compared our results to theirs in the experiments
%section.

% other parts
%Other related work includes unsupervised systems such as
%\cite{zou2014natural}, which calculates scores of candidate skeletons
%using TF-IDF and then choose the best one to represent the target relation.
%%As for concrete mapping format, MapOnto \cite{an2006discovering} uses Horn clauses when produces mapping rules between two schemas. Others \cite{zhang2012ontological} generate complex SQL queries consisting of operations like join, union, project and select as mappings.
%Graph-based representations \cite{reddy2014large} is usually
%used in exploiting structural and conceptual similarity between NL
%and KB. Zou et al. \cite{zou2014natural} interpret a natural language
%question as a semantic query graph where each vertex represents an argument
%and each edge is associated with a relation phrase.
%Compared to these works, our schema graph is more complex with constraints.

