%# -*- coding: utf-8-unix -*-
% !TEX program = xelatex
% !TEX root = ../thesis.tex
% !TEX encoding = UTF-8 Unicode

\section{相关工作}%related
\label{sec:tabel-related}

% %\label{sec:el}
% Entity linking has been a popular topic in NLP for a long time as it is the basic step for machines to understand natural language and an important procedure of many complex NLP applications such as information retrieval and question answering. Entity linking requires a knowledge base to which entity mentions can be linked, the most popular ones including Freebase~\cite{bollacker2008freebase}, YAGO~\cite{suchanek2007yago} and Wikipedia~\cite{cai2013wikification}, where each Wikipedia article is considered as an entity. 
% %Most works focus on linking to Wikipedia and thus the task is also named as Wikification. The typical procedure of entity linking contains two stages: candidate generation, where the surface forms (mention) in the query text which could be linked to certain entities in the KB are identified, and a set of candidate entities are proposed for each entity mention; candidate ranking, where the candidates for each mention are ranked (usually based on context) and the best one is returned as linking result. 
% Due to its fundamental role in many applications, the task of entity linking has attracted a lot of attention, and many shared tasks have been proposed to promote this study~\cite{ji2010overview,cano2014microposts2014,carmel2014erd}.
% %Wikipedia was first explored by Bunescu and Pasca~\cite{pasca2006using}, where an SVM kernel was used to compare the lexical context of an entity mention to each candidate's Wikipedia page. Since each entity mention needed to train its own SVM model, the experiment was limited. Later, Mihalcea and Csomai~\cite{mihalcea2007wikify!:} proposed a system called Wikify! for the Wikification task. They applied word sense disambiguation to this task, and experimented with two methods to link detected candidates to a Wikipedia page: 1) comparing the mention's lexical context to content of disambiguation page; 2) training a Naive Bayes classifier for each ambiguous mention. 
% %Later approaches made use of the observation that entity disambiguation in the same document should be related. Cucerzan~\cite{cucerzan2007large-scale} maximized the agreement between the context data stored for each candidate entity and the contextual information in the document, and also the agreement among the category tags of the candidate entities. Milne and Witten~\cite{milne2008learning} took a similar approach but relied on unambiguous terms in the context. Han and Zhao~\cite{han2009named} constructed a large-scale semantic network from Wikipedia, then computed similarity between query and candidate entity based on the semantic network. Ratinov et al.~\cite{ratinov2011local} formalized this task into a bipartite graph matching problem and proposed a score function which considered both local similarity and global coherence. Zhang et al.~\cite{zhang2011wikipedia} employed a Wikipedia-LDA model and the contexts were modeled as a probability distribution of Wikipedia categories. The similarity score between candidates and entities were computed based on the category distribution. 
% %Cai et al.~\cite{cai2013wikification} proposed to first enrich the sparsely-linked articles by adding more links iteratively and then use the resulting link co-occurrence matrix to disambiguate the mentions in an input document. Yang and Chang~\cite{yang2016s-mart:} proposed a tree-based structured learning framework, S-MART, which is particular suitable for short texts such as tweets. 
% 

对互联网表格的研究最早开始于Cafarella等人的工作\cite{cafarella2008webtables}，
文中指出大约有1.54亿表格可以作为高质量的关系数据源。
例如文献\parencite{munoz2014using,sekhavat2014knowledge}关注于从表格中寻找
不同列之间的关系，从而实现向知识库中补充新的三元组。
这些工作都假定实体链接已完成，而若要对更广范围的表格数据进行关系挖掘，
表格链接始终是其前置步骤，链接准度直接决定了后续步骤的质量。
%如概述中指出，  表格关系可以补充三元组
%Munoz:找关系 挖掘不同行之间，但是是同样的两列之间的实体关系，映射至DBPedia 构成三元组，即挖掘语义。  并补充缺失
%在维基百科内部（已有链接）
%按已有match比例尝试一些predicate，然后学习一个triple是对还是错
%Sekhavat: Tabular KBC 同样是假设linking已好的情况 利用PATTY作为跳板，根据EP之间存在的pattern，寻找到rel的关系
%条件概率  （相当于EP有一系列的外部文本pattern，而不是仅有KB）
%rel来自YAGO
%(Fuck...但两者根本都是column-column-rel和predicate的一一对应)

和纯文本上的实体链接任务不同，表格文本上的链接聚焦于表格中的每一个单元格，
并且对于任何一个待链接的单元格，其它同行或同列的单元格与其有着更加密切的语义联系。
%不同的研究工如何利用好表格的半结构化
%如何利用同行列实体间的关联
目前已有的表格链接研究主要基于特征工程。
Limaye等人\cite{limaye2010annotating} 以YAGO为知识库，解决更加宽泛的表格链接任务，
包括将单元格链接至实体、列头链接至类型，以及两列之间的关系链接至谓词，
同时创建了WebManual数据集。
作者提出了一个概率图模型用于同时完成不同的链接子任务，
并通过人为定义的多种势函数表示单元格、实体、类型、谓词语间的组合特征，
整个表格链接的目标函数为多种势函数的连乘，不同子任务的决策互相影响，
使得模型在捕捉单个单元格与实体相匹配的同时，也能兼顾实体与列头类型的一致性，
以及不同列实体间与特定谓词的相关性。
%全局优化
%概率图模型，定义了potential，用于描述pair或这triple feature among type,entity,relation
%都是feature indicator
%1. cell vs entity (tfidf-cos, jaccard, soft-cos ...)
%2. header vs type (the same)
%3. entity vs type (e \in T)
%4. type vs rel (soft schema match)
%5. entity vs rel
%特征都比较直观简单，but，
%Experiments show that attacking the
%three subproblems collectively and in a unified graphical inference
%framework give clear accuracy benefits compared to
%making local decisions
Bhagavatula等人\cite{bhagavatula2015tabel}利用了表格上下文的词汇信息，
对于待链接的单元格，将其行或列方向上的其它单元格文本合并形成上下文词袋，
与候选实体所对应的词汇进行相似度计算，得到多个相似度特征用于模型训练，
并采用迭代更新方式进行预测。
%graph model，iterative update配上features
%每个cell确定了context之后独立计算
%上下文特征（word level，entity level）
%coherence（MW）
Wu等人\cite{wu2016entity}首次尝试对中文表格进行链接，
提出的模型首先构建由单元格和所有候选实体组成的连通图，
然后在图中进行类似PageRank算法\cite{page1999pagerank}
的随机游走，以选择最佳链接结果，因此是一种非监督学习方式。
候选实体是否同行列决定了图中是否存在直接相连的边，
而单元格与实体、实体与实体之间所连边的权重则由预定义的相似度公式计算，
使用了编辑距离、词袋相似度、实体于三元组中共现等特征。
区别与以上研究，本文的工作基于深度学习，尝试不依赖常用的相似度计算公式，
而是利用神经网络挖掘表格和目标实体在多个粒度上的特征。


% %\subsection{Cross-Lingual Entity Linking}
% %\label{sec:cl}
% Starting from 2011 the annual TAC KBP Entity Linking Track has been using the multi-language setting~\cite{ji2010overview,ji2014overview,ji2015overview}, where the languages involved are English, Chinese and Spanish. 
% %Most systems for this task were adaptations of mono-lingual entity linking systems: either first do entity linking on foreign languages and then translate the results to English via language links, which requires a comprehensive knowledge base in the foreign languages; or first translate the query into English by some machine translation tool and then apply English entity linking algorithms, whose performance greatly relies on the machine translator. 
% %Some other systems tried to avoid the usage of such assumptions. McNamee et al.~\cite{mcnamee2011cross} first experimented with cross-lingual entity linking on documents. They first used a machine translation tool developed by Irvine et al.~\cite{irvine2010transliterating} to transliterate the detected query mentions into English and transform the task into a mono-lingual one. Then they extracted some features and ranked the candidates with SVM-rank. However, to train this model, parallel corporas which are well aligned at sentence level are required.
% Most methods managed to bridge the language gap through language-independent spaces.
% Fahrni et al.~\shortcite{fahrni2011hits} presented HITS' system for cross-lingual entity linking. Their approach consisted of three steps: 1) obtain a language-independent concept-based representation for query documents; 2) disambiguate the entities using an SVM and a graph-based approach; 3) cluster the remaining mentions which were not assigned any KB entity in step 2.
% Zhang et al.~\shortcite{zhang2011wikipedia} leveraged a modified version of Latent Dirichlet Allocation, which they call BLDA (Bilingual LDA) and bridged the gap between languages via topic space. 
% %They trained the topic model on English-Chinese Wikipedia page pairs (indicated by inter-language links) and disambiguated candidate entities by computing the inner product of the topic distributions of the query text and the entity Wikipedia page. Their approach does not require supervised learning and performs well with a conservative candidate generation stage. 
% Wang et al.~\shortcite{wang2015language} proposed an unsupervised graph-based method which matches a knowledge graph with a graph constructed from mentions and the corresponding candidates of the query document.
% Tsai et al.~\shortcite{tsai2016cross} trained a multilingual word and title embeddings and ranked entity candidates using features based on these multilingual embeddings. 
% %They used canonical correlation analysis~\cite{hotelling1936relations} to project the embeddings of two languages into the same space, whose goal is the same as the translation layer in our model.


跨语言的实体链接的主要目的是将文本中的实体短语链接至另一个语言构建的知识库上，
近几年的TAC-KBP数据集\cite{ji2010overview,cano2014microposts2014,carmel2014erd}
中包含了跨语言的实体链接任务。
%以2015年的TAC-KBP任务为例，
%%Ji: Overview of the TAC-KBP2015 entity discovery and linking tasks 数据集
%目标知识库为英文维基百科，待链接的纯文本则来自英语、汉语、西班牙语这三种语言。
%相对于英文而言，其它语言下的知识库较为匮乏，
%因此跨语言实体链接的意义在于借助信息量更大的英文知识库，
%更好地理解外文文本中的信息。
为了解决此类问题，%跨语言场景中的实体链接任务，
McNamee等人\cite{mcnamee2011cross}提出了一种基线方法，
利用已有的翻译工具将外文文本转换为英语，
再使用传统的单语言链接模型完成任务。
%这样的方法，好处在于模型简单易于实现，
%但同时也具有一个很大的缺陷，就是
%此法的缺陷在于对已有翻译工具准确率的高度依赖：
%一方面，文本翻译过程仅生成单一结果，一旦错误则对后续链接步骤影响很大；
%另一方面，翻译工具如同黑盒，无法根据训练数据进行优化。
%TODO:后续如果有时间，可以聊聊cross-lingual word embedding，但肯定不是限制该干的事情。
为了尽可能减少对翻译工具的高度依赖，
模型需要能学习同一个实体或概念在不同语言下的抽象表达，
并通过特定运算体现出不同抽象表达之间的联系，以完成语义的跨语言兼容。

基于跨语言词向量的链接模型是一种可行的解决方案，
跨语言词向量的相关内容已在\secref{sec:rw-linking-cle}中介绍。
Tsai等人\cite{tsai2016cross}首先分别训练英文和外文的词向量，
再用典型相关分析（CCA）学习各自语言的转移矩阵，使得不同语言词向量位于同一连续空间，
之后依据该词向量计算短语和实体在不同粒度上下文中的余弦相似度，形成多个特征进行训练。
Sil等人\cite{sil2017neural}提出了更加复杂的深度学习模型，
以学习短语上下文和实体在句子级别和单词级别的相似特征，
同时在实验中比较了CCA、均方误差等多种生成跨语言词向量的方式。
%Sil 2016
%Wang
%fahrni
除了跨语言词向量以外，Zhang等人提出的跨语言主题模型\cite{zhang2013cross}
也可用于描述不同语言上的相同语义。
传统的LDA主题模型\cite{blei2003latent}旨在描述文档的语义表示，
通过对 ``{文档—主题}'' 与 ``{主题—单词}'' 间的概率进行建模，
将一个文档表示为抽象主题上的概率分布。
考虑到同一个实体在不同语言中的维基页面，
虽然单词不同，但其主题十分相似，
因此双语LDA模型中，同一个抽象主题对应不同语言上的两个``{主题-单词}'' 概率分布，
从而外语上下文和英语维基页面之间可以在主题层面上概率分布比较，实现链接过程。


本文的工作是表格链接和跨语言实体链接两者的综合体现，同时也是首次对此问题进行研究。
