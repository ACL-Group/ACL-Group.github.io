\section{Conclusion}
\label{sec:conclude}
We analyze two possible reasons behind the repetition problem in abstractive
summarization: (1) attending to the same location in source,
and (2) attending to similar but different sentences in source. 
In response, 
we presented two methods to modify existing CNN seq2seq model, i.e.,
a section-aware attention mechanism (ATTF)
and a sentence-level backtracking decoder (SBD). 
The proposed models are able to train a model that produces 
summaries with natural level repetition that are fluent and coherent. 
%The proposed model is able 
%to produce more fluent and coherent summaries with minimal repetitions.
%Besides, 
It means that the summaries generated by our model are more accurate and 
readable. This can help user quickly get the main information from large of textual data,
saving the reading time and improving reading efficiency.
As some other natural language generation (NLG) tasks based on seq2seq model with attention mechanism
are orthogonal to our proposed methods,
they can also be enhanced with our proposed models.
%which can redistribute attention and reduce repetition.
%We find that the basic CNN seq2seq model 
%still has some problems, such as generating repeated word sequence. 
%We also argue that ROUGE is not a perfect evaluation metric for the abstractive 
%summarization. Our future work will focus on these two aspects.

