Review 1:
Reject1-'Insufficient analysis':
A good result of ATTF is shown at Line 613~615, summarizing the document from Table 1.
A not-so-good result of ATTF is shown in Table 6, summarizing Example 1 at Line 167~170.

Reject2-'Backtracking(SBD)':
Refer to G1.
The Example 1 is the source document of summaries in Table 6 and Figure 6.

Reject3&Reject4-'SBD vs ATTF':
Refer to G1.
We will fix the notations in the final version.

Reject5-'Human-evaluation':
The Readability scores(human-evaluation) in Table 5 is introduced at Line 525~545.

Review 2:
Weak1-'SOTA':
Refer to G2.
The proposed approach can reduce repetition from transformer-based models, too. 

Weak2-'Significance test':
We use Kruskal-Wallis test(Albert, 2017) as significance test. 
All p-values (Table 4) are less than 0.05: 3.41e-32(R-1), 2.12e-45(R-2), 0.01(R-L). 
We will add these into the final version.

Weak3-'SBD vs ATTF':
Refer to G1 and Review2-Weak2.

Review 3:
Weak1&Reject2-'SOTA':
Refer to G2.

Weak2&Reject1-'R-L':
We used the package ROUGE-1.5.5. 
R-L score is determined by the length of LCS,
hence it only captures the lexical similarity between generated summary and 
reference summary. Since abstractive summary of a document is not unique,
R-L is not the best evaluation metric. Thus, we use Readability(human-evaluation) 
to gauge the quality of each summary(Table 5).

General:
Thank you all for the precious comments and suggestions.

G1-'SBD vs ATTF':
We explained in Introduction that there are two sources of repetition:
i) attending to the same segment at different timesteps and
ii) attending to the different but similar sentences in source document.
ATTF and SBD respectively solve the above two problems. 
ATTF is applied at training time and aims at reducing repetition in the model itself. 
SBD is a practical enhancement used only at testing time.
ATTF has a lower R-2 score than SBD because
ATTF may produce some repetition caused by the repeated information
in the source(Example 1 and Figure 6).
However, ATTF has higher readability scores than SBD(Table 5).
Overall, ATTF+SBD is a good combination that increases the readability while 
reducing repetition.  

G2-'State-of-the-art model':
We aim at reducing repetition in abstractive summarization.
Existing repetition reduction techniques are shown in Table 3. Although some of them were
implemented on top of RNN, some on CNN, they are all independent of the underlying seq2seq models. 
This paper seeks to evaluate the effectiveness of our repetition reduction technique,
so we need to compare our proposed approaches with others on the same seq2seq model, to be fair.
We choose the vanilla CNN seq2seq model as our basic model to improve on,
because it's fast and enjoys the best accuracy among
the other vanilla seq2seq models such as RNN seq2seq model and LSTM seq2seq model(Gehring, 2017). 
We did not implement the repetition reduction methods on top of SOTA seq2seq models, 
because the tricks employed in these SOTA approaches may interact with the repetition 
reduction methods and complicate the analysis.
Our main purpose is to evaluate the repetition reduction, which is not necessarily
reflected in the ROUGE(See 2017; Paulus 2017; Fan 2018). 
Due to variable nature of abstractive summarization, ROUGE is not the optimal 
evaluation metric. Human-evaluation and Repeatedness score, in our opinion, 
are better. As shown in Table 5, our proposed models are 
more readable and closer to the natural repetition level.
Since most SOTA models use attention of some sort,
and our model deals with incorrect attention distribution, we can reasonably deduce
that these SOTA models will benefit from our techniques as well.

