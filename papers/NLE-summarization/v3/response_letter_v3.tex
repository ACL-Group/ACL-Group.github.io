%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plain Cover Letter
% LaTeX Template
% Version 1.0 (28/5/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Rensselaer Polytechnic Institute 
% http://www.rpi.edu/dept/arc/training/latex/resumes/
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{letter} % Default font size of the document, change to 10pt to fit more text

\usepackage{newcent} % Default font is the New Century Schoolbook PostScript font 
%\usepackage{helvet} % Uncomment this (while commenting the above line) to use the Helvetica font

% Margins
\topmargin=-1in % Moves the top of the document 1 inch above the default
\textheight=8.5in % Total height of the text on the page before text goes on to the next page, this can be increased in a longer letter
\oddsidemargin=-10pt % Position of the left margin, can be negative or positive if you want more or less room
\textwidth=6.5in % Total width of the text, increase this if the left margin was decreased and vice-versa

\let\raggedleft\raggedright % Pushes the date (at the top) to the left, comment this line to have the date on the right

\usepackage{xcolor}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{soul}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath,amsfonts,amssymb,amsthm,amsopn}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{array}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{multirow}
\usepackage{listings}
\theoremstyle{definition}
\usepackage{caption2}
\newtheorem{example}{Example}

%\usepackage{caption}
\usepackage{newfloat}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\DeclareFloatingEnvironment[fileext=lot]{table}

\begin{document}
	
	%----------------------------------------------------------------------------------------
	%	ADDRESSEE SECTION
	%----------------------------------------------------------------------------------------
	
	\begin{letter}{Dr. April Harper \\
            Editorial Assistant \\
			Natural Language Engineering}
		
		%----------------------------------------------------------------------------------------
		%	YOUR NAME & ADDRESS SECTION
		%----------------------------------------------------------------------------------------
        \begin{center}
        \large\bf Yizhu Liu, Xinyue Chen, Xusheng Luo and Kenny Q. Zhu \\ % Your name
        %\vspace{20pt} \hrule height 1pt % If you would like a horizontal line separating the name from the address, uncomment the line to the left of this text
        Department of Computer Science and Engineering \\ Shanghai Jiao Tong University \\ 800 Dongchuan Road, Shanghai, China 200240 \\
        liuyizhu@sjtu.edu.cn
         % Your address and phone number
        \end{center} 
        \vfill

        \signature{Yizhu Liu, Xinyue Chen, Xusheng Luo and Kenny Q. Zhu} % Your name for the signature at the bottom
		
		%----------------------------------------------------------------------------------------
		%	LETTER CONTENT SECTION
		%----------------------------------------------------------------------------------------
		
		\opening{Dear Dr. April Harper and Reviewers,} 
		
		We appreciate the opportunity to revise our manuscript. 
		Thank you for the editors' and reviewers' comments concerning our 
		manuscript entitled ``Reducing Repetition in Convolutional Abstractive Summarization" (ID:NLE-ARTC-REG-20-0036). 
		Those comments are all valuable and very helpful
		for revising and improving our paper.
		A point-by-point response to the Editors' and Reviews' comments is below. 
		We believe that the revisions prompted by these comments have strengthened our manuscript.
		\newline\newline
		On behalf of all co-authors,\\
		Yizhu Liu, Xinyue Chen, Xusheng Luo and Kenny Q. Zhu
		\newline\hrule

		\flushleft
		\begin{enumerate}
		%\textit{*The location labels in responses are the locations of revised manuscript.}
		    \item \textbf{Reviewer 1}

			\begin{itemize}
				\item At the end of paragraph 1 on page 4 I do not see how the backtracking decoder would fight logical incorrectness. It seems to me that your proposed mechanism tackles the issue of generating repetition, but I do not see how this should improve logic a priori. The authors should rephrase and motivate the backtracking decoder w.r.t. to their discussion later in the text.

				\item[] \textbf{AUTHORS' RESPONSE}: 
				We proposed sentence-level backtracking decoder to avoid the repetition causing by attending to sentences that are similar in different positions. Trigram decoder (TRI) is the state-of-the-art method to forbid repetition causing by attending to different but similar sentences. However, TRI forces the decoder to never output the same trigram more than once during the beam search at testing. TRI impacts the process of beam search through trigram that cannot reflect the  complete semantic information, which generates logically incorrect summaries (as shown in Table 2).  Instead, we introduced sentence-level backtracking decoder (SBD) that prohibits the repeat of the same sentences at test time. Thus, as shown in Table 2, compared with TRI, sentence-level backtracking decoder can avoid repetition and generate more logical summaries.
				
				Considering the Reviewer's suggestion, we rephrased the sentence-level backtracking decoder in Section 1 as follows:
				``
				... the meddling of beam search at runtime causes another problem: it tends to generate sentences that are logically incorrect. In Table 2 (TRI row), the defender dayot didn’t really play for France, according to the source. That is, the subject and object are mismatched. As trigram cannot reflect the complete semantic information, trigram decoder always generates logically incorrect summaries due to the trigram-based meddling of beam search during testing. In order to avoid the logical incorrectness caused by trigram decoder, we introduce a sentence-level backtracking decoder that prohibits the repeat of same sentence at test time. Compared with trigram decoder, sentence-level backtracking decoder can avoid repetition and generate more logical summaries. Our summary produced for the example is shown in last section of Table 2.
				''
			    We also revised the explanation of SBD at Section 2.3 and 3.4 as follows:
			     \begin{itemize}
			    	\item[-] We added the summary generated by TRI into Table 5 in Section 2.3.
			    	\item[-] The last paragraph of Section 2.3 is changed as ``... Compared with TRI,  SBD does not interrupt the beam search process in the middle of a sentence, hence significantly reducing related grammatical and factual errors. As shown in Table 5, the summary generated by SBD is grammatical and factual. Besides, SBD is capable of producing a more informative summary since it yields more chances to other candidate sentences.''.
			    	\item[-] The analysis of readability in Section 3.4 is enriched by the comparision between TRI and SBD, 
			    	such as ``... As shown in Table 8(b), TRI achieves the best score on repeatedness, 
			    	but lower readability score than other models. Also, the readability of ATTF drops after adding TRI.
			    	The models with SBD are more readable than them with TRI. 
			    	This is because that TRI interrupt the process of beam search through trigrams that cannot reflect the complete grammatical structure and semantic information. 
			    	TRI always generates summaries with more grammatical and factual errors. 
			    	SBD forbids the repetition at sentence-level during testing, which considers complete grammatical structure and semantic information.
			    	As shown in Table 2 and Table 5, SBD weakens the influence of the meddling of beam search during generation
			    	and generates more readable summaries.
			    	The higher ROUGE scores shows that SBD enhances performance of CNN and ATTF by reducing the repetitive unreadable sentences. ATTF+SBD scores highest on readability.''.
			    \end{itemize}
				
				\item The choice of each term appearing in the RHS of equation (9) should be discussed. I recommend to briefly explain simple thought experiments: i.e. what happens when one filter is dominated by zeros, why do we choose exactly the minimization term, etc.
				\item[] \textbf{AUTHORS' RESPONSE}: 
                We briefly revised the explanation of variables in the equation (9) and discussed the choice of each term appearing in the RHS of equation (9). 
				
				The revised version is as follows:  
				``
				We construct two multi-hot vectors $g_{s}$ and $g'_{s}$ for each \textit{segment} $V_{s}$.
				The dimensions of them are the
				same as $A_{s}^{l}$. For $g_{s}$, we set elements on the position of tokens
				belonging to sections in $\mathbb{U}_{s}$ to 0, and other
				positions to 1. 
				$g'_{sj}$ denotes $j$-th element of $g'_{s}$, which is the flipped version of $\prod \limits_{q=0}^{s}g_{qj}$. 
				The filter on $a_{ij}^{l}$ in Equation (5) is given as:
				\begin{equation}
					\tilde{a}_{ij}^{l} = a_{ij}^{l}\prod_{q=0}^{s}g_{qj} + \min \limits_{A_{s}}\left(\frac{A_{sj}^{l}}{v_{s}-v_{s-1}-1}\right)g_{sj}' \nonumber
				\end{equation}
				where $v_{s}$ is the maximum value in 
				$\mathbf{v}$ that is smaller than $i$,  and $\tilde{a}_{ij}^l$ is the filtered
				attention score. $A_{sj}$ is the attention score between $j$-th token
				of the source document and the $s$-th \textit{segment}. 
				$g_{sj}$ and $g_{sj}'$ denote whether $j$-th token
				of the source document has been attended.
				When $\prod \limits_{q=0}^{s}g_{qj}$ is zero, $g_{sj}'$ is $1$.
				This means that the $j$-th token of the source document has been attended before.
				We penalize the attention score of attended tokens in source document
				We take the minimum attention score between tokens in source document and summary 
				(i.e. $\min \limits_{A_{s}}\left(\frac{A_{sj}^{l}}{v_{s}-v_{s-1}-1}\right)$ )
				as the attention score between the $i$-th token in target and the $j$-th token in source.
                ''  
				
				\item What type of tokenization do you use in your experiments? Please, specify.
				\item[] \textbf{AUTHORS' RESPONSE}: 
				We described the type of tokenization we used in experiments at Section 3.2 as follows:
                ``In the following experiments, we tokenize source documents and targets 
				using the word tokenization method from NLTK (Natural Language Toolkit). 
				The NLTK module is a massive toolkit, 
				aimed at helping with the entire Natural Language Processing (NLP) methodology.
				''

				\item Regarding ATTF, what happens if punctuation is not present or if the hypothesis is that other type of partitioning of the source/ target is better? I would suggest the authors to compare punctuation with a baseline of uniform partitioning in a reasonable number of parts of their sources/ targets. Are there any other interesting baselines?
				\item[] \textbf{AUTHORS' RESPONSE}: 
                According to the Reviewer's suggestion, we compared segments in different types and gave some examples in Table 3.

				The details are shown in Section 1 as follows:
				
				``
                %\end{table}
                %As shown in above table,
                We can get segments in different ways.
                As shown in Table 3, we compare the segments in different types.
                The baseline with sentence as a segment (sentence-level segment) 
                always loses important information in reference summary, such as ``silas randall timberlake''.
                The first sentence in generated summary based on sentence-level segment attends to 
                the second sentence in source. 
                The attention score of second sentence in source is minimized,
                and this source sentence is no longer attended. 
                So, the model with sentence-level segment loses the important information ``silas randall timberlake''
                during decoding.
                The baseline with N-gram as a segment (N-gram segment) may bring about grammatical and semantic problems.
                Suppose that $N$ equals to 3, as shown in Table \ref{tab:punct},
                the green part of generated summary based N-gram segment does not attend to the ``the couple announced''
                in source document. 
                As N-gram cannot be seen as a complete and accurate semantic unit,
                the decoder of model with N-gram segment attends to the segment in source with inaccurate grammar and semantics.
                Thus, the generated summary based on N-gram segment has grammatical and semantic errors.
                We use punctuations to separate the source or target into segments,
                since punctuations play an important role in written language to organize
                the grammatical structures and to clarify the meaning of sentences.
                (Jones, 1996;Briscoe et al., 1997;Kim et al., 2019;Li et al., 2019).
                It is very simple but effective. 
                In this paper, a segment means 
                a sentence or clause delimited by punctuation,
                which carries syntactic and semantic information. 
                Specifically, we calculate the attention in terms of segments, 
                larger semantic units than tokens and smaller semantic units than sentences, 
                which intuitively helps with emphasis of attention and POIs in source.
                	%\begin{table}[th!]
                \begin{center}
                	\captionof{table}{(Table 3 in revised manuscript) The summary generated by attention filter mechanism with different type of segments. \protect\\
                		The parts of text in different colors denote segments. The segments in the same color attend to \protect\\
                		the same POI in the source.}
                	\begin{tabular}{|l|}%{|p{7cm}|rl|}
                		\hline 
                		\textbf{Source} \\
                		\hline 
                		(1)justin timberlake and jessica biel , welcome to parenthood . \\
                		(2)the celebrity couple announced the arrival of their son , silas randall timberlake, ... \\
                		(3)silas was the middle name of timberlake ’s maternal grandfather bill bomar , \\
                		who died in 2012 .\\
                		(4)the couple announced the pregnancy in january , ... \\
                		(5)it is the first baby for both .  \\
                		\hline 
                		\textbf{Reference} \\
                		\hline 
                		timberlake and jessica biel welcome son silas randall timberlake. \\
                		the couple announced the pregnancy in january . \\
                		\hline 
                		\textbf{Sentence-level segment} \\
                		\hline \textcolor{blue}{the couple announced the arrival of their son .} \\
                		\textcolor{brown}{it is the first baby for both} \textcolor{black}{.}  \\
                		\hline 
                		\textbf{N-gram segment} \\
                		\hline \textcolor{blue}{the couple announced} \textcolor{green}{silas randall timberlake} \textcolor{red}{pregnancy in january }  \textcolor{black}{.}\\
                		\hline
                		\textbf{Segment: a sentence or clause delimited by punctuation} \\
                		\hline \textcolor{blue}{the couple announced the arrival of their son ,} \textcolor{green}{silas randall timberlake .} \\
                		\textcolor{red}{the couple announced the pregnancy in january .} \\ 
                		\textcolor{brown}{it is the first baby for both} \textcolor{black}{.} \\
                		\hline 
                	\end{tabular}
                	\label{tab:punct}
                \end{center}
				''
 
				\item I find it particularly satisfying that the authors have discussed the testing time and speed of generation in Table 10. Another question that is important is: can the authors give predictions how their methods would scale for other architectures, say RNNs and Transformers? I.e. would there be a roughly 6 times slowdown (for ATTF+SBD) as it is apparent in Table 10 when compared to the vanilla CNN?
				\item[] \textbf{AUTHORS' RESPONSE}: 
				We have given predictions on the quality and speed of models with ATTF +SBD.
				The details are added into the last paragraph  Section 3 as follows:
				
				``
				As ATTF deals with incorrect attention distribution
				between the inputs of encoder and decoder to 
				reduce repetition in generated summaries,
				the seq2seq models with attention mechanism between encoder and decoder
				can be improved via ATTF.
				The SBD can only be used at testing, which is suitable for the models with decoder.
				Since RNN-based and transformer-based seq2seq models,
				including attention mechanism between encoder and decoder,
				always suffer from repetition in generated summaries, 
				we can reasonably deduce that these models will benefit from our proposed ATTF and SBD as well.
				The higher ROUGE scores (Table 7) of our model means that
				the summaries generated by our model are more similar to their corresponding reference summaries.
				The natural-level repeatedness and higher readability score (Table 8) of our model means 
				that our model can produce summaries with higher quality.
				ATTF is applied to the attention mechanism between encoder and decoder,
				which impacts the time of decoding at training and testing.
				SBD only impacts the time of decoding during test.
				%So ATTF+SBD makes the models slowdown in different degrees 
				%because of different time costs for encoding.
				%The more time the encoder takes, the less the model slow down.
				ATTF+SBD takes about the same amount of time for additional models to slow down.
				For RNNs and transformers, after adding ATTF+SBD,
				there would be less than 6 times slowdown
				(As shown in Table 11, for the vanilla CNN, there is a roughly 6 times slowdown after adding ATTF+SBD.)
				since RNNs and transformers spend more training time and testing time on encoding than CNN.
				''
				
				\item I believe that the manuscript would also benefit from a discussion on training time. With ATTF does the model converge faster or slower with what kind of factor? Please discuss that in the paper.
				\item[] \textbf{AUTHORS' RESPONSE}: 
				We have added training time of difference models into Table 11 and
				discussed the results in Section 3.4 as follows:
				``
				The training speed and testing speed of CNN+ATTF 
				is faster than RNN seq2seq model and Transformer
				as CNN can be trained in parallel and ATTF .
				The gap of training/testing time between ATTF and Transformer is not much large,
				but the memory usage of Transformer is much larger than ATTF.
				Because Transformer has more trainable parameters than ATTF.
				'' 
				
				According to the Reviewer's comments, we also discussed the factors that impact the covergence rate of the model with ATTF (Section 3.4) as follows:
				``
				The convergence rate of models with ATTF depends on three aspects:
				{\em dataset}, {\em basic model} and {\em experimental settings}.
				For {\em dataset}, ATTF redistributs the attention distribution between source documents and summaries during decoding,
				which dynamically searches the attended segment in source by the predicted segments in summary.
				Thus, the convergence rate of the models with ATTF depends on the length of source documets and summaries.
				The ATTF applied on better {\em basic models} converges faster,
				because better basic models learn the alignment between source documents and summaries better.
				The {\em experimental setting} also impacts the convergence rate of the model with ATTF.
				At the beginning of training, a large learning rate makes the model converge faster. 
				When most of the samples have been trained, 
				the model converges rapidly by decreasing the learning rate.
				''
				
				\item A minor suggestion is to rename the constant `sz` to something standard, say $beta$, etc.: the readers might get confused with the constant `s` and the latent variables `z`.
				\item[] \textbf{AUTHORS' RESPONSE}: 
				Considering the Reviewer's suggestion, 
				we renamed the constant 'sz' as $\beta$ in Section 2.2 and Section 3.2 as follows:
                \begin{itemize}
                \item[-] ``If the size of $P_{U_t}$ is larger than $\beta$, a predefined constant'' (Section 2.2)
                \item[-] ``We set the threshold $\beta$ to 3'' (Section 3.2)
                \end{itemize}


			\end{itemize}

			\item \textbf{Reviewer 2}
			\begin{itemize}
                \item The article does not discuss self-attention-based models which constitute the basis of many recent state-of-the-art systems. It would be useful to read the benefits of CNN-based models compared to Self-Attention-Based models and the reason you prefer this model.
				\item[] \textbf{AUTHORS' RESPONSE}: 
                Considering to Reviewer's comments, we discussed the reason we used CNN-based 
                models in our paper. 
                Without additional, complicated structures or features, 
                compared with self-attention-based models,
                the computation complexity of CNN-based models is less. 
                Thus, the CNN-based models are more applicable.
                %can be trained in faster speed and less memory usage.

                The details are in Section 1 and Section 3.3 as follows:
                \begin{itemize}
                	\item[-] ``Self-attention-based model is 
                	the basis of many recent state-of-the-art systems,
                	which always need multi-layer self-attention 
                	and have greater computational complexity than CNN seq2seq models 
                	(Vaswani et al. 2018).
                	Thus, we take CNN seq2seq models as the target model to improve on and
                	compare with in this paper.'' (Section 1)
                	\item[-] ``The vanilla CNN seq2seq model and vanilla self-attention-based model have the similar
                	feature capture capabilities.
                	With long inputs, the self-attention-based models
                	will have greater computational complexity, such as Transformer. 
                	As the inputs of summarization is very long, the self-attention-based models
                	always need much more time during training and testing.
                	Besides, the self-attention-based models contain more training parameters,
                	which need the memory usage at training and testing time. .'' (Section 3.3)
                \end{itemize}
            
                \item The Readability score was decided by only two experts which may lead to significant bias. It would be useful to repeat these experiments with the larger number of experts.
				\item[] \textbf{AUTHORS' RESPONSE}: 
                Considering the Reviewer's suggestion, we  asked two other experts to give a readability socre for each generated summary.
                
                The details are shown in the last paragraph of Section 3.2. The Cohen’s Kappa coefficient between them is 0.66, indicating  agreement. The readability score of Table 8 (revised version) are updated. As shown in Table 8, the readability scores of our proposed models are higher, which means that the summaries generated by our proposed methods are more readable.

				
			\end{itemize}

			\item \textbf{Reviewer 3}
			\begin{itemize}
			    \item As one of its core contributions, the manuscript mentions the identification of the repetition problem in abstractive summaries, which sounds to be an overstatement. The same problem has been mentioned in multiple previous works as also illustrated by the fact that Section 4.2 lists several papers that focus on the repetition reduction for abstractive summarization.
			    \item[] \textbf{AUTHORS' RESPONSE}:  
			    We have revised the parts of our manuscript that refer to "identification of the repetition problem".
			    The details are as follows:
			    \begin{itemize}
			    	\item[-] In Abstract: ``In this paper, we find the reasons behind repetition problem in abstractive summarization and solve the repetition problem.'' 
			    	\item[-] In Conclusion: ``In this paper, we focus on the repetition problem in abstractive summarization based on CNN seq2seq model with attention mechanism.'' 
			    \end{itemize}
			    				
				\item The manuscript fails to mention and compare its performance with the Diverse Convolutional Seq2SeqModel (DivCNN Seq2Seq) [1], even though the goal of DivCNN Seq2Seq is very closely related, i.e. to generate a diverse and quality abstractive summary for source documents without excessive repetition from the source document.
				\item[] \textbf{AUTHORS' RESPONSE}:  
                We added the introduction of DivCNN Seq2seq into Section 1, Section 3.3 and Section 4.2
                as follows:
                 \begin{itemize}
                	\item[-] ``The other is Diverse Convolutional Seq2Seq
                	Model (DivCNN) (Li et al. 2019), 
                	which introduces DPPs into deep neural network (DNN) attention adjustment
                	and uses status of DNN as quality and diversity (QD-score) for abstractive summarization.
                	DivCNN first selects the attention distribution of the subsets of source with high QD-score
                	and then adds it into model loss as a regularization.
                	DivCNN does not directly redistribut the attention,
                	so it may still attend to similar POIs.
                	In order to improve QD-score, DivCNN tends to attend to scattered subsets of sentences in source document, 
                	which leads to semantic incoherence. 
                	As shown in Table 2 (DivCNN row ),
                	the content about the 16-year-old 
                	is inconsistent with source document.''  (Section 1)
                	\item[-] ``DivCNN uses Determinantal Point Processes methods(Micro DPPs and
                	Macro DPPs) to produce attention distribution (Li et al. 2019a). DPPs consider both quality and diversity, which helps model attend to different subsequence in source document.''  (Section 3.3)
                	\item[-] ``Li et al. (2019a) add Determinantal Point Processes methods (DPPs)
                	into deep neural network (DNN) attention adjustment
                	and takes attention distribution of
                	subsets selected from source document by DPPs as the part of loss..''  (Section 4.2)
                \end{itemize}
            
                We compared our proposed models with DivCNN Seq2seq 
                in terms of ROUGE scores, repeatedness and readability.
                We updated the Table 7 and Table 8 by filling in the scores of DivCNN,
                and analyzed the experimental results (Section 3.4).
                Our proposed model (ATTF+SBD) outperforms DivCNN on all evaluation metrics.
                Compared with ATTF+SBD, DivCNN adjusts attention distribution in an indirect manner that adds the attention of the subsets (with hight quality-diversity-score) selected from source document into the loss. So, DivCBNN may still attend to similar but different sentences, resulting in lower ROUGE scores and higher repeatedness. As DivCNN is trained to attend to diversity subsets, 
                which means that the selected subsets are more scattered (shown in Figure ) and leads to semantic incoherence. Thus, the readability score of DivCNN is lower.
                              
				\item A shortcoming of the experiments is that the method was only evaluated on the DailyMail/CNN dataset. In order to see how general the proposed approach is, it would have been desirable to test it on additional benchmark datasets that are frequently used for evaluation, such as the Newsroom, Gigaword, ELI5 and XSUM datasets. This would be especially helpful to see the effects of the hyperparameter choices employed in the model (such as the thresholds sz and n utilized by ATTF and SBD, respectively).
				\item[] \textbf{AUTHORS' RESPONSE}:  
                According to the Reviewer's suggestion, we test the models on Newsroom and DUC to evaluate the generalizability of our proposed models.
                We did not use Gigaword and XSum, because they use singel-sentence as a summary.  
                However, we primarily deal with multi-sentence summaries with 
                sentence-level repetition since there is almost no word repetition in multi-sentence summaries. Besides, Gigaword is not free.
                As ELI5 is a Question Answering dataset that is not suitable for abstractive summarization tasks, we also didn't use ELI5 in our experiments.
                We use DUC 2002 as dataset because it is a test-only dataset used in abstractive summarization. DUC 2002 consists of long source documents and multi-sentence summaries written by human.
                
                We introduced Newsroom and DUC 2002 in Section 3.1 and analysis the generalization of our proposed ATTF+SBD (Table 12) in the "Generalization" part of Section 3.4.
                The details are as follows:
                 \begin{itemize}
                	\item[-] ``Also we tried our model on other two abstractive summarization datasets about news, which are Newsroom (Grusky et al., 2018) and DUC 2002. For Newsroom, there are 1,321,995 document-summary pairs, which are divided into training (76\%), development (8\%), test (8\%), and unreleased test (8\%). At testing, we use the 8\% released test data. DUC-2002 (DUC) is a test set of document-summary pairs. We use the models trained on CNNDM to do the test on DUC and demonstrate the generalization of the models. ''  (Section 3.1)
                	\item[-] `` Table 12 shows the generalization of our abstractive system to other two datasets, Newsroom and DUC 2002, where our
                	proposed models achieve better scores than vanilla CNN seq2seq model in terms of ROUGE scores, readability and repeatedness. 
                	We use the same settings of $\beta=3$ ($sz$ in previous version) in Section 2.2 and $n=5$ in Eq.(11), because the proportion of segments with length greater than 3 and reference summaries with LCS greater than 5 were about 90\%. As show in Table 12, our proposed models can be better generalization on other datasets about news, along with repetition reduction and the improvement of readability.
                	This shows that our proposed models can be generalized well.
                	''  (Section 3.4)
        
                \end{itemize}

				\item The choice for k in Eq. 8 would also require some additional justification.
				\item[] \textbf{AUTHORS' RESPONSE}:  
				We have explain why we choose $k=v_{s}-v_{s-1}-1$ in Eq. 8 as follows:
				
				``
                To find the most attended \textit{sections}, 
                we sort the elements inside the filter vector, 
                $A_{s}^{l}$, in descending order, 
                and record the top $k$ elements' positions in 
                the source document as: 
                \begin{equation}
                    \mathbf{p}=(p_{0},...,p_{k}) \nonumber
                \end{equation}
                where $k=v_{s}-v_{s-1}-1$. 
                $k$ is the same as the number of tokens in the $s$-th \textit{segment},
                because each token in a \textit{segment} is aligned with at least one token in source document, according to the principle of the seq2seq model. Thus, for the $s$-th \textit{segment}, we observe top $k$ most attended tokens in source document to find the most attended \textit{sections}. 
				''

                \item The manuscript only reports the correlations between the repeatedness scores of the gold standard and the generated summaries. Additionally, it would be interesting to see the correlation between the repeatedness of the generated summary and the human readability judgement in order to see the usefulness of the repeatedness criterion the authors proposed.
				\item[] \textbf{AUTHORS' RESPONSE}:  
				Considering the Reviewer's suggestion, we reported the correlation between repeatedness of the generated summary and the human readability judgement
				in Table 10.
				The correlation scores are about 0.7, which means that the repeatedness score are useful. The repetition in summaries will affect coherence between sentences and the readability of summaries. The correlation scores between the repeatedness of the generated summary and the human readability are listed in the table below.
				\begin{center}
				\captionof{table}{(Table 10 in revised manuscript) The correlation between the repeatedness of the generated summaries and \\
				the human readability judgement.}
				\begin{tabular}{|l|c|c|c|}
					\hline
					& pearson  & spearman & kendall's tau \\
					\hline
					ATTF & 0.78 & 0.74 & 0.70 \\
					SBD* & 0.75 & 0.71 & 0.68 \\
					ATTF+SBD* & 0.75 & 0.74 & 0.69 \\
					\hline
				\end{tabular}
                \end{center}
				\item Additional remarks:
				\begin{itemize}
                \item[-] The presence of factual errors is mentioned among the criteria for fluency, however, one could arguably generate totally fluent sentences that contain factual errors.
                \item[-] The paper would benefit from a thorough proofreading as it contains relatively many run-on sentences and non-grammatical structures.
                \item[-] The consistency of the manuscript could also be improved. For instance page 9 mentions TRI (''reducing related grammatical and factual errors compared with TRI''), whereas it is only mentioned at page 13 for the first time that TRI refers to the trigram decoder.
                \item[-] The first sentence in the second paragraph of section 4.1 and the first sentence of section 4.2 are basically the same. Additionally, the list of those citations included in the duplicate sentence would be better elaborated by providing some additional details for those recent methods as well.
                \end{itemize}
				\item[] \textbf{AUTHORS' RESPONSE}:  
				\begin{itemize}
                \item[-] The 'Fluent' in Readability in previous manuscript has been classified as 'Fluent' and 'Factual'. The details are in Section 3.2 as follows:
                \begin{itemize}
                \item[] (3) Fluent: How grammatical the sentences of a summary are? 
                \item[] (4) Factual: Are there any factual errors in the summary?
                \end{itemize}
                \item[-] We revised the paper with thorough proofreading.
                \item[-] We explained that the TRI denotes trigram decoder
                (Page 4 of revised version), when we first mentioned trigram decoder.
                \item[-] We deleted the first sentence of section 4.2 which was similar to first sentence in the second graph of section 4.1. We briefly modified the following related work and explained the methods listed in citations in detail.
                \end{itemize}


			\end{itemize}
		\end{enumerate}
                
	
	
		We appreciate the hardwork by the editors and the reviewers,
		and hope that our revision meets their requirements.
		
		Once again, thank you very much for  your comments and suggestions.
		
		
		\closing{Sincerely yours,}
		
		
		%\encl{Curriculum vitae, employment form} % List your enclosed documents here, comment this out to get rid of the "encl:"
		
		%----------------------------------------------------------------------------------------
		
	\end{letter}
	
\end{document}
