\section{Related Work}
\label{sec:related}
In this section, we 
discuss neural-based abstractive summarization
and some previous work on repetition reduction methods in
abstractive summarization.

\subsection{Neural-based Abstractive Summarization}
Automatic summarization
condenses long documents into short summaries
while preserving the important information of the documents.
\citep{RadevHM02,AllahyariPASTGK17,Tian18}
There are two general approaches to automatic summarization: 
Extractive summarization and Abstractive summarization.
Extractive summarization selects sentences 
from the source articles, which can produce
grammatically correct sentences~\citep{BokaeiSL16,VermaL17,NaserasadiKS19,ZhongLWQH19}.
Abstractive summarization is a process of {\em generating} a concise and 
meaningful summary from the input text, possibly with words or sentences 
not found in the input text. 
A good summary should be coherent, 
non-redundant and readable~\citep{YaoWX17}.
Abstractive Summarization is one of the most challenging and 
interesting problems in the field of Natural Language Processing (NLP)
\citep{CareniniC08,PallottaDB09,SankarasubramaniamRG14,BingLLLGP15,RushCW15,LiHZ16,YaoWX17,MohamedO19,LierdeC19,NguyenCNN19}.

Recently, neural-based (encoder-decoder) models~\citep{RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17,LiuL19,ZhangWZ19,WangQW19}
have made some progress for abstractive summarization.
Most of them use recurrent neural networks (RNN) with different attention 
mechanisms~\citep{RushCW15,NallapatiZSGX16,SeeLM17,PaulusXS17,ZhangWZ19}. \citet{RushCW15} are the first to apply the 
neural encoder-decoder architecture to text summarization. 
\citet{SeeLM17} enhance this model with a pointer generator network 
which allows it to copy relevant words from the source text.
RNN models are difficult to train because of the 
vanishing and exploding gradient problems.
Another challenge is that the current hidden state in a RNN is 
a function of previous hidden states, so RNN cannot be easily
parallelized along the time dimension during training and evaluation, 
and hence training them for long sequences becomes very expensive in 
computation time and memory footprint.

%Convolutional neural network (CNN) 
%models~\citep{gehring2017convs2s,FanGA18,LiuLZ18,Zhang2019AbstractTS} can  alleviate
%the above challenges to a certain extent. 
To alleviate the above challenges,
Convolutional neural network (CNN) 
models~\citep{gehring2017convs2s,FanGA18,LiuLZ18,Zhang2019AbstractTS} 
are applied into seq2seq models.
\cite{gehring2017convs2s} proposes a CNN seq2seq model equipped with
Gated Linear Units \citep{DauphinFAG17}, residual connections \citep{HeZRS16}
and attention mechanism. 
\cite{LiuLZ18} modifies basic CNN seq2seq model with a summary length
input and trains a model that produces fluent summaries of desired length.
\cite{FanGA18} presents a controllable CNN seq2seq model to
allow users to define high-level attributes of generated
summaries, such as source-style and length.
\cite{Zhang2019AbstractTS} adds a hierarchical attention mechanism to CNN seq2seq model.
CNN-based models can be parallelized during
training and evaluation. The computational complexity of
these models is linear with respect to the length of sequences.
CNN model has shorter paths between pairs of input and
output tokens, so that it can propagate gradient signals more
efficiently.
CNN model enables much faster training and more stable gradients 
than RNN. 
\cite{bai2018empirical} showed that CNN is more powerful than 
RNN for sequence modeling.
Therefore, in this work, we choose the vanilla CNN seq2seq model as 
our base model.

\subsection{Repetition Reduction for Abstractive Summarization}
Repetition is a persistent problem in the task of 
neural-based summarization. 
It is tackled broadly in two directions in recent years. 

One direction involves {\em information selection} or 
{\em sentence selection} before generating summaries.
\cite{P18-1063} proposes an extractor-abstractor model, which uses an extractor  
to select salient sentences or highlights and then employs 
an abstractor network to rewrite these sentences.
\cite{SharmaHHW19} and \cite{SanghwanB19} are also use extractor-abstractor model 
with different data preprocessing methods.
All of them can not solve repetition in seq2seq model.
\cite{TanWX17} and \cite{D18-1205,D18-1441} encode
sentences using word vectors
and predicts words from sentence vector in sequential order 
whereas CNN-based models are naturally parallelized. 
While transferring RNN-based model to CNN model, 
the kernel size and the number of 
convolutional layers can not be easily determined when
converting between sentences and word vectors. 
Therefore, we do not compare our models to those models in this paper. 

The other direction is to improve the 
{\em memory of previously generated words}.
\cite{SuzukiN17} and \cite{LinSMS18} 
deal with word repetition in single-sentence summaries, 
while we primarily deal with multi-sentence summaries with 
sentence-level repetition. 
There is almost no word repetition in multi-sentence summaries.
\cite{JiangB18} add a new decoder without attention mechanism.
In CNN-based model, attention mechanism is necessary to connect encoder 
and decoder.
Therefore, our model also is not compared with the above models in this paper. 
The following models can be transferred to CNN seq2seq model and
are used as our baselines.
\cite{SeeLM17} integrates coverage mechanism, 
which keeps track of what has been summarized, as a feature that helps 
redistribute the attention scores in an indirect manner,
in order to discourage repetition. 
\cite{TanWX17} uses distraction attention
\citep{ChenZLWJ16}, which is identical to coverage mechanism. 
\cite{GehrmannDR18} adds coverage penalty to loss function
which increases whenever the decoder directs more than 1.0 of total attention
towards a word in encoder.
This penalty indirectly revises attention distribution and results in
the reduction of repetition.
\cite{elikyilmazBHC18} uses semantic cohesion loss,
which is the cosine similarity between two consecutive sentences, as part of
the loss that helps reduce repetition.
\cite{DivC2C19} add Determinantal Point Processes methods (DPPs)
into deep neural network (DNN) attention adjustment
and takes attention distribution of
subsets selected from source document by DPPs as the part of loss.
\cite{PaulusXS17} proposes intra-temporal attention \citep{NallapatiZSGX16} and 
intra-decoder attention which dynamically revises the attention distribution while decoding. 
It also avoids repetition at test time by directly banning the generation of 
repeated trigrams in beam search. 
\cite{FanGA18} borrows the idea from \cite{PaulusXS17} and 
builds a CNN-based model. 

Our model deals with the attention in both encoders and decoders. 
Different from the previous methods, 
our \textit{attention filter mechanism} does not 
treat the attention history as a whole data structure,  
but divides it into sections (\figref{fig:model_main}). 
Previously, the distribution curve of accumulated attention scores 
for each token in the source document tends to be flat, 
which means critical information is washed out during decoding.
Our method emphasizes previously attended sections 
so that important information is retained.

Given our observation that repetitive sentences in the source are
another cause for repetition in summary, 
which cannot be directly resolved by manipulating attention values, 
we introduce \textit{sentence-level backtracking decoder}. 
Unlike \cite{PaulusXS17}, 
we do not ban repetitive \textit{trigrams} in test time. 
Instead, our decoder regenerates a sentence that is similar to previously generated ones.
With the two modules, our model is capable of generating summaries with a
natural level of repetition while retaining fluency and consistency.

\cut{%%%%%%%%%%%
In this paper, we aim at reducing repetition in abstractive summarization
and evaluate the
effectiveness of our repetition reduction technique, so we need to compare our proposed
approaches with others on the same seq2seq model, to be fair. 
We did not take all of the seq2seq models with better ROUGE score as baselines, because
these models do not deal with repetition problem in abstractive
summarization effectively. 
Since most seq2seq models use attention of some sort, and our model deals
with incorrect attention distribution, we can reasonably deduce that these models
will benefit from our techniques as well. 
}%%%%%%%%%%%


