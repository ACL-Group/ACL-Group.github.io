%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plain Cover Letter
% LaTeX Template
% Version 1.0 (28/5/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Rensselaer Polytechnic Institute 
% http://www.rpi.edu/dept/arc/training/latex/resumes/
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{letter} % Default font size of the document, change to 10pt to fit more text

\usepackage{newcent} % Default font is the New Century Schoolbook PostScript font 
%\usepackage{helvet} % Uncomment this (while commenting the above line) to use the Helvetica font

% Margins
\topmargin=-1in % Moves the top of the document 1 inch above the default
\textheight=8.5in % Total height of the text on the page before text goes on to the next page, this can be increased in a longer letter
\oddsidemargin=-10pt % Position of the left margin, can be negative or positive if you want more or less room
\textwidth=6.5in % Total width of the text, increase this if the left margin was decreased and vice-versa

\let\raggedleft\raggedright % Pushes the date (at the top) to the left, comment this line to have the date on the right

\usepackage{xcolor}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{soul}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath,amsfonts,amssymb,amsthm,amsopn}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{array}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{multirow}
\usepackage{listings}
\theoremstyle{definition}
\usepackage{caption2}
\newtheorem{example}{Example}

%\usepackage{caption}
\usepackage{newfloat}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\DeclareFloatingEnvironment[fileext=lot]{table}

\begin{document}
	
	%----------------------------------------------------------------------------------------
	%	ADDRESSEE SECTION
	%----------------------------------------------------------------------------------------
	
	\begin{letter}{Dr. April Harper \\
            Editorial Assistant \\
			Natural Language Engineering}
		
		%----------------------------------------------------------------------------------------
		%	YOUR NAME & ADDRESS SECTION
		%----------------------------------------------------------------------------------------
        \begin{center}
        \large\bf Yizhu Liu, Xinyue Chen, Xusheng Luo and Kenny Q. Zhu \\ % Your name
        %\vspace{20pt} \hrule height 1pt % If you would like a horizontal line separating the name from the address, uncomment the line to the left of this text
        Department of Computer Science and Engineering \\ Shanghai Jiao Tong University \\ 800 Dongchuan Road, Shanghai, China 200240 \\
        liuyizhu@sjtu.edu.cn
         % Your address and phone number
        \end{center} 
        \vfill

        \signature{Yizhu Liu, Xinyue Chen, Xusheng Luo and Kenny Q. Zhu} % Your name for the signature at the bottom
		
		%----------------------------------------------------------------------------------------
		%	LETTER CONTENT SECTION
		%----------------------------------------------------------------------------------------
		
		\opening{Dear Dr. April Harper and Reviewers,} 
		
		We appreciate the opportunity to revise our manuscript. 
		Thank you for the editors' and reviewers' comments concerning our 
		manuscript entitled ``Reducing Repetition in Convolutional Abstractive Summarization" (ID:NLE-ARTC-REG-20-0036.R1). 
		Those comments are all valuable and very helpful
		for revising and improving our paper.
		A point-by-point response to the Editors' and Reviews' comments is below. 
		We believe that the revisions prompted by these comments have strengthened our manuscript.
		\newline\newline
		On behalf of all co-authors,\\
		Yizhu Liu, Xinyue Chen, Xusheng Luo and Kenny Q. Zhu
		\newline\hrule

		\flushleft
		\begin{enumerate}
		%\textit{*The location labels in responses are the locations of revised manuscript.}
		    \item \textbf{Reviewer 1}

			\begin{itemize}
				\item The FastRNN baseline is now missing from Table 11. Why not including it?
				

				\item[] \textbf{AUTHORS' RESPONSE}: 
				We didn't use FastRNN as baseline because FastRNN is not an end-to-end model compared with other methods in Table 11.
				FastRNN is a two-stage framework consisting of an extractor and an abstractor. The abstractor in FastRNN is the RNN seq2seq model in Table 11. To generate a summary, the FastRNN first extracts several salient sentences from source document by extractor and then uses the abstractor to summarize these salient sentences. The extractor is a pointer network with the same encoder as RNN in table 11 and its input is a sequence of sentences. For abstractor, its input and output are sequences of words in a single sentence. However, the input and output of other models in Table 11 are sequences of words in complete source document and summary. FastRNN is accelerated by shortening the input and output of the RNN. In facts, the model used in FastRNN are the same as RNN in Table 11.
				\\ \hspace*{\fill} \\
				Considering the Reviewer's comments, we added the FastRNN as baseline in Table 11 and explained its speed at Section 3.4 as follows:
				
				``
				... The training speed and testing speed of FastRNN are faster than RNN
				because FastRNN is not an end-to-end model. FastRNN is a two-stage framework, which first uses an extractor to extract several salient sentences from source document and then uses an abstractor to summarize each salient sentence. The final summary is the concatenation of these summarized salient sentences. The extractor in FastRNN is a pointer network with a sequence of sentences as input. The encoder of extractor is the same as RNN. The FastRNN adopts RNN as abstractor and trains extractor and abstractor in parallel, which speeds up the encoder and decoder of RNN seq2seq model by shortening the input and output. As an end-to-end model, the input and output of our CNN+ATTF model are sequences of words in complete source document and summary, which are much longer than extractor and abstractor of FastRNN. The training speed of CNN+ATTF is similar to FastRNN as the CNN can be trained in paralled. The testing speed is faster than FastRNN because FastRNN should extract sentences first and then abstract each sentence during test.
				''

				
				\item The manuscript is overall clear and easy to follow, but following Reviewer 3's recommendation, I also recommend to proofread the manuscript thoroughly. Below I give just a few examples:
				 \begin{itemize}
				 \item[*] in abstract: ``reasons behind repetition problem'' $<-$ ``reasons behind the repetition problem'', ``prevent decoder'' $<-$ ``prevent the decoder'', etc.
				 \item[*]  in introduction: ``Recent study'' $<-$``A recent study'', ``due to its intrinsic'' $<-$ ``due to their intrinsic'', ``italized'' $<-$ ``italicized'', ``DivCNN row )'', etc.
				 \item[*]  There are many other missing articles and typos.
				 \end{itemize}
			   	\item[] \textbf{AUTHORS' RESPONSE}: 
                We corrected the grammatical and spelling mistakes mentioned in the comments.
                We also corrected other typos, such as ``second choices'' $->$ ``second choice'' in Section 2.3.
                
                Besides, we added the missing articles in Related Work (Section 4)
                as follows:
                
                ``
                ''
                


                
			\end{itemize}

			\item \textbf{Reviewer 2}
			\begin{itemize}
                \item Parts of interest (POIs) seem not defined in the Introduction. It would be nice to define them in the introduction or give an example (or in Figure). Later the reader finds that POIs are sections in the source document.
				\item[] \textbf{AUTHORS' RESPONSE}: 
                Considering to Reviewer's suggestion, we highlighted the ``Parts of interest (POIs)'' in introduction. Then we defined POIs and colored the POIs in Table 1 and Figure 1 as examples for explanation. The details are as follows:
                ``
                POIs are the segments of the source document that are attended by the segments in its corresponding summary, such as the green and red segments of source document in Table 1 and Figure 1.
                '' (Section 1)
                
       
                \item It would be nice if the source sentences also receive colors in Table 2, so the reader can easily check which source sentence a target segment is focusing on.
				\item[] \textbf{AUTHORS' RESPONSE}: 
                According to the Reviewer's suggestion, we added the source document to Table 2 and used different colors to describe the attended segments of source document. The revised Table 2 is as follows:
                \begin{center}
                	\captionof{table}{(Table 2 in revised version) Generated summaries of the source in Table 1. The  \protect\\ sentences in \textit{italicized} are repetitve 
                		sentences. Each summary segment and its attended POI in  \protect\\ source document are in the same color.}
                	\small
                	\begin{tabular}{|p{14.3cm}|}
                		\hline \bf Source Document \\
                		\hline ... \textcolor{blue}{manchester city are rivalling manchester united and arsenal for valenciennes teenage defender dayot upamecano .} \textcolor{green}{the 16-year-old almost joined united in the january transfer window ,} \textcolor{brown}{only for him to opt to stay in france for a few more months .}  \textcolor{purple}{centre-back umecano has played for france at u16 and u17 level .} 
                		monaco , inter milan and paris stgermain had also expressed interest. \textcolor{red}{fourth-placed city face aston villa at the etihad stadium on saturday .} ...\\
                		\hline \bf Intra-temporal attention (ITA) \\
                		\hline \textcolor{red}{\textit{manchester city face aston villa at the etihad stadium on saturday .}} \\
                		\textcolor{red}{\textit{manchester city face aston villa at the etihad stadium on saturday .}} \\
                		\hline \bf Intra-temporal $+$ Intra-decoder (ITDA) \\
                		\hline \textcolor{blue}{\textit{manchester city are rivalling manchester united and arsenal }for valenciennes teenage .}\\
                		\textcolor{red}{manchester city face aston villa at the etihad stadium on saturday .} \\
                		\textcolor{blue}{\textit{manchester city are rivalling manchester united and arsenal }. }\\
                		\hline \bf Coverage model (COV) \\
                		\hline \textcolor{red}{\textit{manchester city face aston villa at the etihad stadium on saturday .}} \\
                		\textcolor{blue}{manchester city are rivalling manchester united and arsenal for valenciennes .}\\
                		\textcolor{red}{\textit{manchester city face aston villa at the etihad stadium on saturday .}} \\
                		\hline \bf Coverage penalty (COVP)\\
                		\hline \textcolor{red}{\textit{manchester city face aston villa at the etihad stadium on saturday .}} \\
                		\textcolor{red}{\textit{manchester city face aston villa at the etihad stadium on saturday .}} \\
                		\textcolor{blue}{manchester city are rivalling manchester united and arsenal .}\\
                		\hline \bf Semantic cohesion loss (SCL) \\
                		\hline \textcolor{blue}{\textit{manchester city are rivalling manchester united and arsenal for} defender dayot upamecano .}\\
                		\textcolor{blue}{\textit{manchester city are rivalling for} valenciennes teenage.} \\
                		\hline \bf Diverse Convolutional Seq2Seq  Model (DivCNN) \\
                		\hline \textcolor{red}{manchester city face aston villa at the etihad stadium on saturday . } \\
                		\underline{\textcolor{green}{the 16-year-old}} has played \textcolor{brown}{for} \underline{\textcolor{brown}{france}}  \textcolor{brown}{for a few months.}
                		\vspace{0.2mm} \\
                		\hline \bf Trigram decoder (TRI) \\
                		\hline \underline{\textcolor{blue}{defender dayot upamecano}} has played \textcolor{purple}{for} \underline{\textcolor{purple}{france}} \textcolor{purple}{at unk and unk level .}\\ 
                		\textcolor{red}{manchester city face aston villa at the etihad stadium on saturday .} \\
                		\hline \bf Ours (Attention Filter + Sentence-level Backtracking decoder) \\
                		\hline \textcolor{red}{manchester city face aston villa at the etihad stadium on saturday .} \\
                		\textcolor{green}{the 16-year-old almost joined united in the january transfer window .}\\
                		\textcolor{blue}{manchester city are rivalling manchester united and arsenal for teenage defender daypot upamecano .}\\
                		\hline
                	\end{tabular}
                   \label{tab:strong_methods}
                \end{center}
                
                
                \item In equation (7), $a^l_i$ seems not properly defined. How is it exactly calculated?
                \item[] \textbf{AUTHORS' RESPONSE}: 
                The $a^l_i$ in equation (7) is a vector whose elements are the attention scores between the $i$-th token in the summary and all the tokens in the source document.
                
                The $j$-th element of $a^l_i$ is the attention score of the $i$-th token in the summary and the $j$-th token in the source document, which is the $a^l_{ij}$ calculated as equation (5).
                \\ \hspace*{\fill} \\
                Considering the Reviewer's comments, we explained $a^l_i$ as follows:
                
                ``
                \begin{equation}
                	A_{s}^{l} = \sum_{i=v_{s-1}+1}^{v_{s}-1}a_{i}^{l} \nonumber
                \end{equation}
                where $a_i^l=(a_{i0}^l, a_{i1}^l,..., a_{ij}^l,..., a_{iD}^l)$ is also a $D$-dimensional vector that records the attention scores of the $i$-th token in the summary over all the tokens in the source document. 
                ''(Section 2.2)
                 
                \item In Table 4, using colors to highlight the attendance in the source would make the Table clearer.
                \item[] \textbf{AUTHORS' RESPONSE}: 
                According to the Reviewer's suggestion, we used colors to highlight the attendance in the source document and summary in Table 4.
                The revised Table 2 is as follows:
                \begin{center}
                	\captionof{table}{\label{tab:attn_exp} (Table 4 in revised version) Summary generated by the basic CNN model and ATTF \protect\\
                	model. The segment of summary uses the same color as its attended section in the source \protect\\ document.}
                	\small
                	\begin{tabular}{|l|l|}%{|p{7cm}|rl|}
                		\hline 
                		\multicolumn{2}{|c|}{\bf Source document} \\
                		\hline
                		\multicolumn{2}{|c|}{\tabincell{l}{(1)justin timberlake and jessica biel, (2)welcome to parenthood. 
                				(3)\textcolor{red}{the celebrity couple announced the} \\
                				\textcolor{red}{arrival of their son,} 
                				(4)... 
                				(5)\textcolor{blue}{the couple announced the pregnancy in january,} (6)...  
                				(7)\textcolor{brown}{it is the first baby}\\
                				\textcolor{brown}{for both .} }} \\
                		\hline 
                		\bf Basic CNN model (CNN) & \bf ATTF (our) \\
                		\hline 
                		\tabincell{l}{(1)\textcolor{red}{the couple announced the the arrival of their son.} \\
                			(2)\textcolor{blue}{the couple announced the pregnancy in january.} \\ 
                			(3)\textcolor{blue}{the couple announced the pregnancy in january.}} 
                		& \tabincell{l}{(1)\textcolor{red}{the couple announced the arrival of their son.} \\
                			(2)\textcolor{blue}{the couple announced the pregnancy in january.} \\
                			(3)\textcolor{brown}{it is the first baby for both.}} \\
                		\hline
                	\end{tabular}
                \end{center}
                
                \item What is the computational complexity of the Sentence-level Backtracking Decoder (SBD) and how it related to the complexity of beam search?
                \item[] \textbf{AUTHORS' RESPONSE}: 
                
                \item The paper presents experiments comparing the current model with Transformers (Speed section). Could the authors elaborate more about the complexity of the models? - - How the computational complexity of CNN seq-to-seq is related to Transformers? (e.g. transformers has a complexity of O($n^2$) )
                \item[] \textbf{AUTHORS' RESPONSE}: 
                
                \item One alternative to the SBD would be to learn to avoid similar sentences during training using a modified loss function (e.g. check previous tokens and penalize n-grams during generation modifying the vocabulary probability).
                \item[] \textbf{AUTHORS' RESPONSE}: 

				
			\end{itemize}

			\item \textbf{Reviewer 3}
			\begin{itemize}
			    \item The organisation of this paper need to be improved, especially the approach part. Now it is very difficult to follow and the denotations/equations are messy.
			    \begin{itemize}
			    \item[a.] The definition of segment and sections are confusing, and only mentioned in the introduction. There should be a subsection in Section 2 describing them in details. And the reason you use clauses instead of ngrams or sentences is only supported by cases but not numbers. There should be a detailed comparison and analysis.
			    
			    \item[b.] equations 3-6 are confusing. In E3, h itself is used for word output, and in the last paragraph of 2.1, you said 'Finally, c is added to h, which forms the input for the next decoder layer or the final output'. These should be all modified into a clear version.
			    
			     \item[c.] Section 2.2 is very confusing, and I spent a lot of time to figure it out clearly. Basically, the main equation should be E9, where you want to punish previously attended source tokens. However, current organization is messy and denotations are confusing. This section should be re-written, and summarised into an algorithm table if necessary.
			    
			     \item[d.] about $g'_s$, not sure what you mean by flipped version, do you mean 1-X?
			    
			     \item[e.] After E9, I think attention scores will not be a probability distribution any more. Why do you not re-normalize it?
			     \end{itemize}
			    \item[] \textbf{AUTHORS' RESPONSE}:  
			    We have revised the parts of our manuscript that refer to "identification of the repetition problem".
			    The details are as follows:
			    \begin{itemize}
			    \item[a.] The definition of segment and sections are confusing, and only mentioned in the introduction. There should be a subsection in Section 2 describing them in details. And the reason you use clauses instead of ngrams or sentences is only supported by cases but not numbers. There should be a detailed comparison and analysis.
			    
			    \item[b.] equations 3-6 are confusing. In E3, h itself is used for word output, and in the last paragraph of 2.1, you said 'Finally, c is added to h, which forms the input for the next decoder layer or the final output'. These should be all modified into a clear version.
			    
			    \item[c.] Section 2.2 is very confusing, and I spent a lot of time to figure it out clearly. Basically, the main equation should be E9, where you want to punish previously attended source tokens. However, current organization is messy and denotations are confusing. This section should be re-written, and summarised into an algorithm table if necessary.
			    
			    \item[d.] about $g'_s$, not sure what you mean by flipped version, do you mean 1-X?
			    
			    \item[e.] After E9, I think attention scores will not be a probability distribution any more. Why do you not re-normalize it?
			    \end{itemize}
			    				
				\item Some claims about tri-gram blocking are not accurate. In intro, you said 'While this simple but crude method avoids the repeat of any kind completely, it ignores the fact that some amount of repetition may exist in natural summaries. On the other hand, the meddling of the beam search at runtime causes another problem: it tends to generate sentences that are logically incorrect. ' I am not sure why TRI is a meddling of the beam search while SBD is not. In popular implementations, including ONMT and Huggingface, TRI is implemented by setting the beam contained tri-gram repentance to a low value, and I don't see an obvious different between this and SBD. And also, the claim the TRI will introduce logically incorrect content is not supported by any experiments.
				\item[] \textbf{AUTHORS' RESPONSE}:  
                We added the introduction of DivCNN Seq2seq into Section 1, Section 3.3 and Section 4.2
                as follows:
         
                              
				\item The novelty of evaluation metrics is limited, as we already got novel gram and many previous human evolution methods for summarization.
				\item[] \textbf{AUTHORS' RESPONSE}:  
                According to the Reviewer's suggestion, we test the models on Newsroom and DUC to evaluate the generalizability of our proposed models.
                We did not use Gigaword and XSum, because they use singel-sentence as a summary.  
              

				\item The paper misses a lot of references. Many new methods or pre-trained summarization models were not mentioned, including [1],[2],[3]
				\item[] \textbf{AUTHORS' RESPONSE}:  
				We have explain why we choose $k=v_{s}-v_{s-1}-1$ in Eq. 8 as follows:
				
	

                \item The experiments are not solid. The main baseline result on CNN/DM is too low. A simple Transformer model can beat this baseline by a large margin [1]. If the authors really want to show the superiority of their method, a stronger comparison should be conducted.
				\item[] \textbf{AUTHORS' RESPONSE}:  
				Considering the Reviewer's suggestion, we reported the correlation between repeatedness of the generated summary and the human readability judgement
				in Table 10.
				
				
				\item A comparison of ATTF and CNN+TRI should be provided, since the last is a simple but strong framework, and we should see how the attention-based modification perform against simple tri-gram blocking.
				\item[] \textbf{AUTHORS' RESPONSE}:  
				
				\item page7, tokens minus the punctuation token $->$ tokens after removing the punctuation tokens
				\item[] \textbf{AUTHORS' RESPONSE}:  
				According to Reviewer's suggestion, we changed ``$U_i$ and $V_i$ are both sequence of tokens minus the punctuation tokens.''
				to 
				``$U_i$ and $V_i$ are both sequence of tokens after removing the punctuation tokens.''. (Page 8 of revised version).
				
				\item minor issues: page 5, the introduction of DPPs here is confusing, and we do not know what it is shortened for.
				\item[] \textbf{AUTHORS' RESPONSE}:  
				According to Reviewer's comment, as  Determinantal Point Processes methods (DPPs) are used in our baseline Diverse Convolutional Seq2Seq Model (DivCNN) (Li et al. 2019a), we re-introduced DivCNN and DPPs as follows:
				``
				The other is Diverse Convolutional Seq2Seq
				Model (DivCNN) (Li et al. 2019a), 
				which introduces Determinantal Point Processes methods () into deep neural network (DNN) attention adjustment
				and uses status of DNN as quality and diversity (QD-score) for abstractive summarization.
				DivCNN first selects the attention distribution of the subsets of source with high QD-score
				first and then adds it into model loss as a regularization.
				DivCNN does not directly redistribut the attention,
				so it may still attend to similar POIs.
				In order to improve QD-score, DivCNN tends to attend to scattered subsets of sentences in source document,
				which leads to semantic incoherence. 
				''
                

			\end{itemize}
		\end{enumerate}
                
	    [1] Kulesza, Alex, and Ben Taskar. "k-DPPs: Fixed-size determinantal point processes." ICML. 2011.
	
		We appreciate the hardwork by the editors and the reviewers,
		and hope that our revision meets their requirements.
		
		Once again, thank you very much for  your comments and suggestions.
		
		
		\closing{Sincerely yours,}
		
		
		%\encl{Curriculum vitae, employment form} % List your enclosed documents here, comment this out to get rid of the "encl:"
		
		%----------------------------------------------------------------------------------------
		
	\end{letter}
	
\end{document}
