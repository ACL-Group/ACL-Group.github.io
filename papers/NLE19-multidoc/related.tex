\section{Related Work}
\label{sec:related}

Repetition is a persistent problem in the task of 
neural-based summarization. 
Repetition in summarization is tackled broadly from two directions in recent years. 

One involves {\em information selection} or sentence
selection before generating summaries.
%The existing %hierarchical 
%RNN-based models %to deal with summarization
%can not be converted to CNN seq2seq models. 
Chen~\cite{P18-1063} uses an extractor agent 
to select salient sentences or highlights and then employs 
an abstractor network to rewrite these sentences.
It can not solve repetition in seq2seq model.
Tan~\cite{TanWX17} and Li~\cite{D18-1205,D18-1441} encode
sentence using word vectors
and predicts words from sentence vector in sequential order 
whereas CNN-based models are naturally parallelized. 
While transferring RNN-based model to CNN model, the kernel size and the number of 
convolutional layer can not be easily determined at conversion between sentence and word vector. 
Therefore, we do not compare our models to those models. 
%\KZ{What about Chen? Also not parallel?
%Have you really considered carefully whethere they can be adapted to our framework?}

The other direction is to improve the 
{\em memory of previously generated words}.
Suzuki~\cite{SuzukiN17} and Lin~\cite{LinSMS18} 
deal with word repetition in single-sentence summaries, 
while we primarily deal with multi-sentence summaries with 
sentence-level repetition. 
There is almost no word repetition in CNN-based model.
Jiang~\cite{JiangB18} adds a new decoder without attention mechanism.
In CNN-based model, attention mechanism is necessary to connect encoder 
and decoder.
%\KZ{The following ``thus'' is very strange. Why we compare with the following and not
%the above two?? You need to make the case stronger!}
%Thus, we convert the following models to construct our baselines.
Thus, our model also is not compared with the above models. 
The following models can be transferred to CNN seq2seq model and
are used as our baselines.
%See\cite{SeeLM17}, Paulus\cite{PaulusXS17} 
%and Fan\cite{FanGA18} choose to adopt sequence-to-sequence approach, 
%where source document and summary are treated as two long sequences, 
%without the idea of separate sentences. 
See~\cite{SeeLM17} integrates coverage mechanism, 
which keeps track of what have been summarized, as a feature that helps 
redistribute the attention scores in an indirect manner,
in order to discourage repetition. 
Tan~\cite{TanWX17} uses distraction attention
\cite{ChenZLWJ16}, which is identical to coverage mechanism. 
Paulus~\cite{PaulusXS17} proposes intra-temporal attention and 
intra-decoder attention which dynamically revises the attention distribution while decoding. 
It also avoids repetition in test time by directly banning the generation of 
repeated trigrams in beam search. 
%These two models are RNN-based. 
Fan~\cite{FanGA18} borrows the idea from Paulus~\cite{PaulusXS17} and builds a CNN-based model. 

Our model deals with the attention in both encoders and decoders. 
Different from the previous methods, 
our \textit{attention filter mechanism} does not 
treat the attention history as a whole data structure,  
but divides it into sections (\figref{fig:model_main}). 
%\KZ{I actually think it might be better to draw a diagram
%to show this in approach.} 
Previously, the distribution curve of accumulated attention scores 
for each token in the source document tends to be flat, 
which means critical information is washed out during decoding.
Our method emphasizes previously attended sections 
so that 
important information is retained.

Given our observation that repetitive sentences in the source are
another cause for repetition in summary, 
which cannot be directly resolved by manipulating attention values, 
we introduce \textit{sentence-level backtracking decoder}. 
Unlike Paulus \cite{PaulusXS17}, 
we do not ban repetitive \textit{trigrams} in test time. 
%Instead, 
Our decoder regenerates a sentence that is similar to previously generated ones.
With the two modules, our model is capable of generating summaries with a
natural level of repetition while retaining fluency and consistency.
