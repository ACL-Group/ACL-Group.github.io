% nleguide.tex
% v1.0, released 31 Jan 2019
% Copyright 2019 Cambridge University Press

\documentclass{nle}
\bibliographystyle{nlelike}

\usepackage{natbib}
\usepackage{url}
\usepackage{soul}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}
%\usepackage{tablefootnote}
\usepackage{helvet}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amsthm,amsopn}
\usepackage{epsfig}
\usepackage{graphicx}
%\usepackage{array}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{subfigure}
\theoremstyle{definition}
\usepackage{booktabs}
\usepackage{caption2}
\newtheorem{example}{Example}


\ifpdf%
\usepackage{epstopdf}%
\else%
\fi

\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}

\begin{document}
\label{firstpage}

\lefttitle{Reduce Repetition in CNN-based Summarization}
\righttitle{Natural Language Engineering}

\papertitle{Article}

\jnlPage{1}{00}
\jnlDoiYr{2019}
\doival{10.1017/xxxxx}

\title{Reducing Repetition in Convolutional Abstractive Summarization}

\begin{authgrp}
\author{Yizhu Liu}
\affiliation{Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China \\
        \email{liuyizhu@sjtu.edu.cn}}
\author{Xinyue Chen}
\affiliation{Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China \\
        \email{sherryicss@gmail.com}}
\author{Xusheng Luo}
\affiliation{Search and Recommendation Team, Alibaba Group, Hangzhou, China \\
        \email{lxs140564@alibaba.com}}
\author{ Kenny Q. Zhu}
\affiliation{Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China \\
        \email{kzhu@cs.sjtu.edu.cn}}
\end{authgrp}

\history{(Received xx xxx xxx; revised xx xxx xxx; accepted xx xxx xxx)}
%\received{20 March 1995; revised 30 September 1998}

\begin{abstract}
Convolutional sequence to sequence (CNN seq2seq) models have met success in abstractive summarization. However, their outputs often contain repetitive word sequences and logical inconsistencies,
limiting the practicality of their application. 
In this paper, we find the reasons behind the repetition problem in CNN-based abstractive summarization through observing the attention map between the summaries with repetition and their corresponding source documents and mitigate the repetition problem.
We propose to reduce the repetition in summaries by 
Attention Filter mechanism (ATTF) and Sentence-level Backtracking Decoder (SBD),
which dynamically redistributes attention over the input sequence 
as the output sentences are generated. 
The ATTF can record previously attended locations in the source document directly and prevent the decoder from attending to these locations. The SBD prevents the decoder from generating similar sentences more than once via backtracking at test.
The proposed model outperforms the baselines 
in terms of ROUGE score, repeatedness, and readability. 
The results show that this approach 
generates high-quality summaries with minimal repetition,
and makes the reading experience better.
\end{abstract}

\maketitle
\input{intro}
\input{approach}
\input{eval}
\input{related}
\input{conclude}

~\\
\noindent
Competing Interests: Yizhu Liu and  Xinyue Chen are the students of Shanghai Jiao Tong University. Xusheng Luo is employed at Alibaba Group. Kenny Q. Zhu is employed at Shanghai Jiao Tong University.


\bibliography{mybibfile}

\label{lastpage}

\end{document}
