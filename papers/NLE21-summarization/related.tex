\section{Related Work}
\label{sec:related}
In this section, we 
discuss neural-based abstractive summarization
and some previous work on repetition reduction methods in
abstractive summarization.

\subsection{Neural-based Abstractive Summarization}
Automatic summarization
condenses long documents into short summaries
while preserving the important information of the documents.
\citep{RadevHM02,AllahyariPASTGK17,Tian18}
There are two general approaches to automatic summarization: 
Extractive summarization and Abstractive summarization.
Extractive summarization selects sentences 
from the source articles, which can produce
grammatically correct sentences~\citep{BokaeiSL16,VermaL17,NaserasadiKS19,ZhongLWQH19}.
Abstractive summarization is a process of {\em generating} a concise and 
meaningful summary from the input text, possibly with words or sentences 
not found in the input text. 
A good summary should be coherent, 
non-redundant and readable~\citep{YaoWX17}.
Abstractive Summarization is one of the most challenging and 
interesting problems in the field of Natural Language Processing (NLP)
\citep{CareniniC08,PallottaDB09,SankarasubramaniamRG14,BingLLLGP15,RushCW15,LiHZ16,YaoWX17,NguyenCNN19}.

Recently, neural-based (encoder-decoder) models~
\citep{RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17,LiuL19,WangQW19,BART19,LiuJZ21}
have made some progress for abstractive summarization.
Most of them use recurrent neural networks (RNN) with different attention 
mechanisms~\citep{RushCW15,NallapatiZSGX16,SeeLM17,PaulusXS17}. \citet{RushCW15} are the first to apply the 
neural encoder-decoder architecture to text summarization. 
\citet{SeeLM17} enhance this model with a pointer generator network 
which allows it to copy relevant words from the source text.
RNN models are difficult to train because of the 
vanishing and exploding gradient problems.
Another challenge is that the current hidden state in an RNN is 
a function of previous hidden states, so RNN cannot be easily
parallelized along the time dimension during training and evaluation, 
and hence training them for long sequences becomes very expensive in 
computation time and memory footprint.

To alleviate the above challenges,
Convolutional neural network (CNN) 
models~\citep{gehring2017convs2s,FanGA18,LiuLZ18,Zhang2019AbstractTS} 
are applied into seq2seq models.
\cite{gehring2017convs2s} propose a CNN seq2seq model equipped with
Gated Linear Units \citep{DauphinFAG17}, residual connections \citep{HeZRS16}
and attention mechanism. 
\cite{LiuLZ18} modify the basic CNN seq2seq model with a summary length
input and trains a model that produces fluent summaries of desired length.
\cite{FanGA18} present a controllable CNN seq2seq model to
allow users to define high-level attributes of generated
summaries, such as source-style and length.
\cite{Zhang2019AbstractTS} add a hierarchical attention mechanism to CNN seq2seq model.
CNN-based models can be parallelized during
training and evaluation. The computational complexity of
these models is linear with respect to the length of sequences.
CNN model has shorter paths between pairs of input and
output tokens so that it can propagate gradient signals more
efficiently.
CNN model enables much faster training and more stable gradients 
than RNN. 
\cite{bai2018empirical} showed that CNN is more powerful than 
RNN for sequence modeling.
Therefore, in this work, we choose the vanilla CNN seq2seq model as 
our base model.

\subsection{Repetition Reduction for Abstractive Summarization}
Repetition is a persistent problem in the task of 
neural-based summarization. 
It is tackled broadly in two directions in recent years. 

One direction involves {\em information selection} or 
{\em sentence selection} before generating summaries.
\cite{P18-1063} propose an extractor-abstractor model, which uses an extractor  
to select salient sentences or highlights and then employs 
an abstractor network to rewrite these sentences.
\cite{SharmaHHW19} and \cite{SanghwanB19} also use extractor-abstractor model 
with different data preprocessing methods.
All of them can not solve repetition in seq2seq model.
\cite{TanWX17} and \cite{D18-1205,D18-1441} encode
sentences using word vectors
and predict words from sentence vector in sequential order 
whereas CNN-based models are naturally parallelized. 
While transferring RNN-based model to CNN model, 
the kernel size and the number of 
convolutional layers can not be easily determined when
converting between sentences and word vectors. 
Therefore, we do not compare our models to those models in this paper. 

The other direction is to improve the 
{\em memory of previously generated words}.
\cite{SuzukiN17} and \cite{LinSMS18} 
deal with word repetition in single-sentence summaries, 
while we primarily deal with multi-sentence summaries with 
sentence-level repetition. 
There is almost no word repetition in multi-sentence summaries.
\cite{JiangB18} add a new decoder without attention mechanism.
In CNN-based model, attention mechanism is necessary to connect encoder 
and decoder.
Therefore, our model also is not compared with the above models in this paper. 
The following models can be transferred to CNN seq2seq model and
are used as our baselines.
\cite{SeeLM17} integrates coverage mechanism, 
which keeps track of what has been summarized, as a feature that helps 
redistribute the attention scores in an indirect manner,
in order to discourage repetition. 
\cite{TanWX17} use distraction attention
\citep{ChenZLWJ16}, which is identical to coverage mechanism. 
\cite{GehrmannDR18} add coverage penalty to loss function
which increases whenever the decoder directs more than 1.0 of total attention
towards a word in encoder.
This penalty indirectly revises attention distribution and results in
the reduction of repetition.
\cite{elikyilmazBHC18} uses semantic cohesion loss,
which is the cosine similarity between two consecutive sentences, as part of
the loss that helps reduce repetition.
\cite{DivC2C19} add Determinantal Point Processes methods (DPPs)
into deep neural network (DNN) attention adjustment
and takes attention distribution of
subsets selected from source document by DPPs as the part of loss.
\cite{PaulusXS17} propose intra-temporal attention \citep{NallapatiZSGX16} and 
intra-decoder attention which dynamically revises the attention distribution while decoding. 
It also avoids repetition at test time by directly banning the generation of 
repeated trigrams in beam search. 
\cite{FanGA18} borrows the idea from \cite{PaulusXS17} and 
builds a CNN-based model. 

Our model deals with the attention in both encoders and decoders. 
Different from the previous methods, 
our \textit{attention filter mechanism} does not 
treat the attention history as a whole data structure,  
but divides it into sections (\figref{fig:model_main}). 
Previously, the distribution curve of accumulated attention scores 
for each token in the source document tends to be flat, 
which means critical information is washed out during decoding.
Our method emphasizes previously attended sections 
so that important information is retained.

Given our observation that repetitive sentences in the source are
another cause for repetition in summary, 
which cannot be directly resolved by manipulating attention values, 
we introduce \textit{sentence-level backtracking decoder}. 
Unlike \cite{PaulusXS17}, 
we do not ban repetitive \textit{trigrams} in test time. 
Instead, our decoder regenerates a sentence that is similar to previously generated ones.
With the two modules, our model is capable of generating summaries with a
natural level of repetition while retaining fluency and consistency.

\subsection{Pretrained Models for Summarization}
The pretrained transformer language models have success in summarization tasks.

Some of the pretrained summarization models apply pretrained contextual encoders, such as BERT~\citep{Bert19}.
BERT proposes a transformer-based masked language model, where some of the tokens of an input sequence are randomly masked, and the goal is to predict these masked tokens with
the corrupted sequence as input. 
\cite{LiuL19} introduce a document-level encoder based on
BERT which is able to express the semantics
of a document and obtain representations for
its sentences.
\cite{ZhongLCWQH20} leverage the BERT in a Siamese~\citep{Siamese93} network structure to
construct a new encoder for the representation of the source document and reference summary.
\cite{HiBert19} propose a novel HIBERT encoder for document encoding
and apply HIBERT to summarization model.

Others are pretrained on sequence-to-sequence (seq2seq) models.
UniLM~\citep{UniLM19} is a multi-layer transformer network,
which utilizes specific self-attention masks based on three language model (i.e., unidirectional, bidirectional and seq2seq language models)
 to control what context the prediction conditions on. 
The seq2seq language model in UniLM attends to bidirectional contexts for source document and left contexts only
for summary.
For the pretraining seq2seq model, BART~\citep{BART19} uses an arbitrary noising function to corrupt input,
instead of the masked language model. Then, the
corrupted input is reconstructed by training on a transformer seq2seq model.
ProphetNet~\citep{ProNet20} trains on the transformer seq2seq model and 
takes future n-gram prediction as self-supervised.
PEGASUS~\citep{PEGASUS20} uses self-supervised objective Gap Sentences Generation to train 
a transformer seq2seq model. Compared with previous pretrained models, 
PEGASUS masks sentences rather than smaller continuous text spans.
Through fine-tuning the pretrained models or representations on summarization task,
the quality of generated summaries can be improved.

The excellent performance of the pretrained summarization models 
is from large-scale training datasets and heavy network structures, 
which always brings huge consumption of training time and memory space. 
However, the goal of our approach is to reduce repetition in abstractive summarization.
The comparison results of the summaries generated by the vanilla models adding different reducing repetition methods can obviously show the effectiveness of different  reducing repetition methods.
Thus, we take the vanilla model as our basic model and don't compare our proposed approach with the pretrained models. 


