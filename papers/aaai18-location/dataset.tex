\section{Datasets\footnote{Both datasets are available at \url{adapt.seiee.sjtu.edu.cn/~frank/location_relation_data.zip}}}
\label{sec:data}
We create a vocabulary of single-word physical objects by the 
intersection of all entities that belong to ``physical object'' class
in Wikidata\footnote{\url{www.wikidata.org}} and 
all the ConceptNet concepts. We then manually filtered out some 
words that have the meaning of an abstract concept, which 
results in 1169 physical objects in total.

We use a cleaned subset of the Project Gutenberg corpus \cite{lahiri:2014:SRW} 
in this work\footnote{\url{web.eecs.umich.edu/~lahiri/gutenberg_dataset.html}}, containing 3,036 English books written by 142 authors.
%Sentences in novels are more likely to describe real life scenes 
%than other well-known corpus. 
%We compare the density of \lnear~ relations in Gutenberg with other 
%widely used corpora, namely Wikipedia, 
%used by~\citeauthor{mintz2009distant} and New York Times corpus, 
%created by~\citeauthor{riedel2010modeling} and 
%used by~\citeauthor{Lin2016NeuralRE,hoffmann2011knowledge,surdeanu2012multi}. 
%In English Wikipedia dump, out of all sentences which mentions at least two
%physical objects, 32.4\% turn out to be positive. 
%In New York Times corpus~\footnote{\url{http://iesl.cs.umass.edu/riedel/ecml/}},
%the percentage of positive sentences is only 25.1\%. 
%In contrast, that percentage in the Gutenberg corpus is 55.1\%, much higher 
%than the other two corpora, making it a good choice for \lnear~ 
%relation extraction.
%\subsection{\lnear~ Sentences Dataset}
%\label{lsd}
From this corpus, we identify 15,193 pairs that co-occur in more than 10 sentences.
Among these pairs, we randomly select 500 object pairs and 
10 sentences for each pair for the annotators to label true or false. 
%We do not distinguish between \lnear\ and \textsc{atLocation} relations,
%where the latter typically refers to objects which are adjacent to or
%contained by each other, e.g., {\em room} and {\em door}.
%Each sentence is labeled by three annotators who are college students
%and proficient with English. The final truth label of a sentence is decided
%by a majority vote from the four annotators. 
%The Cohen's Kappa among the three annotators is 0.711 which suggests
%substantial agreement. 
%This dataset will be used to train and test models for relation
%classification.
%We compare the statistics of our \lnear\ sentence
%dataset with a few datasets on other well known relations in 
%\tabref{tab:datasets}.
%One can see that our dataset almost double the size of those most
%popular relations in the SemEval task, and the sentences in our
%data set tend to be longer with more words.
From a total of 5000 labeled data samples, 
we randomly choose 4000 as the training set and 1000 as the test set.

%\begin{table*}[th]
%\centering
%\small
%\begin{tabular}{|l|c|c|c|c|} \hline
%Data set & Frequency & Percentage & Words per entry & Chars per word  \\ \hline \hline
%\lnear & 2,754 & 55.1 & 18.6 & 4.51  \\ \hline
%Not \lnear& 2,246 & 44.9 & 19.1 & 4.32  \\ \hline\hline
%Cause-Effect & 1,331  & 12.4 & 17.3 & 4.71\\ \hline
%Component-Whole &1,253 & 11.7 & 17.9 & 4.12 \\ \hline
%Others &1,864 & 17.4 & 17.8 & 4.34 \\ \hline
%\end{tabular}
%\caption{Comparison between our \lnear~ dataset and 
%the most popular relations from SemEval 2010 Task 8 dataset for 
%relation classification}
%\label{tab:datasets}
%\end{table*}

%\subsection{Commonsense \lnear~ object pairs}
We randomly sampled 500 pairs of objects by the number of sentences they
appear in. 
%This tends to give us pairs which are more popular.
We again ask the same annotators to label whether each pair of objects
are likely to locate near each other in the real world. 
Majority votes determine the final truth label as before.
%The inter-annotator agreement here is {0.703}. 
This smaller data set is used to evaluate the 
scoring and ranking of relation triples.
