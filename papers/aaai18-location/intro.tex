\section{Introduction}
%Commonsense knowledge is an important ingredient in machine comprehension and
%inference. 
%Intelligent systems about comprehension and inference 
%can be improved by incorporating commonsense knowledge as background, 
%such as ice is cold, 
%chewing is a subevent of eating, 
%chair and table are typically found near each other, etc. 
%Such commonsense facts could benefit entailment~\cite{dagan2009recognizing}, grounded entailment~\cite{bowman2015large}, or visual recognition tasks~\cite{zhu2014reasoning}.
%Such commonsense knowledge is often represented as relation triples
%between two concepts or objects in ConceptNet,
%which is largely manually curated
%or crowd-sourced by community efforts, and thus does not scale well.
%For example, ConceptNet\footnote{\url{http://conceptnet.io/}} by MIT \cite{speer2012representing}, one of the largest commonsense
%knowledge graph available today, contains only 49 \lnear~
%relation triples. 
%%Many commonly co-located objects such as (house, garden) and 
%%(fork, knife) are not included in this knowledge base. 
%Another problem is that such commonsense knowledge bases are typically contributed by just a very limited number of people due to the cost of manual labor. 
%Thus no meaningful statistical scores can be
%associated with the triples, making rank-based 
%computation difficult. 
%For instance, although ConceptNet gives a confidence
%score (from 0 to infinity) to each triple, most of the triples have the default
%score of 1, simply because the human contributor did not or could not 
%provide a score. 
%If such commonsense knowledge is harnessed automatically
%from open-domain text corpora, both of the above problems can be 
%effectively addressed. 
%Open information extraction not only provides the much needed scale,
%but also valuable statistics that can turn into confidence scores.
%
This paper aims at extracting the commonsense \lnear\ relation between
physical objects, which is defined as two objects typically found 
near each other.
%\footnote{Because some physical objects can be a location itself, this
%relation may include some instances of the \textsc{atLocation} relation,
%e.g., {\em room} and {\em door}.}
We focus on \lnear\ relation for these reasons: 
i) such prior knowledge of \lnear\ relations is helpful for 
object detection in complex image 
scenes. 
For example, in a dimly lit room with settings shown in \figref{fig:dim}, 
if a bright laptop is present on a table,
one may guess that a lamp, a photo frame or books maybe nearby 
based by the commonsense, 
even though these objects are hardly visible due to low light. 
%(The guess can be based on commonsense knowledge learned from room settings scene description in articles and texts.)
%Such prior knowledge helps with the object detection accuracy; 
ii) \lnear~ relation can benefit general reasoning in reading comprehension,
question answering and many other AI tasks;
iii) existing knowledge bases such as ConceptNet have very limited facts for
this relation.

%
%\begin{enumerate}
%\item \lnear relation is useful for object detection in complex image 
%scenes. For example, in a dimly lit room with a dining table and some chairs.
%One may guess that plates and other kitchenware maybe present on the table.
%Such prior knowledge helps with the object detection accuracy. \KZ{give a 
%real image here which is hard to detect by state-of-the-art algo but 
%obeys commonsense.}
%
%%\item \lnear relation can also be useful for automated conversation systems
%%where meaningful context maybe added to the conversation. \KZ{Example?}
%
%\item \lnear relation can benefit general reasoning in reading comprehension,
%question answering and many other AI tasks.
%
%\item Existing knowledge bases such as Concept Net has very limited facts for
%this relation.
%\end{enumerate}

%Automatic extraction of relations from open text has a short but rich history. 
%Attempts have been made to extract isA, causal, correlation, and 
%also open domain relations (e.g., ReVerb, Yago).
%%\KZ{cite and Say more about KBC}. 
%\lnear~relation is unique and poses significant challenges
%for the following reasons:
%
%i) It involves physical (often visible) objects whereas other 
%popular relations involve general concepts or just natural language terms. 
%
%ii) The distribution of \lnear~ relation is not even across domains: 
%it is more prevalent in literary work such as stories 
%and dramas which come with descriptive scenes rather
%than in news, science \& technology related articles or online user 
%generated content. 
%%Our preliminary investigation shows that XXX\% of the sentences 
%%in New York Times and YYY\% of the sentences
%%in Wikipedia contains a \lnear~ relation, 
%%while that percentage for novels and stories in Gutenberg corpus is ZZZ\%. 
%
%iii) Sentences in literature are often complex and nuanced, 
%which makes extraction particularly challenging. Consider the objects
%``bed'' and ``star'' in the following
%sentence: ``{\em Until at last all the promenaders had gone home to bed, 
%and I was alone with the star.}'' Bed is not near the star
%because it's at another location.
%
%iv) Labeling such sentence is a non-trivial task, 
%and obtaining a large training set is difficult and expensive.
%
%\BL{add more related work about KBC here, mainly about Xiang Li (TTIC) and Stating the Obvious; to stress that why we are using raw text}

%Since raw text of novels tend to contain many 
%descriptions of  scene in real life, 
%we argue that it is feasible to obtain unseen 
%{\lnear} relations from raw novel text. 

We propose two tasks in extracting \lnear\ relation.
One is a binary classification problem which judges whether 
a sentence is describing two objects physically closeby.
The other is given the classified results of 
large number of sentences, produce a ranked list of \lnear\
facts as a knowledge base.
%Notice that two objects that are {\em co-located} in a couple of sentences
%may not mean they have the \lnear\ relation as commonsense.
To this end, we created two benchmark datasets: one is 5,000 sentences 
each describing a scene of two physical objects and with a label indicating 
if the two objects are co-located; 
the other consists of 500 pairs of objects with binary labels 
indicating whether a pair is commonly located near or not. 
\begin{figure}[th]
	\center
	\includegraphics[width=0.8\columnwidth]{dim-room.jpg}
	\caption{\lnear~relation assists the detection of vague objects}
	\label{fig:dim}
	\vspace{-0.6cm}
\end{figure}
%is to build a framework to automatically extract 
%large amount of unseen object pairs with \lnear relations 
%from literary text corpora.
%that 
%are not already included in ConceptNet. \KZ{Need to back up this claim in eval}
%Considering the fact that not all sentences mentioning two physical objects 
%indicate that the two objects are nearby each other, 
%our proposed approach is to first construct a sentence-level relation classification model 
%to identify \lnear~ relation and then score each candidate object pair with the model 
%and the sentences containing it.
%Thus, the core problem is to judge whether a given sentence 
%containing two objects is describing a \lnear~ relation between the two objects or not. 
%

%More formally, given a sentence $s$ mentioning a pair of physical objects 
%\textless$e_1,e_2$\textgreater, 
%the problem is to determine whether $e_1$ and $e_2$ are located near each other in a physical scene 
%described in the sentence $s$. 
%
%\begin{table}[th!]
%	\centering
%	\caption{Examples for sentence-level \lnear Classification; The  \textless$e_1,e_2$\textgreater in both instances is \textless dog,cat\textgreater. Since the the first sentence is describing a physical scene about the dog and the cat, 
%		while the dog and the cat in the second sentence do not have to be located near since it is just talking about a general comparison.}
%	\label{tbl:lnc}
%	\begin{tabular}{|c|c|}
%		\hline
%		$s$    &  T/F\\ \hline
%	My dog is older than her cat.	 &  T\\ \hline
%	The King put his dog and cat on the table.     & F  \\ \hline
%	\end{tabular}
%
%\end{table}
%
%\KZ{Can use the two examples earlier. 
%For example, suppose $s_1$ = ``The King put his dog and cat on the table.'', 
%$s_2$ = ``My dog is older than her cat.'', 
%$e_1$ is ``dog" and $e_2$ is ``cat''. 
%Then the answer of $s_1$ is \textit{True} and of  $s_2$ is \textit{False} , 
%since the $s_1$ is describing a physical scene about the dog and the cat, 
%while the dog and the cat in $s_2$ do not have to be located near since it is just talking about a general comparison.
%}

%\BL{Add related work here about the relation classification , and their shortcoming (too many parameters) }

We propose two kinds of baseline methods to solve the classification task.
%The first approach leverages syntactic features and word embedding
%in the sentence in a SVM model. 
%The second approach is a DNN
%model that takes advantage of word embeddings of the 
%two objects and the most relevant sequential information of the sentences 
%indicating the co-location, significantly reducing the size of 
%parameters. 
We then associate a probabilistic score with each co-occurrence of
a pair of objects, and aggregate such scores for all co-occurrences
to produce a commonsense knowledge score for this pair.  
%We construct a dataset consisting of \BL{xxx} instances with labels 
%(\textit{True}/\textit{False}) for training our model. 
%Experiments demonstrate that both of the classification methods 
%outperforms the state-of-art approaches on the \lnear dataset.
%\KZ{Need to back up these in the eval.} 
These methods compare favorably with the current state-of-the-art
	method for general-purpose relation classification problem.
We extract in total 2,067 new pairs previously not in
	ConceptNet, with a precision of 0.68. 

