Review#1:
Q1：our approach over-tailored to SCT rather than general story understanding...
The Story Cloze Task is the only well-recognized task of story understanding now. A host of baselines and state-of-the-art models (Nasrin 2016) based on shallow language understanding struggle to achieve high scores on SCT. Many previous works (Wang, Liu, and Zhao 2017; Zhou, Huang, and Zhu 2019; Roemmele et al. 2017; Srinivasan, Arora, and Riedl 2018; Radford et al. 2018; Chen, Chen, and Yu 2018) have tried their best on the task. This task is well worth exploration. The performance on this test set can reflect the ability for story ending prediction which is consistent with our title. Our method performs well on this particular story understanding task. We will transfer our method to other related tasks in the future work.  

Q2: our results not compared to SCT v1.5...
We have pointed out that SCT 1.5 still has, though to a lesser extent, information leak problem in Sec 3.2 through the ending-only experiment. Following the work of Timothy Niven(2019), we statistically explore the spurious cues in different datasets. We find the unbalanced tokens are 34% and 30% in SCT and v1.5 but only 18% in our new dataset. It is not that important to give the test result on v1.5 though we have had the result. We can add three rows in Table 4 for SCT v1.5 in the camera ready version since extra space is allowed. The results for DSSM are 54.30, 57.83, 54.35 and 58.53(corresponding to Original (%) Simp(%) CE(%) Simp+CE (%) separately); For SKBC, the results are 64.56, 67.30, 65.45 and 67.97; For BERT-base(ours), the results are 56.88, 58.02, 59.79 and 60.97. The conclusion is the same with SCT that simplification and graph embedding can help for ending prediction.  

Q3: Why did you choose the Random…
The 4:2 proportion follows Roemmele et al. (2017)’s work. They generate negative endings with several different methods and this proportion gives the best result. Though the backward strategy may introduce some bias (i.e., if the 5th sentence is identical to one of the previous 4, then this is a negative ending) into the training data, such bias doesn't exist in the test set which was manually created. Hence there is no information leak in the generated training data, not by backward method at least. Previous SOTA Models, when trained with our generated trainig data, perform badly in the ending-only experiment in Table 2. This result shows the effectiveness of this dataset. Although we are the first to use this data generation method to reduce information leak, we are not trying to claim any novelty or technical contribution by this approach.

Review#2:
Q1: However, I think the method proposed requires…
We did not neglect the logical indicators especially negation. “Not” and “dis-” are included in ConceptNet. For example, there are 'CapableOf' and 'NotCapableOf' relation in ConceptNet. We use the concepts in ConceptNet mainly as we observe the stories and find most of the key terms for understanding the story can be found in ConceptNet. Just as the example with right ending in Figure 1. All of the key terms can be found in ConceptNet and they provide sufficient information for understanding stories.

Q2: Minor comments…
We will fix the minor errors in the revised version:
3.We have mentioned BERT, DSSM, SKBC in Sec 1 and 2, so this is not the first time in Sec 2.3.
4.In Sec 2, I will add the definition of y: “The classification target y∈{1,2} corresponding to e1 and e2”.

Q3: In Table 4: why does DSSM+CE...
DSSM is a bag-of-words model and it models the first 4 sentences as a whole. When using CE, we have to sum the embeddings of all 4 sentences together and then concatenate it with the output vector of DSSM as final representation. By doing this, we inevitably lose the order information.

Review#3:
Q1: The paper gave me an impression…
First, please consider with Review#1 Q1. Then, ConceptNet is a KB with comprehensive coverage, which contains not only verbal events(e.g. "happy" in Figure 1). 

Q2: As indicated by authors ROCS…
Please refer to Review#1 Q3. Table 2 shows the endings in new dataset have little information leak. Since this data generation method is proposed by previous research, we didn’t describe it in detail. Besides, the target of our paper is story completion, whereas [1][3][7] are more like script knowledge augmentation or tagging task on specific scenarios, which are different. But we compare with 2 typical script-like representations for simplification in Sec 3.4.

Q3: In section 3.3…
Our experiments show that simplification, when applied to strong models such as BERT(base), GPT, and SkipThought, gives rise to 0.8%, 3.10% and 3.43% gain in accuracy respectively on the SCT test set. The result on the GPT was not included in the current submission and will be added in the revised version.

Q4: The paper is weak on…
Due to the space limitation, we only refer to some of the most important papers. We will add the papers you mentioned into our related work. "Events without probabilistic semantics" is an early definition of script and we will remove it in revised version. Thanks for your suggestions.

Q5: ConceptNet itself…
We do observe some noises in ConceptNet. For example, we found 13 unreasonable triples in 500 randomly sampled triples from ConceptNet. But the noises come from the relation triples but not the concept terms. In our simplification method, we merely use the concept terms and not the relations. Therefore, our approach is not affected by the noises in ConceptNet. Furthermore, numberbatch itself has used some methods to reduce the noise in ConceptNet. 

Q6: The sentence representation…
The vocabulary of numberbatch only contains part of ConceptNet terms. Thus some concepts don't have corresponding embedding in numberbatch and that is why we rather separate these two steps.

