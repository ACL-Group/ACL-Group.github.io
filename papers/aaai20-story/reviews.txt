View Reviews
Paper ID6545
Paper TitleEnhanced Story Representation by ConceptNet for Predicting Story Endings
Reviewer #1
Questions

1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
This paper improves on baseline neural network approaches to the Story Cloze Test evaluation (predict final fifth sentence of a short story given two options), primarily through the filtering content words to those that can be matched to ConceptNet nodes, and using the corresponding latent representations of these nodes as input to a supervised binary classifier. Unlike previous work, these evaluations eschew the ROCStories corpus that is traditionally used for training, and instead opts to devise a modified ROCStories training corpus by using random-sentences and previously-seen sentences as incorrect choices. Although this methodology prohibits direct comparisons with top performing approaches, it does show that simplification and concept embeddings can improve the performance within this learning context.
2. [Relevance] Is this paper relevant to an AI audience?
Relevant to researchers in subareas only
3. [Significance] Are the results significant?
Moderately significant
4. [Novelty] Are the problems or approaches novel?
Novel
5. [Soundness] Is the paper technically sound?
Has minor errors
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Sufficient
7. [Clarity] Is the paper well-organized and clearly written?
Good
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
One of the most interesting areas of current natural language understanding research is in the exploitation of external knowledge resources within contemporary neural language processing pipelines. This paper pursues one approach within this line of work, where the ConceptNet resource is used both to filter and represent the content of sentences for input into task-specific neural architectures. A enabler of this work is the ConceptNet Numberbatch representations, which uses an ensemble approach to learn latent meanings that rely more on a nodes relational neighborhood rather than its linguistic context.

However, the results presented in table 4 suggest that much of the gains in performance over the three baseline models comes not from these conceptual embeddings, but rather from the simplification of the sentences. The important question to ask is: why does throwing out information help in this task. The authors argue that this step reduces noise and variance in the training data, but an alternative way of expressing this is that the discarded information is largely irrelevant to the Story Cloze Test. Indeed, the generic conceptual relationships in ConceptNet seem closely aligned with the script-like setup of the SCT five-sentence format. I think it would be a mistake to assume these results will hold up in other language understanding tasks. In this paper, ConceptNet is the right tool for the job, because the job is so narrowly focused on one aspect of language understanding, albeit an important one.

The finding of systematic bias in the SCT, where strong performance could be demonstrated by ignoring the story context, does cast doubt on the validity of the test as a benchmark for commonsense story understanding research. The use of the revised v1.5 in these evaluations is a good move, even if it has its flaws. But the comparative performance of your best results should also be viewed against the best system results on this revision, not merely against the variants you've made using the modified training data set. You note that your aim is not to beat the state of the art, but a direct comparison against top results helps put the significance of your work in context.

These two points, that the approach is overly tailored to the Story Cloze Test rather than general story understanding, and that the work is not compared against top results on the v1.5 test set, work against the overall significance of this research finding. Outside of the SCT community, it remains to be seen if the approach has broad applicability. Inside the SCT community, the raw results won't be viewed as strong enough.
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
Why did you choose the Random and Backward methods in your approach to training dataset generation, and why in the 4:2 proportion? These methods are very contrived, and likely introduce their own set of biases - especially the backward method.
10. [OVERALL SCORE]
6 - Marginally above threshold
15. Please acknowledge that you have read the author rebuttal. If your opinion has changed, please summarize the main reasons below.
I acknowledge that I have read the author rebuttal.
Reviewer #2
Questions

1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
This paper concerns about the task of predicting endings for narrative stories. The authors propose to improve story representations in existing models aimed at this task, by simplifying sentences to a few key concepts and incorporating commonsense knowledge regarding relationships between these key concepts. In particular, a publicly available knowledge graph, ConceptNet, is utilized to provides the set of key concepts to be extracted from sentences, as well as relationships among these concepts. The authors claim that sentence simplification eliminate noise and variance in the text, and incorporating commonsense knowledge from ConceptNet improves understanding of stories. The authors reported experimental results from multiple aspects to show that the improvement technique proposed enhances variable language models in Story Cloze Test.
2. [Relevance] Is this paper relevant to an AI audience?
Likely to be of interest to a large proportion of the community
3. [Significance] Are the results significant?
Moderately significant
4. [Novelty] Are the problems or approaches novel?
Somewhat novel or somewhat incremental
5. [Soundness] Is the paper technically sound?
Technically sound
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Somewhat weak
7. [Clarity] Is the paper well-organized and clearly written?
Satisfactory
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
The paper is very relevant to AI audience. The contribution is incremental. The approach is straightforward and simple but still novel. The experimental results seem to support the author's claim. Overall, the paper is easy to follow.

However, I think the method proposed requires more theoretical justifications to be convincing. While incorporating commonsense knowledge from ConceptNet to improve performance in SCT task seems intuitive, it is hard to see how simplifying sentence into a set of concepts that exist in ConceptNet can be generalizable. This simplification neglects all concepts not in ConceptNet, as well as logical indicators in sentences such as negation. Determining whether a concept in sentence is "key" concept or not based on whether it exists in ConceptNet requires justification. 

Minor comments:
- Introduction, the 3 bullets summarizing contribution: The first bullet is missing period or semi-colon at the end; the second bullet has a colon in the end
- In 2.2: "where Numberbatch(Â·) denotes the concept vector of concept c" -> "where Numberbatch(c) denotes the concept vector of concept c"
- In (3): DSSM, BERT and SKBC were mentioned before they were introduced
- In (3): please define y
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
In Table 4: why does DSSM+CE has worse performance than DSSM?
10. [OVERALL SCORE]
5 - Marginally below threshold
15. Please acknowledge that you have read the author rebuttal. If your opinion has changed, please summarize the main reasons below.
I acknowledge that I have read the author rebuttal.
Reviewer #3
Questions

1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
The paper proposes methods for representing stories by extracting key information with the help of a knowledge base (ConceptNet). Authors show that using only the key concepts from the story helps to perform better on story cloze task as compared to using the complete story text.
2. [Relevance] Is this paper relevant to an AI audience?
Relevant to researchers in subareas only
3. [Significance] Are the results significant?
Moderately significant
4. [Novelty] Are the problems or approaches novel?
Somewhat novel or somewhat incremental
5. [Soundness] Is the paper technically sound?
Has minor errors
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Somewhat weak
7. [Clarity] Is the paper well-organized and clearly written?
Good
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
Strengths:
1. Authors use of commonsense knowledge base like ConceptNet helps to filter out irrelevant information and experimentally show that it helps to improve performance on story cloze task.
2. The paper is mostly well written and experimentally proves it claims.

Weaknesses and Suggestions:

1. The paper gave me an impression that the authors are solving a dataset and not a research problem per se. What is relevant and what is not relevant information is very application specific. For example, if the task is to understand underlying emotions in the story then many of the abstract quantities and adjectives would be relevant but if the task is to solve story cloze then events (mainly in form of verbs) would be relevant. Authors extract relevant events using ConceptNet and generalize it to be a better story representation based on just one dataset (ROCS) and one task. 
2. As indicated by authors ROCS is crowdsourced and is noisy and biased. Authors create a new version of the dataset (Section 3.2, page 6) but the described method (Random and Backward) is not clear. It is also not clear how does it overcome the shortcomings of the original dataset? Why didn't authors try out the same task on other script based datasets like those in [1], [3], [7]? Demonstrating on other datasets would also show the generalization of the proposed method.
3. In section 3.3 (Page 7, Table 5), authors compared BERT with the proposed model. But this is not a fair comparison because BERT has been trained on a much smaller and limited corpus. Transformer based models work best when trained on much larger corpora. Are authors trying to claim that with limited data it is better to extract key information and then use a regular pre-trained text classification model? Nevertheless, it is misleading since if a pre-trained model is being used, why not use a full fledged (i.e. trained on a very large corpora) pre-trained model in the first place? There has been work which has already shown that if there is enough data extracting key information is not helpful than using just a language model.
4. The paper is weak on related work section. There has been lot of work on scripts which is not addressed in the paper: [2], [4], [5], [6], [7]. Authors should address these as well. More at the fundamental level the definition of "Scripts" is wrong. Scripts are sequences of events describing a prototypical activity ([5]). Scripts are kind of recipe for commonsense knowledge about everyday activity and do not necessarily need to have associated probabilistic semantics with them. 

[1] Learning script knowledge with web experiments, ACL 2010
[2] Event Embeddings for Semantic Script Modeling, CoNLL 2016
[3] InScript: Narrative texts annotated with script information, LREC 2016
[4] Modeling Semantic Expectations: Using Script Knowledge for Referent Prediction, TACL 2017
[5] Modeling Common Sense Knowledge via Scripts, 2017
[6] A hierarchical bayesian model for unsupervised induction of script knowledge, EACL 2014
[7] Inducing Neural Models of Script Knowledge, CoNLL 2014
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
1. ConceptNet itself has been crowdsourced and is very noisy, authors do not address this and how would one deal with it?
2. The sentence representation model proposed by authors has two components Simp and CE. Authors experimentally show that the combination of both the component gives better results. Did authors try to combine them into a single step? Basically, initialize the embeddings of extracted concepts with numberbatch embeddings and then pass these through sequential encoder?
10. [OVERALL SCORE]
6 - Marginally above threshold
15. Please acknowledge that you have read the author rebuttal. If your opinion has changed, please summarize the main reasons below.
Yes I have read the author rebuttal.

View Meta-Reviews
Paper ID6545
Paper TitleEnhanced Story Representation by ConceptNet for Predicting Story Endings
META-REVIEWER #1

META-REVIEW QUESTIONS

3. Detailed Comments
Even after the author response, some reviewers were concerned that the paper did not provide adequate evaluation. In the discussion, one reviewer wrote "My concern is that they are stripping down the SOTA model and then showing that these do not perform well, but this is not a fair way to compare. " The reviewers did not see evidence that the ideas in the submission were generalisable to other settings, or supported by theoretical insights.
