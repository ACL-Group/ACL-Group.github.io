\section{Related Work}
\label{sec:related}

Repetition is a persistent problem in the task of 
neural-based summarization. 
Repetition in summarization is tackled broadly from two directions in recent years. 

One involves {\em information selection} or sentence
selection before generating summaries.
Chen~\shortcite{P18-1063} uses an extractor  
to select salient sentences or highlights and then employs 
an abstractor network to rewrite these sentences.
It can not solve repetition in seq2seq model.
Tan~\shortcite{TanWX17} and Li~\shortcite{D18-1205,D18-1441} encode
sentence using word vectors
and predicts words from sentence vector in sequential order 
whereas CNN-based models are naturally parallelized. 
While transferring RNN-based model to CNN model, the kernel size and the number of 
convolutional layer can not be easily determined at conversion between sentence and word vector. 
Therefore, we do not compare our models to those models. 

The other direction is to improve the 
{\em memory of previously generated words}.
Suzuki~\shortcite{SuzukiN17} and Lin~\shortcite{LinSMS18} 
deal with word repetition in single-sentence summaries, 
while we primarily deal with multi-sentence summaries with 
sentence-level repetition. 
%There is almost no word repetition in CNN-based model.
There is almost no word repetition in multi-sentence summaries.
Jiang~\shortcite{JiangB18} adds a new decoder without attention mechanism.
In CNN-based model, attention mechanism is necessary to connect encoder 
and decoder.
Thus, our model also is not compared with the above models. 
The following models can be transferred to CNN seq2seq model and
are used as our baselines.
See~\shortcite{SeeLM17} integrates coverage mechanism, 
which keeps track of what have been summarized, as a feature that helps 
redistribute the attention scores in an indirect manner,
in order to discourage repetition. 
Tan~\shortcite{TanWX17} uses distraction attention
\cite{ChenZLWJ16}, which is identical to coverage mechanism. 
Gehrmann~\shortcite{GehrmannDR18} adds coverage penalty to loss function
which increases whenever the decoder directs more than 1.0 of total attention
towards a word in encoder.
This penalty indirectly revises attention distribution to reducing repetition.
{\c{C}}elikyilmaz~\shortcite{elikyilmazBHC18} uses semantic cohesion loss,
which is cosine similarity between two consecutively sentences, as part of
loss that helps reduce repetition.
Paulus~\shortcite{PaulusXS17} proposes intra-temporal attention \cite{NallapatiZSGX16} and 
intra-decoder attention which dynamically revises the attention distribution while decoding. 
It also avoids repetition in test time by directly banning the generation of 
repeated trigrams in beam search. 
%These two models are RNN-based. 
Fan~\shortcite{FanGA18} borrows the idea from Paulus~\shortcite{PaulusXS17} and 
builds a CNN-based model. 

Our model deals with the attention in both encoders and decoders. 
Different from the previous methods, 
our \textit{attention filter mechanism} does not 
treat the attention history as a whole data structure,  
but divides it into sections (\figref{fig:model_main}). 
%\KZ{I actually think it might be better to draw a diagram
%to show this in approach.} 
Previously, the distribution curve of accumulated attention scores 
for each token in the source document tends to be flat, 
which means critical information is washed out during decoding.
Our method emphasizes previously attended sections 
so that important information is retained.

Given our observation that repetitive sentences in the source are
another cause for repetition in summary, 
which cannot be directly resolved by manipulating attention values, 
we introduce \textit{sentence-level backtracking decoder}. 
Unlike Paulus \shortcite{PaulusXS17}, 
we do not ban repetitive \textit{trigrams} in test time. 
%Instead, 
Our decoder regenerates a sentence that is similar to previously generated ones.
With the two modules, our model is capable of generating summaries with a
natural level of repetition while retaining fluency and consistency.
