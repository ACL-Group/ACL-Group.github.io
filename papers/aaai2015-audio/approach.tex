\section{Approach}
\label{sec:approach}

\begin{figure*}[th]
\centering
\epsfig{file=figures/all.eps, width=1.5\columnwidth}
\caption{The Auditory Scene Recognition Framework}
\label{fig:sys}
\end{figure*}

Our ASR framework can be roughly divided into two parts: the {\em text 
modeling} and the {\em audio modeling}. In the text modeling part, we seek to
derive probability distribution of predefined auditory scenes given 
a primitive audible event concepts. For example, the probability
distribution conditioned on event ``car'' may be 
\begin{eqnarray*}
Pr(street | car) &=& 0.6\\ 
Pr(station | car) &=& 0.2\\
Pr(park | car) &=& 0.18 \\
Pr(cafe | car) &=& 0.02
\end{eqnarray*}
We obtain such probability distribution by first 
collecting a vacabulary of audible event
concepts such as ``car honk'' and ``engine'', and then by mining the 
relationships between these concepts and the scene terms from large 
volume of text corpus, in particular, movie and TV drama transcripts. 
In the audio modeling part, we first download audio samples of all audible
events from our vacabulary and then train Gaussian Mixture Models (GMMs)
for each event using the corresponding samples. 
During the end-to-end scene recognition phase, the input audio clip 
is segmented into pieces, and passed to an inference engine,
which infers the probability distribution on events for each segment.
Finally, based on the event-scene relations obtained in the text modeling part,
the engine determines the most likely scenes.
\fig{sys} shows an overview of our system. Next, we describe the different
components of our framework. 

%Firstly, we build a probability model of events and scenes from drama scripts. Next, using some knowledge base and sound search engine, we can build a word set of audible event. Then we focus on this audible set and download some related audio clips as training data. At the same time, we can label these audio clips automatically using the key words in searching. After that, for
%each event, we train GMM classifiers using ZCR and MFCC as features, in order to detect audible event from audio clips. Finally, we can get the result by combining the detection result and the probability model of events and scenes.
%
%Text part
\input{approach/event_vocab}
\input{approach/event_context}
%Audio part
\input{approach/audio_model}


