\subsection{Build Event-Scene Probability Model}
\label{sec:mapping}
This section decribes how to compute the conditional probability 
$P(scene | event)$, for the given $n$ scenes in question and all 
terms in the event vocabulary. We first extract fragments of text
corresponding to the input scenes 
from large number of TV drama or movie transcripts, and then extract
audible events from the text. Finally we compute the probability model
between events and scenes. 

\subsubsection{Extraction of Scene Contexts}
In this paper, we choose to use movie and TV drama transcripts 
\footnote{Movie scripts were downloaded from \url{http://www.imsdb.com},
while TV series transcripts were downloaded from 
\url{http://simplyscripts.com/tv.html}.} as 
our primary source to obtain the event-scene relations,
because i) they contain large number of interesting scenes; and ii)
most of them mark clearly the boundaries of a scene, such as those
in \fig{ET}.
%We can make use of such patterns to extract the text context of a scene such
%as ``street.'' as well as its noun synonyms,  e.g., ``avenue'' and 
%``boulevard.'', from WordNet \cite{miller1995wordnet}.
%In addition, because event terms that occur in human conversations 
%are not necessarily events that happen
%in that scene, we remove all conversations which also have clear patterns
%from the context.
%\tabl{scripts} shows a subset of
%dramas and movies which were considered as data sources

%\begin{table}[th]
%\caption{Selected Movies/TV Transcripts}
%\label{tab:scripts}
%\centering
%\small
%\begin{tabular}{llr}
%\toprule
%Title & Type & Length\\
%\bottomrule
%\rowcolor[gray]{.8} The Big Bang Theory & TV & 126 episodes  \\
%					Friends & TV & 229 episodes  \\
%\rowcolor[gray]{.8} How I Met Your Mother & TV & 135 episodes  \\
%					Prison Break & TV & 23 episodes  \\
%\rowcolor[gray]{.8} Lost & TV & 118 episodes  \\
%					Sherlock & TV & 6 episodes  \\
%\rowcolor[gray]{.8} Family Guy & TV & 104 episodes  \\
%					South Park & TV & 232 episodes  \\
%\rowcolor[gray]{.8} Arrested Development & TV & 22 episodes  \\
%					Scrubs & TV & 150 episodes  \\
%\rowcolor[gray]{.8} Modern Family & TV & 84 episodes  \\
%					House M.D. & TV & 177 episodes  \\
%\rowcolor[gray]{.8} Supernatural & TV & 167 episodes  \\
%					The Vampire Diaries & TV & 82 episodes  \\
%\rowcolor[gray]{.8} Firefly & TV & 11 episodes  \\
%					True Blood & TV & 34 episodes  \\
%\rowcolor[gray]{.8} Seinfeld & TV & 179 episodes  \\
%					Wall-E & Movie & 97 minutes  \\
%\rowcolor[gray]{.8} V for Vendetta & Movie & 132 minutes  \\
%					Twilight & Movie & 121 minutes  \\
%\rowcolor[gray]{.8} Toy Story & Movie & 81 minutes  \\
%					Titanic & Movie & 194 minutes  \\
%\rowcolor[gray]{.8} Kung Fu Panda & Movie & 92 minutes  \\
%					King-Kong & Movie & 187 minutes  \\
%\rowcolor[gray]{.8} I am Sam & Movie & 132 minutes  \\
%					The Avengers & Movie & 142 minutes \\
%\rowcolor[gray]{.8} Avatar & Movie & 162 minutes  \\
%					2012 & Movie & 158 minutes  \\
%\rowcolor[gray]{.8} 500 Days Of Summer & Movie & 95 minutes \\
%					E.T. & Movie & 115 minutes  \\
%\bottomrule
%\end{tabular}
%\end{table}

\begin{figure}[th]
\centering
\fbox{\parbox{\columnwidth}{
\small
%\begin{verbatim}
%...
%Elliott and Mike walk down the driveway.\\ 
%They are on their way to school.\\
...They discuss E.T., arguing about how smart he is.

[This is just a transition scene.]\\
EXT: STREET: DAY\\
Mike and Elliot walk towards a bus stop ...
%a group of children are waiting. \\
%Mike's friends torment Elliott about his "goblin."
%...
%\end{verbatim}
}}
\caption{A Snippet from the Transcript of Movie E.T.}
\label{fig:ET}
\end{figure}


%They not only describe what happend in a context, but also present this information in a good manner. In well-written transcripts, when plot switchs, there is a particular sentence in a certain pattern indicating current context, such as "CUTS TO: Captain Raydor's Office" or "Scene: Outlets in Los Angeles". This make it easy to extract the context from a short sentence and help us collect events for contexts accurately.
%
%We use 30 hot American tv series, as shown in \tabl{scripts}.



\subsubsection{Extraction of Events from Contexts}
\label{subsec:lists}
Contexts extracted in \fig{ET} may contain event terms such as ``walk'',
``bus'' and ``children'' that find exact match in our vocabulary.
The vocabulary also contains compound terms such as ``open door'', 
``ring bell'' which may not find exact match in the context. To extract
as many events as possible from the context, besides exact matches for
terms in the vocabulary, we also parse the context using a
dependency parser, and pay special attention to the following relations:
{\em direct object}, {\em indirect object}, {\em noun compound modifier},
{\em nominal subject} and {\em passive nominal subject}.
Each of these relations relates either a noun and a verb, or between
two nouns. The reason we focus on these dependencies is that sound is generally
made by a motion or action and its agent or recipient (single verb or 
verb-noun cases) 
or some object (single noun or noun-noun cases such as ``coffee cup'') alone. 
A word pair $(w_1, w_2)$
with the above five relations from the context is considered an audible
event, if there is a compound event term $w_1w_2$ or $w_2w_1$ in the 
vocabulary. 
 
%
%\begin{enumerate}
%\item Use Stanford NLP to conduct sentence splitting, POS tagging and lemmatization and dependency analysis.
%\item Check all nouns in first sentence (indicating sentence) to see if there is any noun each context's synset . If yes, we label this paragraph with the context.
%\item Based on result provided by POS tagging, we record noun lists and verb lists with frequency, respectively. For multi-word nouns, we just keep the last word, e.g, "safe-guard" becomes "guard".
%\item Analyze the sentence dependency and extract direct object pairs. Each pair is combined alphabetically with space.
%\item Merge noun, verb and pair lists with context same label, respectively. As far, for each context, there are three event lists.
%\item Audible concept set bulit in\ref{subsec:buildingvocabulary} and the three lists intersect to pick out events we concern about.
%\end{enumerate}
%

\subsubsection{Event-scene Distribution}
Our problem is to classify an input audio clip into one of $n$ predefined
scenes. The intuition is that humans recognize a scene by its most important,
and distinctive events. We model this by $P(scene | event)$. For example, if
flushing the toilet is a very distinctive event for the scene ``toilet'', 
then we expect $Pr(toilet | flush)$ is significantly higher than 
$Pr(other\_scene | flush)$.

We compute the probability as
\begin{equation}
\label{eqn:prob}
Pr(scene=s | event=e)  = \frac{TF(s, e)}{\sum_{s \in S} TF(s, e)},
\end{equation}
where $s$ is a scene in the set of $n$ input scenes $S$, 
$e$ is an event in the vocabulary, and $TF(s, e)$ is the number of occurrences
of $e$ in the context of scene $s$.

%Now, we get events and its frequency of each context. Since we assume human beings recognize context audio by catching some notability events, we hope there are some typical events existing in every context, e.g, when there is the sound of toilet flushing, we can barely deny that it is in toilet. So we calculate and normalize TF (term-frequency) of each event, and then filter by it.
%So far, we get the event-context model according to \eqn{model}.
%\begin{equation}
%\label{eqn:model}
%p(scece_j|event_i) = tf_{i, j}
%\end{equation}

