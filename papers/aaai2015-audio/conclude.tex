\section{Conclusion}
\label{sec:conclude}

We presented a novel framework which combines text mining 
and audio signal processing for recognizing auditory scenes. This framework
is unsupervised in the sense that no manual labeling of the audio training
data is needed. Instead of training audio scene data directly, 
%like most existing work does, 
we train GMMs on primitive audible events downloaded from online 
sound search engines. The framework leverages
large text corpus of online transcripts to mine statistical
models between a scene and its constituent events. Experiments for
10-scene classification showed promising results of 42\% accuracy which is
on par with best performing algorithms
reported at 2013 IEEE AASP scene recognition challenge.
%
%Our system does not need manually labeled training data. Instead, we analyzed the drama scripts which can easily collected from Internet. At the same time, we used some knowledge base and sound search engine to build our audible events set, and downloaded and labeled audio clips automatically. When we analyzing the text data, we also built a probabilistic model between scenes and events. Then we trained GMM models for audible events. Using these models, we can infer scenes from an audio clip. In our experiments, our system can achieve $37\%$ accuracy in a 10-scene classification work.
%
%TODO(kenny): do we need to write future work here?

%\begin{itemize}
%\item Enlarge text corpus to get more information.
%\item Make the quality of training data higher, including audio clips and text corpus.
%\item Use more relations found by Stanford NLP.
%\item Besides using only events to recognize scene, take some global features in testing clips into consideration for recognition.
%\item Use some other model like universal background model (UBM) to reduce the impact of noise.
%\end{itemize}
