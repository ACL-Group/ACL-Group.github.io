\section{Related Work}
\label{sec:related}

%\KZ{Too much focus on accuracy numbers.. Should be more on approaches and
%differences wherein.}

ASR and related problem has been extensively studied in the past.
Some researchers work on sound event detection/classification/recognition,
which focuses on some specific sound events, ignoring the
contexts or scenes that produce the events. Other researchers
work on recognizing the contexts or scenes, but most of them use one model for
recognizing the context without detecting the sound events therein.
Recently, there has been some new efforts to use a
small set of pre-defined event to infer the context \cite{heittola2010audio},
which partly inspires this paper's research.

%\subsection{Auditory Scene Recognition}

Gaunard \et\shortcite{679661} proposed an HMM-based environmental noise recognition system. It uses discrete HMM as the model and linear prediction 
cepstral coefficients (LPCCs) as features. 
The system can outperforms human subjects for classifying 5 types 
of environmental noise, namely car, truck, moped, aircraft, and train.
Peltonen \et\shortcite{5745009} studied the efficiency of different acoustic 
features, models, and the effect of test sequence length. 
They proposed two system, %using different features and models. 
one using band-energy ratio as features and trained by 1-NN classifier, 
the other using MFCC as features and trained by GMM. 
%The best recognition rate is around $68.4\%$ for 26 different scenes.
Eronen \et\shortcite{1561288} investigated the feasibility of an audio-based 
context recognition system, and showed that linear data-driven transformations, 
\ie Independent Component Analysis (ICA) and Linear Discriminant Analysis (LDA) 
could improve recognition accuracy slightly. 
%Their system can achieve $58\%$ accuracy for 24 common contexts. They also did some listening tests, and found that human beings can achieve $69\%$ accuracy on the same data set.
Chu \et\shortcite{5109766} performed an empirical feature analysis and 
use the matching pursuit (MP) algorithm to obtain effective features from 
a large feature set, including MFCC, LPCC, band energy ratio, 
zero-crossing, energy, etc. 
%The recognition rate of their system is $82.3\%$ over 14 audio contexts.
Weninger \et\shortcite{5946409} focused on animal vocalizations. 
They compared left-to-right HMM, cyclic HMM, recurrent neural networks, 
and SVM, and achieved up to $64.0\%$ accuracy on a 5-class task, 
and $81.3\%$ on a 2-class task.

One of the approaches to consider audio events in context inference 
is introduced by Cai \et\shortcite{1621215}. 
They proposed a flexible framework to recognize 5 audio contexts, 
including excitement, humor, pursuit, fight and air-attack, 
using 10 predefined events. The main technologies they used are HMM, 
Grammar Network, and Bayesian network. 
%Their system can achieve $91.7\%$ accuracy for event detection, 
%and $82.4\%$ accuracy for context inference.
Recently, another event-based audio context recognition is 
proposed by Heittola \et\shortcite{heittola2010audio}. 
They use a histogram of audio events which are detected by 
GMM/HMM presented in \cite{mesaros2010acoustic}. 
%where an accuracy of $24\%$ was obtained when classifying isolated 
%sound events into 61 classes. 
After a histogram of audio events was built, context recognition can 
be performed by using cosine distance to calculate 
the similarity. Their system obtains 89\% accuracy when recognizing 
10 audio contexts. However, they used a predefined set
of events, which is equivalent to collecting all the relevant events and their
training samples in our paper, only manually!

Giannoulis \et\shortcite{6701819} described the IEEE AASP 
challenge at \url{http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge} on 
acoustic scene classification and detection of sound events within a scene.
Note that the sound event detection here means to detect a temporal region
in the audio which corresponds to a specific event class. This is different
from the definition of event detection in this paper. More relevant is the
scene classification task which seeks to classify audio clips into 
10 different scenes: bus, busystreet, office, openairmarket, 
park, quietstreet, restaurant, supermarket, tube, tubestation. 11 classification
systems competed for the task. It turns out that ``Recurrence Quantification Analysis applied to MFCC time-series, classified by SVM'' done by Roma et al, 
achieves 75\% accuracy, outperforming the rest. This system was 
compared with our system on our data set in \secref{sec:eval} and logs
44\% accuracy.  
%This paper overviewed of systems 
%\cite{elizaldevector,geigerrecognising,krijnders400tone,liauditory,namacoustic,nogueirasound,emanuele,patil2002multiresolution,rakotomamonjy,romarecurrence,zbilut2006recurrence} submitted to the challenge and summarized the results.
%The best of the results were compared with our results in Table \ref{tab:ac}. 
%Chum \et\shortcite{chumieee} proposed a GMM and HMM based system, 
%using magnitude response, loudness, spectral sparsity and 
%temporal sparsity as features. They achieved accuracy of $72\%$.
%Elizalde \et\cite{elizaldevector} proposed an {\em i-vector} system\cite{dehak2009support,5947437}, together with MFCC features, which can achieve an accuracy of $65.8\%$.
%Geiger \et\cite{geigerrecognising} introduced a SVM based system, using many low-level features, such as MFCC, band energy, etc. An accuracy of $73\%$ is achieved by their system using majority voting scheme.
%Krijnders \et\cite{krijnders400tone} proposed a SVM based system using tonalness as feature, achieved $53\%$ accuracy.
%Li \et\cite{liauditory} developed a treebagger classifier using MFCC and other spectral features. It can achieve $72\%$ accuracy.
%Nam \et\cite{namacoustic} introduced the feature learning approach to audio scene classification. They use RBM\cite{lee2007sparse}and perform selective max-pooling to form scene-level feature vector for SVM training. Their system can achieve $75\%$ accuracy.
%Nogueira \et\cite{nogueirasound} proposed a SVM based system using spectral, temporal and spatial features, achieve accuracy of $69\%$.
%Olivetti \cite{emanuele} proposed two approaches, dissimilarity representation and normalized compression distance, to embed audio into a vectorial feature space. A random forest\cite{rf} algorithm was used for classification, and the system can achieve accuracy of $80\%$.
%Patil \et\cite{patil2002multiresolution} proposed a framework that provided a analysis of the spectro-temporal modulations in acoustic signal, and built a SVM classifier, which can achieve accuracy of $73\%$.
%Rakotomamonjy \et\cite{rakotomamonjy} used a constant Q transform in feature extraction. Their system can achieve $75\%$ accuracy by applying a SVM classifier.
%Roma \et\cite{} proposed a SVM based classifier with MFCC feature and 
%RQA\cite{zbilut2006recurrence} features. It can achieve accuracy of $71\%$.
%
%\subsection{Sound Event Detection}
%Sound event detection is to detect some pre-defined sound events in a long audio sample. Usually, a large number of labeled audio samples of events are use as training data.
%
%Heittola \et\cite{Heittola2013EURASIP} proposed a context-based sound event detection system. They used the ground truth of the context of audio to help them detect the sound events in the audio. They modeled the context using GMM, and the sound events were modeled as 3-state left-to-right HMMs. It is shown that there system can benefit from the context information.
%
%Some work focus on specific sounds, such as gunshots\cite{1521669}, birds\cite{Fagerlund:2007:BSR:1288980.1289030}, etc. Some work focus on context-based sound event detection, which only consider about specific sounds, like kitchen\cite{Kraft05temporalica}, bathroom\cite{bathroom}, etc. Their work usually train GMM, HMM or SVM models as classifiers.
%
%We use sound event detection as a part of our work. Instead of directly training models from audio samples, we break the audio into several short segments. Then we cluster those segments and train a GMM for each cluster.
%
%\subsection{Audio Processing Using Knowledge}
%
%Cano and Koppenberger\cite{Cano2004:anno} proposed a solution to
%automate audio annotation. Sound samples are gathered and are tagged
%with unambiguous concepts in WordNet. A 20-nearest-neighbor classifier
%is trained to annotate more audio samples using normalized Manhattan distance.
%Based on 15 sound effects, an annotation test on 261 audio files showed
%an accuracy of 91\%.
%
%Based on this trial, the authors further built sound effect
%taxonomy\cite{Cano2004:tax} and processed audio retrievals \cite{CanoKGHRW04}
%on it. For the taxnomy\cite{Cano2004:tax}, they implemented a
%classification scheme for sound effect management on top of WordNet,
%which solves the ambiguity inherent to natural language. This system both
%regulates the labels for annotation, and leads to a robust framework for
%sound information retrieval. Futher, the researchers presented a sound effect
%retrieval system \cite{CanoKGHRW04} that incorporates content-based
%audio techniques and semantic knowledge provided by WordNet.
%
