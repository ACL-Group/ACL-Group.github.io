We appreciate the valuable comments given by the reviewers and we thank
them for the hard work. Below are our responses to some of the questions
or comments.

R1:
1) "It is not clear to me why the authors do not simply start with 
all the terms that have sounds according to the sound search engine..."
Answer: Indeed, none of the sound search engines provide such vocabulary of
audible events. Neither do they allow the crawling of all sound clips from
their websites. Therefore, we chose to use them as a filtering mechanism
to verify whether a term is likely to be an audible event.

2) "It may have been interesting to use as seed terms instead the scenes that 
are the targets for classification..."
Answer: Our objective is to obtain sound samples for primitive audio events
from the sound search engine. Using the scene names such as "train station"
or "toilet" directly will give us sound samples of these scenes directly,
which counters our original intention of training models for complex 
scenes without using audio samples of these scenes.

3) "...use general-purpose text collocations to estimate the probability 
of ..."
Answer: We use TV and movie dramas as our primary source of co-occurrences because
such texts are more likely to contain narratives or descriptions about
common scenes in real world. However, the size of such corpora might be a
limitation to our approach and hence we are considering general purpose 
text as well in future work.

R2:
1) ..What does it mean to "cluster" candidates "under different super-concepts?"
Answer: The following sentences and Fig 3 explain the process. We represent each
new candidate term by its superconcepts in Probase, which form a vector. 
For example, the superconcepts of "hunt dog" are "dog", "animal", etc. 
Then we cluster these vectors together by cosine similarity as the semantic distance. 
For each cluster of terms, which presumably represent one type of similar objects,
we identify their most popular super concept as their common "type". For example, hunting
dog, labrador and puppy may be clustered together and their common type is "dog".
Subsequently, other entities under "dog", such as "police dog" maybe included in the
candidate pool, effectively "growing" the pool.

2) "During clustering, we represent each new term as a vector if its 
superconcept in Probase ..." is totally incomprehensible -- it's not a sentence!"
Answer: "if" is a typo, and should be replaced by "of".

2) What are "scene contexts" (2.2)? How is their length determined? 
How are they extracted? what clustering algorithms are used? 
How is the parameter K in the Scene Inference phase (2.4) chosen?
Answer: Scene context refers to a segment of text in transcript that describes a scene.
Such text segment is usually clearly marked with delimiters in the transcripts we obtained
and hence can be easily identified. The context also has a name, e.g. "street" in Fig. 4,
which can be used to match names of the scene that we are classifying into. We used K-means
clustering in Sec. 2.3 and in this paper K was set to 10 empirically, which means 10
aspects of an event. Similarly K=10 in scene inference (2.4).

3) "Moreover, the language should be polished..."
Answer: We will definitely proofread the paper and polish it for the revision.

R3:
1) "During the end-to-end scene recognition phase, the input audio clip is segmented into pieces” By a human, or by an algorithm?
Answer: The segmentation is done automatically into 500ms pieces, 
similar to the segmentation done during training (Para. 1 of Sec 2.4), 
which is detailed in "Model Training" of Sec. 2.3.

2) "We will remove such noises in the filtering phase.” With an algorithm or by hand?
Answer: This is also done automatically by an algorithm, as specified in "filtering phase"
of Sec.2.1: "All clips which are shorter than 0.1 seconds or longer than 30 seconds 
are removed, because these are usually not a single event by our experience. 
Finally we filter out terms which have fewer than 10 resulting clips, 
and keep the rest in our pool."
