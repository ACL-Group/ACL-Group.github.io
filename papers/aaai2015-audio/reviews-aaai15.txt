Reviews
Review 1
Significance of the Contribution:	8: (+++)
Soundness and Positioning with Respect to Related Work:	7: (++)
Depth of Theoretical and/or Experimental Analysis (as appropriate):	8: (+++)
Quality of Presentation:	7: (++)
SUMMARY RATING:	3: (+++)
Comments for the Authors:	This paper on the topic of auditory scene recognition. The interesting aspect of this paper is that it proposes to build a vocabulary of audible event terms, which are then used in the task of auditory scene recognition. Evaluation is conducted as follows: given a sound segment, classify the audio sample into one of 10 scenes: bar, beach, cafeteria, church, concert, office, park, street, toilet and train. They are provided with only 10 training clips for each, and 10 testing clips. 

In this paper, the authors propose to segment each audio sample into sub-segments, and they obtain the probability of events for each sub-segment. If, e.g., the event "sea" is detected, then that increases the probability of the scene "beach". The final classification of the audio sample is the most probable of the scenes (multi-class classification).

The method for mapping events to scenes could be explained a little more clearly in the paper. As I understand it, they start from the concept terms must frequently associated with "sound", "noise", musical instruments etc. as found in ProBase. So that "music" is associated with "sound", as is "gunshot", e.g. Then, they look up the concept term using a sound search engine. It is not clear to me why the authors do not simply start with all the terms that have sounds according to the sound search engine (or perhaps there was no api for that). 

It may have been interesting to use as seed terms instead the scenes that are the targets for classification and follow those concepts, filtering by the sound search engine.

An alternative for the Event-Scene probability model, which uses movie and TV drama transcripts as the primary source would be to take the target scenes (beach, train etc) and use general-purpose text collocations to estimate the probability of toilet|flush, e.g. General purpose text would provide a much richer knowledge source than movie and TV transcripts.

However, as this is the first work to combine text mining with audio event detection, these are suggestions for further research in an interesting area and direction.
Review 3
Significance of the Contribution:	6: (+ (slightly positive))
Soundness and Positioning with Respect to Related Work:	6: (+ (slightly positive))
Depth of Theoretical and/or Experimental Analysis (as appropriate):	5: (- (slightly negative))
Quality of Presentation:	5: (- (slightly negative))
SUMMARY RATING:	-1: (- (slightly negative))
Comments for the Authors:	It was not clear is some places what was done by algorithms, and by humans. As such, I could not reproduce these results. 

The idea of using a “comprehensive concept knowledge base” is nice, but it is hard to know how much of the results are due to human choices and human feature-engineering. There are a LOT of choices and magic numbers in the paper.



“During the end-to-end scene recognition phase, the input audio clip is segmented into pieces” By a human, or by an algorithm?

“We will remove such noises in the filtering phase.” With an algorithm or by hand?





framework doesn’t require any such
framework does not require any such

Different from traditional approaches
In contrast to traditional approaches

a vacabulary of audible event concepts such a
a vocabulary of audible event concepts such a
Review 3
Significance of the Contribution:	7: (++)
Soundness and Positioning with Respect to Related Work:	6: (+ (slightly positive))
Depth of Theoretical and/or Experimental Analysis (as appropriate):	5: (- (slightly negative))
Quality of Presentation:	5: (- (slightly negative))
SUMMARY RATING:	-1: (- (slightly negative))
Comments for the Authors:	I have slightly increase my ranking based on authors feedback.
However, most of my points still stand.
-------------------






It was not clear is some places what was done by algorithms, and by humans. As such, I could not reproduce these results. 

The idea of using a “comprehensive concept knowledge base” is nice, but it is hard to know how much of the results are due to human choices and human feature-engineering. There are a LOT of choices and magic numbers in the paper.



“During the end-to-end scene recognition phase, the input audio clip is segmented into pieces” By a human, or by an algorithm?

“We will remove such noises in the filtering phase.” With an algorithm or by hand?





framework doesn’t require any such
framework does not require any such

Different from traditional approaches
In contrast to traditional approaches

a vacabulary of audible event concepts such a
a vocabulary of audible event concepts such a
Review 2
Significance of the Contribution:	5: (- (slightly negative))
Soundness and Positioning with Respect to Related Work:	4: (--)
Depth of Theoretical and/or Experimental Analysis (as appropriate):	3: (---)
Quality of Presentation:	2: (----)
SUMMARY RATING:	-4: (----)
Comments for the Authors:	The paper describes what the authors claim is a new approach to auditory scene recognition, which does not classify an audio clip directly into scene classes, but rather tries to detect more elementary sound events in the clip, based on which it then predicts the scene class. Recognizers for the elementary events are learned from sound examples from the web, and the association between scenes and elementary event names is automatically extracted from a text corpus of movies and TV series scripts.
While I cannot determine with certainty whether this is a really novel approach (I am not an expert in this particular field of scene recognition), I can judge the quality of the presentation, and it is because of the quality of presentation that, in my opinion, the paper cannot be published at the conference in its current form. The paper is very hard to read. While one does get the general idea quite well (not from the abstract, though), many parts of the algorithm are explained in a confusing way. For instance, I was unable to understand the details of the "growing phase" in Section 2.1: "We further expand the set of new candidates by clustering them under different super-concepts". -- what does it mean to "cluster" candidates "under different super-concepts"? The next sentence "During clustering, we represent each new term as a vector if its superconcept in Probase and compute distance between any to by cosine similarity" is totally incomprehensible -- it's not a sentence ("if its super-concept .. WHAT?). The presentation of the "Model training" phase in 2.3 is even more confusing. It is not clear what clustering algorithm is used, it is not clear how the number of clusters is determined or emerges (at least not to me), etc.
Generally, there is a lack of precise information about the exact parameters and methods used, in several places. What are "scene contexts" (2.2)? How ist their length determined? How are they extracted? what clustering algorithms are used? How is the parameter K in the Scene Inference phase (2.4) chosen?
Moreover, the language should be polished by a native English speaker (e.g., by introducing definite and indefinite articles in the right places).
For these reasons, I believe the paper is not ready for publication -- independently of whether the experimental results are promising or not. (Actually, I do find the results shown in Table 2 rather poor, and would have expected a deeper discussion of this problem -- it's not just "bark" and "station" that I see as problematic here.)
