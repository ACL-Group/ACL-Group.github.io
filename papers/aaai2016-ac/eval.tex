\section{Experimental Results}
\label{sec:eval}

In this section, we first show how we prepare the data for argument
conceptualization. Then, we use some example concepts
generated by our algorithm to show the advantage of our algorithm (AC)
against selectional preference (SP), FrameNet~\cite{baker1998berkeley} and ReVerb~\cite{fader2011identifying},
as well as our baseline approach (BL) which considers equal
weight for each argument (see \secref{sec:problem}).
We also quantitatively evaluate the accuracies of AC, BL and SP
on Probase. Finally, we apply our algorithm to
an NLP task known as argument
identification~\cite{gildea2002necessity,abend2009unsupervised,meza2009jointly}
and show that concepts generated by AC
achieve better accuracy against BL, SP,
Reverb and a state-of-the-art semantic role labeling tool (using FrameNet) on both taxonomies.

\subsection{Experimental Setup}
\label{sec:preprocess}

We use our algorithm to conceptualize subjects and objects
for 1770 common verbs from Google syntactic
N-gram~\cite{goldberg2013dataset,googlengram} using
Probase and WordNet as isA taxonomies.
\footnote{All evaluation data sets and results
are available at \url{http://202.120.38.146/ac}.}
From 1770 verb set, we
sample 100 verbs with probability proportional to
the frequency of the verb. This set of 100 verbs (Verb-100) is
used for quantitative experiments including evaluating the
accuracy of argument concepts and the accuracy of
argument identification.

All argument instances we use in this work come
from {\em Verbargs} and {\em Triarcs} packages
of the N-gram data.
From the labeled dependency trees, we extract
subject-verb dependency pairs (nsubj, agent) and
object-verb dependency pairs (dobj, nsubjpass).
We expand the subject or object, which is a word, into
a phrase recognizable by Probase/WordNet by sliding a window
across the subtree rooted at the argument word.

For the system parameters, we set the maximum overlap
threshold between two concepts to 0.2, and the number of concepts
$k$ to $\{5, 10, 15\}$ to evaluate argument concepts of
different granularity.
In practice, the number $k$ can be set differently for different verbs,
which we view as an advantage of the framework.


\subsection{Conceptualization Results}
We compare the concepts learned by AC with the concepts learned by
BL, FrameNet elements, Reverb arguments, and concepts learned by SP.
ReVerb is an open information extraction system
that discovers binary relations\footnote{ReVerb extracts general relations
instead of verb predicates, e.g., XXX heavily depends on YYY.} from the web
without using any predefined lexicon. ReVerb data contains
15 million subject-predicate-object triple instances without
any abstraction or generalization.

Table \ref{tab:results} shows 3 example verbs and their argument concepts (AC \& BL),
FrameNet semantic roles (FN), ReVerb argument instances (RV) as well as
selectional preference (SP) concepts for the verbs' subjects and objects.
The number of concepts $k$ is set to 5 for AC \& BL, and the top 5 instances/concepts are showed for
RV and SP.
We can observe that the semantic roles in FN are too general, while RV instances
are too specific. Both inevitably lose information:
FN is a manually constructed lexicon by experts thus
cannot scale up well, while ReVerb is automatically extracted from massive
English sentences and hence comes with abundant errors (e.g., {\em ive}
as a subject of ``enjoy'').
SP does not consider semantic overlaps between argument concepts.
BL assumes that all argument instances of a verb are of
equal importance, which is not true in practice. It tends to generate uninformative concepts such as ``factor'' and ``feature''.
Compared to the other methods, AC generates concepts with
tunable granularity and low semantic overlap. These concepts are more comprehensive
and more accurate.

To quantitatively compare our algorithm to BL and SP,
we ask three native English speakers to
annotate whether the concepts generated by AC, BL and SP are
the correct abstraction of the verb's arguments.
The majority votes are used as the ground truth. We compute the percentage of
correct concepts as accuracy, and report the accuracy of
AC, BL and SP in \tabref{tab:precision}.
AC generates more accurate concepts than BL and SP mainly because AC
considers the quality of argument instances extracted from dependency
and the semantic overlap between concepts.
BL performs worse than SP because the noise caused by parsing
error is not considered, but SP considers the association between the verb and
arguments which implicitly gives a low rank to the incorrect arguments.

%\vspace{-1.5em}
\begin{table}[th]
\centering
%\small
\scriptsize
\caption{Accuracy of AC, BL and SP concepts}
\begin{tabular}{cIc|c|c|c|c|c}
\whline
\multirow{2}{*}{k} & \multicolumn{3}{c|}{Subject} & \multicolumn{3}{c}{Object}\\
\cline{2-7}
& AC & BL & SP &  AC & BL & SP \\
\whline
5 &\bf 0.88 & 0.49 & 0.58 &\bf 0.97 & 0.63 & 0.62 \\
\hline
10 &\bf 0.86 & 0.47 & 0.56 &\bf 0.94 & 0.61 & 0.65 \\
\hline
15 &\bf 0.85 & 0.43 & 0.58 &\bf 0.91 & 0.60 & 0.66 \\
\whline
\end{tabular}
\label{tab:precision}
\vspace{-1.5em}
\end{table}

\begin{table*}[th]
  \centering
  %\scriptsize
  \small
  \caption{Example subject/object concepts from 5 lexicons}
    \begin{tabular}{cIrIl|l|l|l|l}
    \whline
    Verb  &       & \multicolumn{1}{c}{AC Concepts} & \multicolumn{1}{|c}{BL Concepts} & \multicolumn{1}{|c}{FrameNet} & \multicolumn{1}{|c}{ReVerb} & \multicolumn{1}{|c}{SP Concepts} \\
    \whline
    \multirow{2}[6]{*}{accept} & Subj  & \tabincell{l}{person,community, \\ institution,player,\\ company} & \tabincell{l}{topic,name, \\ group,feature, \\ product} & \tabincell{l}{Recipient, \\ Speaker, \\ Interlocutor}  &  \tabincell{l}{Student,an article, \\ the paper,Web browser, \\ Applications} & \tabincell{l}{world,group, \\ person,term, \\ safe payment method}   \\
          \cline{2-7}
          & Obj   & \tabincell{l}{document,payment, \\ practice,doctrine, \\ theory} & \tabincell{l}{factor,feature, \\ product,activity, \\ person} & \tabincell{l}{Theme, \\ Proposal}  & \tabincell{l}{the program,publication, \\ HTTP cookie,the year, \\ credit card}  & \tabincell{l}{topic,concept, \\ matter,issue, \\ word}   \\
    \hline

    \multirow{2}[6]{*}{enjoy} & Subj  & \tabincell{l}{group,community, \\ name,country, \\ sector} & \tabincell{l}{name,topic, \\ group,feature, \\ product} & Experiencer  &  \tabincell{l}{people,ive,Guests, \\ everyone,someone} & \tabincell{l}{world,stakeholder, \\ group,person, \\ actor}   \\
          \cline{2-7}
          & Obj   & \tabincell{l}{benefit,time,hobby, \\ social event, \\ attraction} & \tabincell{l}{factor,activity, \\ feature,product, \\ person} & Stimulus  &  \tabincell{l}{life,Blog,Breakfirst, \\ their weekend,a drink}  & \tabincell{l}{benefit,issue, \\ advantage,factor, \\ quality}   \\
    \hline

    \multirow{2}[6]{*}{submit} & Subj  & \tabincell{l}{group,community, \\ name,term, \\ source} & \tabincell{l}{topic,name, \\ group,feature, \\ product} & Authority  &  \tabincell{l}{no reviews,Project, \\ other destinations, \\ HTML,COMMENTS} & \tabincell{l}{large number,number, \\ stakeholder,position, \\ group}   \\
          \cline{2-7}
          & Obj   & \tabincell{l}{document,format, \\ task, procedure, \\ law} & \tabincell{l}{factor,feature, \\ activity,product, \\ person} & Documents  & \tabincell{l}{one,review, \\ a profile,text, \\ your visit dates}  & \tabincell{l}{document,esi online tool, \\ material,nickname, \\ first name}   \\
    \whline
    \end{tabular}
  \label{tab:results}
%\vspace{-1.5em}
\end{table*}

\subsection{Argument Identification}

In the argument identification task, we use the inferred argument concepts
to examine whether a term is a correct argument to a verb in a sentence.
To evaluate the accuracy of argument identification, for each verb in Verb-100,
we first extract and randomly select 100 sentences containing the verb from the Engish Wikipedia
corpus. We then extract \pair{verb}{obj} and \pair{verb}{subj} pairs from these 10,000 sentences.
Apart from parsing errors, most of these pairs are correct
because Wikipedia articles are of relatively high quality.
We roughly swap the subjects/objects from half of these pairs with
the subject/object of a different verb, effectively creating
incorrect pairs as negative examples.
For example, if we exchange ``clothing'' in ``wear clothing'' with the ``piano''
in ``play piano'', we get two negative examples ``wear piano''
and ``play clothing''.
Finally, we manually label each of the 20,000 pairs to be correct or not,
in the context of the original sentences.  As a result, we have a test
set of 10,000 \pair{verb}{obj} and \pair{verb}{subj} pairs in which
roughly 50\% are positive and the rest are negative.

We compare AC with BL, SP, ReVerb and Semantic Role Labeling (SRL) as follows:
\begin{itemize}
\item {\bf AC \& BL \& SP}: Check if the test term belongs to any of the $k$ argument concepts (isA relation) of the target verb.
\item {\bf ReVerb}: Check if the test term is contained by the
verb's list of subjects or objects in ReVerb.
\item {\bf SRL}: SRL aims at identifying the semantic arguments
of a predicate in a sentence, and classifying them into different
semantic roles.
We use ``Semafor''\cite{chen2010semafor}, a well-known SRL tool,
to label semantic arguments with FrameNet in the sentences,
and check if the test term is recognized as a semantic argument of
the target verb.
\end{itemize}

%\vspace{-1.5em}
\begin{table}[th]
  \centering
  \scriptsize
  %\small
  \caption{Accuracy of argument identification}
    \begin{tabular}{cIcIc|c|c|c|c|c|l@{}}
        \whline
        \multirow{2}{*}{} & \multirow{2}{*}{k} & \multicolumn{3}{c|}{Probase} & \multicolumn{3}{c|}{WordNet} & \multirow{2}{*}{\diagbox[dir=SE,height=2em,trim=rl]{RV}{SRL}} \\ %\multirow{2}{*}{RV} & \multirow{2}{*}{SRL} \\
        \cline{3-8}
             & & AC & BL & SP & AC & BL & SP &   \\
        \whline
            \multirow{3}{*}{Subj}
            & 5 & \bf 0.81 & 0.50 & 0.70 & 0.55 & 0.54 & 0.54 & \multirow{3}{*}{\diagbox[dir=SE,height=3em,trim=rl]{0.54}{0.48}} \\ %\multirow{3}{*}{0.54} & \multirow{3}{*}{0.48}\\
        \cline{2-8}
            & 10 & \bf 0.78 & 0.50 & 0.72 & 0.57 & 0.54 & 0.55 &  \\
        \cline{2-8}
            & 15 & \bf 0.77 & 0.49 & 0.72 & 0.58 & 0.54 & 0.56 &  \\
        \whline
            \multirow{3}{*}{Obj}
            & 5 & \bf 0.62 & 0.51 & 0.58 & 0.50 & 0.46 & 0.50 & \multirow{3}{*}{\diagbox[dir=SE,height=3em,trim=rl]{0.47}{0.50}} \\ %\multirow{3}{*}{0.47} & \multirow{3}{*}{0.50}\\
        \cline{2-8}
            & 10 & \bf 0.62 & 0.52 & 0.58 & 0.52 & 0.47 & 0.52 &  \\
        \cline{2-8}
            & 15 & \bf 0.62 & 0.52 & 0.59 & 0.53 & 0.47 & 0.52 &  \\
        \whline
    \end{tabular}
\vspace{-1em}
  \label{tab:argumentidentify}
\end{table}

We set $k = 5, 10, 15$ for AC, BL and SP.
The accuracies are shown in \tabref{tab:argumentidentify}.
From Table \ref{tab:argumentidentify}, we observe that
the accuracy of AC is higher than that of BL, SP, ReVerb and SRL.
Due to its limited scale, ReVerb cannot recognize many argument
instances in the test data, and thus often labels true arguments
as negative. SRL, on the opposite side, tends to label everything
as positive because the SRL classifier is trained based
on features extracted from the context, which remains the same
even though we exchange the arguments. Thus, SRL still labels the
argument as positive.
Comparing with BL and SP, AC considers the coverage of
verb arguments, the parsing errors and overlap of concepts to
give an optimal solution with different values of $k$.
Consequently, our algorithm generates a set of concepts
which cover more precise and diversed verb argument instances.
The accuracy decreases when we use WordNet as the taxonomy
because WordNet covers 84.82\% arguments in the test data
while Probase covers 91.69\%. Since arguments that are not
covered by the taxonomy will be labeled as incorrect
by both methods, the advantage of our algorithm is reduced.

