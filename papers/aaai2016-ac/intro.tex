\section{Introduction}
Verb plays the central role in both syntax and semantics of natural language sentences.
The distributional hypothesis~\cite{harris1954distributional,miller1991contextual}
shows that it is possible to represent the meaning of a word by the distributional
properties of its context, e.g., its surrounding words in a window.
A verb has a unique role in a sentence because it maintains dependency relation
with its syntactic arguments such as the subject and the object.
Therefore, it is possible to use the
distribution of immediate arguments of a verb
to represent its meaning, such as ReVerb~\cite{fader2011identifying}.
Such an approach is a form of
``bag-of-words'' (BoW) approach. The common criticisms of the BoW approach
are i) perceived orthorgonality of all words despite some of them sharing
similar or related meanings; ii) its high dimensionality and high
cost of computation; and
iii) poor readibility to humans.

To ameliorate these limitations, a natural solution is to represent the
arguments by their abstract types, rather than the words themselves.
It is reasonable to assume that a verb represents different meanings,
or different senses, if it's used with different types of arguments.
To that end, FrameNet~\cite{baker1998berkeley} and VerbNet~\cite{kipper2000class}
are examples of human-annotated lexicons that include verbs and their
meanings (called {\em frames}) and the different types of
their arguments (called {\em thematic roles} or {\em semantic roles}).
Due to the excessive cost of constructing such
lexicons, as well as their intentional shallow semantic nature,
the abstraction of verb arguments is very coarse-grained. For example,
in FrameNet, the verb ``eat'' has just one frame, namely ``Ingestion'',
and its direct object has just one role, ``Ingestibles''.
Furthermore, the lexical coverage of these
resources are very limited. FrameNet, which is the most popular and best
maintained among the three, consists of just 3000 verbs and 1200
frames.

The BoW approach is too fine-grained while the semantic role approach
is too coarse-grained.
In this paper, we seek to strike a balance between these two
extremes. Our goal is to automatically infer a tunable set of
human-readable and machine-computable abstract concepts for
the immediate arguments~\footnote{We only consider subjects and
direct objects in this paper, though other arguments may be inferred
as well.} of each verb from a large text corpus.
By ``tunable'', we mean that the granularity of
the concepts can be parameterized by the size of the set to be returned.
The larger the set, the finer-grained the semantics.
The vocabulary of the concepts
comes from an existing taxonomy of concepts or terms such as
Probase~\cite{wu2012probase}  or WordNet~\cite{miller1998wordnet}.
For instance, the direct object of verb ``eat'' may be conceptualized
into ``food'', ``plant'' and ``animal''.

One potential solution toward this goal is selectional preference
(SP), originally proposed by Resnik\shortcite{resnik1996selectional}.
Class-based SP computes whether a class of terms is a preferred argument
to a verb. Together with a taxonomy of concepts,
SP can produce a ranked list of classes that are the most appropriate
subjects or objects of a verb. However, for the purpose of representing verbs,
SP has the following drawback: it doesn't allow the granularity of the concepts
to be tuned because it computes a selectional preference score between the
verb and {\em every} possible concept in the taxonomy. The top $k$ concepts
do not necessarily cover all the aspects of that verb because these
concepts may semantically overlap each other.
Clustering-based SP and LDA-based SP~\cite{ritter2010latent}
find tunable classes with low overlaps, but the classes are either
word clusters or probabilistic distributions of words,
which are not abstracted into concepts. Without
associating the classes to concepts in taxonomies,
the model loses the ability of generalization.
For example, if ``eat McDonalds'' does not appear in the training
data, clustering- and LDA-based SP cannot recognize ``McDonalds''
as a valid argument to ``eat'', since ``McDonalds'' is not
a member of any inferred clusters or word distributions.

In this paper, we first introduce the notion of taxonomy (\secref{sec:tax})
and define the argument conceptualization problem,
which asks for $k$ concepts drawn from a taxonomy
that generalize as many possible arguments
of a verb as possible, and with bounded overlap with each other
(\secref{sec:problem}). We present the system
to generate tunable argument concepts through a branch-and-bound
algorithm (\secref{sec:algo}) and show in experiments that our system can
generate high quality human-readable and machine-computable
argument concepts (\secref{sec:eval}).
