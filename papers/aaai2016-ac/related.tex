\section{Related Work}
\label{sec:related}
We first review previous work on selectional preference,
which can be seen as an alternate way of
producing abstract concepts for verb arguments, then discuss
some known work on semantic representation of verbs.

\subsection{Selectional Preference}
The most related work to our problem (AC) is
selectional preferences (SP), which
aims at computing preferences over the classes of arguments by a verb,
given the fact that some arguments are more
suitable for certain verbs than others. For example,
``drink water'' is more plausible than ``drink desk''.
While our problem defines a controllable level of abstraction
for verb arguments, selectional preference
often outputs the preference scores for all possible classes of
arguments.  Moreover, SP doesn't consider the overlap between classes,
which results in highly similar classes/concepts.

There are several approaches to computing SP.
The original {\em class-based} approaches
generalize arguments extracted from corpus to human readable
concepts using a taxonomy such as WordNet.
The most representative of such approach was proposed
by Resnik~\shortcite{resnik1996selectional},
which is used as a comparison in this paper.
Instead of WordNet,
Pantel et al.\shortcite{pantel2003clustering}
proposed a clustering method (named CBC) to
automatically generate semantic classes, which are nonetheless
not human readable.
There is another work about SP~\cite{fei2015illinois}. However,
it doesn't abstract the arguments which is different from us.
Other approaches including {\em cut-based} SP~\cite{li1998generalizing},
{\em similarity-based} SP~\cite{clark2001class,erk2007simple},
and {\em generative model-based} SP~\cite{ritter2010latent}
are less relevant to our problem,
because they cannot generate human readable classes.

\subsection{Semantic Representation of Verbs}
From past literature, the semantics of a word (including verbs) can be
represented by the {\em context} it appears in~\cite{mikolov2013efficient,mikolov2013distributed,mikolov2013linguistic,levy2014dependencybased}.
There are a number of ways to define the context. The simpliest is by the
words from a surrounding window.
A slightly different type of context takes advantage of structural information
in the corpus, e.g., Wikipedia. The third type of context comes from
a knowledge base or lexicon,
such as WordNet. For a verb, the gloss, its hypernyms, hyponyms, antonyms and
synonyms can be used as its context~\cite{meyer2012exhibit,yang2006verb}.
Finally, most recently, the dependency structure surrounding the verb in a
sentence has been used as its context~\cite{levy2014dependencybased}.
This is also the approach adopted in this paper.

With different types of context, a common way to represent a verb is by
a vector of distributional properties, extracted from the contexts within
a large corpus. For example, LSA \cite{deerwester1990indexing}
uses the window of words as context,
while ESA \cite{gabrilovich2007computing} uses the TF-IDF score of the word w.r.t.
the article it appears in to form a vector of Wikipedia concepts.
Another popular approach is to map the word distribution in the context
into another high-dimensional space, which is known as word embedding~\cite{mikolov2013distributed}.
Our approach can be thought of as mapping the words in the context, in this
case, the subject and object arguments into a hierarchical concept space.
