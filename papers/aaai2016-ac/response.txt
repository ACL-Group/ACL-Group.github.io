We thank the reviewers for the valuable comments and suggestions. We will proofread the paper and fix all minor errors. Below are our responses to some of the questions:

R1:
1."how the algorithm deals ...":
Our framework infers argument concepts drawn from *one* taxonomy. The integration of different taxonomies is out of the scope of this work.

2."some practical application":
Examples of such applications include verb similarity, WSD and co-reference resolution, which will be our future work.

R2:
1. "...in a real problem":
See R1.2.

2. Specific comments:
1)"Therefore, it is possible...":
We mean that we can represent a verb by the distribution of its immediate argument words from a large corpus; ReVerb is one such corpus providing this statistical information.
2)"Such an approach is...":
The approach refers to representing verbs using word distributions, and therefore a "bag-of-word" approach.
3)"...its high dimensionality...":
We are referring to the dimension of the word distribution, which consists of all distinct words that serves as an argument of a verb, which may be very large.
4)"goo 2013" refers to the dataset of google ngram:
http://commondatastorage.googleapis.com/books/syntacticngrams/index.html.
5)"[1] is a nice example...":
We will cite this paper. However, it doesn't abstract the arguments which is different from us.

R3:
1."the effect of changing parameter":
Due to space constraints, we didn't show argument concepts for different k in the paper, but in the website instead. 

2."how accuracy of the results was decided":
The annotators are instructed to label a concept as correct if it can act as an argument of the verb.

3."the way the problem been parameterized":
Our parameterization is inspired by k-means clustering. k is determined by the application scenario and not by the verb. Since we want to represent verb by a set of concepts, the size of this set is the most natural and direct way to tune its semantic granularity. Setting a coverage threshold to tune this granularity is indirect and more difficult to do.

4."...compare accuracy of different algorithms ... of k":
Given input k, we want to compare the correctness of concepts generated by different algorithms, which evaluates the quality of the representations.

5."...poor performance of BL and SP":
AC aims to cover most *correct* arguments (Eq. 5). BL has good coverage while SP focuses on the correlation strength, while neither handles *incorrect* arguments. Hence in Table 1, they perform worse. Algorithms that have better coverage will perform better in argument identification but SP concentrates on a cluster of good concepts which can *overlap* with each other while BL contains *noises*, hence outperformed by AC.

6."I am confused by ... is superior":
In Table 3, an argument instance is considered correct if it's an entity of *any* of the inferred concepts, while Table 1 considers *all* concepts. The subjects of verbs are generally limited to few concepts (e.g., person), which cover most instances in corpus and in the test sentences. Therefore, it's relatively easier to identify subjects than objects given k concepts.