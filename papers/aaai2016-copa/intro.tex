\section{Introduction}
\label{sec:intro}

%\ZY{Lots work of open information extraction from large text corpus
%discover general knowledge for further natural language processing applications.
%The discovered knowledge is ususally represented as triples denoting certain relationship
%between two entities or concepts.
%This paper focus on reasoning about causality by extracting and modeling causal knowledge
%which means specify the relationship of triples as causal relation.}

Causal reasoning, a central challenge in artificial intelligence,
has been actively studied by both linguists and computer scientists.
It aims at understanding the generalized causal dependency
between events and actions. %\ZY{not commonsense, not daily life?}
Reasoning causality between short texts measures the likelihood of
causality between the events in one piece of text to another.
To further illustrate the problem, we present a question from
Choice of Plausible Alternatives (COPA)
evaluation~\cite{roemmele2011choice}
which consists of one thousand multiple-choice questions requiring
commonsense causal reasoning to answer correctly.
Specifically, each question is composed of a premise and two
alternatives, where the task is to select the more plausible
alternative as a cause (or effect) of the premise.

\begin{example}
\label{ex:copa}
\noindent
\begin{itemize}
\item[] Premise: \emph{I knocked on my neighbor's door.} What
happened as an effect?
\item[] Alternative 1: \emph{My neighbor invited me in.}
\item[] Alternative 2: \emph{My neighbor left her house.}
\end{itemize}
\end{example}

This example shows that a key challenge is
harvesting causality knowledge that the action of
{\em knocking} causes that of {\em invitation}.

% move to approach, the utility of the constructed causalnet
%Causality knowledge is the most natural and fundamental but complex knowlege
%which benefits many NLP tasks such as text reasoning, understanding and so forth.
%The strength of causality encoded between short texts is not so obviously comparable
%due to the complexity and uncertainty of causal relation.
%For example, the following are two groups of short texts that encode causality:
%\begin{itemize}
%\item[(1)] T1: It was a surprise party. T2: The guests of the party hid behind the couch.
%\item[(2)] T1: The woman knew her friend was going through a hard time.
%T2: She tolerated her friend's difficult behavior.
%\end{itemize}
%In both groups, T2 is a reasonable effect of T1 which means .
%It is even difficult for human to make a clear decision whether or not the causality encoded in group (1)
%is stronger than that of group (2).
%So causality reasoning mainly focus on comparing causality strength when fixing T1 or T2 then select
%the other one from several choices.
%Questions in Choice of Plausible Alternatives (COPA) evaluation~\cite{roemmele2011choice}
%describes the most common scenarios of causal reasoning.

%\ZY{
%%move to approach, why we choose explicit causality as our data set
%Text corpus can explicitly as well as implicitly encode causality.
%Explicit causality usually directly indicate the causal relation in text
%with causal patterns or indicators (e.g., cause, because).
%Implicit causality naturally encoded in discourse without indicators, which is more complex
%and difficult to recognize. For example, \textit{}.
%Text always describe the most informative things according to its context,
%which is a very natural and useful convention followed by most writers.
%For example, people will write \textit{} rather than \textit{}, since . }

%%Previous efforts for causal reasoning mainly focus on }
%\ZY{which developed for validating the ability of discovering the key causal
%knowledge by data-driven approaches},
%\ZY{The authoring methodology of COPA ensured the
%question set with sufficient breadth of the topics. And the questions in COPA
%data set validated by two raters have a quite high inter-rater agreement. In addition, the incorrect alternative was set closer in content to the premise which makes this task more difficult for purely associative
%methods.}
%\ZY{The COPA evaluation judged the degree of plausibility for
%commonsense causal implication. }
%\ZY{ Note that, the COPA causal implications are judged not strictly positive
%or negative but in degrees of plausibility. Thus, it is more than causality
%extraction which also need to properly model causal strength amongst text
%spans.
%}
%
%%\begin{itemize}
%%\item[] Premise: \emph{The runner wore shorts.}. What is the
%%cause?
%%\item[] Alternative 1: \emph{The forecast predicted a hot day.}
%%\item[] Alternative 2: \emph{She planned to run along the beach.}
%%\end{itemize}

Existing work on harvesting causality knowledge has been conducted in
two directions.

%Second category, pursuing the opposite emphasis of depth in
%understanding sentences, seeks to overcome the limitation of
%the first approach.
First direction, pursuing high {\em precision} of
causality knowledge, usually requires expensive manual efforts.
For example, ConceptNet~\cite{HavasiSALAM10} leverages human efforts
to encode causal events as common sense knowledge.
Khoo et al.~\shortcite{khoo2000extracting} hand-crafted
lexico-syntactic patterns from the dependency tree to recognize
causal knowledge.  Rink et al.~\shortcite{rink2010learning}
automatically generated such patterns encoded with
lexical, syntactic and semantic information to extract
causality knowledge, but the approach requires initial training data,
which determines the quality and quantity of the generated patterns.
Such iterative approach also tends to bring in ill-formed patterns
and unreliable results.
Other approaches reported in~\cite{gordon2012copa} build on deeper
lexico-syntactic analysis of sentences,
to identify knocking and inviting in our example as
\emph{events}, and determine whether causality between two events hold.
However, knowledge acquired by these approaches,
based on human and in-depth analysis, inherently lack coverage.
	
Second direction, harvesting causality from large
text corpus with a data-driven approach, seeks to overcome
the limitation in breadth of the first direction.
The best known approach~\cite{gordon2011commonsense} here,
outperforming the approaches in the first direction~\cite{gordon2012copa},
leverage personal stories as a source of information about casuality and use
Pointwise Mutual Information (PMI) statistics~\cite{Mihalcea2006:CKM}
between words in the premise and alternative, to identify the pairs with
high correlation.
In our example, while we expect the words \emph{knock} and \emph{invite}
co-occur frequently in text, which indicates a potential causality;
the words \emph{door} and \emph{house} are also observed frequently together.
Misidentifying both as causality may incorrectly give the second
alternative as the result.
Thus term causality from lexical co-occurrence alone is noisy.
Furthermore, causal relations are inherently directional.
If the premise in our example asks for a cause, then \emph{call}
might provide a strong signal to \emph{knock}, though PMI statistics
would model the two directions equally likely.
%However, lexical co-occurrence can be a false alarm.
%are also observed frequently together, but identifying this pair as
%causality leads to falsely identifying the second sentence as a
%result. This observation suggests that Further more, co-occurrence is undirectional,
%while direction is crucial in causality.
%COPA task is directional
%such that our example question can be asked in another direction, that is,
%asking what is a cause of knocking.
%In this direction, \emph{call} can be a strong
%cause, but it cannot be a result, though PMI statistics would model
%the two directions equally likely.
%Approaches trade off between coverage and precision of
%extracted causality knowledge.
Therefore, current data-driven approaches may address the coverage
limitation of causality acquisition, but suffer from low precision in return.


%\ZY{Though PMI, co-occurrence based causality metric, has been an effective
%indicator in prior literature, it suffers from the following limitations: }

In contrast, our goal is to pursue both coverage and precision
in modeling causality.
%\ZY{In this paper, we leverage the data-driven approach to extract substantial causality
%knowledge from large web corpus. Based on this, we propose a novel metric
%which properly modeled reasoning about causality for short text.}
To pursue coverage, we propose a data-driven approach of harvesting
a comprehensive \emph{term-based causality network} from a large web corpus.
To pursue precision, we leverage explicit causal indicators
(e.g., cause, because), to prune substantial non-causal co-occurrences and
introduce cause and effect roles to each term in the network, and
capture the directions in causality reasoning.
%The identification of cause/effect roles, obtained by leveraging
%our extracted causality network, benefits the improvement of
%co-occurrence base metric modeling causality.
With the differentiated cause and effect roles, our causality network carries
directional {\em causal co-occurrences}, e.g., from the corpus,
$knock$ causes $invite$ $m$ times, while $invite$ causes
$knock$ $n$ times. This directional information gives rise to new ways of
computing causality strength between terms and between texts, thus yields much
better results in commonsense causal reasoning.
%For example, after introducing causal/effect roles, we can simplely reformulate
%the PMI equation to obtain an adapted version of PMI.
%We name such variation of co-occurrences as causality co-occurrences.
%Further more, we propose a more powerful causality co-occurrence based metric to
%model the causality strength
%between terms, adopting simply combination of which,
%achieves (?) results on causality reasoning tasks for short texts.

%conduct lexico-syntactic analysis of the sentences to
%extract events and identify events that are strongly causal using the causality network
%and other semantic resources such as WordNet~\cite{Miller1995}.

In sum, this paper makes the following contributions:
\begin{itemize}
\item We harvest a term-based causality co-occurrences network from large
web text by carefully designed causal cues (see \secref{sec:network});
%This network encodes both causal direction and strength between terms and
%is first of its kind to the best of our knowledge.
\item We develop a new statistical metric that captures
causal strength between any two pieces of text (see \secref{sec:causalstrength}
and \secref{sec:reasoning});
%adapting to model causality more properly by replacing lexical
%co-occurrences with causality co-occurrences. Base on this,
%we then propose a reasonable and effective metric for casuality reasoning
%by combining two directional information.

%\item ==We redefine causal strength $u \rightarrow v$ to reflect directions, by combining conditional probability
%of $u$ being the cause of the pairwise causality and $v$ being its effect.==

%\item To quantify causality between phrases, we aggregate term causality
%leveraging both syntactic and semantic understanding on the premise
%and the alternatives. For syntactic understanding, we parse sentences to
%extract \emph{event} from premise and alternatives, consisting of head words
%in verb and objects. For semantic understanding, we leverage
%semantic knowledge on each term in the event obtained from WordNet,
%to properly discount causality from ambiguous terms.
\item Our proposed framework achieves state-of-the-art accuracy of $70.2\%$
on the difficult COPA task, and also outperforms existing methods on
causality detection between phrases by signifant margins (see \secref{sec:eval}).
\end{itemize}

%The rest of the paper is organized as follows. \secref{sec:approach}
%established our 3-part causality computation framework.
%\secref{sec:eval} evaluates the accuracy of the extracted causal network,
%the effectiveness of causal reasoning between sentences as well as phrases,
%and the ability to determine the direction of causality. \secref{sec:related}
%discusses some related work before \secref{sec:conclude} concludes the paper.
%\ZYBEGIN{}We demonstrate the effective causal directional property of our
%extracted causal network. \ZYEND{} In addition, we validate the accuracy of our
%causality detection using manually labeled causal relations from ConceptNet as
%ground truth \ZY{to show the effectiveness of our causality network}.
