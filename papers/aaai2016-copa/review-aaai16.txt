Dear Kenny Q.,

Thank you for your submission to AAAI-16.  Author responses will be accepted between now (Wednesday October 28) and 11:59 PM PDT Friday October 30.

During this time, you will be able to read the current reviews for your paper and have the option to submit a response of up to 500 words.  
Please keep the following points in mind:

* An effective response will identify any factual errors in the reviews and focus on any questions posed by the reviewers.  It is normally ineffective to attempt to provide new research results, dispute matters of judgment, or reformulate the presentation.  Try to be as concise and to the point as possible.

* Submitting an author response is optional; it is not a requirement.

* The reviews are provided as submitted by the PC members, without any coordination between them.  Thus, there may be inconsistencies.  
Furthermore, these are not the final versions of the reviews, since they will be updated to take into account your feedback and any discussions among PC members.  It might also be necessary to solicit additional reviews after the author response period has closed.

* The program committee will take author responses into account during the discussion period.  We will strongly encourage reviewers to update their reviews to reflect the discussion and react to your response; however, we cannot guarantee that all reviews will be so updated.

* No edits or deletion of comments are possible after the response has been submitted on EasyChair.  Only a corresponding author is able to enter the response. 

The reviews on your paper are attached to this letter.  To submit your response you should log on the EasyChair Web site
https://easychair.org/conferences/?conf=aaai16 and select your submission on the menu.

Thank you,

Dale Schuurmans and Michael Wellman
AAAI-16 Program Cochairs

===========================
Scales used in the reviews:

Significance of the Contribution
Does the paper contribute a major breakthrough or an incremental advance? 
3: substantial, novel contribution
2: modest or incremental contribution
1: minimal or no contribution

Soundness
Is the technical development correct or does the paper exhibit inaccuracies? 
3: correct
2: minor inconsistencies or small fixable errors
1: major errors

Scholarship
Is the work well positioned with respect to the existing literature? If relevant references are missing, please provide examples in your comments to the authors.
3: excellent coverage of related work
2: relevant literature cited but could expand
1: important related work missing or mischaracterizes prior research

Clarity
Assess the clarity of the presentation and reproducibility of the results.
3: crystal clear
2: more or less readable
1: hard to follow

Breadth of Interest
Would the paper attract broad interest or is it targeted to a narrow audience?
4: broad interest across AAAI audience
3: some interest beyond specialty area
2: interest limited to specialty area
1: out of scope

SUMMARY RATING
Scores from + to +++++ indicate that the submission could be accepted (with either a poster presentation or a talk). The stronger your feeling, the higher your score. Scores from - to ----- indicate that the submission should be rejected. The summary rating does not need to be an "average" of the other ratings. 
Range: +++++ (= Best), ++++, +++, ++, +, -, --, ---, ----, ----- (= Worst)

==============================================================

----------------------- REVIEW 1 ---------------------
PAPER: 1353
TITLE: Computing Strength of Causality between Short Texts
AUTHORS: Zhiyi Luo, Yuchen Sha, Kenny Q. Zhu, Seung-Won Hwang and Zhongyuan Wang

Significance: 2 (modest or incremental contribution)
Soundness: 2 (minor inconsistencies or small fixable errors)
Scholarship: 1 (important related work missing or mischaracterizes prior research)
Clarity: 2 (more or less readable)
Breadth of Interest: 3 (some interest beyond specialty area) SUMMARY RATING: -1 (- (weak reject))

----------- SUMMARIZE THE MAIN CONTRIBUTION OF THE PAPER ----------- 
This paper proposed a novel method of constructing a causality network from texts.
The idea, using both necessity causality and sufficiency causality for the task, seems interesting.

----------- COMMENTS FOR THE AUTHORS ----------- 1. It is in doubt that Eq(1) and Eq(2) cope with the definition of ``necessity causality'' and ``sufficiency causality''. Eq(1) and Eq(2) are almost the same (though alpha was introduced) so that it does not seem to be able to make distinction between the two different causalities.

2. Though this paper's main contribution is to construct a causality network, it does not consider characteristics of the network such as transitivity, direction, many-to-many relations etc. in calculating the causality strength. The current approach seems just a co-occurrence based scoring without considering such characteristics.

3. Is it possible to acquire "knock" causes "invite" in Example 1 with the proposed method? It seems that the proposed method is good at extracting from texts noun-to-noun causality rather than verb-to-verb causality. 

4. It is not clear that how was a causality network constructed from sentence-to-sentence causality or phrase-to-sentence causality, which can be recognized by inter-sentence causal cues such as ``If A, B'' and ``A, therefore B''

5. Why did the authors introduce ``implicit causality'' in Section 2.2? Can such implicit causality be captured from the constructed network?

6. Is this sentence ``Based on this one-to-many relation...'' correct? The constructed causality network seems to has a many-to-many causal relation between terms.

7. Some recent works such as (Hashimoto et al., 2014;2015) would be relevant to this work. 
Especially, (Hashimoto et al., 2015) chained causality extracted from texts for generating future scenario, which may be relevant to the causality network in this work.

---
* Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer and Jong-Hoon Oh. Generating Event Causality Hypotheses through Semantic Relations. Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI-15)
* Chikara Hashimoto, Kentaro Torisawa, Julien Kloetzer, Motoki Sano, Istvan Varga, Jong-Hoon Oh and Yutaka Kidawara. Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014)

---
 
8. Some of notations were introduced without definition. For example, ``W=x'' in PMI (W=x) in Table 2 and ``|T1|'' and ``|T2|'' in Eq. (7).

----------------------- REVIEW 2 ---------------------
PAPER: 1353
TITLE: Computing Strength of Causality between Short Texts
AUTHORS: Zhiyi Luo, Yuchen Sha, Kenny Q. Zhu, Seung-Won Hwang and Zhongyuan Wang

Significance: 2 (modest or incremental contribution)
Soundness: 2 (minor inconsistencies or small fixable errors)
Scholarship: 2 (relevant literature cited but could expand)
Clarity: 2 (more or less readable)
Breadth of Interest: 3 (some interest beyond specialty area) SUMMARY RATING: 1 (+ (weak accept))

----------- SUMMARIZE THE MAIN CONTRIBUTION OF THE PAPER ----------- 
The paper presents an approach to extracting causal relations and measuring causality strength using a casual network. Causal patterns are used to extract causal pairs with frequencies from a web corpus. These weighted pairs constitute the causal network. The paper proposes a measure of causality strength, which is used to compute causality strength between two text fragments. The approach is evaluated using the COPA challenge dataset. The results outperform the previous best approach. The approach is also evaluated against SemEval-2010 Task 8 data and is shown to be able to detect causality direction.

The experiments are not always clearly described, which should be fixed in the final version, if the paper gets accepted.

----------- COMMENTS FOR THE AUTHORS -----------
>The blue bars (left) are the number
of distinct pairs and the orange ones (right) show the total number of pairs. Inter-sentence cues like “if” and “because”
harvested the largest number of pairs. But more specific patterns such as “reason” and “inasmuch” find more diverse pairs, since the number of distinct pairs is relatively large compared to the total pairs extracted.

When I look at Fig. 2, I see that "reason for" and "inasmuch" resulted in less distrinct pairs than "if"... Or do you mean ratio? How did you compute the ratio and what is it supposed to show?

When you compared against ConceptNet, where did you get the non-causal pairs from? Did you just take all possible related pairs from ConceptNet? If this is the case, then it seems to be a bit random to me in terms of coverage. What one could do instead is taking a set of N terms and computing causality for each pair of terms from the set and then comparing it against some manually annotated database (e.g. ConceptNet). Btw, how do you determine, if there is a causal relation? Do you use a threshold on causality strength or just take each pair of terms no matter what strength it has?

Btw, WordNet also contains causal relations for verbs. It would be interesting to compare your coverage with the coverage of WordNet.

>The incorrect alternative was purposely set semantically
close to the premise, which makes this task more difficult for purely associative methods.

I cannot parse this sentence.

In the Related Work section, please mention work on bootstrapping causal relations, e.g. by Kozareva.

----------------------- REVIEW 3 ---------------------
PAPER: 1353
TITLE: Computing Strength of Causality between Short Texts
AUTHORS: Zhiyi Luo, Yuchen Sha, Kenny Q. Zhu, Seung-Won Hwang and Zhongyuan Wang

Significance: 2 (modest or incremental contribution)
Soundness: 3 (correct)
Scholarship: 2 (relevant literature cited but could expand)
Clarity: 3 (crystal clear)
Breadth of Interest: 4 (broad interest across AAAI audience) SUMMARY RATING: 1 (+ (weak accept))

----------- SUMMARIZE THE MAIN CONTRIBUTION OF THE PAPER ----------- 
This paper presents a data driven approach to do causality reasoning between short texts. The main testbed is Choice of Plausible Alternatives (COPA) questions, which is to select a more plausible alternative that follows the premise. The proposed framework is to automatically harvest a network of causal effect terms from a large web corpus.  The idea is to first count the frequency of every two words co-appearing in certain patterns in the training data, based on which the causal correlation strength between these two words is calculated. The choice in COPA is then determined by comparing the causal correlation strength between each of the candidate text and premise text, which is computed by simply summing up the the causal strength between every pair of one word occurring in the premise and the other occurring in the candidate text. According to the authors, this method outperforms other existing methods.

----------- COMMENTS FOR THE AUTHORS ----------- 
The paper contributes to an incremental advance in causality detection in textual data, which is an area of great interest. No major error is found. The presentation is clear. I find the work interesting. On the other hand, the experiment do not seem sufficiently improving the state-of-the art methods.

In Table 2, why is the training data for different methods are from different data source? In this case, one can't just compare accuracy alone, right?

While the method can be useful for certain (simple) domains, like COPA, I don't see it really applies to many other (real) causal reasoning problems For instance, I don't see how the causality reasoning problems in (Morgenstern 2015, cited in the paper) can be solved using the techniques in this paper. 

How can the result be further improved? Are the causal cues in Table 1 sufficient? Some remarks about the future direction would be good to mention.

------------------------------------------------------

