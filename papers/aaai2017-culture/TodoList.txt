################New

0. Get the Twitter Corpus Done and Preprocessed
1. Do Chinese Word Segmentation on Weibo.csv Data
2. Find Weibo Entity Linking Tools (State of the Art)
3. Find Twitter Entity Linking Tools (State of the Art)
4. Improve Linear Transformation 
	4.1. Do not use the human annotation result as Training data Anymore
		 Reason: Waste our label; not very accurate 
	4.2. Use the translation pairs whose confidence >=0.8(a certain threshold) as training data.
	4.3. Or we can use all the translation pairs and we modify the loss function with weight information
5. Use Smaller But Specific Similarity Space 
	5.1. Find several culture related lexicons in the Coling paper 
	5.2. Use English lexicon to generate a Chinese lexicon which maybe lager than the English one; And then we can simply use the Translation space approach.
	P.S. This may make our approach more explainable

6. Use Concurrence Information to describe every entity (May use SVD to attack the sparse problem.)
7. A mixed corpus and embedding way: randomly replace words with its translation and then train the embedding 






#################Old 
Todo-List:

1.Improve the Ground Truth
Importance: ***
Owner: Bill
	1.1 Cite more social science papers to be more convincing; Argue why labeling images is reasonable.
	1.2 Better the result of the Bing Images 

2.Try Deep Learning Approach 
Importance: ***
Owner: Hanyuan
	

3.Crawl the new Corpus From SNS
Importance: ***
Owner: Frank
Remark:Crawl the data of named entities one by one. Available Sources: Quora.com, Twitter, Zhihu.com, Weibo.com

4.Figuring out the difference between #Semantic Difference# and #Cultural Difference#
Importance: *
Owner: Bill

5.Sentimental analysis on named entities.
