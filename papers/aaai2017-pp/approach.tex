\section{Approach}
\label{sec:approach}


In this section, we discuss how a natural language relation
is paraphrased into knowledge base schemas.
Given a relation $r$ with a list of instances extracted from Open IE system,
we first generate a set of candidate schemas from these (head, tail) pairs,
and then discover the most suitable representations among them.
Due to the lack of direct (relation, schema) training data, 
we propose a data-driven (\KZ{Shall we say distance supervision?})
approach and learn a probability distribution over all the candidates.

%Given a natural language relation $r$ with a list of its entity pairs,
%we first generate a set of candidate schemas from these pairs, 
%extract features from both the schemas and the relation pattern itself, 
%and then compute a probability distribution over all the candidates 
%given the relation pattern and the features according to the 
%following log-linear model:
%\begin{equation}
%  p(s|r; \vec{w}) = 
%    \frac { 
%      exp \{ \vec{f} (s, r) \cdot \vec{w} \} 
%    } { 
%	  \sum\limits_{s' \in CAN(r)} { 
%	    exp \{ \vec{f} (s', r) \cdot \vec{w} \} 
%	  }
%    },
%\end{equation}
%\noindent
%where $\vec{f} (s, r)$ is the feature vector encoding the schema
%along with the relation, and $\vec{w}$ represents the vector of
%feature weights, and $CAN(r)$ is the set of generated candidate schemas for 
%the relation $r$.
%
%Since gold training data in the form $\langle relation,\, schema \rangle$
%is hard to obtain in large volume, the feature weights $\vec{w}$ is trained
%by distant supervision. Specifically, we design a data-driven function 
%to label any schema with a ``silver'' score, which reflects the 
%approximate quality of the schema, and then perform learning by minimizing 
%the following loss function on a set of candidate
%schemas generated from a training set of relations:
%\begin{equation}
%\small
%J(\vec{w}) = \lambda \| \vec{w} \|_2^2 \! - \!  
%  \frac {1} {N} \! 
%  \sum\limits_{i=1}^{N} {
%    \! log \! \sum\limits_{s \in CAN(r_i)} { 
%	  \! lb(s, r_i) p(s|r_i; \vec{w}) 
%	}
%  },
%\end{equation}
%\noindent
%where $N$ is the number of relations in the training data,
%$\lambda$ is L2 regularization parameter, 
%$lb(s, r)$ is the silver label function giving a score between 0 and 1. 
%The loss function is a negative log-likelihood function 
%of correct schemas over all the candidates.
%
%In the following sections, we describe how to generate the candidate
%set, the silver label function and the feature functions in detail.
%

% then 3 step in detail
% 1. candidate generation (BFS + DFS)
\input{candgen}
% 2. schema probability distribution
\input{schema}


% 2. silver labeling (ratio function)
%\input{label}
% 3. learning step (feature selection)
%\input{feature}
