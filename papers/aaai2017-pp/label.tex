\section{Silver Label Function}
\label{sec:label}

%18 sents.
%1. intro (2 sent)
In this section, we design a function to label the approximate quality of
a schema based on the input relation instances as well as the KB itself.
Previously during candidate schema generation, we have used the support as
a rough measure of schema quality to prune the search space. 
However, it hasn't taken into account of the entity pairs that are 
covered by the schema but not in the input instances. 
Our silver label function is designed to capture the trade-off 
between the support in the input vs. the coverage of other entity pairs 
in the KB.
%With a schema $s$, a relation $r$ and its training instances as 
%input, the silver labeling function $lb(s, r)$ approximately 
%measures the correctness of the schema representing the relation.
%The function follows a data-driven idea: a schema is more
%confident to be correct, if it covers more positive relation 
%instances and less negative instances. 
%%2. incompleteness (3 sent)
%A straightforword function can be derived by just counting the 
%proportion of instances covered in both positive and negative side.
%However, the incompleteness fact of KB has not been taken into 
%consideration: given a positive instance $\langle e_1, e_2 \rangle$,
%where $e_1$ is a rare entity in KB, and its relationship with other
%entities are not well connected, even a best schema (annotated by 
%human) can't cover this entity pair, should this pair contributes
%a negative evdience equally with those popular pairs not covered by
%a schema?
%The answer is no, so treating all positive and negative instances 
%equally is not the best way to produce labeling function.
%
%%3. group by e1 + formula (7 sent)
%Now we introduce our solution to handle this problem.
We first introduce some notations: 
\begin{itemize}
  \itemsep0em
  \item $E_{subj}(r), E_{obj}(r)$: the set of distinct subject entities and 
object entities in the input instances of $r$;
  \item $obj_s(e_1), sub_s(e_2)$: given a schema $s$, 1) the set of all object
entities of a subject entity $e_1$ in KB and 2) the set of all subject entities
of an object entity $e_2$ in KB; 
  \item $obj_r(e_1), sub_r(e_2)$: all distinct object (or subject) entities
of $e_1$ (or $e_2$) in the input instances;
%  \item $NS_r(e_1), NS_r(e_1)$: all distinct $e_2$ (or $e_1$) where $\langle e_1, e_2 \rangle$ is in negative instances,
  \item $obj_{sr}(e_1) = obj_s(e_1) \cap obj_r(e_1)$;
  \item $sub_{sr}(e_2) = sub_s(e_2) \cap sub_r(e_2)$.
\end{itemize}

We define a confidence score function of subject entity 
$e_1$ under schema $s$ generated from relation $r$: 
\begin{equation}
\label{eqn:scp}
\small
  sc_+(e_1, s, r) = \left\{
  	\begin{aligned}
	\! 1 / ( 1 \! + \! \ln \frac 
	  {\left| obj_r(e_1) \right|} 
	  {\left| obj_{sr}(e_1) \right|} 
	)    & ~ & obj_{sr}(e_1) \! \neq \! \varnothing  \\
	\! 0 & ~ & obj_s(e_1) \! = \! \varnothing        \\
	\! \frac {1} {
	  1 \! + \! \ln (\left| obj_s(e_1) \right| \! + \! 1)
	} \! - \! 1  & ~ & otherwise    \\
	\end{aligned}
  \right..
\end{equation}
\normalsize

The three branches in \eqnref{eqn:scp} assigns positive, zero and negative
scores to three scenarios respectively. If there are some object entity covered
both in the input data and KB, the schema receives positive confidence score 
and up to 1 if all input instances are covered.
In the second case, the schema from $e_1$ returns no objects, possibly due to
incompleteness of the KB, we have no information as to the quality of $s$.
Therefore we assign a zero score. In the third case, $s$ leads to some
objects in KB, not none of them are in the input data, which suggests that
$s$ may be incorrect and hence assigned a negative score. 

A similar confidence score function for the object entity (in an opposite
direction), denoted as $sc_-(e_2, s, r)$ can be defined by replacing
$obj$ with $sub$ in \eqnref{eqn:scp}.
%The last branch goes even worse: some $e_2$ are covered in the 
%knowledge base, but neither is found in the positives instances.
%Though KB incompleteness is still possible in this scenario, 
%with more $e_2$ covered, the $e_1$ is more popular, which 
%gives us a stronger evidence that $s$ is not correct.
%Therefore, the confidence for the schema goes down from 0,
%and become smaller when its coverage on $e_1$ goes larger, with
%the minimum confidence as -1.

%%4. negative side (3 sent)
%Similar with \eqnref{eqn:scp}, negative relation instances also
%provide confidence scores to the schema, 
%defined in \eqnref{eqn:scn}:
%\begin{equation}
%\label{eqn:scn}
%\small
%  sc_n(e_1, \! s, \! r) \! = \! \left\{
%    \begin{aligned}
%	\! -1 / ( 1 \! + \! ln \frac
%	  {\left| NS_r(e_1) \right|}
%	  {\left| NC_{sr}(e_1) \right|}
%	)            & ~ & NC_{sr}(e_1) \! \neq \! \varnothing  \\
%	\! 0         & ~ & CV_s(e_1) \! = \! \varnothing        \\
%	\! 1 \! - \! \frac {1} {
%	  1 \! + \! ln (\left| CV_s(e_1) \right| \! + \! 1)
%	}            & ~ & otherwise \\
%	\end{aligned}
%  \right.,
%\end{equation}
%\normalsize
%
%\noindent
%where the first scenario makes the confidence score smaller than 0
%(negative instances are covered by $s$); again the second scenario
%faces the incompleteness problem and we can't decide the 
%quality of schema by this $e_1$; the last scenario shows us a
%confidence score larger than 0, since all entity pairs it covered 
%on $e_1$ are not negative, and the score increases monotonically 
%when the size of coverage increases.
%
%\KQ{I didn't mention the penalty function right now..}
%
%5. summary  (3 sent)
%In summary, each distinct $e_1$ (and $e_2$) from the training data 
%of $r$ gives us confidence scores on the side of both positive and 
%negative instances, ranging from -1 to 1.
The silver label function $lb(s, r)$ sums the confidence scores for
all entities in $E_{subj}(r)$ and $E_{obj}(r)$ in both directions,
and normalizes scores to the value ranging from 0 to 1:
\begin{equation}
\small
\begin{aligned}
  lb(s, r) = \frac{1}{2} (1 & + \sum\limits_{e_1 \in E_{subj}(r)} { \frac {sc_+(e_1, s, r)} { \left| E_{subj}(r) \right| } } \\
             & + \sum\limits_{e_2 \in E_{obj}(r)} { \frac {sc_-(e_2, s, r)} { \left| E_{obj}(r) \right| } })
%  lb_n(s, & r) = \sum\limits_{e_1 \in E_1(r)} { \frac {sc_n(e_1, s, r)} { \left| E_1(r) \right| } }
%               + \sum\limits_{e_2 \in E_2(r)} { \frac {sc_n(e_2, s, r)} { \left| E_2(r) \right| } }\\
%  lb(s, & r) = \frac {1} {2} ( 1 + \alpha \cdot lb_p(s, r) + (1 - \alpha) \cdot lb_n(s, r) ), \\
\end{aligned}
\end{equation}
%\noindent
%where $\alpha$ controls the tradeoff between positive and negative
%instances. Since Open IE systems only provide positive instances, 
%our model will automatically generate negative instance based on
%random sampling. We explain it in detail at \secref{sec:eval}.

%1-2. introduce the HP size
%Then we focus on how to calculate the coverage size for one skeleton
%on the whole knowledge graph.
%Since a more general skeleton always covered more entity pairs than
%a more specific one in $KB$, the coverage size would be a useful 
%evidence in both schema generation and learning step.
%%3. brute force is intractable
%We could get the exact coverages of a skeleton by brute force 
%searching its ground graphs over the KB, but it's very time consuming.
%%4-8. explain in detail 
%Here we propose a randomized approach to estimate the coverage size.
%Supoose a skeleton has $n$ predicates with $n+1$ variables 
%$x_0,\, x_1,\, ...\, x_n$ (for convenience of explanation, we use
%$x_0$ and $x_n$ to indicate positions of target subject and object
%entites respectively only in this section).
%Firstly, we only consider the predicate between $x_0$ and $x_1$,
%extracting all the ground graphs (only $g_0,\, g_1$ connected by 
%the predicate), and keep a sampled list
%\footnote{We set the maximal number of samples
%to 100,000 in the implementation.}
%of graphs by randomly picking.
%Next, we follow the predicate between $x_1$ and $x_2$, expanding 
%previous ground graphs into 3 entities, and also sample them randomly.
%We perform the expansion step iteratively, until all variables in 
%the skeletons are processed, resulting in a list of ground graphs 
%with $n+1$ entities.
%The estimated coverage of the skeleton is the size of distinct 
%$(g_0,\, g_n)$ pairs in sampled ground graphs, divided
%by sampling rates at each iteration.
%%9. show example
%We show some candidate skeletons for ``grandson-of'' relation, with 
%coverages over positive training instances and Freebase, displayed 
%in \tabref{tab:bfs-example}. Generally speaking, the first skeleton
%is the most suitalbe one among these candidates.
%
%\begin{table}[ht]
%%\small
%	\centering
%	\caption{Example of candidate skeletons for ``grandson-of'' relation.
%		We show the coverage with percentage over 104 training instances,
%		and estimated coverage over Freebase. $\tau$=3, $cov$=10\%.}
%	\begin{tabular}{|c|c|c|c|}
%		%\toprule
%        \hline
%		skeleton		 	& $cover_{train}$	& $cover_{FB}$ \\
%        \hline
%        parent + parent		& 31 (28.8\%)		& 1.65\e{4}  \\
%        \hline
%        family\_members		& 18 (17.3\%)		& 3.94\e{5} \\
%        \hline
%		\pbox{20cm}{nationality + contains \\ 
%		+ people\_born\_here} 	& 35 (33.7\%)	& 6.95\e{9}	\\
%		\hline
%	\end{tabular}
%	\label{tab:bfs-example}
%\end{table}
%
%
