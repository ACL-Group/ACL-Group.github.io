Many thanks to the previous comments that will help improve this work.
Below are our responses to some of the questions raised in the reviews.


R1:
Thanks for pointing out grammar mistakes, and we will definitely edit
them in the revised version.
The "singer" example is also a typo, which should be "performed in".


R2:
The reason we didn't show the comparison of embedding-based approach is twofold:

	First, our main objective is to show the expressiveness of a more accurate semantic model 
	when the explicit representation changes from the path to the tree structure.
	Embedding model aims to encode all entities, predicates (or even NL relations) into a uniform space,
	and indeed KBC is a sharing application between ours and embedding approach,
	but it's not our main purpose to be the state-of-the-art in KBC across different techniques.
	Due to the space limitation, we focus on the comparison with structure-based approaches.
	BTW, other embedding approaches are also less likely to compare with structure-based approaches in their papers.

	Second, embedding-based approach comes with a scalability issue.
	FB15K and WN18 are the commonly used KB dataset in their evaluations.
	These dataset are relatively small, but it may still takes one day to finish the training step,
	based on our empirical study.
	While the dataset "FB-" in our work is 2 orders of magnitude larger than FB15K and WN18, not mentioning
	the full version of Freebase, it's intractable to compute embedding on such larger KB.
	With a well-designed searching strategy, our model works well in Freebase,
	which is able to handle queries in the real world scenario.

We made some discussions about those relations having no explicit structures.
Some of them are called "trivial relations" (Sec. 4.2) due to a typical KB isn't interested in such trivial facts.
For the remaining relations (Sec. 4.3 part 3), if the certain KB doesn't contain necessary basic predicates for schema inference,
they also have no suitable explicit structures.
Our approach may output improper schemas (at least covering some entity pairs), but other works won't produce good
results neither, unless unstructured text information is involved.


R3:
Compared with AMIE+ (Gal√°rraga et al.), our extra contribution is to control the trade-off between general & specific schemas.
that's what our schema inference step handles: given all the candidate schemas of the relation,
we want to learn parameter \theta, the probability distribution over the candidates.
Refer to figure 4, the probability value tells us which candidate is more preferred to describe the relation.
The notation sc_j indicates the j-th candidate schema, corresponded to the probability \theta_j.
In order to learn the parameters, we maximize the likelihood of generating the object entity, given the relation and the subject.
Formula 2 shows how the likelihood is generated: first picks a (hidden) schema based on \theta distribution,
and then randomly output an object from all results queried by this schema (defined in Formula 3, which is a uniform distribution).
Actually we got the idea from Gaussian Mixture Model, which may help you understand the formula.
AMIE+ lacks such trade-off, and our approach outperforms theirs on KBC evaluation.
On complex relation dataset, the Macro/Micro/Average F1 results are 0.547/0.440/0.470 (ours) vs 0.393/0.301/0.335 (AMIE+).
On ordinary relation dataset, the results are 0.423/0.307/0.367 (ours) vs 0.474/0.249/0.343 (AMIE+).

We considered directly evaluating the quality of schemas by manual labeling, but it suffered from two problems:
First, the evaluation is subjective, even we authors are hard to label a proper score for one partial correct schema.
Second, the labeling task requires expert knowledge of Freebase, which limits the number of annotators.
Instead of showing manual labeling result, we give real examples of our algorithm, which is an alternative
to measure the quality of schemas.

We had tried to compare with the work of Quan Wang et al., but the scenario is a bit different from ours.
the key idea of their work is to merge similar test relations (sharing lots of entity pairs) together,
so that KBC results on one test relation can be improved through sharing common features from other relations.
In our experiments, test relations and relation instances are extracted from different sentences in the web corpus.
With only a few support entity pairs provided, the similarity between two relations could be very small, 
even if they are really similar in semantics. 
Actually, we failed to merge any two testing relations by the proposed clustering algorithm,
so their approach degenerated into a PRA algorithm, and we didn't show the comparison.
And we didn't compare with the embedding approach, because we found it intractable to run
on a large-scale knowledge base (refer to the response of R2).


