\section{Related work}
%\KZ{\textcolor{red}{This section needs to be expanded a bit more more. Need to cover
%code search of various types. You should start with works most relevant to
%this paper, that is topic modeling for code, code search, code mining etc..
%then navigate to other less related work such as tag recommendation, and topic
%models for general text.}}

%\KZ{\textcolor{red}{You definitely need to talk about work on coding mining or code search that
%has leveraged NL semantics, code structure as well as call graph info.
%You should also mention work that has splitted identifiers like we did before,
%if any.}}

%\subsection{Code Mining}
Mining of source code repositories becomes increasingly popular in
recent years. Existing work in source code mining include
code search, clone detection, software evolution,
models of software development processes, bug localization,
software bug prediction and so on. The areas of concern for this
paper are code search and duplicated code detection.

Iyer et al.~\shortcite{iyer2016summarizing} proposed a new model called CODE-NN
that uses Long Short Term Memory (LSTM) networks with attention
to produce sentences that can describe C\# code snippets and SQL queries.
Iyer et al.'s work has strong performance on two tasks,
code summarization and code retrieval.
Adrian et al.~\cite{kuhn2007semantic} utilized the information of
identifier names and comments to mine topic of source code repositories.
Punyamurthula~\shortcite{punyamurthula2015dynamic} used call graphs just like
our work to extract the metadata and dependency information from the
source code and used these information to analyze the source code
and get its topics.

In code search, most search engines solve the problem by keyword extraction
and signature matching. Maarek et al.~\cite{maarek1991information} used keywords extracted form man pages written in natural language and their work is an early example of the work based on keywords. Rollins and Wing~\shortcite{rollins1991specifications} proposed an approach to find code with the signatures present in code. And Mitchell~\shortcite{mitchell2008hoogle} combined signature matching with keyword matching. Then Garcia et al.~\cite{garcia2016semantic} focused on querying for semantic characteristics of code and proposed a new approach which combines semantic characteristics and keyword matching.

In other related domains, Cai~\cite{cai2016code} proposed a new method
for code parallelization through sequential code search. That method also
can be used for clone detection. Williams and Hollingsworth~\shortcite{williams2005automatic}
described a method to use the source code change history of a
software project to drive and help to refine the search for bugs.
Adhiselvam et al.~\cite{adhiselvam2015enhanced} used MRTBA algorithm
to localize bug to help programmers debug.
%Semantic clustering \cite{kuhn2007semantic} utilizes identifier names and comments to mine topic of source code. (using LSI) \textcolor{red}{Identifier}

%Summarizing Source Code \cite{iyer2016summarizing} using LSTM to describe C\# code snippets and SQL queries. And this paper preform on two tasks code summarization and code retrieval.

%An Enhanced Approach for Software Bug Localization using Map Reduce Technique based Apriori (MRTBA) Algorithm \cite{adhiselvam2015enhanced} is about bug localization, one domain of code mining. \textcolor{red}{Bug Localization}
%\subsubsection{Code Search}
%Semantic Code Browsing \cite{garcia2016semantic} combines the two approaches of semantic and keyword-based search (we can find some related work about code search here which are based on keyword and signature). \textcolor{red}{Semantic}

%Semantics-Based Code Search \cite{reiss2009semantics}, there are some related works about code search based on semantic information. \textcolor{red}{Semantic}

%Dynamic model generation \cite{punyamurthula2015dynamic} uses call graph to search code and there are mostly all related work about code search. \textcolor{red}{Call Graph}

%Code Parallelization through Sequential Code Search \cite{cai2016code}. \textcolor{red}{clone detection}

%\subsection{Tag Recommendation}
%As mentioned earlier, in our definition the most important components for an action are the arguments of the predicate.
%Argument conceptualization tries to abstract the subject and object arguments of a verb by categorizing them into set of noun concepts.
%For example, for the verb ``accept'', the set of subject noun concepts might contain ``person'', ``community'' and
%the set of object noun concepts might contain ``document'', ``payment''.
%
%Earlier efforts of such tasks including Semantic role labeling (\cite{gildea2002automatic} and
%\cite{palmer2005proposition}), which labels the semantic meaning of the arguments of the verb, for example
%in the sentence ``He killed the enemy with a rifle.'' ``He'' is the agent, ``enemy'' is the patient and ``rifle'' is the instrument.
%However the choices of such role labels are rather limited and could be further expanded through Knowledge graphs. Especially in our
%application we hope the role to be more specific than it is in Semantic role labeling.
%
%Selectional constraints studies what are appropriate arguments for a particular verb.
%\cite{resnik1996selectional} proposed class-based selectional preference which decides if a class of terms is
%prefered to a verb. For example ``water'' is more prefered than ``table'' for the verb ``drink''. The main problem with this
%method is it does not consider overlap between classes which results in very similar classes.
%
%\cite{gong2015representing} proposed a data-driven approach which models the conceptualization problem as
%finding $k$-clique with maximum combined weights. They managed to build up an inventory of arguments concepts for
%more than 1,700 unique verbs. Our work is a further development of their project by converting action concepts into
%noun concepts.
Our work is similar to tag recommendation, a topic that has attracted
attention from many researchers.
%The task of tag recommendation is to produce some descriptive
%tags of texts or images.
%There are two kinds of such recommendations, one is
%personalized tag recommendation while the other one is non-personalized.
%The method which consider users' interest and other features is called personalized
%tag recommendation.
Rendle and Schmidt-Thieme~\shortcite{rendle2010pairwise} proposed a tag recommendation
method based on Tucker Decomposition (TD) and Canonical Decomposition (CD)
called Pairwise Interaction Tensor Factorization (PITF).
This method can produce tags which the
user has never seen, however, related to the user's hobbies.
Song et al.~\shortcite{song2008real} worked on a real-time way and their method
is a non-personalized one. Other people worked on tagging pictures, such
as Sigurbj{\"o}rnsson and Van Zwol~\shortcite{sigurbjornsson2008flickr}.
They made contributions to recommending the description of pictures on Flickr.

%\subsection{Probabilistic Model}
Probabilistic models are used to solve problems in natural language processing, too. Hindle et al.~\shortcite{hindle2012naturalness} and Tu et al.\cite{tu2014localness}
showed that n-gram model can improve the ability for automatically
complement when people are coding.
Movshovitz-Attias and Cohen~\shortcite{movshovitz2013natural} used a model similar to n-gram,
to predict annotations from given source code pieces.
Maddison and Tarlow~\shortcite{maddison2014structured} proposed a code-generating system.
However, such kind of probabilistic models are introduced to mine the motivation of
writing code, instead of what it means. This is the difference from our work.

%\subsection{Topic Model}
%There are many researchers using ways which process natural language text to
%work on source code repositories. Maletic and Marcus\cite{maletic2000using} were
%the first people who introduced LSI, Laten Semantic Indexing, to analyze software
%systems. Adrian et al\cite{kuhn2007semantic} worked on the improvement of Maletic's
%analysis. They first extracted the hidden semantics from source code repositories,
%using LSI. After that, they used hierarchical clustering from the hidden semantics
%extracted above. Such kind of processing can classify the source code by their
%function. After all a tag will be added onto each method in the repositories.
%
%Latent Dirichlet Allocation(LDA) was also used on source code analysis. Trevor
%Savage\cite{savage2010topic} and Girish Maskeri\cite{maskeri2008mining} are two
%examples.
