We would like to thank all reviewers for their valuable and constructive feedback. We will revise our paper accordingly based on the comments.


#Response to Assigned_Reviewer_2:


1. We believe our contribution is significant for the following reasons:  


1.1. As we all agreed, the BSL is an important part of our contribution; therefore, while evaluating the significance of our work, people are supposed to compare “SocVec:xxx” with “xxx-BL” instead of “xxx-BSL” solely. In other words, the substantial improvement of using BSL on existing cross-lingual embedding methods is also part of our contribution (e.g. the improvement of Duong-BSL over Duong-BL).  We will clarify this in the revised version.


1.2. Plus, our work has a great advantage in terms of time efficiency over those methods that need re-training on the original corpora like Duong and MultiCluster. With their setup, we have to obtain new bilingual word embeddings by retraining on the corpora, every time when we have a updated BL or BSL. This can be extremely time-consuming, and therefore efficient examination of different  BSL can be difficult. Whereas, our method is only based on monolingual word embeddings and B(S)L, and does not need re-training on the corpora even if the pivot lexicon is changed. 


1.3. We argue that the two proposed novel tasks together with the released datasets are also a significant contribution. It can attract more researchers in computational social science to do cross-cultural study and social science research with bilingual word embedding techniques for research problems like ours. 


2. We do agree that a few mined culturally differences have platform-specific characteristics due to the choice of corpora. However, we argue that the two microblog platforms are among the most popular ones in their languages, and therefore the common culture differences and the temporal variations of cultural differences can both be valuable and beneficial for the studies of social science. Separating temporal influences from the social media corpora can be another interesting future research topic about our proposed task.


3. The construction of social vocabularies is basically the extraction of words from widely-used publicly available text/sentiment analysis systems that we mentioned in the last paragraph of the Experimental Setup. We will talk about more details in the revised version.


4. As we argued before in 1.1 and 1.2, simple comparison between our proposed SocVec methods with Duong-BSL is unfair for it not only utilizes our BSL and re-train on the corpora with such parallel supervision.


5. We selected four most representative bilingual word representation models (LT, multicca, multicluster, Doung) according to a survey (A survey of cross-lingual embedding models (Ruder, 2017)). We will add the comparison with the work you mentioned. However, we found it performs worse substantially than MultiCluster (using their released evaluation website tool on the benchmark datasets, which is accessible in their paper).


6. Thanks so much for pointing out the typos and we will do more careful proofreading in the revision accordingly.




#Response to Assigned_Reviewer_3:


1. The English social words that do not have Chinese translations are pruned in the pre-processing step. We will clarify this detail in the revised version.
2. C_i is defined as a word vector instead of a word, which is stated in the first sentence of the paragraph above the equations of pseudo-word generators. The notation max(C_1^(i),...,C_N^(i)) here means the maximum value of the i-th dimension of all the N word vectors, as we described after the “Max.”
3. The pseudo word generator computes the values by the their word vectors in the respective embedding space with four possible options that we introduced in Page 3.


#Response to Assigned_Reviewer_4:


Both tasks are about computing cross-cultural similarities/differences between words. 


1. The first task is about the cross-cultural differences of named entities in social media context. We asked human annotators to annotate the scores of 700 most frequently mentioned named entities in a relatively objective way. The evaluation of the scores is tested on widely-used metrics for learning-to-rank research. The input is a list of named entities of interest, and the output is then the scores of each named entities indicating the extent of their cultural differences.


2. For the second task, we believe it is clearly stated that our goal is to find terms in another language that can help people understand “slang” terms across language. Thus, the second task can be seen as bilingual lexicon induction in this clear specific context. By extracting slang translation from example sentences,  we are actually making the baseline methods stronger and the comparison more fair. The input is a slang term in language1 and the output is a set of words in language 2 that can help people understand the meaning of the input slang term.


Admittedly, introduction of the two tasks is relatively compact, since we introduced two novel tasks and datasets with extensive experiments. We will elaborate on the setup and the connection of the two tasks more clearly in the revised version. We believe with the additional page for revision it should be easier for us to keep the details and the more introduction at the same time.


3. The intuition and motivation of using social/culture related terms as additionally supervision is natural. It is just like sentiment analysis model naturally need to focus on the sentiment words in the sentence. The sensitivity is tested by the experiments shown in Table 2 and talked about in the right column of Page 5. We show the results of using other lexicons with same sizes and we conclude that the social vocabulary is indeed important.










SPC:
reviewer_3: We think the review is irresponsible about the [Relevance] and [Soundness]. It says our work “has minor errors”, but what it said is just the misunderstanding of the definition of C_i, which is clearly defined in the paper. Reading the definition more carefully, the reviewer will not have any misunderstanding about pseudo-word generators.


reviewer_4: We agree that more elaboration on the two tasks is helpful in the revised version, but all the questions of reviewer #2 can be answered easily with the current version if one reads carefully. We thus think the opinions on [Evaluation] and [Clarity]are not reasonable or fair.


Please note that the two reviewers are out of area. We find their negative opinions are mainly because of this and uncareful reading. We sincerely hope senior PC and area chairs could deal with our paper more properly. We also argue that the two novel tasks are important and interesting for a wide range of researcher in (computational) social science.
