\subsection{Task 1: Mining cross-cultural differences of named entities in social media}
\label{sec:mcdne}
This task is to discover and quantify cross-cultural differences of concerns towards name entities. 
We first explain how we obtain the ground truth from human annotators, then present several baseline methods to this problem and finally 
show and discuss our experiment results in detail.

\subsubsection{Ground Truth}
\label{sec:mcdne_truth}
Harris~\shortcite{harris1954distributional} states that the meaning of 
words is evidenced by the contexts they occur with. 
Likewise, in this work, we assume that the cultural properties of an entity 
can be captured by the terms they always co-occur with in large text corpus. 
Thus, for each named entity, we present human annotators with two lists of 20 most co-occurred words with 
the named entity, from Twitter and Weibo respectively. 
We select 700 named entities for annotators to label, which
are the most frequently mentioned both in Twitter and Sina Weibo. 
Annotators are instructed to rate the relatedness between the 
two word lists with one of following labels: ``very different'', 
``different'', ``hard to say'',  ``similar'' and 
``very similar''.\footnote{All four annotators are native Chinese speakers 
but have excellent command of English and lived in the US extensively. 
Annotators are educated to have shared understanding of the five-level 
labels with selected examples. We do not choose ranking based annotation method as it demands annotators to look 
at 40+40 words for the two terms in two languages before a decision can
be made, which is more expensive and harder to administer in our opinion.}
We chose to annotate the data in this way mainly to avoid
subjectivity and be efficient. 
We argue that since the words presented to the annotators come from 
social media messages, the social elements are already embedded in 
these words.

We then map the labels to numerical scores from 1 to 5
and use the average scores from the annotators as the ground truth 
for score ranking and binary classification evaluation.
For the binary classification problem, 
an entity is considered culturally similar 
if the score is larger than 3.0, and culturally different otherwise.
The inter-annotator agreement is 0.672 by Cohen's kappa coefficient, 
suggesting substantial correlation, according to the Wikipedia entry
of Cohen's kappa.
%\BL{ (0.531 without hanyuan)}
\begin{table*}[th!]
	\footnotesize
	\centering
	\caption{{Selected culturally different named entities, with Twitter and Weibo's trending topics manually summarized}}
	\begin{tabular}{|L{1.5cm}|L{5cm}|L{8cm}|}
		\hline
		\textbf{Entity} & \textbf{Twitter topics} & \textbf{Weibo topics}
		\\ \hline\hline
		Maldives & coup, president Nasheed quit, political crisis & holiday, travel, honeymoon, paradise, beach \\ \hline
		Nagoya & tour, concert, travel, attractive, Osaka & Mayor Takashi Kawamura, Nanjing Massacre, denial of history\\  \hline
		%		Quebec & Conservative Party, Liberal Party, politicians, prime minister, power failure & travel, autumn, maples, study abroad, immigration, independence   \\ \hline
		%		Philippines & gunman attack, police, quake, tsunami & South China Sea, sovereignty dispute, confrontation, protest  \\ \hline
		Yao Ming & NBA, Chinese, good player, Asian  & patriotism, collective values, Jeremy Lin, Liu Xiang, Chinese Law maker, gold medal superstar   \\ \hline
		University of Southern California & college football, baseball, Stanford, Alabama, win, lose & top destination for overseas education, 
Chinese student murdered, scholars, economics, Sino American politics \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_4}
\end{table*}
\vspace{-10pt}
\subsubsection{Baseline and Our Methods} 
We propose eight benchmark methods. 
The first three are \emph{distribution}-based, while the next two 
are \emph{transformation}-based. 
The last three, namely MultiCCA,
MultiCluster and Duong are three popular bilingual word representation models for general use.   
Distribution-based methods compare lists of surrounding
English and Chinese terms, denoted as $L_E$ and $L_C$, 
by computing cross-lingual relatedness between two lists, 
though different baselines differ in the
selection of words and the way of computing similarities.
Transformation-based methods compute the vector representation 
in English and Chinese corpus respectively, and
then train a transformation.
Bilingual word representations based methods use the existing state-of-the-art models and then compute the similarities between two bilingual word vectors as $clsim$.
% of words, known $L_E$ and $L_C$. The differences of these methods are the selecting method of terms and %the computation method of two word lists.  ii) the second type of baseline methods are first obtain the %comparable vectorial representation of the English title and Chinese title of the given entity, and then just %calculate the similarity between two comparable vectors.
\begin{itemize}
 	\item \textit{Bilingual Lexicon Jaccard Similarity (BL-JS)}~~
 	%	The $L_E$ and $L_C$ of both BL-JS and WN-WUP  are the same as the lists that annotators judge.
 	BL-JS uses the bilingual lexicon to translate $L_E$  to a Chinese word list 
 	$L_E^*$ as a medium and then calculates the Jaccard Similarity between 
 	$L_E^*$ and $L_C$ as $J_{EC}$. Similarly, we can compute $J_{CE}$. 
 	Finally, we compute $\frac{J_{EC}+J_{CE}}{2}$ as the cross-cultural similarity 
 	of this given name entity.
 	
 	\item 	\textit{WordNet Wu-Palmer Similarity (WN-WUP)}~~ Instead of using 
 	the bilingual lexicon and Jaccard Similarity, WN-WUP uses Open Multilingual 
 	Wordnet~\cite{wang2013building,bond2013linking} to calculate the average 
 	similarity of two lists of words from different languages.
 	
 	\item \textit {Word Embedding based Jaccard Similarity (EM-JS)}~~ EM-JS is 
 	very similar to BL-JS, except that its $L_E$ and $L_C$ are generated by 
 	ranking the similarities between the name of entities and all English words 
 	and Chinese words respectively. 
 	
 	\item \textit {Linear Transformation (LTrans)}~~
 	We follow the steps in Mikolov et al.~\shortcite{Mikolov:2013tp} 
 	to train a transformation matrix between \textit{EnVec} and \textit{CnVec}, 
 	using 3000 translation pairs with confidence of 1.0 in the bilingual lexicon. 
 	Given a named entity, this solution simply calculates cosine similarity 
 	between the vector of its English name and the \textit{transformed} vector 
 	of its Chinese name. 
 	
 	\item 	\textit {Bilingual Lexicon Space (BLex)}~~
 	This baseline is similar to \textit{SocVec} but it does not 
 	utilize social word vocabularies and solely uses the bilingual lexicon.
 	
 	\item	\textit{MultiCCA}
 	\cite{ammar2016massively} This method takes two mono-lingual word 
embeddings and a bilingual lexicon as input and develop a bilingual word 
representations.  We use both the Microsoft bilingual lexicon (BL)
and the bilingual social lexicon (BSL) we constructed as the bilingual lexicon
to compare their effectiveness. Dimensionality is tuned from 
$\{50,100,150,200\}$ in all methods.
 	\item 	\textit{MultiCluster} \cite{ammar2016massively} 
 	This method requires re-training the bilingual word embeddings from the two mono-lingual corpora with a bilingual lexicon. We also use our BSL as 
an additional test (MultiCluster-BSL). 
 	\item	\textit{Duong} 
 	\cite{duong2016learning}
 	Similar to MutltiCluster, this method retrains the embeddings from 
mono-lingual corpora with an EM style training algorithm. 

\item \textit{Our SocVec-based method} With the help of our constructed \textit{\socvec},~given a named entity with its English and 
Chinese name, we simply compute the similarity between their 
\textit{SocVec}s as its cross-cultural difference score. 
\end{itemize}

\subsubsection{Experimental Results}

For qualitative evaluation, \tabref{tab:mcdne_res_4} shows some of 
the most culturally different entities mined by our method. 
The hot and trending topics on Twitter and Weibo are 
manually summarized to help explain the cultural difference. 
The perception of these entities diverges widely between English and
Chinese social networks, thus suggesting
significant cross-cultural differences.

\begin{table}[th]
	\small
	\centering
	\caption{{Comparison of Different Methods}}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Method} & \textbf{Spearman} & \textbf{Pearson}  & \textbf{MAP} \\ \hline\hline
		BL-JS& 0.276 & 0.265 & 0.644   \\ 
		WN-WUP  & 0.335 & 0.349 & 0.677 \\ 
		EM-JS & 0.221 & 0.210  & 0.571\\ 
		LTrans& 0.366 & 0.385  & 0.644  \\
		BLex& 0.596 & 0.595  & 0.765 \\ \hline 
MultiCCA-BL(dim=100)&0.325&0.343&0.651\\  
MultiCCA-BSL(dim=150)&0.357&0.376&0.671\\ 
MultiCluster-BL(dim=100)&0.365&0.388&0.693\\ 
MultiCluster-BSL(dim=100)&0.391&0.425&0.713\\ 
Duong-BL(dim=100)&0.618&0.627&0.785\\ 
Duong-BSL(dim=100)&0.632&0.651&0.813\\ \hline 
		SocVec:opn& 0.668 & 0.662   & \textbf{0.834} \\ 
		SocVec:all& \textbf{0.676} & \textbf{0.671}  & \textbf{0.834}\\ 
		SocVec:noun & 0.564 & 0.562 & 0.756 \\ 
		SocVec:verb & 0.615 & 0.618 & 0.779 \\ 
		SocVec:adj. & 0.636 & 0.639 & 0.800 \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_1}
\end{table}
\begin{table}[th]
	\centering
	\small
	\caption{{Evaluation of Different Similarity Functions}}
	\label{tab:mcdne_res_2}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Similarity} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline\hline
		PCorr. & 0.631 & 0.625 & 0.806\\ 
		L1 + M & 0.666 & 0.656 & 0.824 \\  
		Cos & \textbf{0.676} & 0.669 & \textbf{0.834} \\ 
		L2 + E & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
\end{table}

\begin{table}[th]
	\centering
	\small
	\caption{{Evaluation of Different Pseudo-word Generators}}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Generator} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline \hline
		Max. & 0.413 & 0.401 & 0.726\\ 
		Avg. & 0.667 & 0.625 & 0.831\\ 
		W.Avg. & 0.671 & 0.660 & 0.832 \\  
		Top & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_3}
\end{table}

In~\tabref{tab:mcdne_res_1}, we evaluate the benchmark methods and 
our approach with three metrics: Spearman and 
Pearson correlation on the ranking problem, and Mean Average Precision (MAP)
on the classification problem. 
The \textit{BSL} of \textit{SocVec:opn} uses only OpinionFinder as English socio-linguistic vocabulary, while \textit{SocVec:all} uses the union of Emapth and OpinionFinder vocabularies.\footnote{Having tuned the  parameters, we use the best parameters for the \textit{SocVec} methods: 5-word context window and 
150 dimensions used in training monolingual word vectors,
cosine similarity as the \textit{sim} function within the 
\textit{\socvec}~space, and ``\textit{Top}'' as the pseudo-word generator.} 
To show the effectiveness of social-linguistic vocabulary versus other type
of words as the bridge between the two cultures, we also compare the
results using sets of nouns, verbs and adjectives within the 
same \textit{SocVec} framework.
All vocabularies under comparison are of similar sizes 
(around 5000), which also indicates that the improvement of our method 
is not just the result of sparsity.
Results show that \textit{SocVec} models, and in particular, the
\textit{SocVec} model using the social words as cross-lingual media, 
performs the best. 

We also evaluate the effectiveness of four different similarity options in 
\textit{\socvec}, namely, Pearson Correlation Coefficient 
(\textit{PCorr}.), L1-normalized Manhattan distance (\textit{L1+M}), 
Cosine Similarity (\textit{Cos}) and  L2-normalized Euclidean distance (\textit{L2+E}).
%It is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking.
From~\tabref{tab:mcdne_res_2}, we conclude that among these four options, \textit{Cos} and \textit{L2+E} perform the best. 
%Although it is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking, we can find that 
\tabref{tab:mcdne_res_3} shows effect of using four different 
pseudo-word generator functions, from which we can infer that ``\textit{Top}'' generator function performs best for 
it reduces the noise brought by the less probable translation pairs. 

