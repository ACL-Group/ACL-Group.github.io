Review #1

Appropriateness:	5
Clarity:	4
Originality:	3
Soundness / Correctness:	4
Meaningful Comparison:	3
Substance:	4
Impact of Ideas / Results:	3
Impact of Accompanying Software:	1
Impact of Accompanying Dataset / Resource:	1
Recommendation:	3
Reviewer Confidence:	4
Comments

This paper discusses building a set of "sociolinguistic" bilingual word embeddings which are applied to two socially-relevant tasks, identifying words which have cross-cultural difference, and extracting a bilingual slang lexicon. I like the cross-cultural focus of this paper and the idea behind identifying cross-cultural difference. It's a cool idea to use a lexicon of social terms (please don't call it a sociolinguistic lexicon! Sociolinguists would hate this use of the term) to create the pivot into the new language that is focused on social difference. The approach itself is clear, simple, and apparently effective.

My main problem is that I'm not entirely convinced that the social terms are playing the role that the authors think they are; though the authors show that the social terms are better than just using all terms in this setup, there might be other reasons why everything is a bad choice, and I think a more thorough investigation of the effects of different kinds of lexicons is warranted. I also think the paper is missing a comparison to a more general, state-of-the-art bilingual word embedding system, for instance Duong et al (2016) (which like the method used here only requires a bilingual lexicon), and maybe other attempts to create more specific kinds of word embeddings, e.g Tang et al (2015). I think that it's quite plausible that the authors are correct, but I want more evidence than is provided here.

The annotation for first experiment is not about cross-cultural social difference, its simply labelling of the similarity of two word lists. In some sense, this is very clever, because it makes the task much more objective, but, on the other hand, it makes the task kind of circular, since both the method and the solution are defined in terms of distributional similarity; the social element is gone. I would have liked some deeper ground truth, perhaps in terms of a small set of terms which are superficially synonyms but which have very different cultural connotations. Here, all that matters is the words have appeared in different contexts in the time frame of the corpus.

For the other experiment, the authors are perhaps arguing that the vectors must encode more social information that helps the mapping from slang to slang : to be a good mapping, it must both mean the same thing and be slang, so the vector must particularly encode some notion of being slang. Fair enough, though (as with the first experiment) it could also do better by having a better distributional semantics representation as well, it doesn't prove that it has learned social difference. And the examples taken from the bilingual dictionary presented at the end of 3.3.1 do not back up this interpretation, since "foolish", "stubborn", "rude", and "impetuous" are not slang terms. Producing an dictionary explanation and producing a bilingual lexicon are not the same thing, and the difference is really important.

The idea of using average cosine similarity between word sets is actually a pretty terrible one, if for no other reason that a system could trivially produce identical vectors for every word in the vocabulary and get maximum performance. BTW, why is the average cosine difference greater than 1? Are the numbers in the table 6 X 100%?

There's a basic asymmetry in the model related to whether the original words or psuedowords are used for each language. It seems that English used regular words, and Chinese used the psuedo words. Did you consider the reverse? Is there anyway you could remove this asymettry, perhaps by going in both directions? (creating a vector of roughly twice the size?).

Review #2

Appropriateness:	5
Clarity:	4
Originality:	4
Soundness / Correctness:	4
Meaningful Comparison:	4
Substance:	4
Impact of Ideas / Results:	4
Impact of Accompanying Software:	1
Impact of Accompanying Dataset / Resource:	4
Recommendation:	4
Reviewer Confidence:	4
Comments

The research in this paper describes detection of cross-cultural differences in named entities and bilingual lexicon induction in for Internet slang.

-The paper reads easily. The data, method and experiments are well-described. -The results are well-described and linked with the research questions very well. -There are examples to explain the topic of discussion in hand. -Although the authors seem to be strong on the computational linguistics analyses, they lack references and explanations both on cross-cultural analyses and sociolinguistics. Here are some comments & recommendations to improve the paper and the awareness of authors.

-The correct spelling is "sociolinguistics" not "socio-linguistics". -What do "sociolinguistic vocabularies" or "socio-linguistic words" refer to? These notions are vague and the authors lack references to explain these terms to the readers. There is not a single reference to sociolinguistic research or computational aspects of sociolinguistics. The authors are recommended to read extensively on sociolinguistics and computational aspects of sociolinguistics in order to be able to use these terms and/or explain them in their research. -It is not clear how this research contributes to cross-cultural studies. What do the authors mean by "cross-cultural"? Which scientific discipline do they refer to? What are the key references they base their claims on? -Which Chinese dialect is this paper about? Mandarin? Cantonese? -Harris (1954) is a very old reference. Please search for more updated literature in the relevant field. -Footnote (7): what does it mean "four annotators are native chinese speakers but bilingual"? Which dialect do they speak? Are they Mandarin-English bilinguals? -The conclusion is not very explanatory. Why is this research important? What is the main contribution?

Review #3

Appropriateness:	5
Clarity:	4
Originality:	3
Soundness / Correctness:	4
Meaningful Comparison:	3
Substance:	4
Impact of Ideas / Results:	3
Impact of Accompanying Software:	4
Impact of Accompanying Dataset / Resource:	3
Recommendation:	3
Reviewer Confidence:	4
Comments

This paper presents (1) a method for combining word embeddings across languages for (2) the cross-cultural tasks of assessing differences in named entity use and slang dictionary building, with (3) open source tools and data.

While I enjoyed this paper, I will start with my main gripe: the approach is extremely similar in spirit to the multiCCA method proposed by Ammar et al. (https://arxiv.org/abs/1602.01925v2), and the authors overlook the entire body of work cited in that paper on the topic of cross-lingual (or multi-lingual) word embeddings. It's very possible that this is an oversight, but the paper should at minimum place itself in the broader context of this work and would be even better if it compared empirically against the other approaches (multiCCA and multiCluster do not require parallel corpora).

That said, the contributions of the novel tasks and the open-source tool counterbalance this shortcoming for me. The evaluations seem fairly thorough (comparisons of different methods, similarity functions, vector summary functions, etc), and the authors strike a good balance between quantitate and qualitative results for both tasks.

I enjoyed this paper, I think it fits with EMNLP. However, I think a more complete treatment in context of cross-lingual word embedding methods should be a requirement for acceptances.

