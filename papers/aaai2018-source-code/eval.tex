\section{Evaluation}
Our evaluation comes in two parts. In the first part, we evaluate Code-RNN
model's ability to classify different source code blocks into $k$ known
categories. In the second part, we show the effectiveness of our comment
generation model by comparing with several state-of-the-art approaches in
both quantitative and qualitative assessments. The source code of our
approach as well as all data set is available at \url{https://adapt.seiee.sjtu.edu.cn/CodeComment/}.

\subsection{Source Code Classification}
\subsubsection{Data Set}
%To evaluate wether \textbf{Code-RNN} is sufficient to extract method's function information, we apply \textbf{Code-RNN} to the function classification problem of methods. For classification, there are no existing data set for us to use, so we conduct
The goal is to classify a given Java method (we only use the body block without name and parameters) into a predefined set of
classes depending on its functionality.
Our data set comes from the Google Code Jam contest
(2008$\sim$2016), which there are multiple problems, each associated
with a number of correct solutions contributed by
programmers.~\footnote{All solutions are available at \url{http://www.go-hero.net/jam/16}.} Each solution is a Java method. The set of solutions for
the same problem are considered to function identically and belong
to the same class in this work. We use the solutions (10,724 methods) of
6 problems as training set and the solutions (30 methods) of the other 6
problems as the test set. Notice that the problems in the training data
and the ones in the test data do not overlap. We specifically design the
data set this way because, many methods for the same problem tend to use
the same or similar set of identifiers, which is not true in real world
application.  The details of training set and test set are shown in
Table \ref{table:method_data}.



\renewcommand{\multirowsetup}{\centering}
\begin{table}[th]
\caption{Data Sets for Source Code Clustering}
\label{table:method_data}
\center
\scriptsize{
\begin{tabular}{|c|c|c|c|}
 \hline
  & Problem & Year &\# of methods  \\
 \hline \hline
 \multirow{6}{1.5cm}{Training Set} & Cookie Clicker Alpha & 2014 &%1008 301 330
 1639\\
 \cline{2-4}
  & Counting Sheep & 2016 &%1091 310  321
  1722\\
 \cline{2-4}
  & Magic Trick & 2014 &%1383 419 432
  2234\\
 \cline{2-4}
 & Revenge of the Pancakes & 2016 &%740 234 240
 1214\\
 \cline{2-4}
 & Speaking in Tongues & 2012 &%1064 316 309
 1689\\
 \cline{2-4}
 & Standing Ovation & 2015 &%1362 417 447
 2226\\
 \hline
 \multirow{6}{1.5cm}{Test Set} & All Your Base & 2009 & 5 \\ %&122\\
 \cline{2-4}
 & Consonants & 2013 & 5 \\ %&123\\
 \cline{2-4}
 & Dijkstra & 2015 & 5 \\ %&120\\
 \cline{2-4}
 & GoroSort & 2011 & 5 \\%&129\\
 \cline{2-4}
 & Osmos & 2013 & 5 \\ %&126\\
 \cline{2-4}
 & Part Elf & 2014 & 5 \\ %&103\\
 \hline
\end{tabular}
}
\end{table}

%\subsubsection{Inter-node Code-RNN}
%
%\KZ{This section is too short and not sure what you are trying to say.}
%This model delete all leaf nodes of original Code-RNN that means the text in the source code are all deleted from parse tree and the neural network only contains structure information of source code now. And this model's structure and equations are same as \textbf{Code-RNN}. Forward-propagation and back-propagation are like Code-RNN too.


\subsubsection{Baselines}

We compare Code-RNN with two baseline approaches.
The first one is called language embedding (\emph{LE}) and only treats the
source code as a sequence of words, minus the special symbols
(e.g., ``\$'', ``('', ``+'', $\cdots$). All concatenated words are preprocessed
into primitive words as previously discussed. Then the whole code can be
represented by either the sum (\emph{LES}) or the average (\emph{LEA})  of word vectors
of this sequence, trained in this model.%pretrained from \KZ{from where?} 
This approach basically focuses on the word semantics only and ignores the
structural information from the source code.

The second baseline is a variant of Code-RNN, which preprocesses the code parse
tree by consistently replacing the identifier names with placeholders before
computing the overall representation of the tree. This variant focuses on
the structural properties only and ignores the word semantics.


\subsubsection{Result of Classification}
At test time, when a method is classified into a class label,
we need to determine which test problem this class label refers to.
To that end, we compute the accuracy of classification for
all possible class label assignment and use the highest
accuracy as the one given by a model.
%The values of rename test set and original test set are in the TABLE \ref{table:purity}. And Code-RNN Average model is the largest one of all.

Table \ref{table:purity} shows the purity of the produced classes, the F1
and accuracy of the 6-class classification problem by different methods. It is
clear that Code-RNN (avg) perform better uniformly
than the baselines that use only word semantics or only structural information.
Therefore, in the rest of this section, we will use Code-RNN(avg) model to
create vector representation for a given method to be used for
comment generation.
The F1 score for each individual problem is also included in
Table \ref{table:F1_30}.
%Once again, Code-RNN (avg) beats the other methods in most of the methods.

\begin{table}[th]
\caption{Purity, Average F1 and Accuracy}
\label{table:purity}
\center
\scriptsize{
\begin{tabular}{lccc}
 \hline
 & Purity &  F1 & Accuracy \\ %& All\\
 \hline
 LEA & 0.400 &  0.3515  & 0.3667\\% & \textbf{0.3734}\\
 LES & 0.3667 & 0.2846 & 0.3667 \\%& 0.3582\\
CRA(ni) &  0.4667 & 0.4167 & 0.4667\\
CRS(ni) &  0.4667 & 0.4187 & 0.4667 \\
 CRA & \textbf{0.533} &  \textbf{0.4774} & \textbf{0.5} \\% & 0.3513\\
 CRS & 0.4667 & 0.3945 & 0.4333\\% & 0.3610\\
 \hline

\end{tabular}

\begin{tablenotes}
  \item[1] LEA = Language Embedding Average model;
  LES = Language Embedding Sum model;
  CRA = Code-RNN Average model;
  CRS = Code-RNN Sum model; (ni) = no identifier.
\end{tablenotes}
}
\end{table}




\begin{table}[th]
\caption{F1 scores of individual problems}
\center
\scriptsize{
\label{table:F1_30}
\begin{tabular}{c@{\ \ \ \ }c@{\ \ \ \ }c@{\ \ \ \ }c@{\ \ \ \ }c@{\ \ \ \ }c@{\ \ \ \ }c@{\ \ \ \ }c}
 \hline
 %problem name
 &Dijkstra&Part Elf&All Your Base&GoroSort&Consonants&Osmos \\
 \hline
 %Language Embedding Average
 LEA
 & 0.25 & {0.33} & 0.43 & {0.33} & 0.4 & 0.36 \\
 %Language Embedding Sum
 LES
 & 0.33 & 0 & 0.53 & 0 & 0.53 & 0.31 \\
 CRA(ni) & 0.6 & 0 & {0.44} & {0.40} & {0.56} & 0.5 \\

 CRS(ni) & {0.62} & 0.29 & {0.67} & {0.5} & 0.44 & 0 \\
 %Code-RNN Average
 CRA
 & {0.67} & 0 & {0.6} & {0.57} & {0.53} & {0.52} \\
 %Code-RNN Sum
 CRS
 & {0.73} & {0} & 0.44 & 0.55 & 0.4 & 0.25 \\
\hline
\end{tabular}
}

\end{table}






\subsection{Comment Generation Model}
\subsubsection{Data Set}
We use ten open-source Java code repositories from GitHub for this
experiment (see Table \ref{table:repos}). In each of these repositories
we extract descriptive comment and the corresponding method pairs. Constructor
methods are excluded from this exercise.  These pairs are then used
for training and test. Notice that all the method names and parameters are excluded from
training and test.




\subsubsection{Baselines}
We compare our approach with four baseline methods.

\begin{itemize}
\item \emph{Moses}\footnote{Home page of Moses is \url{http://www.statmt.org/moses/}.}
 is a statistical machine translation system.
We regard the source codes as the source language and the comments as
the target, and use Moses to translate from the source to the target.
\item \emph{CODE-NN}~\cite{iyer2016summarizing} is the first model to use neural network to create sentences for source code. In this model author used LSTM and attention mechanism to generate sentences. The original data set for CODE-NN are
StackOverFlow thread title and code snippet pairs. \footnote{Data comes from
\url{https://stackoverflow.com/}. Source code of CODE-NN is available
from \url{https://github.com/sriniiyer/codenn}.}. In this experiment,
we use the comment-code pair data in place of the title-snippet data.
\item We apply the \emph{sequence-to-sequence (seq2seq)} model used
in machine translation \cite{Britz:2017} and treat the code as
a sequence of words and the comment as another sequence.
\item A. Karpathy and L. Fei-Fei~\cite{karpathy2015deep} proposed a meaningful method to generate image descriptions. It also used Recurrent NN and representation vector, so we apply this method to comment generation model. The main equations are:
\begin{align}
b_{v} &= W_{hi}V_{m}\\
h_{t} &= f(W_{hx}x_{t} + W_{hh}h_{t-1} + b_{h} + b_{v})\\
y_{t} &= softmax(W_{oh}h_{t} + b_{o})
\end{align}
where $W_{hi}$, $W_{hx}$, $W_{hh}$, $W_{oh}$, $x_{i}$ and $b_{h}$, $b_{o}$ are
parameters to be learned, and $V_{m}$ is the method vector.
We call this model \emph{Basic RNN}.
%We also combine our method representation vector with LSTM and GRU.
%To include our method vector into these two models, we do the following
%transformation:
%\begin{align}
%\tilde{h_{t}} &= tanh(W_{h} h_{t} + W_{v} V_{m})\\
%y_{t} &= softmax(W_{oh}\tilde{h_{t}} + b_{o})
%\end{align}
%where $h_{t}$ is the output state vector of LSTM or GRU cell and $V_{m}$ is the method vector.
\end{itemize}
Moses and CODE-NN has its own terminate condition. Seq2Seq, Basic RNN and
our model run 800 epochs during training time.
For one project, we separate the commented methods into three parts:
training set, validation set and test set. We tune the hyper parameter on
the validation set. The results of ten repositories are shown
in Table \ref{table:rouge}.

\subsubsection{Evaluation Metric}
We evaluate the quality of comment generation by the Rouge
method\cite{lin2004rouge}.
%Rouge has been used in many summary works\cite{donahue2015long,haghighi2009exploring,rush2015neural} as measurement standard.
Rouge model counts the number of overlapping units between
generated sentence and target sentence.
We choose Rouge-2 score in this paper where word based 2-grams are used as
the unit, as it is the most commonly used in evaluating automatic text
generation such as summarization.







\begin{table}[th]
\caption{Rouge-2 Values for Different Methods}
\center
\scriptsize{
 \begin{tabular}{@{\ }p{0.08\columnwidth}@{\ \ \ \ }c@{\ \ }c@{\ \ }c@{\ \ }p{0.065\columnwidth}@{\ \ }c@{\ \ }c@{\ \ }c@{\ \ }p{0.08\columnwidth}@{\ \ }c@{\ \ }c@{\ }}
 \hline
  %\diagbox{Model}
  & neo4j &cocos2d & jersey & aima-java & guava & Smack & Activiti & spring-batch & libgdx & rhino\\
 \hline
MOSES
& 0.076 & 0.147 & 0.081 & 0.144 & 0.134 & 0.145 & 0.104 & 0.147 & 0.212 & 0.082\\
\hline
CODE-NN
& 0.077 &{0.136} & 0.105 & 0.124 &0.153 & 0.135 & 0.103 & 0.184 & 0.208 & \textbf{0.171}\\
\hline
Seq2seq
& 0.039 & 0.115 & 0.183 & 0.108 & 0.152 & 0.109 & 0.158 &0.171  &  \textbf{0.247} & 0.169 \\ \hline
Basic RNN*
& 0.133 & 0.152 & 0.214 & {0.207} & 0.156 & 0.150 & \textbf{0.203} & \textbf{0.237} & 0.218 & 0.163\\
\hline
Code-GRU*
& \textbf{0.141} & \textbf{0.158} & \textbf{0.230} & \textbf{0.209} & \textbf{0.164} & \textbf{0.162} & 0.200 & 0.213 &  0.233 & 0.165\\
\hline
% CGRUAtt
%& {0.184} & 0.136 &{0.145} & {0.155} &  & {0.167} & {0.139} & {0.217} & 0.237\\
%\hline
 \end{tabular}
 \begin{tablenotes}
  \item[1] *: both models use the method representation vector from Code-RNN.
\end{tablenotes}
}

\label{table:rouge}
\end{table}



\begin{figure*}[!htb]
\centering
\scriptsize{
%/**
%     * If the lock state matches the given stamp, performs one of
%     * the following actions. If the stamp represents holding a write
%     * lock, releases it and obtains a read lock.  Or, if a read lock,
%     * returns it. Or, if an optimistic read, acquires a read lock and
%     * returns a read stamp only if immediately available. This method
%     * returns zero in all other cases.
%     *
%     * @param stamp a stamp
%     * @return a valid read stamp, or zero on failure
%     */
% Project: jersey
%    jersey.repackaged.jsr166e.StampedLock.tryConvertToReadLock
\rule{\linewidth}{0.3mm}
\\[0.2cm]
\begin{minipage}{0.48\linewidth}
\centering
\hspace{.2cm}
\begin{lstlisting}
Project: jersey
    public long tryConvertToReadLock(long stamp) {
        long a = stamp & ABITS, m, s, next; WNode h;
        while (((s = state) & SBITS) == (stamp & SBITS)) {
            if ((m = s & ABITS) == 0L) {
                if (a != 0L)   break;
                else if (m < RFULL) {
                    if (U.compareAndSwapLong(this, STATE, s, next = s + RUNIT))
                        return next;}
                else if ((next = tryIncReaderOverflow(s)) != 0L) return next;}
            else if (m == WBIT) {
                if (a != m)     break;
                state = next = s + (WBIT + RUNIT);
                if ((h = whead) != null && h.status != 0)
                    release(h);
                return next;}
            else if (a != 0L && a < WBIT)   return stamp;
            else  break;}
        return 0L;}
\end{lstlisting}
\end{minipage}
 \hfill
\begin{minipage}{0.48\linewidth}
\begin{tabular}{|c|p{0.75\columnwidth}|}
 \hline
   {Gold} & if the lock state matches the given stamp performs one of the following actions if the stamp represents holding a write lock releases it and UNK a read lock or if a read lock returns it or if an optimistic read acquires a read lock and returns a read stamp only if immediately available this method returns zero in all other cases\\
 \hline
   {MOSES} & if the lock state matches the given if the lock state matches the given gets of processing sbits state matches the given sbits string of the lock hold abits l l break component of rfull that runs sets of processing of runit or create a new pattern if this inc reader overflow l create a human readable description of component of wbit break if the lock of processing of wbit runit h whead by the given status release the given action if the sum associated with the given component l lock state matches the given action wbit get returns break l
   \\
 \hline
   {CODE-NN} & returns code true if the lock is not a link org glassfish jersey server mvc\\
 \hline
   {Seq2Seq} & UNK a new item to the list of superclass timeout version\\
 \hline
   {Basic RNN*} & get a UNK to a link javax ws rs core UNK\\
 \hline
  {Code-GRU*} & if the lock state matches the given stamp performs one of the following actions if the stamp represents holding a write lock returns it or if a read lock if the write lock is available releases the read lock and returns a write stamp or if an optimistic read returns\\
 \hline
 \end{tabular}
 \end{minipage}
 \\[0.2cm]
 \rule{\linewidth}{0.3mm}
 \\[0.2cm]
\vfill
 \begin{minipage}{0.48\linewidth}
 \hspace{.2cm}
% /**
%     * Calculates dot product of two points.
%     *
%     * @return float
%     */
% project: cocos2d
%    org.cocos2d.types.CGPoint.ccpDot
\begin{lstlisting}
project: cocos2d
    public static float ccpDot(final CGPoint v1, final CGPoint v2) {
        return v1.x * v2.x + v1.y * v2.y;    }
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
\begin{tabular}{|c|p{0.75\columnwidth}|}
 \hline
   {Gold} & Calculates dot product of two points \\
 \hline
   {MOSES} & subtract another subtract another the given vector \\
 \hline
   {CODE-NN} & rotates two points \\
 \hline
   {Seq2Seq} & returns the closest long to the specified value \\
 \hline
   {Basic RNN*} & calculates cross product of two points \\
 \hline
   {Code-GRU*} & calculates cross product of two points \\
 \hline
 \end{tabular}
 \end{minipage}
 \\[0.2cm]
\rule{\linewidth}{0.3mm}
\\[0.2cm]
\vfill
 \begin{minipage}{0.48\linewidth}
% /** Creates an int buffer based on a newly allocated int array.
%         *
%         * @param capacity the capacity of the new buffer.
%         * @return the created int buffer.
%         * @throws IllegalArgumentException if {@code capacity} is less than zero.
%         * @since Android 1.0 */
% project: libgdx
%    java.nio.IntBuffer.allocate
\hspace{.2cm}
\begin{lstlisting}
project: libgdx
    public static IntBuffer allocate (int capacity) {
      if (capacity < 0) {
        throw new IllegalArgumentException();
      }
      return BufferFactory.newIntBuffer(capacity);}
\end{lstlisting}
% org.mockito.asm.tree.analysis.Frame.push
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
\begin{tabular}{|c|p{0.75\columnwidth}|}
 \hline
   {Gold} & creates an int buffer based on a newly allocated int array\\
 \hline
   {MOSES} & based on the creates a new backing buffer\\
 \hline
   {CODE-NN} & creates a byte buffer based on a newly allocated char array\\
 \hline
   {Seq2Seq} & creates a float buffer based on a newly allocated float array \\
 \hline
   {Basic RNN*} & creates a char buffer based on a newly allocated char array\\
 \hline
   {Code-GRU*} & creates a long buffer based on a newly allocated long array\\
 \hline
 \end{tabular}
 \end{minipage}
 \\[0.2cm]
 \rule{\linewidth}{0.3mm}


}
\caption{Examples of generated comments and corresponding code snippets}
\label{fig:comment_example}
\end{figure*}


\subsubsection{Examples of Generated Comment}
Fig. \ref{fig:comment_example} shows the comments generated by
the competing methods for three example Java methods coming from
different repositories.
Because we delete all punctuation from the training data,
the generated comments are without punctuation. Nonetheless, we can see that
comments by our Code-GRU model are generally more readable and meaningful.

In the first example, we can see that CODE-NN, Seq2Seq and Basic RNN's results are
poor and have almost nothing to do  with the Gold comment.
Even though both MOSES produces a sequence of words that look similar to the Gold
in the beginning, the rest of the result is less readable and does not have
any useful information. For example, ``if the lock state matches the given'' is output
repeatedly. MOSES also produces strange terms such as ``wbit'' and ``runit'' just
because they appeared in the source code.
In the contrast, Code-GRU's result is more readable and meaningful.

%In the first example, CODE-NN, Seq2Seq and our models are not the same as
%original comment but our model's results have the same meaning while
%CODE-NN's result change the type of value from ``double'' to ``long'' and Seq2Seq change to ``int''.
%In the third example, there are two identical types of values, but CODE-NN
%produce different types.

In the second example, there is not any useful word in the method body
so the results of MOSES, CODE-NN and Seq2Seq are bad.
Code-RNN can extract the structural information of source code and embed it
into a vector, so both models that use this vector, namely Basic RNN and
Code-GRU, can generate the relevant comments.

%We also note that Code-GRU performs better than Basic RNN. For example
%in the second example.
In the third example, although all results
change the type of the value, that is, Basic RNN changes ``int'' to ``char''
while Code-GRU changes to ``long''.  ``long'' and ``int'' are both numerical
types while ``char'' is not. Thus Code-GRU is better than Basic RNN.
For the result of Seq2Seq, although ``float'' is also a numerical type,
it is for real numbers, and not integers.

%For the first example, our model use the word ``mapped'' to replace the word ``dispatch'' in the comment, and these two words have the similar meaning. The situation of replacing words also appears in the third example. ``sets the parent of the {\color{red}{new}} statement'' in the original comment while ``sets the parent of the {\color{red}{declaration}} statement'' in the generated comment by our model. Our comment specify the kind of statement not just the ``new statement'' in the original comment.
%
%In the second example, comparing with the original comment our model's generated comment maintain the most important part ``the number of vertices''.
%
%However, the comments generated by our model has one problem is that the front part is quite different with the original comments while the latter part almost the same as original comments. The third example also has this problem. This may because we feed the method vector at every step and after some steps the influence of method vector is big enough to generate the correct comments. So we can highlight the influence or weight of method vector at the beginning steps of Recurrent Neural Network to improve our model in the future work.

