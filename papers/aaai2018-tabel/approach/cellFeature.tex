\subsection{Mention and Context Feature}
\label{sec:cell}

As shown in \figref{fig:overview}, the \textbf{m}ention and \textbf{c}ontext feature represents
the relevance or compatiblity between the mention table $X$ and the entity table $E$.
Both features aggregate the individual features of each cell,
and thus share a similar neural network structure.
%similar structure
%aggrgation from each cell
%mention feature & context feature

We first introduce the \textbf{m}ention feature.
For the surface form $x_{ij}$,
%let $\bi{v}_{ij}^{(m)}$ denote the translated vector representation
%of the original mention embedding $\bi{x}_{ij}^{(m)}$.
we concatenate the translated embedding $\bi{v}_{ij}^{(m)}$ with the entity embedding $\bi{e}_{ij}$~\cite{socher2013reasoning,socher2013recursive},
then feed into a fully connected layer,
obtaining the hidden feature between $x_{ij}$ and $e_{ij}$ at mention level.
We apply vector averaging over all cells to be linked
and finally produce the hidden mention feature $\bi{h}^{(m)}$
between the whole mention and entity table. 
We formulate the steps as follows:

\begin{equation}
  \label{eqn:cell}
  \begin{aligned}
    & \bi{f}_{ij}^{(m)} & = & \mbox{ReLU}(W^{(m)} [\bi{v}_{ij}^{(m)}; \bi{e}_{ij}] + \bi{b}^{(m)}) \\
    & \bi{h}^{(m)}      & = & \frac{1}{|P|} \sum_{(i,j) \in P} \bi{f}_{ij}^{(m)},
  \end{aligned}
\end{equation}
\noindent
where $W^{(m)}$ and $\bi{b}^{(m)}$ are model parameters.

%As shown in \figref{fig:overview}, the output of our model is a score, which represents the linking confidence between a mention table and a candidate entity table. This score comes from two categories. One is to measure how similar or compatible two tables are. We employ two features called cell feature and context feature to capture the compatibility between mention table and candidate entity table.
%
%After translation layer, each mention embedding $v_{\textbf{m}_{ij}}$ is converted into the same vector space as entity embedding. We concatenate translated mention embedding $\widetilde{v_{\textbf{m}_{ij}}}$ with entity embedding $v_{\textbf{e}_{ij}}$ and then go through a fully connected layer to get a hidden feature for a pair of cells $\langle \textbf{m}_{ij}, \textbf{e}_{ij}\rangle$. After averaging among all cells which need to be linked, we get the cell feature, which now represents a pair of tables $\langle T_M, T_E \rangle$.
%
%\begin{equation}
%f_{cell}(T_M, T_E) = \frac{1}{|P|}\sum_{\langle i,j\rangle \in P}FC([\widetilde{v_{\textbf{m}_{ij}}}, v_{\textbf{e}_{ij}}])
%\end{equation}
%
%Where $FC$ represents fully connected layer, and $[v_1, v_2]$ means vector concatenation.

The \textbf{c}ontext feature follows the similar idea.
Instead of using the surface form $x_{ij}$ itself,
mentions in the same row or column (excluding itself) contain strong relatedness
and hence be regarded as the surrounding context.
In this way, we define the context embedding $\bi{x}_{ij}^{(c)}$
as the average mention embedding of those surrounding cells:
%formulated as follows:
%we discover the semantics derived from the surrounding cells.
%we choose context embedding $\bi{v}_{ij}^{(c)}$ to concatenate with the entity embedding~\cite{socher2013reasoning,socher2013recursive}.
%The context embedding $\bi{x}_{ij}^{(c)}$ of cell $x_{ij}$ is the average of
%all mention embeddings of cells which are in the same row or the same column as 
%$x_{ij}$ (exclude itself), formulated as follows:
\begin{equation}
%  \begin{gathered}
    \bi{x}_{ij}^{(c)} = \frac{1}{|R+C-1|}(
      \sum_{(i,k), k \neq j} \bi{x}_{ik}^{(m)} +
      \sum_{(k,j), k \neq i} \bi{x}_{kj}^{(m)}
    ).
%    \bi{v}_{ij}^{(c)} = W_t \bi{x}_{ij}^{(c)} + \bi{b}_t.
%  \end{gathered}
\end{equation}
\noindent
After applying the translation module,
the context embedding $\bi{v}_{ij}^{(c)}$ of each cell is used to generate
the hidden context feature of the mention-entity table pair, denoted by $\bi{h}^{(c)}$.
The calculation is almost the same as \eqnref{eqn:cell},
but just replacing all the mention embeddings by the context ones.
%Despite that table offers rare ``strict'' context information for entity disambiguation,
%mentions in the same row or column contain strong relatedness and can be regarded as surrounding context. 
By learning the mention and context features,
we can capture a general sense of semantic relatedness of all mention-entity pairs from two tables.


%\begin{equation}
% \begin{aligned}
%   & \bi{v}_{ij}^{(c)} & = & \frac{1}{|R+C-1|}(
%     \sum_{(i,k), k \neq j} \bi{v}_{ik}^{(m)} +
%     \sum_{(k,j), k \neq i} \bi{v}_{kj}^{(m)}
%   ) \\
%   & \bi{f}_{ij}^{(c)} & = & relu(W^{(c)} [\bi{v}_{ij}^{(c)}; \bi{e}_{ij}] + \bi{b}^{(c)}) \\
%   & \bi{h}^{(c)}      & = & \frac{1}{|P|} \sum_{(i,j) \in P} \bi{f}_{ij}^{(c)},
% \end{aligned}
%\end{equation}
%
%\begin{equation}
%	f_{cxt}(T_M, T_E) = \frac{1}{|P|}\sum_{\langle i,j\rangle \in P}FC([\widetilde{vcxt_{\textbf{m}_{ij}}}, v_{\textbf{e}_{ij}}])
%\end{equation}
%
%\begin{equation}
%vcxt_{\textbf{m}_{ij}} = \frac{1}{|\pi_{ij}|}(\sum_{\langle i,k\rangle \in P, k \neq j} v_{\textbf{m}_{ik}} + \sum_{\langle k,j\rangle \in P, k \neq i} v_{\textbf{m}_{kj}})
%\end{equation}
%\noindent
%where $\bi{h}^{(c)}$ denotes the hidden context feature of the mention-entity table pair.
%Despite that table offers rare ``strict'' context information for entity disambiguation,
%mentions in the same row or column contain strong relatedness and can be regarded as surrounding context. 
%By learning cell feature and context feature from mention table and candidate entity table,
%we can capture a general sense of semantic relatedness of all mention-entity pairs from two tables.
