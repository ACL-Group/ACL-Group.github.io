\section{Implementation Details}
\label{sec:impl-detail}

We show the detailed implementations of our model, which include 
candidate generation, translation model pre-train, and parameter tuning.

\noindent
\textbf{Candidate Generation:}
we first use several translation tools provided by
Google\footnote{http://translate.google.cn},
Baidu\footnote{http://fanyi.baidu.com} and
Tencent\footnote{http://fanyi.qq.com}. 
After retrieving English translations, we use several heuristics
to find candidate English entities in Wikipedia for each Chinese mention.
We add all anchor entities whose anchor text exactly match the each 
translation with confidence $1.0$. Then we remove all the stop words from 
the translations and from the anchor texts of Wikipedia, and compute the 
Jaccard similarity between the two modified forms in order to fetch more 
candidate entities. We use the Jaccard similarity score as the confidence
score here.

\noindent
\textbf{Translation Model Pre-Train:}
we collect a bilingual lexicon of common words using
Bing Translate API~\footnote{http://www.bing.com/translator},
containing 91,346 translation pairs at word level.
Each pair has a confidence score ranging from 0 to 1.
We remove the pairs with score less than 0.5,
and further select those pairs in which both the Chinese and English word
perfectly match the name of an article in Wikipedia.
In total 3,655 translation pairs are picked as our pre-train dataset.
%After filtering, we finally pick 3655 translation pairs out of 23,863 pairs as our pre-train dataset, with score no smaller than 0.5 for each pair.

\noindent
\textbf{Parameter Tuning:}
%We implement RankNet~\cite{burges2010ranknet} as the pairwise ranking algorithm.
%we tune the following parameters in our joint model:
%we list the parameter tuning details as below.
\begin{itemize}
\item The size of candidates per mention (denoted by $N_{cand}$) is
in the range of \{1, 3, 5, 10, 20, 30, 40, 50\}, 
\item The number of negative entity tables per mention table (denoted by $N_{tab}$) is in \{9, 19, 49, 99\},
\item The dimension of cell, context and overall features ($d_{cell}$, $d_{cont}$ and $d_{out}$) are in \{20, 50, 100, 200\},
\item The learning rate $\eta$ is in \{0.0002, 0.0005, 0.001\},
%\item The L1- and L2- regularization $l_1, l_2$ in \{0.0001, 0.0002, 0.0005, 0.001\}.
\item We apply dropout layers~\cite{srivastava2014dropout} on each hidden feature vector to avoid overfitting.
The keep probability $p$ of dropout layers is in \{0.5, 0.6, 0.7, 0.8, 0.9\}.
\end{itemize}

%All the parameters are tuned on the validation set, the detail evaluation metric is discussed in \secref{sec:exp-e2e-results}.


