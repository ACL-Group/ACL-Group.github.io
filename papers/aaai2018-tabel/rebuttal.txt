R1.

[question 1.] The intuition that entities in same column share the same type was present in previous methods. But we are the first to embed this idea into 
a neural network model by a feature calculated using element-wise variance. We argue it is a novel and effective design.

[question 2.] Our method/algorithm actually makes no use of the Chinese wiki entities or entries at all. However, in the evaluation, because 
it is difficult to construct large test data set, in which Chinese mentions are linked directly to  English wiki entries,
we resort to wiki interlanguage links to help us construct large number of ground truth examples. That's why the mentions in our test data
have both English and Chinese wiki entries. 

R2.

[question 1.] If by web table recognition, you mean "understanding of web tables", i.e. the ability to extract entities and their 
relations from the web tables, then the work in this paper, which is linking entities to an existing knowledge base is an important step toward that goal. 

[question 2.] Figure 1 is indeed our running example. The same example is used in the ablation evaluation. In the revised version, we will add more details such as the candidates of each cell to this example and use the same examaple in the candidate generation step of the approach section. Thanks for the advice!

R3.

Thank you and we appreciate your support!

R4.

[question] This paper did not claim that table linking is a problem any harder than regular text entity linking. We instead argue that
this is a different problem to which existing regular EL methods don't directly apply. This is because i) the context for mentions in plain text is a sequence of words that is one-dimensional, whereas the context for a table cell is both a column and a row and thus two-dimensional; ii) words that are further way in the context of a mention in text are assumed to be less important in existing methods, whereas no such assumptions can be made about the cells in the context of a mention since the order of the columns and rows in a table is often insignificant; iii) the coherence feature that we take advantage in this paper applies only to a table column or a row, but not to word
sequences in text. 

[comment 2.] In section 3.5, we made the following statement: "To this end, we use a local search descent algorithm to approximate 
the optimal solution." Thus, we are not claiming the novelty of this algorithm. The local search descent algorithm used here is an efficient greedy heuristic while simulated annealing is a stochastic method that typically takes longer to converge though is more likely to achieve global optimal. Our algorithm will converge after 6 rounds of search on average. The micro acc. of our initial state as a solution is 0.537 (comparing to 0.629 at the end of local search descent). 

[comment 3.] First, given the definition of our cross-lingual table linking problem, we assume no foreign language knowledge base except the English knowledge base is available. 
Therefore, a translation tool of some sort is the only possible bridge between the two languages. Second, as the top half of Table 2 
and Fig. 3 show, given candidates obtained by the same translation tool (Baidu), our ranking model clearly outperforms other existing approaches. 

[comment 4.] "Baidu only" means to use only one translation tool (i.e. Baidu) to generate only one English mention per table cell. It doesn't mean that we only generate the topmost candidate. Instead, we still generate a set of candidate entities based on the English mention. In the second paragraph of section 5.4, candidate size is set to 30 in our experiments. Thus, the HITS@1 numbers in Table 1 and Baidu-only numbers in Table 2 have different meanings and they should not match. The reason we include Baidu-only model in our evaluation is the other two systems only handle the case of one English mention per cell therefore only one translation tool is to be used (also mentioned in paragraph 2 of section 5.4). We pick Baidu because it gives the most reasonable translation results. The key point here is that we stick to the same tool for all competing methods, so the comparison is fair (see the top half of Table 2).

