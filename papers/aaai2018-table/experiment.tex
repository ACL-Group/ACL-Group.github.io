\section{Experiments}
\label{sec:eval}



%In this section, we evaluate our table linking system and compare with other previous works
%on the cross-lingual task.
%In this section, we introduce the datasets and state-of-the-art systems used in our experiments,
%and explain how to adapt a mono-lingual entity linker into our scenario.
%We show the end-to-end results of all the systems on cross-lingual and mono-lingual dataset, 
%and perform ablation experiments to investigate the importance of different components used in the whole task.
%Finally, we discuss and analyze the errors in our system.
\noindent
\textbf{Experiment Setup:}
we use the Feb. 2017 dump of English and Chinese Wikipedia
%\footnote{{\scriptsize https://dumps.wikimedia.org/}}
as the text corpora for training word and entity embeddings.
Our cross-lingual table linking dataset consists of 150 web tables with Chinese mentions and
the linked English entities\footnote{
{\scriptsize The original Chinese tables are created by Wu et al.~\shortcite{wu2016entity}, which contains 120 tables extracted from Chinese Wikipedia. We collect another 30 and transform all the Chinese entities into English via inter-language links of Wikipedia. In total we collect 3,818 mentions with 2,883 linkable positions.}}.
%The original Chinese tables are created by Wu et al.~\shortcite{wu2016entity},
%which contains 123 tables extracted from Chinese Wikipedia, and each mention is labeled by
%its corresponding Chinese Wiki article.
%We collect another 30 Chinese tables with similar size from Web and
%transform all the Chinese entities into English via inter-language links of Wikipedia,
%producing the labeled English entities for 81\% of the entire mentions.
%In addition, we discard long-tail tables,
%if the shape or the number of labeled English entities of which is too small.
%In total, we collected 3818 mentions from 150 tables, with 2883 linkable positions
%mentions been linked to English entities
%(19.22 per table).
We randomly split the dataset into training / validation / testing sets (80 : 20 : 50 tables).
%The dataset is publicly available in http://xxxxxx.
%and will publish the dataset later.

\noindent
\textbf{End-to-end Results:}
we compare our cross-lingual table linking model with three different models. 
$TabEL_B$ \cite{bhagavatula2015tabel} and $TabEL_W$ \cite{wu2016entity} are state-of-the-art mono-lingual table linking systems. $TextEL$ \cite{zhang2013cross} is a cross-lingual text linking system. To make fair comparison, we firstly convert each mention into the most likely English translation, then run the mono-lingual models on these translated English tables.
For $TextEL$, we traverse each mention in row order and flatten the whole table into a word sequence, turning the table into unstructured texts. In addition, we evaluate our approach using either translation pre-train ($W_t$ and $\bi{b}_t$) or without it.

\vspace*{-\baselineskip}
\begin{table}[ht]
	\small
	\centering
	\caption{Accuracies on cross-lingual table linking task}
	\label{tab:main-result}
	\begin{tabular} {c|c|cc}
		\hline
		Approach          & Micro Acc.   & Macro Acc.    \\
		\hline
		$TabEL_B$         &  0.512       & 0.507         \\
		$TabEL_W$         &  0.514       & 0.519         \\     %N_{cand} = 10
		$TextEL$          &  0.472       & 0.458         \\
	%	Ours (Baidu Only) &  0.576       & 0.573         \\
		\hline
		Ours (- pre-train) &  0.606    &  0.591        \\ 
		Ours (+ pre-train)  &  \textbf{0.629}       & \textbf{0.614}         \\
		\hline
	\end{tabular}
\end{table}


We report the experimental results in \tabref{tab:main-result}.
Our model outperforms the other baseline models, improving the result by up to 12.1\%.
%Our full model even improves the Micro Accuracy by an absolute gain of 0.053,
%showing the importance of combining multiple translating tools.
Besides, the pre-train step also raises the Micro Accuracy by 0.023.
%3. compared with TabEL, ()
Both $TabEL_B$ and $TabEL_W$ suffer from the error propagation problem, % in the translating step,
since only the top translation is considered,
%as the mono-lingual approaches take translated mentions
%as direct input, a poor translating quality harms the final result.
whereas our approach generates candidate entities from multiple translated mentions,
%takes Chinese mention as the input, such end-to-end approach
which alleviates the error brought by translation.

\vspace*{-\baselineskip}
\begin{table}[ht]
	\small
	\centering
	\caption{Ablation test on validation set}
	\label{tab:ablation-features}
	\begin{tabular} {c|c|c}
		\hline
		Feature Combination &   Micro Acc.  & Decrease in Acc. (\%) \\
		\hline
		Mention Only           &   0.604    & 12.7 \\
		Context Only        &   0.576    & 16.7   \\
		Coherence Only      &   0.279    & 59.6     \\
		Mention + Context      &   0.652    & 5.78    \\
		\hline
		Full                &   0.692    & 0.00  \\
		\hline
	\end{tabular}
\end{table}


\noindent
\textbf{Ablation Study:}
we evaluate table linking results using different feature combinations.
As shown in \tabref{tab:ablation-features},
all features in our model make a positive contribution to the final accuracy.
%Both the mention and context feature directly represents the latent semantic association
%between the embedding of an individual mention
%(either from itself or from its neighborhoods)
%and the candidate entity.
The mention feature is the most important one, since it encodes
the most direct information between the mention and the target entity.
%We observe that when using coherence feature only, a significant decrease is incurred, largely due to the lack of dominant and direct semantic association between mention-entity pairs.
%We observe that Micro Accuracy drops when adopting these two features only,
%but it still outperforms other ablations,
%Nevertheless, t
The coherence feature is complementary to the others,
as it aims at discovering the latent correlation in a global perspective, 
modeling whether different candidate entities in one column 
are close to each other.
%for example, sharing the same (or similar) type,
%even though no explicit type or category information is attached to the entity.
% it also shows that our coherence feature is able to extract the latent correlation
%between the target concepts, even though no explicit type or category information
%is attached to each candidate entity.
