\subsection{Embedding and Translation Module}
\label{sec:translation}

Let $\bi{x}^{(m)}$ denote the mention embedding of the surface form $x$,
and $\bi{e}$ denote the entity embedding of a candidate $e$.
Typically, a mention contains up to three words,
thus we simply represent $\bi{x}^{(m)}$ as the average embedding of the contained words it contains.
%Since the mention table and entity table are written in different languages,
We train word embeddings and entity embeddings on two corpus of different languages separately.

The vector spaces of embeddings in different languages are naturally incompatible,
and it's hard for us to directly compare or calculate them.
To tackle this problem, we employ a bilingual translation layer
to map embeddings from one language space to another.
Taking the mention embedding $\bi{x}^{(m)}$ in Chinese,
the layers translates it into an English mention embedding $\bi{v}^{(m)}$ through linear transformation:
$\bi{v}^{(m)}=W_t \bi{x}^{(m)} + \bi{b}_t$, where $W_t$ is the translation matrix and $\bi{b}_t$ is the bias,
both of which are model parameters
and will be updated during training step.

In addition, we pre-train the translation parameters by leveraging a small number of
bilingual word pairs $(w^{(ch)}, w^{(en)})$, or call them translation seeds.
The loss function of pre-train step is defined as follows:
\begin{equation}
\label{eqn:translation}
L(W_{t}, \bi{b}_{t}) = \sum_i \Arrowvert W_{t} \bi{w}_i^{(ch)} + \bi{b}_{t} - \bi{w}_i^{(en)} \Arrowvert_2.
\end{equation}
Refer to \secref{sec:impl-detail} and \secref{sec:exp-setup}
for detail information of the embedding initialization and translation pre-train.


%As mentioned in model overview, the input of joint model are two tables, mention table and entity table, where each of them is represented as a table of vector embeddings. Since a mention or entity name typically contains up to three words, we simply represent them as the average of embeddings of words they contain. The word embeddings are trained on large scale text corpus. Since mention table and entity table are written in two different languages, we train word embeddings on two corpus of different languages separately. Thus, the embeddings of a mention and its referent entity are naturally incompatible and we can't directly compare or calculate them. To solve this problem, We employ a bilingual translation layer to map embeddings in one language space to another language space. Through this translation layer, a non-English mention embedding $v_m$ can be translated into an English mention embedding $\widetilde{v_{m}}$ roughly through $\widetilde{v_m} = W_{t} v_m + b_{t}$, where $W_{t}$ is the translation matrix and $b_{t}$ is the bias. Notice that $W_{t}$ and $b_{t}$ are model parameters and will be updated during training so that the translation step will be more and more accurate. \KZ{Do we need to entity link the english corpus to train the embedding?
%Since the entity table contains the entities and not names?}
%
%In order to find a good starting point to train the model and jump out of local optima, we train $W_{t}$ and $b_{t}$ in advance. We use a small number of bilingual word embedding pairs $\langle v_{wc}, v_{we} \rangle$ to train the parameters. The loss function is as follows.
%\begin{equation}
%\label{eqn:translation}
%L(W_{t}, b_{t}) = \Arrowvert W_{t} v_{wc} + b_{t} - v_{we} \Arrowvert_2
%\end{equation}
%The list of bilingual word embedding pairs are called translation seeds. We learn a initial translation matrix by minimize the loss and then feed the weights into the model before training.
%\KZ{So this is just to obtain the initial values for $W_t$ and $b_t$ 
%to be trained in the main training process?  How do you obtain the seed pairs?}
