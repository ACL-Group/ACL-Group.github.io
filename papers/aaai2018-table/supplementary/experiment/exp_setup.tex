\subsection{Experimental Setup}
\label{sec:exp-setup}


%In this section, we introduce how we perform word embedding on Wikipedia,
%and how the cross-lingual table linking dataset is constructed.

\noindent
\textbf{Wikipedia and Word Embeddings:}
we use the Feb. 2017 dump of English\footnote{https://dumps.wikimedia.org/enwiki/}
and Chinese\footnote{https://dumps.wikimedia.org/zhwiki/} Wikipedia
as the text corpora for training word and entity embeddings.
The dumps contain 5,346,897 English and 919,696 Chinese articles (entities).
For the purpose of entity embedding,
all the entities occurred in anchor texts are regarded as special words.
E.g., the anchor text ``Rockets'' in the sentence
``the \underline{Rockets} All-Star player James Harden ... ''
is replace by the special word ``[[Houston\_Rockets]]'' as the entry of the English entity.
%The advantage is that,
%by learning embeddings of both common and special words in a uniform vector space,
%each entity can be represented by the embedding of its identical word,
%which is more precise than the aggregation of word embeddings in the entity's name.
%Besides, in order to enlarge the number of anchor texts in the corpora,
%we automatically add more anchor texts to both Chinese and English Wikipedia:
%for each article page, we simply find all the surface form of phrases exactly matching the article name,
%and then transform these phrases into an anchor text, linking to the current article.
We adopt Word2Vec~\cite{mikolov2013distributed} to learn the initial embeddings from both corpus respectively,
the embedding dimension is set to 100.

\noindent
\textbf{Table Linking Dataset:}
%since cross-lingual table linking is a new task, we do not have any existing benchmarks,
%and we have to construct a dataset by ourselves.
%Our cross-lingual table linking dataset consists of 150 web tables with
our cross-lingual table linking dataset consists of 150 web tables with
Chinese mentions and linked English Wiki articles.
The original Chinese tables are created by Wu et al.~\shortcite{wu2016entity},
which contains 123 tables extracted from Chinese Wikipedia, and each mention is labeled by
its corresponding Chinese Wiki article.
We collect another 30 Chinese tables with similar size from Web and
transform all the Chinese entities into English via inter-language links of Wikipedia,
producing the labeled English entities for 81\% of the entire mentions.
In addition, we discard long-tail tables,
if the shape or the number of labeled English entities of which is too small.
%if the shape is smaller than 5*3, 
%or if the number of labeled English entities is smaller than its columns.
%The whole dataset comes from 2 sources. We collect xx tables from Wutianxing \shortcite{},
%and construct the remaining xx tables by human annotation: among xxx tables extracted from
%Chinese Wiki dataset, we randomly sample xx tables with at least x rows and x columns.
%For each cell in the table, if it should be linked to a Chinese Wiki page
%but the link is missing in the original table, we manually add the link to this cell.
%This work is done by x annotators in x hours.
%Then we transform all the Chinese concepts into English concepts via inter-language links in Wikipedia.
In total, we collected 3818 mentions from 150 tables, with 2883 linkable positions
%mentions been linked to English entities
(19.22 per table).
We randomly split the dataset into training / validation / testing sets (80 : 20 : 50 tables) 
%The dataset is publicly available in http://xxxxxx.
and will publish the dataset later.

%\textbf{Datasets.} 
%Chinese web table from the the previous work by XXXX \shortcite{}.
%Size: consists of xx cell mentions with labeled ch-wiki concept from xx tables.
%By looking up inter-lang link from ch to En, we collect xx cells with labeled En concepts
%(x.xx cells per table).
%We split the dataset into training / validation / testing sets (xx : yy : zz).


%\noindent
%\textbf{State-of-the-art Comparisons}

\subsection{State-of-the-art Comparisons}
\label{exp:soat}

%To the best of our knowledge, we are the first work for cross-lingual entity linking 
%in web tables. 
Since there are no previous work that directly handle the cross-lingual table linking,
we select comparison systems from two perspectives.
The first perspective is mono-lingual table linking,
we compare with Bhagavatula et al. ~\shortcite{bhagavatula2015tabel}
and Wu et al. ~\shortcite{wu2016entity}.
We call their systems $TabEL_B$ and $TabEL_W$ in short.
In order to make a fair comparison in our bilingual scenario,
%we bridge the language gap as follows.
%For $TabEL_{En}$,
%For both systems, we apply the translation procedure in \secref{sec:candgen},
we convert each mention into the most likely English translation (\secref{sec:candgen}),
then run the mono-lingual models on these translated English tables.% and produce the final linking results.
%For $TabEL_{Ch}$, we map each predicted Chinese concept to the English concept
%by looking up the inter-language link.
%As mentioned in \secref{sec:intro}, we never use inter-language link in the learning step,
%however, $TabEL_{Ch}$ leverages this gold mapping data without any translating error,
%which makes the experiment unfair to other systems.
%Due to this reason, we regrad this work as an oracle baseline.

The second branch is cross-lingual text linking,
we compare with Zhang et al. 2013~\shortcite{zhang2013cross},
a bilingual-LDA based method, called $TextEL$.
%This work aims at entity linking from foreign languages to English on unstructured texts,
%and authors proposed the BLDA model, which models each foreign text and English Wiki article
%as the probabilistic distribution of latent bilingual topics in a uniform space.
In this case, we traverse each mention in row order and flatten the whole table into a word sequence,
%we flatten the whole mention table into a word sequence
and mark the word intervals for mentions to be linked. 
By turning the table into unstructured texts,
$TextEL$ is able to learn more flexible context information,
but it may be hard to capture the correlation of entities in the same column.
