Review1:


C1: what percentage of slots are multi-token or multi-character expressions?
[comment 1.]: We run the statistics of ESCA dataset and find that 96% percentage of slots are multi-characters. We will present the statistics in our final version.

C2: Fig 3 is not really readable.
[comment 2.]: Figure 3(a) presents the curve of F1 scores of different models and Figure 3(b) is the result of our ablation test (refer to Section 3.3 *Ablation Test* and Figure 4(a) for details).
And we will make Figure 3 more clear in our final version.

C3: The work is incremental given the Zhai et al's work.
[comment 3.]: We compare Zhai et al's work (and Vu et al's) with ours on the ESCA dataset in the last paragraph of Section 3.3 *Evaluation on ECSA*, where our model achieves higher performance due to our multi-task framework with cascade connection. We will add these two numbers into Table 3 in our final version.

Q1: What determines the ordering of these tasks? What about other tasks and other orders?
[question 1.]: Briefly, it is our e-commerce KB structure that determines the ordering of three tasks. Our target task (also the hardest task) is slot filling and it benefits more from the other two tasks than the other way around. Performances achieved with different orderings can be found in Section 3.3 *Ablation Test*.

Q2: The paper should include some analysis of results. For example, what is gained by multi-tasks? What is no entity tagging was performed?
[question 2.]: 1) In the second paragraph of Section 3.3 *Evaluation on ECSA*, we experiment by using the ground-truth segment type or named entity type as extra features, which demonstrates the gain achieved by our cascade multi-task learning. 2) Please refer to Section 3.3 *Ablation Test* and Figure 3(b) for the answer to the question "What is no entity tagging was performed?". We will add more details about the analysis in our final version.

Q3: how are the connections between property keys and values are made?
[question 3.]: Our e-commerce KB is built manually, and each value key is connected to a unique property key. For example, the value key "Nike" solely belongs to the property key "brand".


Review2:


C1: Multi-task learning is better, but to label the named entity tagging and segment tagging is high time cost and labor cost ...
[comment 1.]: Labeling manually is indeed of high cost. As mentioned in Section 3.1 *ECSA dataset*, we adopt a simple dynamic algorithm to automatically label the data with the help of an existing e-commerce KB. It can be considered as a long-distance supervision method as you mentioned. Thanks for your advice!

C2: would the proposed model be improved, if this work replaces the CRF component by LSTM?
[comment 2.]: LSTM is generally used to encode rich semantic features of sentences, while CRF is widely used to encode inter-dependencies among the output tags in sequence labeling tasks. Most state-of-the-art sequence labeling models combine LSTM and CRF (CRF as a downstream module of LSTM, so called LSTM-CRF) to improve the prediction performance [Reimers et al.,Huang et al.].

[Reimers et al.] “Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks”
[Huang et al.] “Bidirectional LSTM-CRF Models for Sequence Tagging”

C3: How about the state-of-the-art models, ...
[comment 3.]: We compare Zhai et al's work (and Vu et al's) with ours on the ESCA dataset in the last paragraph of Section 3.3 *Evaluation on ECSA*, where our model achieves higher performance due to our multi-task framework with cascade connection. We will add the comparison to Table 3 in our final version.

Q1: Noticed that this paper released the ECSA dataset, ...
[question 1.]: We will release the dataset and codes after acceptance.

Q2: Are there some Chinese characters unknown in the testing data of ESCA?
[question 2.]: There are 12% percentage of characters in the testing set which are absent from the training data. We will present the statistics in our final version.


Review3:


C1: although it claims multi-tasking, ...
[comment 1.]: 
It is true that labels in three tasks are with different granularity. And labels in three tasks maintain a reasonable order (seg -> ne -> slot).
The idea behind "multi-task learning" is that, instead of learning each task independently, different tasks in multi-task learning can share some common representations, which will bring extra benefits to each other. Our model exactly utilizes the intuition of multi-task learning but extends it by cascade connection to make different tasks more compact correlated.
The idea of using different granularity of slot labels can be applied in many other scenarios such as Chatbot (not limited to shopping setting), as long as there is a structural KB.

C2/C3/C4:
[comment 2/3/4.]: Thanks for your advice! We will revise our paper accordingly in the final version.

Q1: how many runs did you perform for the numbers in experiments?
[question 1.]: We run each experiment for 5 times and take the average score. The standard deviation on ATIS is about 0.2% and it's 0.1% on ESCA.

Q2: will you release this dataset and code?
[question 2.]: We will release the dataset and codes after acceptance.

Q3: why make your own ATIS validation set ...
[question 3.]: Actually we are using exactly the same data splitting as the link provided. We will present experiment setup more clearly in our final version.