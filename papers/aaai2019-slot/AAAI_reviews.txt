Reviewer #1
Questions
1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
This paper proposes a new slot filling approach	that is	jointly
trained with segmentation and named entity tagging. The approach is
specifically suitable for languages like Mandarin where words are not
separated by a space. The benefits of the proposed approach are
demonstrated on a shopping domain dialog corpus.	However	the results do
not seem to improve over the previously proposed neural sequence
chunking approach.
2. [Relevance] Is this paper relevant to an AI audience?
Likely to be of interest to a large proportion of the community
3. [Significance] Are the results significant?
Moderately significant
4. [Novelty] Are the problems or approaches novel?
Somewhat novel or somewhat incremental
5. [Soundness] Is the paper technically sound?
Has minor errors
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Sufficient
7. [Clarity] Is the paper well-organized and clearly written?
Satisfactory
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
It would be good to see	similar stats on the shopping data as well:
what percentage of slots are multi-token or multi-character
expressions? 

Figure 3 is not really readable.

The work is incremental given the Zhai et al's work.
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
What determines	the ordering of	these tasks? What about	other tasks
and other orders?

The paper should include some analysis of results. For example,	what
is gained by multi-tasks? What is no entity tagging was performed?

What is	not very clear to me from the paper is how are the connections
between property keys and values are made. For example, what if a user
request includes multiple properties, how do we not mis and match?


10. [OVERALL SCORE]
7 - Accept
11. [CONFIDENCE]
Reviewer is knowledgeable in the area


Reviewer #2
Questions
1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
In order to improve slot filling task, this paper propose a deep cascade multi-task learning model, and co-train three tasks (named entity tagging, segment tagging and slot filling) in the same framework with a goal of optimizing. And the data sparsity problem would be alleviated by using a multi-task learning framework.
2. [Relevance] Is this paper relevant to an AI audience?
Likely to be of interest to a large proportion of the community
3. [Significance] Are the results significant?
Significant
4. [Novelty] Are the problems or approaches novel?
Novel
5. [Soundness] Is the paper technically sound?
Has minor errors
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Sufficient
7. [Clarity] Is the paper well-organized and clearly written?
Good
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
Some comments are listed as follows:
(1) Multi-task learning is better, but to label the named entity tagging and segment tagging is high time cost and labor cost. Is it possible to utilize distant supervised model to assist multi-task learning?
(2) As this paper claimed that “CRF is the most popular way to control the structure prediction”, however, we also find that pure LSTM (94.85%) and deep LSTM (95.08%) have achieved the traditional CRF (92.49%) on ATIS dataset. Thus, would the proposed model be improved, if this work replaces the CRF component by LSTM.
(3) Considering that almost all the methods (including the proposed method) reach very close score of around 0.96 on ATIS dataset, it maybe not meaningful enough to continue evaluating on this dataset. However, the baselines on the proposed dataset ECSA are weak. How about the state-of-the-art models, such as BiLSTM-LSTM (Zhu and Yu 2017) and Neural Sequence Chunking (Zhai et al. 2017).
Some typos:
(1) “are” -> “is” at Line 2, Para 5 of Sec 1.
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
Some questions:
(1) Noticed that this paper released the ECSA dataset, how to fetch it? Is it publicly available?
(2) Are there some Chinese characters unkonwon in the testing data of ESCA?
10. [OVERALL SCORE]
6 - Marginally above threshold
11. [CONFIDENCE]
Reviewer is knowledgeable in the area


Reviewer #3
Questions
1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
This paper proposes a deep multi-task learning approach with cascade and residual structure to solve the problem of slot labeling(filling) for NLU, in an online shopping assistant setting.
2. [Relevance] Is this paper relevant to an AI audience?
Relevant to researchers in subareas only
3. [Significance] Are the results significant?
Not significant
4. [Novelty] Are the problems or approaches novel?
Somewhat novel or somewhat incremental
5. [Soundness] Is the paper technically sound?
Has minor errors
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Somewhat weak
7. [Clarity] Is the paper well-organized and clearly written?
Satisfactory
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
This paper deals with the problem of slot labeling with a multi-tasking approach. While the writing of this paper seems OK and the approach and experiment seems sound, I have some concerns with paper:
1, although it claims multi-tasking, it seems to me that from figure 2 that all tasks are just about the different granularity of slot labels (all B, I tags are aligned so these tasks are so similar). So the effectiveness of the improvements may just come from merging rare fine-grained labels into frequent coarse-grained labels. A multi-task learning should have other "partially overlapping" tasks such as POS tagging, maybe also intent classification? The segment label task seems to be more specific to Asian language? Also, the different granularity of slot labels seems specific to shopping setting (e.g., 3.2 mentioned no named entity tagging for ATIS)?
2, char sequence labeling should be clearly mentioned, not just a footnote on the second page. Otherwise people may easily assume a word has multiple chars, even for Chinese.
3, The approach section needs an overall description of the following components.
4, related work is too short, given a large empty space in reference page.
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
how many runs did you perform for the numbers in experiments? It seems to me for small dataset such as ATIS the standard deviation of multiple runs could be 0.5%.
will you release this dataset and code?
why make your own ATIS validation set given the link you provided has a split of validation set?
10. [OVERALL SCORE]
5 - Marginally below threshold
11. [CONFIDENCE]
Reviewer is knowledgeable in the area