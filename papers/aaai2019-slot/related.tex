\section{Related Work}
\label{sec:related}
%\KZ{I think you can expand this section significantly.}
%\xusheng{Slot filling and sequence labeling basics}
%There are mainly three lines of research that are related to our work:
%conversational search between a user and system,
%slot filling for dialog system and multi-task learning in natural language processing.
%\textbf{Conversational Search} is first defined by Filip and Nick \cite{radlinski2017theoretical},
%and nowadays is a hot research direction in both academic and industry \cite{ren2018conversational,yan2017building}.
%It is a system for retrieving information that permits a mixed-initiative back
%and forth between a user and agent, where the actions of agent are
%chosen in response to a model of current user needs within the
%current conversation, using both short- and long-term knowledge of the user.
%It is the most relevant work to our E-commerce Shopping Assistant system,
%for we can regard it as a conversational E-commerce search engine.
%Our system follows \emph{Partial Item System - Free Text User} conversation setting.
%And when a system asks a user to fill in a particular aspect of an information
%need, this is usually referred to as slot filling.
\textbf{Slot Filling} is considered a sequence labeling problem
that is traditionally solved by generative models.
%such as Hidden Markov Models (HMMs) \cite{wang2005spoken},
%hidden vector state model \cite{he2003data}, 
%and discriminative models such as 
%conditional random fields (CRFs) \cite{raymond2007generative,lafferty2001conditional} 
%and Support Vector Machine (SVMs) \cite{kudo2001chunking}.
In recent years,
deep learning approaches have been explored
due to its successful application in many NLP tasks.
Many neural network architectures have been used such as
simple RNNs \cite{yao2013recurrent,mesnil2015using}, 
convolutional neural networks (CNNs) \cite{xu2013convolutional},
LSTMs \cite{yao2014spoken} 
and variations like encoder-decoder \cite{zhu2017encoder,zhai2017neural} 
and external memory \cite{peng2015recurrent}.
In general, these works adopt a BiLSTM as the major labeling architecture
to extract various features, 
then use a CRF layer \cite{huang2015bidirectional} to model 
the label dependency.
We also adopt a BiLSTM-CRF model as baseline and claim that
a multi-task learning framework is working better than directly 
applying it on Chinese E-commerce dataset.
%\KZ{Nobody has applied multi-task learning to the slot filling problem? Are
%we the first? If so we should say it.}
Previous works
only apply joint model of slot filling and intent detection \cite{zhang2016joint,liu2016joint}.
Our work is the first to propose a multi-task sequence labeling model with novel cascade and residual connections based on 
deep neural networks to tackle real-world slot filling problem.

%\xusheng{Multi-task exploration in sequence labeling}
\textbf{Multi-task Learning (MTL)} has attracted increasing attention
in both academia and industry recently.
By jointly learning across multiple tasks \cite{caruana1998multitask}, we can
improve performance on each task and reduce the need for labeled data.
There has been several attempts of using multi-task learning on 
sequence labeling task \cite{peng2016multi,peng2016improving,yang2017transfer},
where most of these works learn all tasks at the out-most layer.
SÃ¸gaard and Goldberg \shortcite{sogaard2016deep} is the first to 
assume the existence of a hierarchy between the different tasks in a stacking BiRNN model.
%and to make lower level tasks affect the lower levels of the representation in a stacking BiRNN model.
Compared to these works, our DCMTL model further improves this 
idea even thorough with cascade and residual connection.


