\section{Related Work}

There are numerous datasets available driving the process of machine intelligence in commonsense reasoning. Choice of Plausible Alternatives (COPA) \cite{roemmele2011choice} are constituted by one question and two choices, requiring the model to select the correct answer by causal reasoning. %either the cause or the result.\JQ{either 这句话看不懂哇} 
Situations with Adversarial Generations (SWAG) \cite{zellers2018swag} and HellaSwag \cite{DBLP:journals/corr/abs-1905-07830}
%\mx{HELLASWAG} 
also focus on the causal inference. Models need to choose the best one from four alternative choices as the result of a given statement. ROCStories Corpora \cite{mostafazadeh2016corpus} %extends further the context information
providing more context information, asks participants to choose reasonable ending after a four-sentence story. SQUABU \cite{davis2016write} and ARC \cite{clark2018think} are two QA tasks for testing the scientific knowledge. CommonsenseQA \cite{talmor2019commonsenseqa} is also a QA dataset where the model needs to select the correct concept as the answer of given question from four candidates using background knowledge.

%Noticed that the above commonsense dataset mostly concerned about the relation inference between two sentences or document and sentence which could offer considerable context information. 
It should be noticed that the existing commonsense datasets are mostly concerned about either causal relation between two sentences, or an answer to a question which may need commonsense to judge. Those datasets all offer a relatively longer context different from short phrases in our released dataset. 
However, as far as we know, there are no public dataset focuses on short text for commonsense reasoning.

Short text have several challenges: absent of context information, not standard on syntax and more ambiguous, which makes understanding short text be a more challenging task.%\JQ{这个重复了？intro里有了，去掉？}
 
Some previous researches used twitter posts\footnote{http://trec.nist.gov/data/tweets}, Chinese Weibo\cite{he2016extracting}, product review\cite{pang2005seeing} or news title\cite{vitale2012classification} where the average word numbers in each sample are mostly more than ten or even much longer. They did short text classification by integrating external knowledge\cite{wang2017combining,chen2019deep} or adapting memory network\cite{zeng2018topic} for reducing ambiguity. 

Other researches on short text mainly aimed at web search query understanding \cite{hua2015short,wang2015query}. %The length of web search query satisfies more our short text definition.
The length of web search query is more close to the definition of our short text. %which is always constituted by a few words. 
However those public web query dataset \cite{pass2006picture,liu2011users,he2018dureader} is quite different from samples in our dataset. According to the analysis of AOL query log\footnote{https://github.com/wasiahmad/aol\_query\_log\_analysis}, 25.17\% queries contain person/location/organization entity such as "michigan sex offender". For the rest queries, they also mostly focus on searching a fact or an opinion, which is irrelevant with commonsense. 
%\mx{does it need to get statistics for randomly select 300 queries from sougou query?}. 
We also sampled 100 queries in one-day query log\cite{liu2011users} from Sougou\footnote{https://www.sogou.com/}, which is one of the largest Chinese search engine. Among which, 
43\% of queries asks for a source of video, song, essay or website; 
13\% for an opinion, and
10\% for an occurred effect. 
34\% queries are about a concrete object, while only 3\% of them are associated with commonsense modifiers.
In our dataset, commonsense phrases are in the majority as all samples are constituted by a thing with one or more modifiers, such as "cute children dress" or "cotton swimwear".%\JQ{ok这个词不太ok}

%According to web query statistics in the paper of He et al.\shortcite{he2018dureader},  

%Compared to the previous commonsense dataset, 

%Current researches make great efforts for solving difficulties on short texts. Wang et al. \shortcite{wang2015query} and Hua et al. \shortcite{hua2015short} apply graph structure and Probase \cite{wu2012probase} to understand short query. The methods of \cite{dai2006detecting}, \cite{shen2006query} and \cite{sun2012short} extract auxiliary context information using search engines. Other methods integrate external knowledge such as \textit{isA} or \textit{isPropertyOf} relation in Probase or Yago \cite{suchanek2007yago} to help the neural network get better performance, such as CNN \cite{wang2017combining}, attention-based LSTM \cite{chen2019deep}. Zeng et al. \shortcite{zeng2018topic} proposed topic memory network which encodes latent topic representations for short text classification. For better measuring distance between short texts, Li et al. \shortcite{li2019classifying} adapt and optimize the original Word Mover's Distance \cite{kusner2015word} to get a competitive and efficient performance.