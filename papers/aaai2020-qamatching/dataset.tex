\section{Dataset Construction}
\label{sec:data}
Previous QA datasets are in the form of independent QA pairs~\cite{yang2015wikiqa} and do not
provide surrounding dialogue context. Although He et al.\shortcite{he2019learning} solved a similar problem, their customer
service dataset is not open to public due to privacy 
concerns. Wei et al.~\shortcite{wei2018task} 
published their dialogue dataset collected from an online forum. 
However, their work focuses on the dialogue policy learning and 
the data doesn't preserve the original utterances. 

There is also no published qualified dialogue dataset for the QA matching task. The IRC dataset~\cite{elsner2008you} and Reddit dataset~\cite{jiang2018learning} are both multi-party dialogues instead of two-party dialogues. Twitter Triple Corpus~\cite{sordoni2015neural} and Sina Weibo~\cite{shang2015neural} are not multi-turn dialogues 
with only two or three turns. 
MultiWOZ 2.0~\cite{budzianowski2018multiwoz}, CamRest676~\cite{wen2016network}, 
and Stanford Dialog Dataset~\cite{eric2017key} are multi-turn dialogues with dialogue act annotations but these QA pairs appear next to each other. 
If we shuffle the well-ordered dataset randomly or by some rules, it's unnatural and incorrect because the original dialogue context is destroyed as shown in Table \ref{tab:example}. 
The two-party Ubuntu dataset~\cite{lowe2015ubuntu} meets these requirements
and we annotated 1000 dialogue sessions. 
However, the statistics and experiments show that the Ubuntu dataset is not a good 
QA matching dataset especially for long distance QAs, given the limited manpower for
annotation. More details will be shown in \secref{sec:results}.

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{ccccc}
		\toprule[1.2pt]
		Turn & QA & Original & Shuffled\\
		\midrule[1pt]
		\tabincell{c}{r1:Where can I enjoy my holiday?\\I want to go somewhere near the sea and warm.}& Q1 & 1 & 1 \\
		%\hline
		r2:Maybe \textit{Xiamen} is a good choice.&   A1   &    2  &  4  \\
		%\hline
		 r1:Is \textit{there} anything delicious?
		 & Q2 & 3 & 2 \\
		r2:Wow, that’s quite a lot. There are……
		 & A2 & 4 & 3 \\
		\bottomrule[1.2pt]
	\end{tabular}
	\vspace{-0.25cm}
	\caption{An example of shuffling well-ordered dialogues. Unreasonable co-reference appears after shuffling.}
	\label{tab:example}
\end{table}

%Dialogue datasets do contain QAs~\cite{lowe2015ubuntu} but these QAs almost exclusively
%appear next to each other in the dialogue session. In other words, long-range
%QAs are very rare in these datasets. 


Hence, we create a new dataset suitable for this task. 
Nearly 160,000 distinct dialogues are collected from 
an online health forum\footnote{An dialogue example online: 
	\url{https://www.120ask.com/shilu/0cjdemaflhj6oyw9.html}}.%\footnote{\url{https://www.120ask.com/}}. 
All the personal information was removed in advance by the website. 
After some basic data cleaning methods such as deleting the irrelavant 
sentences like ``Please pay ** coins to continue consultation'', 
we labelled 1000 randomly selected two-party multi-turn dialogues with 
Q (question), A (answer) and O (others) labels. 
A small amount of turns (0.24\%) are considered by the annotators 
to be both a question and an answer, and these are treated as questions
uniformly. The Fleiss' Kappa between three annotators 
was 0.75, indicating substantial agreement.

On average, each dialogue has 19.68 doctor turns and 17.32 patient turns. 
Most turns are made up of a sentence fragment, so the number of words 
for each turn is on average less than 10 
words~\footnote{There are on average 9.80 questions with 8.89 words, 
10.78 answers with 6.62 words, 16.41 casual chit chats with 6.99 words 
in each session according to annotated dialogues.}. 
$21.9\%$ of the questions have no answers, $22.7\%$ of the questions 
have more than one answer and the remaining questions have the only answer. 
For questions that do have answers, each of them is matched to 1.41 answers 
on average.


%The distribution of questions and answers is almost balanced.
The annotated dialogues are split into training/development/test sets by 7:1:2. 
The distribution of the QA pairs by distance is shown in Table \ref{tab:dataInfo}.

\begin{table}[th]
        \scriptsize
    \centering
    \begin{tabular}{cccccc}
    \toprule[1.2pt]
    \diagbox{Dataset}{Distance} & 1 & 2 & 3 & 4 & $\geq5$ \\
    \midrule[1pt]
    Training  & 3439 & 2068 & 1029 & 450 & 554\\
    %\hline
    Development & 454   &   331   &    167  &  76   &  99  \\
    %\hline
    Test  & 947 & 592 & 274 & 136 & 168 \\
    \bottomrule[1.2pt]
    \end{tabular}
	\vspace{-0.25cm}
    \caption{The distribution of QA pairs by Q-A distances.}
    \label{tab:dataInfo}
\end{table}

%To meet the need for pairwise models which score the probability of each Q-NQ pair being a QA pair, we reconstructed the labeled dialogues into Q-NQ pairs with distance, history and binary golden label.
We reconstructed the labeled dialogues into Q-NQ pairs with distance, history and binary golden label used for our models. A NQ from a party is paired with every earlier Q from the other party. If the pair is a QA pair, it is labeled as True(T). Otherwise, it is labeled as False(F). The distribution of positive and negative data in three datasets is shown in Table \ref{tab:pairdata}.

\begin{table}[th]
        \scriptsize
        \centering
        \begin{tabular}{cccc}
                \toprule[1.1pt]
                \diagbox{Label}{Dataset} &Training& Development& Test\\
                \midrule[0.8pt]
                True &7540 & 1226  & 2116\\
                %\hline
                False & 80631 & 14889 & 23893  \\
                \bottomrule[1.1pt]
        \end{tabular}
    	\vspace{-0.25cm}
        \caption{The distribution of positive and negative Q-NQ pairs on three datasets.}
        \label{tab:pairdata}
\end{table}

