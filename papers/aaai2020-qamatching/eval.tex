\section{Experiment Setup}
\label{sec:eval}

In this section, we firstly list the baselines and the ablations of our full model. Next, we define the evaluation metrics. Finally, we show the details of hyperparameters in our model.

\subsection{Baselines and Our Method}
\subsubsection*{Greedy strategy (GD)} 
A simple baseline \textit{Greedy} is that, when a question is posed by $RQ$, we can directly match the following several NQs said by $\overline{RQ}$ as the answers. It stops when meeting another Q or a turn said by $RQ$. 
There are a few variants in this methods. GD1 selects only one satisfying answer, 
and GDN selects multiple satisfying answers. The methods with \textit{Jump} 
(\textbf{J}) skip the non-question sentence uttered by $RQ$ when matching the 
NQs. 
%Four rules are adopted and their results of sample in Figure \ref{fig:sample} are shown in Table \ref{tab:res-simple}.
% Disadvantages: This baseline does not consider internet delay. Besides, side messages will all be assigned to one question.


%\begin{table}[h]
%	\small
%   \centering
%    \begin{tabular}{p{1.3cm}<{\centering}p{1.3cm}<{\centering}p{1.3cm}<{\centering}p{2.2cm}<{\centering}}
%        \toprule[1pt]
%         Greedy Rules & ID of Question & Ground Truth & Predicted Alignments\\
%         \midrule[0.5pt]
%         GD1 & $U_3$ & $U_4$ & $U_4$ \\
%         GDN & $U_3$ & $U_4$ & $\{U_4,U_5,U_6\}$ \\
%         GD1+J & $U_3$ & $U_4$ & $U_4$ \\
%         GDN+J & $U_3$ & $U_4$ & $\{U_4,U_5,U_6,U_8\}$ \\
%         \bottomrule[1.2pt]
%    \end{tabular}
%    \caption{Identified QA pairs by simple greedy rules.}
%    \label{tab:res-simple}
%\end{table}

\subsubsection*{Distance}
A simple model takes a 10-dimensional one-hot distance vector as the input of a fully-connected layer and outputs the score for each Q-NQ pair.  


\subsubsection*{Word-by-word match LSTM (mLSTM)}
This model is proposed by Wang et al.~\shortcite{wang2015learning}, used for natural language inference. It performs word-by-word matching based on an attention mechanism, with the aim of predicting the relation between two sentences.


\subsubsection*{Recurrent Pointer Network (RPN)}
The model proposed by He et al.~\shortcite{he2019learning} is the previous
state-of-the-art method for a similar task, and is implemented with some modifications to fit our task. We use two parallel RPN to distinguish questions from two parties. Comparing the classification loss and regression loss proposed in this paper, we choose the one that performs best on our test set.


\subsubsection*{Our Models}
By disabling some of its components, our model comes in three different
variants:
\begin{itemize}

    \item \textbf{Distance Model (DIS)} removes the mutual attention. It directly puts Q and NQ with the distance into Part II of the full model in Figure \ref{fig:model1}.
    \item \textbf{History Model (HTY)} disables the distance information at the prediction layer.
    \item \textbf{History-Distance Model (HDM)} is the full model we have explained in Section \ref{sec:method}.
\end{itemize}


\subsection{Evaluation Metrics}
%We first separately evaluate the performance of each model on all non-questions. With the ground truth, we can calculate the accuracy of two kinds of labels: A and O, corresponding to Taccuracy (\textbf{Tacc}) and Faccuracy (\textbf{Facc}) respectively.

Once we have identified all of the QA pairs, we count the true positive, 
false positive and false negative for each question. 
To measure the quality of the matched QA pairs, micro-averaging 
precision (\textbf{P}), recall (\textbf{R}) and F1-score (\textbf{F1}) are 
calculated,  with all questions in test dataset treated equally. 
Another metric, accuracy (\textbf{Acc}), is used when evaluating QA pairs matched
with a specific distance. Accuracy is calculated by the percentage of ground
truth QA pairs that are predicated positive by the models.

\subsection{Implementation Details}
 
We use Jieba\footnote{\url{https://github.com/fxsjy/jieba}} to do Chinese word segmentation and pre-train the 100 dimentional word embeddings with Skip-gram model~\cite{mikolov2013efficient}. For our proposed models, we use LSTM with hidden state size equaling 128 and 256 for Part I and Part II of the model respectively. We adopt Adam optimizer with 0.001 learning rate and 0.3 dropout. 
Learning rate decay is 0.95 and the training process terminates if the 
loss stops reducing for 3 epochs. All experimental results are 
averaged over three runs.
