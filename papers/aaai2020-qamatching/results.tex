\section{Results and Analysis}
\label{sec:results}
%In this section, we first show the end-to-end results on QA matching and accuracy on variable distances. Then we do the ablation tests to show the specific architectural decisions.  
In this section, we show the end-to-end results and ablation tests for the specific architectural decisions.
\subsection{Overall Performance}
The main results of different models are shown in Table \ref{tab:mainResults}. The last row lists the human performance, regarding as the upper bound of this task due to some ambiguities. 
%We make the following observations. 

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{p{1cm}<{\centering}p{1cm}<{\centering}ccc}
		\toprule[1.5pt]
		Models &P&R& F1\\
		\midrule[1pt]
		GD1&69.84&44.73&54.53\\
		GDN  &70.03&69.11&69.57\\
		GD1+J&70.38&50.40&58.74\\
		GDN+J&51.47&82.90&63.51\\
		\hline
		mLSTM&58.17&4.20&7.84\\
		Distance&71.57&69.34&70.44\\
		RPN&72.40&68.63&70.46\\
\hline
		DIS&78.46$^\star$&70.34&74.70$^\star$\\
		HTY&75.40$^\star$&76.42$^\star$&75.90$^\star$\\
		HDM&76.44$^\star$&78.44$^\star$&\textbf{77.43}$^\star$\\
		\hline
		Human &85.11&84.21&84.66\\
		\bottomrule[1.5pt]
	\end{tabular}
	\vspace{-0.25cm}
	\caption{The end-to-end performance of all methods on test dataset. Scores marked with $^\star$ are statistically significantly better than the RPN with $p<0.01$ of our models.}
	\label{tab:mainResults}
\end{table}

 

The results of the rule-based methods are not bad, which indicate that questions are actually followed by their answers in many cases. The GDN increases the F1-score to 69.57\% compared with Greedy-1 because it can solve the case of simple fragmented answers. For GDN+J, the recall is the best among all the methods while accuracy and F1-score suffer. 
The reason is that GDN tends to match NQ with Q as much as possible, so many chit chats will be regarded as answers, which reduces the precision.

Model mLSTM fails because it is difficult to solve the QA matching problem with 
only two short texts without history. The word distribution between the questions 
and answers are quite different and maybe unrelated without background knowledge. 
Distance achieves good scores which shows that the distance is very important factor 
when identifying QA relations in dialogues. People tend to answer the question at the moment they see it except in incremental QA cases.RPN obtains competitive results. It mainly benefits from taking the dialogue session as a whole which contains all the information in a session. 
 
Our proposed models achieve the best results compared with above models. 
The HDM improves the F1-score to 77.43\%, significantly better than RPN by t-test with
$p<0.01$. 
Although the recall of HDM is not better than GDN+J and the precision is 
lower than DIS, the overall quantity and quality of QA pairs identified 
are the best based on the highest F1-score. 
In addition, the comparable results achieved by HTY demonstrate that QA matching not only depends on 
the distance but also relies on the history information.
This shows that HDM model successfully combines both the distance and history 
information.


\subsection{Variable Distance Matching}

 According to the results above, we can find that the model Distance, RPN, DIS, HTY and HDM are competitive. Thus, we further analyze the accuracy of these five models on variable distances (Table \ref{tab:longrangeResults}).

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{p{1.5cm}<{\centering}ccccc}
		\toprule[1.3pt]
		 Models &1&2&3&4&$\geq5$\\
		\midrule[1pt]
		Distance      &\textbf{100.0}&88.01&0.0&0.0&0.0 \\
		RPN  &89.37&69.37&50.12&36.96&13.10\\	
		DIS &96.23&\textbf{89.13}$^\star$&17.03&2.45&0.0\\
		HTY &94.37&78.89$^\star$&57.42&38.48&\textbf{28.17}$^\star$\\
		HDM &95.99&83.16$^\star$&$\textbf{59.37}^\star$&\textbf{40.44}&24.80$^\star$\\
		\bottomrule[1.3pt]
	\end{tabular}
	\vspace{-0.25cm}
	\caption{The Acc (\%) of matched QA pairs on variable distances.}
	\label{tab:longrangeResults}
\end{table}

It shows that Distance and DIS work well on SQAs but deteriorate rapidly 
as the distance between QAs grows, indicating that relying solely on 
the distance information is insufficient. 
On the other hand, using dialog context or history, 
the matching accuracy of RPN and HTY is generally lower on SQAs but higher on 
LQA. Our full model (HDM) actually is a good trade-off 
by incorporating both distance and history information. This also accords 
with the decision process made by human annotators.

\subsection{Ablation Tests}
We justify the design of our model in the following two aspects:

\subsubsection*{Different definitions of history}

To show the effectiveness of using the turns between Q and NQ 
as history, we devise the following variants on the HDM model 
for comparison:
\begin{itemize}
	\item \textbf{Q-history Model (QH)} where history refers to all of the turns before Q.%has the same structure of HDM where the history is all the turns before Q.
	\item \textbf{A-history Model (AH)} where history refers to all of the turns before NQ.%has the same structure of HDM where the history is all the turns before NQ. 
\end{itemize}

The main results with different choices of history are shown in Table \ref{tab:historychoice}\footnote{The complete results are shown in Appendix due to the space limitation.}.%Due to the space limitation, we only listed the significantly different results. The complete results are shown in Appendix.}.
Our final model (HDM) outperforms QH and AH, indicating that the turns between Q and NQ are significant when figuring out the relation of Q-NQ pair. The turns before Q is actually not that important for matching Q and NQ. 
Although there is an overlap between the history we defined and 
the turns before NQ, 
the turns before Q brings more noises than benefits for
the end-to-end performance. This suggests that our definition of 
history as the turns between Q and NQ is reasonable and effective.

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{p{1.5cm}<{\centering}|c|ccc}
		\toprule[1.3pt]
		Models &F1&Acc@3&Acc@4&Acc@$\geq5$\\
		\midrule[1pt]
		QH&74.56&21.53&21.53&0.79 \\	
		AH&73.84&15.81&10.78&4.76 \\
		HDM&\textbf{77.43}&\textbf{59.37}&\textbf{40.44}&\textbf{24.80}\\
		\bottomrule[1.3pt]
	\end{tabular}
	\vspace{-0.25cm}
	\caption{The matching results on different choices of history.}
	\label{tab:historychoice}
\end{table}

\subsubsection*{Different ways of attending to the history}

To evaluate the effectiveness of mutual attention for aggregating the 
history, we devised variants of the HDM model for comparison:
\begin{itemize}
	\item \textbf{Non-mutual Model (NM)} where $Q$ attends to $H_{RQ}$ and $NQ$ attends to $H_{RNQ}$.%has the same structure of HDM where $Q$ attends to $H_{RQ}$ and $NQ$ attends to $H_{RNQ}$.
	\item \textbf{Identical history model (ID)} where $Q$ and $NQ$ attends to the same history $H_{RQ}\bigcup H_{RNQ}$.%has the same structure of HDM where $Q$ and $NQ$ attends to the same history $H_{RQ}\bigcup H_{RNQ}$.
\end{itemize}

The main result in Table \ref{tab:historyways} reveals that our choice 
of separating the history by role label and mutually attending to each other
does work. The full model (HDM) consistently outperforms both NM and ID. 
When making use of the union of the history from both parties, 
the combined history confused the model with too many turns to consider. 
The results of NM is slightly better than ID especially on LQAs, probably 
due to the better representations of the flow of semantic information on 
individual parties. However, QA relations focus more on interactions 
between parties. In a word, HDM performs better. We also conduct a Z-test on the 
results to show that the improvements on the LQAs are statistically 
significant even with a small sample size.

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{p{1.5cm}<{\centering}|c|ccc}
	\toprule[1.3pt]
	Models &F1&Acc@3&Acc@4&Acc@$\geq5$\\
	\midrule[1pt]
	NM&75.81&57.18&37.50&20.04 \\	
	ID&75.46&54.50&28.43&11.70\\
	HDM&\textbf{77.43}&\textbf{59.37}&\textbf{40.44}&\textbf{24.80}\\
	\bottomrule[1.3pt]
\end{tabular}
	\vspace{-0.25cm}
	\caption{The results on different ways of aggregating history.}
	\label{tab:historyways}
\end{table}
%\caption{The matching results on different ways of aggregating history.}


