Reviewer #1
Questions
1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
This paper addresses the task of word difficulty prediction in English and German. In both cases, the authors train machine learning models on native corpora, and evaluate the performance of the models against CEFR-leveled word lists that assign the difficulty level to words according to the Common European Framework of Reference for languages scheme aimed at non-native speakers. The models employ a range of features including word embeddings.
2. [Relevance] Is this paper relevant to an AI audience?
Of limited interest to an AI audience
3. [Significance] Are the results significant?
Not significant
4. [Novelty] Are the problems or approaches novel?
Not novel
5. [Soundness] Is the paper technically sound?
Has major errors
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Not convincing
7. [Clarity] Is the paper well-organized and clearly written?
Poor
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
This paper has two main weaknesses: firstly, it does not reference a vast amount of relevant research and does not use traditional data for the word complexity identification task, which makes it hard to put this research in context; secondly, parts of this paper are unclear and some implementation details are vague, which would make it hard to replicate the approach.

In more detail:

1. CEFR levels mentioned in Section 1 are not properly introduced and the examples will be unclear to anyone unfamiliar with the scheme. Table 1 uses numerical scores instead of CEFR levels (which range from A1 to C2) and requires at least some explanation (e.g., 1=lowest to 6=highest level of language proficiency). In addition, it is unclear which resource these CEFR-level attributions are based on.
2. The argument that frequency does not always correlate with word difficulty, although logically correct, is not exemplified well in Section 1: just because a particular corpus like COCA doesn't provide the representative frequency distributions does not mean that frequency in general is not a good clue. In addition, there may be differences in the domains / topics covered by a particular corpus. Exemplifying this phenomenon with a bigger corpus or a combination of several corpora from various domains will provide a much more convincing proof.
3. The paper is weak in terms of previous work overview and meaningful comparison: it ignores a vast amount of work on readability assessment in English and German and a whole line of work on complex word identification (for reference, see publications by Matthew Shardlow, papers on the two recent shared tasks on complex word identification from 2016 and 2018, as well as a whole set of papers presenting the systems that participated in these shared tasks). As a result, the approaches and features presented in the paper as novel have actually been applied to this task before. In addition, there are widely accepted datasets for complex word identification in English and German that have benchmark results reported for them (https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/complex-word-identification-dataset.html). If the authors ran their systems on these datasets, it would make their work comparable to the rest of research in this area as well as comparable to the actual state-of-the-art.
4. The paper is vague in terms of the resources and implementation details. In particular, it is unclear which "authoritative word lists" were used (the most comprehensive one for English is Cambridge Advanced Learner's Dictionary, but it doesn't seem like this resource was actually used here). In addition, it is unclear what "huge" corpus was used to estimate the frequency, what "previous state-of-the-art" methods the authors refer to in Section 3.4, and so on.
5. The examples of the bigram and trigram vectors in Figure 2 are uninformative.
6. It is unclear what represents ground truth in these experiments. Some CEFR-annotated resources are mentioned but it's unclear what are those resources. Provided that they are word lists, the fact that word difficulty may change depending on the context and the particular sense in which the word is used is completely ignored. In addition, the authors specifically mention that words with multiple annotations were removed from the ground truth annotations, showing conceptual misunderstanding of the task the authors are attempting in this paper.
7. The details of the clustering algorithm (Section 3.3) are missing. It would be impossible to reimplement this approach based on this paper only.
8. There are exaggerated claims regarding the strength of the model performance: first of all, lack of reference to actual state-of-the-art results makes it hard to judge the performance of the presented systems objectively; secondly, the difference between the model's performance and human performance on the same task is quite big. The claim that "in this paper, the universal dependency and word embedding features are applied to the recognition of word difficulty for the first time and have achieved remarkable results compared with previous work" is inaccurate as this is not the first work that applied this type of features to word complexity detection.
9. The paper needs proofreading.
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
See the detailed comments section.
10. [OVERALL SCORE]
3 - Clear reject
15. Please acknowledge that you have read the author rebuttal. If your opinion has changed, please summarize the main reasons below.
I acknowledge that I have read the author rebuttal, however I will stand by my score for this paper since the author response reinforced my suspicions about the flaws of the proposed approach.
Question 1 about the ground truth labels did not relate to the status of CEFR levels as the authoritative standard for second language learners, but rather to the way the authors derived the ground truth labels. Word complexity (binary or CEFR-levelled) is not a single number: it depends on the context of use and on the particular sense in which the word is used, therefore I find the approach that the authors are following in this paper conceptually flawed.
Question 3 suggested the authors put their work in perspective and compared their approach to the complex word identification task. There are two reasons for that: the two tasks are more similar than the authors are willing to recognise (the main difference is treating the word difficulty as a binary vs. multi-level target); and by actually referring to the previous work that has been done on complex word identification the authors might find that their claims on novelty are not accurate.


Reviewer #2
Questions
1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
This paper studies automatic classification of vocabulary CEFR levels, to aid language learners in vocabulary training. The results presented indicate that a classifier can be trained to reach a level of performance close to the human baseline experiment carried out by the authors.
2. [Relevance] Is this paper relevant to an AI audience?
Relevant to researchers in subareas only
3. [Significance] Are the results significant?
Significant
4. [Novelty] Are the problems or approaches novel?
Somewhat novel or somewhat incremental
5. [Soundness] Is the paper technically sound?
Has minor errors
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Somewhat weak
7. [Clarity] Is the paper well-organized and clearly written?
Poor
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
I find the paper interesting, and the results may have practical applications in education. The authors investigate a wide range of features, and include human baselines in their evaluation.

Unfortunately these advantages are outweighed by numerous problems at all levels, from poor presentation to methodological issues. For this reason I can not recommend accepting this paper as it currently stands, but would recommend the authors to rework the paper. In some cases I am not sure if the problems are just a presentation issue or run deeper. I may change my ratings on soundness/evaluation if these questions are cleared up in the author response.

One weakness of the paper is that the discussion is quite thin, with most of the results being presented as "this feature is useful" without much deeper discussion. A striking result from Table 8 is that word embeddings contain a large share of the information needed to perform the task. I would have liked to know why. Table 12 seems like an attempt at a qualitative investigation (but relative to the most naive frequency-only baseline), but it is not discussed in the text and I don't find it informative.

The authors mention that they try a bunch of classifiers (setups and hyperparameters not given) and choose the best. I can't find any information about which is the best, or which data they use to choose the model. Unless more details are given I will assume that this may have influenced the results by giving too optimistic classification accuracy figures.

Some parts of the paper are a bit difficult to understand due to unconventional presentation and/or confusing or incorrect language use. Some examples:

The legend of Figure 1 mentions "Universal Cognition" and "Actual Situation". These are not mentioned in the text, and I can not make sense of the figure.

The last paragraph on page 2 (titled "N-gram Probability") seems to described a character-level n-gram language model, it would be better to stick to standard terminology. The most important detail for these models is how the conditional probabilities like P(c_i | c_{i-1}) in equation (2) are estimated. Any kind of smoothing/priors used?

The paragraph preceding Figure 2 (titled "N-gram Vector") could be reduced to a single phrase like "we use binary vectors where each dimension represents the presence or absence of a particular character n-grams in a word".

I don't understand figure 4. What is each datapoint exactly? Mean CEFR level for all words in a given frequency span? If so, I can see the FO prediction makes sense, but not the others, especially the true label which seems to indicate that CEFR level (= difficulty) increases with frequency until a peak is reached and the level drops back down. And does the Y axis correspond to the CEFR levels from Table 6, or do the numeric values mean something else here?

Typos etc.:
(too many to mention actually, the following is a selection. I would encourage the authors to get someone to proofread the paper)
BERT-LAREG -> BERT_LARGE (p 4)
"is more difficulty than" -> "is more difficult than" (Table 5)
The reference to a report by John Kominek and Alan W Black use their first names only in the citation.
Some references (e.g. the first, Andersson) lack publication information.
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
You mention trying SVM, MLP and logistic regression, then picking "the model with the best results". Which model is this, how did you determine this is the best model, and what are the hyperparameters of the model? Has this method in any way affected the results presented in tables 8--11?

What is the reason that word embeddings are so important to the classification?
10. [OVERALL SCORE]
4 - Reject
15. Please acknowledge that you have read the author rebuttal. If your opinion has changed, please summarize the main reasons below.
i tried to read the author rebuttal, but it is unfortunately not very clearly formulated. The points I did understand were not enough to change my opinion on the paper. In addition, the thorough comments from reviewer #1 point out previous work that I was not aware of, which also speaks against accepting the current paper. I stand by the original recommendation of rejecting.


Reviewer #3
Questions
1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
This paper presents a solid work in automatic classification and comparison of words by difficulty. The result of this work designed to avoid time-consuming and labor-intensive work.
2. [Relevance] Is this paper relevant to an AI audience?
Likely to be of interest to a large proportion of the community
3. [Significance] Are the results significant?
Moderately significant
4. [Novelty] Are the problems or approaches novel?
Somewhat novel or somewhat incremental
5. [Soundness] Is the paper technically sound?
Technically sound
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Somewhat weak
7. [Clarity] Is the paper well-organized and clearly written?
Good
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
I think what the author did in the submission have potential industrial application value, and also likely draw interests of education communities. But I am afraid that this paper more likely to be presented as a workshop paper or a journal paper with more detailed experiments. the details of my comments listed as follows:
Pros:
1: Proposed a novel work to classify and compare words by the difficulty using multi-faceted features.
2: Show improvement to current approaches and shows robustness against different corpus environments of the same language and is capable of predicting word difficulty in different languages.

3: The experiment's methods are well choosed, and systematic. With minor justification and supplementary work, it would be a good paper in some peer-reviewed journal.

Cons:
1: The paper's motivation set to be in low resource languages. But the author test the all model on English and German, so the experiments are not convincing enough.

2: Many features do require things like parsing on a huge group of original text data, which is not likely available3: in the low resource languages.

9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
Please provide the details of your human baseline as an appendix.
10. [OVERALL SCORE]
5 - Marginally below threshold

META-REVIEWER #1
META-REVIEW QUESTIONS
3. Detailed Comments
While the reviewers believe that this work has practical applications in education, the paper suffers from serious presentation and methodological issues, and fails to use standard evaluation datasets. There is also a lack of awareness of related work.