\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai21}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
\usepackage{latexsym}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[english]{babel}
\usepackage[switch]{lineno}
\setcounter{secnumdepth}{2}

\begin{document}
\linenumbers

\section{Inter-annotator Agreement}

We report the inter-annotator agreement measured by Randolph’s $\kappa$ \citep{randolph2005free} in Table \ref{tab:inter-annotator}. It can be seen that \textit{Grammatical} and \textit{Relevant} have high agreement as they are easy to judge. \textit{New Info} has lower agreement possibly because it is harder to decide. For the example in Table \ref{table:quality}, the question "what is the color of the chair ?" may not be annotated as repetitive as the word "color" doesn't appear in the context, though it is actually covered by the specific value "blue grey". \textit{Logical} and \textit{Specific} have the lowest degree of agreement as they are more subjective criteria. According to the table suggested by \citet{landis1977measurement}, all the criteria achieved at least moderate agreement.


\begin{table}[htbp]
  \centering
  \begin{tabular}{l|ccccc}
  \hline
  Criteria & Agreement \\
  \hline
  Grammatical\tiny{[0-1]}  &  0.933 \\
  Relevant\tiny{[0-1]} &  0.853 \\
  Logical\tiny{[0-1]} &  0.659  \\
  New Info\tiny{[0-1]} &  0.701  \\
  Specific\tiny{[0-4]} &  0.546  \\
  \hline
  \end{tabular}
  \caption{\label{tab:inter-annotator} Inter-annotator Agreement measured by Randolph’s $\kappa$ \citep{randolph2005free}}
\end{table}


\section{Group-level Qualitative Analysis}
\label{sec:quality}

We provide a group-level evaluation example in Table \ref{table:quality}. We can see that the diversity of MLE is very limited (it gets \textit{\#Useful} of only 1, though all 3 questions are valid), and it produces highly generic question. The generations are more diverse for hMup. However, we find that a certain expert of hMup has a style of long and illogical generation, like the second one demonstrated here. (It's abnormal to put chairs \textit{on} a table, and the text is not coherent as it doesn't use a pronoun in the second sentence.) This significantly harms hMup's group-level performance (Table 4) compared to its best single model (Table 3). KPCNet(cluster) produces a diverse and specific generation, and we can clearly see the effect of keyword in its generation. 

For the group-level evaluation on \texttt{Home \& Kitchen}, we also studied the system-level Pearson correlation between the automatic metrics and human judgements. Pairwise-BLEU has a correlation  of 0.915 with \textit{\#Redundant} ($p<0.01$), -0.835 with \textit{\#Useful} ($p<0.05$). Avg BLEU is shown only correlates well with \textit{Logical} (correlation: 0.849, $p<0.05$).

\begin{table*}[htbp]
  \centering
  \begin{tabular}{c|lcc}
      \hline
      product & \makecell[l]{homelegance 2588s accent dining chair, blue grey, set of 2} & {} & {} \\
      \hline
      \makecell[c]{system \\ (\#Useful)} & generation group & specific & problem \\
      \hline
      \makecell[c]{ref \\ (3)} & \makecell[l]{can any of the recent reviewers confirm the seat height ? \\ i see the question was posted in april ... \\ would u please send me the box dimensions ( when buy in \\ a set of 2 ) and the weight ? \\ can someone please tell me the depth of the chair seat \\ from the end of the curved back to the end of the seat ? } & \makecell[c]{2 \\ \\ 3 \\ \\ 3 \\ \\} & {} \\
      \hline
      \makecell[c]{MLE \\ (1)} & \makecell[l]{what is the seat height ? \\ what are the dimensions of the chair ? \\ what are the dimensions ?} & \makecell[c]{2 \\ 2 \\ 1} & {} \\
      \hline
      \makecell[c]{hMup \\ (1)} & \makecell[l]{what is the weight limit for the chair ? \\ i have a table that is a [UNK]. will this chair be able \\ to fit on a table ? \\ is this a set of 2 chairs or just one ?} & \makecell[c]{2 \\ 2 \\ \\ 2} & \makecell[c]{ \\ illogical \\ \\ repetitive} \\
      \hline
      \makecell[c]{KPCNet \\ (2)} & \makecell[l]{what is the \textbf{color} of the \textbf{chair} ? \\ what are the \textbf{dimensions} of the \textbf{seat} ? \\ what is the \textbf{weight} limit ? } & \makecell[c]{2 \\ 2 \\ 2} & \makecell[c]{ repetitive \\ \\  \\ } \\
      \hline
      \end{tabular}
      \caption{\label{table:quality} Example generation group and the human judgements for each system. Here we use KPCNet to stand for KPCNet(cluster) for brevity, and the responded keywords of KPCNet are highlighted. }
\end{table*}

\section{Ablation Test}
\label{sec:ablation}

Below we describe the ablation test to check the influence of the components and hyperparameters of the model. These tests are all conducted on the \texttt{Home \& Kitchen} dataset.

\subsection{Additional Metrics}


To evaluate the quality of our keyword predictor and keyword bridge, we propose these additional automatic metrics:
\paragraph{P@5} Since the number of keywords in ground truth questions are different across each sample. We take the top 5 keywords with the highest predicted probability as selected keyword set $Z^s$, and calculates precision@5 by:
\begin{equation}
  P@5 = \frac{|Z^s \cap Z^T|}{5}
\end{equation}
where $Z^T$ is the union of keywords extracted from all ground truth questions of a sample.

\paragraph{Response Rate} which is the proportion of conditioned keywords that appears in the corresponding generation, and we report the macro average on all the records. We use this to evaluate the controllability of the keyword conditions.

We also report the average generation length(\textbf{Length}) as it is related to almost all metrics proposed above, but neither long or short generation should be considered an indicator of good performance.

\subsection{Ablation Factors}

These are many important factors and parameters in our model. So we divide the ablation test into 2 logical parts: one for keyword predictor (and the effect of data cleaning on it), and another for keyword bridge. 

The ablation factors for keyword predictor are as follows (abbreviated for readability):
\begin{itemize}
  \item \textbf{E}: End2end training of keyword predictor with other component. The training objective is a weighted sum of the 2 objectives (Equation 2 \& 3).
  \item \textbf{S}: Separate training, first train predictor, and then freeze its parameters to train other parts. 
  \item \textbf{H}: Hard label fed to bridge instead of masked soft logits. The label can be provided from ground truth in training and is decided with threshold filtering in inference. If this setting works well, we can then completely separate the parameters of predictor from other parts.
  \item \textbf{C}: Cleaned dataset.
\end{itemize}


The ablation factors for keyword bridge are:
\begin{itemize}
  \item \textbf{NE}: No encoder feature fed back to encoder
  \item \textbf{ND}: No decoder feature fed to decoder
  \item \textbf{Dropout}: We add a dropout layer for the unmasked keywords logits before it passes the latter transformation. Due to the nature of dropout, this part may help ease the noise introduced by the error of keyword predictor. And we study the effect of the strength of this layer.
\end{itemize}

\subsection{Results}

\begin{table*}[htbp]
  \centering
\begin{tabular}{l|ccccc}
\hline
{} & Distinct-3 & BLEU & P@5 & Response & Length \\
\hline
KPCNet(C, S) & 0.1530 & 17.77 & 0.472 & 0.395 & 7.263 \\
-C & 0.1651 & 15.88 & 0.510 & 0.350 & 7.517 \\
-S, +E & 0.1200 & 9.04 & 0.217 & 0.500 & 7.656 \\
+H & 0.2997 & 12.85 & 0.472 & 0.657 & 9.171 \\
\hline
\end{tabular}
\caption{\label{table:ablation1} Ablation test results on \texttt{Home \& Kitchen} for data and keyword predictor at individual-level. The first line is final adopted setting.}
\end{table*}

The ablation test result for data and keyword predictor at individual-level is shown in Table \ref{table:ablation1}. The setting for keyword bridge is fixed: dropout=0.3, both encoder and decoder feature are used. After data cleaning(C), \textit{P@5} dropped because of the reduction of the number of ground-truth keywords. The decreasing of \textit{Distinct-3} and \textit{Length} shows the effect of irrelevant part removing. The improvement on \textit{BLEU} and \textit{Response} indicates the overall benefits brought by the cleaning. End2end training(-S, +E) leads to significant performance degradation on all metrics except slight increase on \textit{Response}. The possible reason is that keyword prediction skews highly towards frequent keywords under this condition. Finally, feeding hard label instead of logits also produce worse result. We can see from the extremely high \textit{Response} and \textit{Length} that this setting suffers severely from over-generation of keywords: model generates illogical long questions to contain as much keywords as possible. We hypothesize that the soft logits can reflect subtle difference on the importance of each conditioned keyword and thus can lead to more robust performance. Moreover, we can achieve a \textit{P@5} of 0.628 with one group of group truth keywords, as compared to 0.472 of the current model, which shows a huge room for improvement of the keyword predictor.

\begin{table*}[htbp]
  \centering
  \begin{tabular}{l|ccccc}
  \hline
  {} & Distinct-3 & BLEU & Response & length \\
  \hline
  Dropout = 0.2 & 0.1729 & 17.11 & 0.452 & 7.164 \\ 
  Dropout = 0.3 & 0.1530 & 17.77 & 0.395 & 7.263 \\
  Dropout = 0.4 & 0.1302 & 18.33 & 0.353 & 6.947 \\
  Dropout = 0.4, NE & 0.1504 & 18.19 & 0.341 & 6.782 \\
  Dropout = 0.4, ND & 0.1219 & 17.47 & 0.317 & 6.525 \\
  Dropout = 0.5 & 0.1177 & 18.53 & 0.319 & 6.662 \\
  \hline
  \end{tabular}
  \caption{\label{table:ablation2} Ablation test results for keyword bridge at individual-level on \texttt{Home \& Kitchen}.}
  \end{table*}

  The ablation test result for keyword bridge at individual-level is shown in Table \ref{table:ablation2}. The setting for keyword predictor is fixed as KPCNet(C, S). We can clearly witness the trend that the higher dropout, the higher controllability keywords will have over generation (Response). As a result, the behavior of KPCNet will be more and more like MLE when dropout grows, with lower generation length, lower keyword response and higher BLEU. We speculate that the dropout imposed on the keywords logits to be masked forces the model to make prediction with incomplete keyword set. Therefore, proper level of dropout can make the model robust to the noise introduced by keyword predictor. Furthermore, the ablation of either encoder bridge or decoder bridge would harm BLEU, response and length, which proved the effect of KPCNet's double-bridge design to guide the generation via attention between the two sides. 
  

\begin{table*}[htbp]
\centering
\begin{tabular}{l|ccccc}
\hline
{} & Grammatical\tiny{[0-1]} & Relevant\tiny{[0-1]} & Logical\tiny{[0-1]} & New Info\tiny{[0-1]} & Specific\tiny{[0-4]} \\
\hline
KPCNet &        \textbf{0.99} &     \textbf{0.99} &    \textbf{0.95} &     0.80 &     1.81 \\
KPCNet(filter) &        \textbf{0.99} &     \textbf{0.99} &    0.94 &     0.85 &     \textbf{1.84} \\
\hline
KPCNet &        0.98 &     0.97 &    0.88 &     0.84 &     1.77 \\
KPCNet(filter) &        0.98 &     0.97 &    0.89 &     \textbf{0.88} &     1.80 \\
\hline
\end{tabular}
\caption{\label{tab:ind-human-eval-2} Comparison between KPCNet with Dropout=0.3 (upper half) and Dropout=0.2 (lower half) with individual-level human judgements on 100 sample products from \texttt{Home \& Kitchen}}
\end{table*}  

We also conducted human evaluation for different value of dropout (Table \ref{tab:ind-human-eval-2}), and found that lower dropout trades logicality for new information. We selected Dropout=0.3 as the final setting for its good balance of all metrics.



\section{Experimental Details}
\label{sec:detail}
\subsection{Data Cleaning}
The following steps are enforced to remove noises as well as remove unhelpful parts for the CQGen task in the original data:

\paragraph{Fixing Unescaped HTML characters} We noticed that there are unescaped HTML special characters in both context and the question. (e.g. ``does it slice like zucchini \textbf{\& amp ;} cucumbers?" is changed to ``does it slice like zucchini \textbf{\&} cucumbers?")

\paragraph{Remove non-question parts} Sometimes there are declarative sentences following the question, which is not the focus of our task. We thus removed them. (e.g. For ``where is this product made ? i contacted customer service and the representative was uninformed and could not offer any information ." We will remove the second sentence.)

\paragraph{Remove noise questions} Some questions contain the comparison between 2 specific entities, which is unlikely to be tackled by our model, so we dropped them. And some questions are too universal (``Does it ship to Canada?"). We consider them as noise and also dropped them.

Note that the data cleaning was only imposed on the training set and the validation set. We preserve exactly the same test set as \citet{rao2019answer} for fair comparison.

\subsection{Hyperparameters and other settings}
For all models, we set the max length of context to be 100, question to be 20. For all variants of KPCNet, we use 2-layer GRU \citep{cho2014learning} with 100 hidden units for both the encoder and decoder. We use a learning rate of 0.0003 to train at most 60 epochs. For MLE, the model structure and parameters are identical to KPCNet, and we follow the setting of \citet{rao2019answer}, using dropout=0.5, learning rate=0.0001 to train 100 epochs. To improve the generation quality, we block bigrams from appearing more than once, and also forbid 2 same words to appear within 3 steps. For sampling-based keyword selection, we sampled 3 keywords from top-$K$ top-$p$ filtered keywords distribution with $K=6, p=0.9$ for 2 times. For clustering-based keyword selection, we produce 2 clusters from the top 6 predicted keywords. For hMup, we use the implementation in fairseq\footnote{\url{https://github.com/pytorch/fairseq/blob/master/examples/translation_moe}}. The architecture is set to 2-layer LSTM \citep{hochreiter1997long} with 100 hidden units, and other settings are identical to KPCNet for fair comparison. The threshold $\alpha$ for the default keyword selection method of KPCNet is manually tuned within range [0.05, 0.1]. The dropout strength is shared among all components of KPCNet and is manually tuned within range [0.2, 0.5]. MLE and KPCNet is implemented in PyTorch. For all manually tuned hyperparameters, we fix all other hyperparameters and random search for value within given range that can achieve the best BLEU on our validation set. The models are trained on a Ubuntu 18.04.4 LTS server with one NVIDIA GeForce RTX 2080 Ti. 

For \texttt{Home \& Kitchen} dataset, all models are operated on 200D word embeddings borrowed from \citet{rao2019answer}, which are pretrained from in-domain data with Glove \citep{pennington2014glove} and are frozen during training, except for hMup, which uses unique embedding to distinguish between experts and thus the embeddings are trained from scratch. The selected threshold $\alpha$ is 0.07, after 3 trials, and the selected dropout is 0.3 after 4 trials. 


For \texttt{Office} dataset, all models are operated on 200D word embeddings that we pretrained from in-domain data with Word2vec\citep{mikolov2013distributed} in gensim\footnote{\url{https://radimrehurek.com/gensim/models/word2vec.html}}, except for hMup. The selected threshold $\alpha$ is 0.07, after 3 trials, and the dropout is initially selected as 0.3 based on the result of \texttt{Home \& Kitchen}.

For hypothesis test in Table 4, we use \texttt{proportions\_ztest} of scipy for the first 3 columns whose range is binary, and \texttt{ttest\_rel} for the other 3 columns. The procedure we assign the underline are: First, we underline the best number at each column. Then we run hypothesis test against every other number. If the difference is not significant, we also underline it, otherwise we don't underline it. 

\bibliography{cqgen}

\end{document}