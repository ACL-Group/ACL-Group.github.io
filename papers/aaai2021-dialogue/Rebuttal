We profoundly appreciate the hardwork and the precious suggestions and comments in the reviews. 

Reviewer #1
1. We agree with the reviewer that there are similarities between our task and summarization, where they are both required to pick up the useful information throughout the given texts. However, summarization usually refers to a natural language generation task, whereas our task is defined as a classification task. Both extractive and abstractive summarization models can not be directly used for this task, but future work in interpersonal relation extraction may take advantage of this viewpoint.
2. Our task is also different from the traditional relation extraction/classification task as mentioned in Sec 2. We focus on recognizing interpersonal relationships, which requires inferences across dialogue sessions.

Reviewer #3
1. As shown in Sec 2.2, existing dialogue-related tasks focus on analyzing in-session characteristics such as emotion recognition and relation extraction between entities and concepts. Different from these tasks, our proposed pair-level interpersonal relation classification task requires inference beyond a single session. The novelty of this task is also recognized by most reviewers. We conclude that there are two main challenges which ware laid out in Sec 6.4. They are a) cross-session consideration and b) incorporation of commonsense knowledge. We will reorganize these sections better in the revised version. 
2. We position this paper as a dataset paper. As such, we mainly focus on proposing the new dataset and the definition of the task instead of a novel method. We provide a simple MRR-based method dealing with the cross-session scenarios. Thanks to your suggestion, we attempted a data augmentation method which segments sessions of the same pair of speakers and glues the segments into new "hybrid" sessions to introduce more cross-session training data. This simple optimization improves the accuracy to 49.13% and 46.93% for 6-class and 13-class pair-level classifications respectively. With your permission, we can add this stronger baseline and its results to the revised version.

Reviewer #4
1. Data Labeling: Although the data annotation and evaluation process are only done by two persons due to the costs, one of them is a native English speaker and the other has more than 20 years of English use experience. The agreement between the two humans is also high as shown in Section 4.2 and Section 6.2. We also provide part of the dataset in the supplementary materials. The quality of the dataset has been approved by Reviewer #6. We invite you to check the dataset at your convenience.
2. Result Analysis: We analyzed the performance of samples in the test set and concluded two future directions for current models in Sec. 6.4. Here, we show another example with 4 sessions for a pair of speakers:
===
B: Excuse me.
A: Oh.
B: Hi, I'm-I'm Tony Lacey.
A: Well, hi!
B: Uh, we just wanted to stop by and say that we really enjoyed your sets.
A: Oh, yeah, really, oh!
B: I though it was ... very musical, and I liked it a lot.
A: Oh, neat ... oh, that's very nice, gosh, thanks a lot.
B: Are you ... are you recording? Or do-
A: No, no, no, not at all.
B: Uh, well, I'd like to talk to you about that sometime, if you get a chance.
A: Oh.  What about?
A: Well, hey, that's, that's nice. Uh.
B: No, I don't-I don't know, but I-I know your work. I'm a big fan of yours.

B: Oh, well, I-if it's inconvenient, eh, we can't do it now ... that's fine, too.  W-w-we'll do it another time.
A: Hey
B: Maybe if you're on the Coast, we'll get together and ... and we'll meet there.
A: Oh.
B: It was a wonderful set.
A: Oh, gosh.
B: I really enjoyed it.

B: We just need about six weeks, in about six weeks we could cut a whole album.
A: I don't know, this is strange to me, you know.
A: Oh.
B: There's a whole wing in this house.
A: Oh yeah, stay here?  U-huh.
B: You can have it to use.  Why-why are you smiling?
A: I don't know. I don't know.

A: Boy, this is really a nice screening room. It's really a nice room.
B: Oh, and there's another thing about New York. See ... you-you wanna see a movie, you have to stand in a long line.
A: Yeah.
B: It could be freezing, it could be raining.
A: Yeah.
B: And here, you just
===
As for human, we can make inferences based on keywords such as "enjoyed your sets", "cut a whole album" and "screening room". Based on our background experience and knowledge, such conversations are more likely happening between two guys with cooperation in making music. The cues here are not obvious in single sessions but are very assuring when four sessions are considered together. Human choose "Official" while the best baseline BERT mistakes it to be "Intimacy" for this sample. The model may be confused by the informal expressions and emotional words such as "wonderful", and it also lacks the cross-session information and commonsense knowledge as mentioned in Sec. 6.4. We can add more case studies into the final version.
3. Q1: As shown in Sec 5.3, the pair-level results are based on the session-level model predictions and MRR metric. We will explain this more clearly in the revised version.
4. Q2: The proportion of the cases where human can predict the relationship with the first conversation is limited. In some cases, it is also true that not all conversations contributed to the prediction as shown in Table 4, which means that future models should also learn to weight-in different conversations like human does instead of considering all the conversations equally. 

Reviewer #5
1. Baselines: We position our paper as a dataset paper, so we didn’t provide exhaustive results of all the competitive models that could potentially deal with this task. Please refer to our second response to Reviewer #3.
2. Kappa Score: The difference of Kappa scores is caused by the different set-ups for the annotation and the evaluation processes. Annotators have access to the movie synopsis and the Internet for the annotation process, while they are only provided with dialogue sessions during human evaluation.
3. Results: As for session-level 13-class classification scores, there is no significant difference between them. The low score and the poor agreement in the human evaluation also reflect that session-level relation classification is an extremely hard problem even for human, and consequently, we believe it's more appropriate to consider the pair-level interpersonal relation classification task at present. We explained this in the last paragraph of Sec 6.1. The poor performance of the BERT model is not only attributed to data imbalance, but also to the lack of consideration on cross-session information. Once we consider the cross-session information, the data imbalance problem would not be much of a problem anymore. This is shown in our new results mentioned in the second response to Reviewer #3.
4. Metric: Because the number of samples among different classes varies significantly as shown in Table 1, we choose F1-macro instead of F1-micro.

Reviewer #6 
1. Metric: Following your advice, we asked human annotators to further label several sessions into a ranked list of 4 classes and then evaluate the results using MRR. The results for human annotators are 82.69%, 74.36% and 56.36% for 4, 6 and 13 classes respectively. The results for BERT are 67.46%, 62.77% and 55.42% for 4, 6 and 13 classes respectively. Indeed this seems to look better than directly predicting a class. The trends of these results with MRR are similar to the results shown in the paper. With your permission, we can include this bit of result into the revised version. 
2. Downstream: The model for this task can be used to improve dialogue systems trained from multiple history sessions. The predicted interpersonal relationship can also be used for applications requiring dialogue reasoning as mentioned in the paper “MuTual: A Dataset for Multi-Turn Dialogue Reasoning” by Cui et al.
