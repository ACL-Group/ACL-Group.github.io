\section{Experiments}
\label{experiments}


In this section, we introduce the baseline models, human evaluation settings and evaluation metrics.




\subsection{Baseline Models}
\label{sec:baselines}
We introduce two naive baseline methods, Random and Majority, and three strong neural
baseline models, CNN, LSTM and BERT. 
The code and dataset are avaliable at Github~\footnote{\url{https://github.com/JiaQiSJTU/DialogueRelationClassification}}.

\textbf{Random:} A relation type is randomly assigned to a dialogue session or a pair of interlocutors.

\textbf{Majority:} The most frequent relation type is assigned to a dialogue session or a pair of interlocutors.


\begin{table}[h!]
	\centering
	\small
	\begin{tabular}{@{}lrrr@{}}
		\toprule[1.5pt]
		\textbf{}                				& \textbf{train} & \textbf{development} & \textbf{test} \\ 
		\hline
		\# Pair of Speakers      		  &   541     &  75     &   78  \\
		\# Sessions             			 &     5,037   &    653    &  610  \\
		\# Turns                			   &   42,564     &    5,210    &  5,352  \\
		Sessions per pair (mean)  	  &    9.31    &   8.71     &  7.82  \\
		Sessions per pair (std)        &    8.18    &   6.35     & 5.96   \\
		Turns per session (mean)    &    8.45     &   7.98     &   8.77 \\
		Turns per session (std)      &    6.96     &   5.60     &  7.93  \\
		\bottomrule[1.5pt]
		
	\end{tabular}
	\caption{Statistics on the splitted datasets.}\label{table:dataset}
\end{table}



\begin{table*}[th]
	\centering
	\small
	\begin{tabular}{@{}llcccccc@{}}
		\toprule[1.5pt]
		&               				& \multicolumn{2}{c}{\textbf{4-class}} & \multicolumn{2}{c}{\textbf{6-class}} & \multicolumn{2}{c}{\textbf{13-class}} \\ 
		& & Acc & F1-macro                 & Acc & F1-macro                  & Acc & F1-macro \\
		\midrule
		\multirow{6}{*}{\textbf{Session-level}}&Random   &23.0$\pm$3.56 &22.67$\pm$3.71 &17.33$\pm$2.62 & 15.80$\pm$3.00& 8.33$\pm$2.62& 6.63$\pm$2.12  \\		
		&Majority    &31.00$\pm$0.00 &11.80$\pm$0.00 &31.00$\pm$0.00 &7.90$\pm$0.00 &26.00$\pm$0.00 &3.20$\pm$0.00 \\
		&LSTM    &29.80$\pm$1.28 &22.87$\pm$1.24 &30.83$\pm$1.16 &11.10$\pm$0.08 &28.50$\pm$1.44 &4.63$\pm$0.45 \\
		&CNN    &42.67$\pm$2.93 & 33.27$\pm$6.63&37.80$\pm$1.31 & 31.40$\pm$6.67 &32.33$\pm$2.46 &9.20$\pm$4.97 \\
		%&CNN    &44.93$\pm$1.23 &39.70$\pm$1.31 &39.03$\pm$0.90 &34.10$\pm$2.18 &30.93$\pm$1.30 & 6.53$\pm$2.31 \\
		&BERT   &47.10$\pm$1.28 &44.53$\pm$1.10 &41.87$\pm$0.81 &39.40$\pm$0.85 &39.40$\pm$0.36 &20.40$\pm$0.67 \\
		&Human &56.00$\pm$6.00 &55.20$\pm$6.30&50.00$\pm$9.00&53.00$\pm$8.10&38.50$\pm$5.50&40.75$\pm$8.15 \\ 
		\midrule
		\multirow{6}{*}{\textbf{Pair-level}}&Random   &28.20$\pm$9.30 &26.90$\pm$9.24 &17.93$\pm$7.89 &16.2$\pm$7.54 &6.43$\pm$2.76 & 5.73$\pm$2.64 \\		
		&Majority    &23.10$\pm$0.00 &9.40$\pm$0.00 &23.10$\pm$0.00 &6.20$\pm$0.00 &19.20$\pm$0.00 & 2.50$\pm$0.00\\
		&LSTM    &25.63$\pm$2.76 &13.13$\pm$5.06 &22.67$\pm$0.61 &6.40$\pm$0.29 &19.20$\pm$0.00 &2.57$\pm$0.05 \\	
		&CNN    &47.47$\pm$2.76 &35.03$\pm$5.80 &38.47$\pm$4.21 &30.40$\pm$9.06 & 22.20$\pm$6.08& 7.07$\pm$6.04\\	
		%&CNN    &52.13$\pm$4.97 &45.00$\pm$4.34 &34.63$\pm$3.63 &30.70$\pm$5.56 & 19.20$\pm$1.06 & 4.27$\pm$1.39\\
		&BERT   &58.13$\pm$0.61 &52.00$\pm$0.86 & 42.33$\pm$2.76&38.00$\pm$1.14 &39.73$\pm$1.79 &24.07$\pm$0.63 \\
		&Human & 75.65$\pm$3.85 &73.00$\pm$4.40 & 72.40$\pm$4.50&73.55$\pm$5.45 &63.45$\pm$1.95 &54.40$\pm$3.00 \\ 
		\bottomrule[1.5pt]
		
	\end{tabular}
	\caption{The classification results(\%) on session-level tasks and pair-level tasks.}
	\label{tab:results}
\end{table*}


\textbf{CNN:}
TextCNN, proposed by Kim ~\shortcite{Kim14}, is a strong text classification models 
based on convolution neural network.
All of the utterances in a dialogue session are concatenated as the input to the 
embedding layer, where 300-dimension pre-trained Glove~\shortcite{pennington2014glove} 
embeddings are used and freezed 
during training. Following the setting of Kim ~\shortcite{Kim14}, we used three convolution layers
with kernel size 3, 4, and 5 to extract semantic information from the dialogue. A dropout layer with 
probability $0.5$ is attached to each convolutional layer to prevent overfitting.
Finally, a linear layer and a softmax function are set for the final predictions. 
The loss function is the negative log likelihood loss. Stochastic gradient descent is 
used for parameter optimization with the learning rate equaling $0.01$. 

\textbf{LSTM:}
The attention-based bidirectional LSTM network by Zhou et al.~\shortcite{ZhouSTQLHX16} 
is implemented as another neural baseline. The same pre-trained Glove embeddings are 
used for the embedding layer. Then high-level features are extracted by a single 
Bi-LSTM layer. The last hidden states of both directions are concatenated as the 
query to do the self-attention among the input words. Finally, the weighted summed 
feature vector can be used to characterize the whole session and used for final relation 
classification with a linear layer and a softmax function. We use AdamDelta as optimizer with
learning rate $0.0003$ following Zhou et al.~\shortcite{ZhouSTQLHX16}.

\textbf{BERT:}
We fine-tuning the uncased base model of BERT released by 
Devlin et al.~\shortcite{DevlinCLT19}. 
All of the utterances in a dialogue session 
are also concatenated with the special token [CLS] as the start of the sequence.
Following the general procedure of fine-tuning BERT, we pass the output hidden state of 
[CLS] token into a fully connected layer for classification and use Adam as optimizer
with learning rate $1e-6$. We fine-tune the dataset for 32 epochs with early stopping
patience equal to 3.

The above baselines can be directly used for session-level classifications. For pair-level classifications, we do the following calculation for each neural baseline based on the session-level trained models: We first calculate the MRR metric of each relation type for each dialogue session. Then, given a pair of interlocutors with multiple sessions, the confidence score for each relation type can be regarded as the average MRR among sessions. Finally, the relation type with the maximum confidence score is regarded as the final prediction for this pair. 




\subsection{Human Evaluation Settings}

To give an upper bound of our proposed DDRel dataset, we hired human annotators to do the relation classification tasks on the test set. Since given the 13-class classification results, the 4-class or 6-class classification results are obvious for human. We only asked annotators to do the 13-class interpersonal relation classification tasks.

We asked 2 volunteers to do the 13-class relationship task on session-level samples.  Each session are showed individually and volunteers are required to choose the most possible relation type. 
Another 2 volunteers are hired to do the 13-class relationship task on pair-level samples. All of the dialogue sessions between a pair of speakers are given to the volunteers to inference the relation types.


\subsection{Evaluation Metrics}
\label{sec:metrics}
Each classification model are trained separately for relation classification tasks on different granularity. We use accuracy and F1-macro scores for evaluation.



