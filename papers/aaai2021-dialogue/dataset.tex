\section{Dataset}
\label{sec:dataset}
Although there are many currently available dialogue datasets, 
most of them are used for training automatic dialogue robots/systems, 
thus they either do not cover the diversity of interpersonal relationships, 
or do not come with relationship labels.
Therefore, we build a new dataset 
composed of $6,300$ sessions of dyadic dialogues with 
interpersonal relationship labels between two speakers, 
extracted from movie scripts crawled from 
the Internet Movie Script Database (IMSDb). More details about the dataset are explained as follows.

\begin{table*}[t]
	\centering
	\small
	\begin{tabular}{@{}lllrrrrrr@{}}
		\toprule[1.5pt]
		\textbf{4 classes} & \textbf{6 classes} & \textbf{13 classes}  & \textbf{\# Sessions} & \textbf{\% Sessions}& \textbf{\# Pairs} & \textbf{\% Pairs} & \textbf{\# Turns} & \textbf{\% Turns}  \\ 
		\hline
		\multirow{4}{*}{Family}&\multirow{2}{*}{Elder-Junior} & Child-Parent   &  414    &   6.57   &  67   &	9.65	&  3,377 & 6.36   \\
		& & Child-Other Family Elder 															    &   91   &  1.44    &  12   &	1.73	&   632  & 1.19  \\
		& \multirow{2}{*}{Family Peer} & Siblings 														  &   211   &   3.35   &  27   &   3.89	&   1,585  & 2.98 \\
		& & Spouse 																						   &   568   &   9.02   &  51   &	 7.34  &   4,784  & 9.01 \\
		\hline
		\multirow{2}{*}{Intimacy}& \multirow{2}{*}{Intimacy} &  Lovers					&  1,852    &  29.40    &  244   &20.75	&   17,474 & 32.89  \\
		& & Courtship	
		&  146    &   2.32   &  15   & 2.16	&  1,323 & 2.49   \\
		\hline
		\multirow{3}{*}{Others}& \multirow{3}{*}{Peer} & Friends 					&  1,049    & 16.65     &  124   &17.87	&  8,900 & 16.75    \\
		& &Neighbors 																					 &   21   &  0.33    &  2   &	0.29& 189 & 0.36      \\
		& &Roommates 																				    &  120    &   1.90   &  8   & 1.15	&   966 & 1.82   \\
		
		
		\hline
		\multirow{4}{*}{Official}& \multirow{1}{*}{Elder-Junior} &Workplace Superior-Subordinate &  536    &   8.51   &  79   &	11.38& 3,958 & 7.45     \\
		&\multirow{3}{*}{Official Peer} &Colleague/Partners										&   710   &  11.27    &  76   &	10.95 & 5,455 & 10.27     \\
		& &Opponents																				  &  203   &  3.22    &  33   &  4.76	&  1,532 & 2.88   \\
		& &Professional Contact																	  &   56   &  8.07    &  56   &	8.07 & 2,952 & 5.56     \\
		
		\bottomrule[1.5pt]
		
	\end{tabular}
	\caption{Statistics on categories of interpersonal relation types.}
	\label{table:relationtypes}
\end{table*}

\subsection{Dataset Extraction and Processing}
Initially, we crawl 995 movie scripts from IMSDb, and 941 of them remain after we automatically match the titles with movies in IMDb\footnote{\url{https://www.imdb.com/}} and filter out those that do not meet following requirements:
\begin{itemize}
	\item Don't have a match in IMDb; 
	\item Not in English; 
	\item Very unpopular(measured by number of raters). 
\end{itemize}

By observing the formats of the scripts and manually defining text patterns, 
we split each script into scenes, extract the sequence of (speaker, utterance) 
pairs for each scene and identify subsequences that meet the following requirements  
as dyadic dialogue sessions: 
\begin{itemize}
	\item Two speakers speak alternately without being interrupted by a third one; 
	\item Each dialogue session contains at least 3 turns. 
\end{itemize}

We set this minimum length requirement to make sure that two speakers are speaking to each other instead of participating in a group discussion. Finally, we count the total number of turns taken between each pairs and filter out those having fewer than 20 turns to make sure the relationship between the two speakers is significant and not as trivial as greetings between strangers. This filtering step also helps reduce the cost of labeling because more sessions can share the same pair of speakers.

\subsection{Annotation Procedure}
Although interpersonal relationships are not static or mutual exclusive, most of them exhibit relative stability over time~\cite{gadde1987stability}, and relationships in movies are usually more clear-cut. Therefore, in this paper, we model relationship as a single, stable label. Such assumption simplifies our task and significantly reduces the workload of labeling, though it introduces 
ambiguity in certain cases such as evolving relationships (e.g., courtship $\rightarrow$ lover $\rightarrow$ spouse) or concurrent ones that do not usually exist together(e.g., enemies falling in love). To avoid these situations, we require the annotator to only assign labels when the relationship is clear, relatively stable and typical.

Our ground truth annotator was provided with the movie title, the pair of characters involved in the dialogue, movie synopsis from IMDb and Wikipedia for each movie, as well as complete access to the Internet, and was asked to choose between one out of tens of classes mentioned in Reis and Sprecher's work~\cite{reis2009encyclopedia} or “Not applicable (NA)” label.
It took the annotator 100 hours across one and a half months to finish the annotation of 300 movies, at a rate of approximately 4.07 minutes per pair. Only 47.11\% of the pairs received a specific label, while others are considered “not applicable”. Finally, 13 kinds of relation types are labeled in our dataset, covering a variety of interpersonal relationships and enough for developing classification methods on this task.

\textbf{Second-Annotator Verification}
Due to excessive cost of the annotation task, we are not able to commit multiple annotators on the labeling task. But to compensate that, we verify the accuracy of annotation by having a second person label 100 pairs with the same experimental settings. The inter-annotator agreement (kappa) is 82.3\% for 13-classes. This indicates that incorrect labels are limited, and the annotation by the first human is reliable.




\subsection{Dataset Statistics}
The current version of the DDRel dataset~\footnote{We have processed $941$ scripts and 
	manually labeled $300$ of them with relationships at present.
	A full dataset (with much larger expected scale) will be made 
	public once ready. The current version can be downloaded at: 
	\url{http://Anonymous.com}.}
%For readers who want to have a copy of the current dataset,	please contact the first arthor.} is described in Table \ref{table:dataset}. 
contains $6,300$ labelled sessions of dyadic dialogues, taking place between $694$ pairs of interlocutors across $300$ movies. 
The average number of turns in each dialogue is $8.43$, 
while it varies greatly (the standard deviation is $6.94$).  The number of sessions for each pair of interlocutors also varies a lot with $avg=9.08$ and $std=7.80$. The whole dataset is split into train/development/test sets by 8:1:1. All of the dialogue sessions between the same interlocutors are assigned to the same subset and there is no overlap between three subsets. Both human and algorithm performance are later measured on the test set to ensure fair comparison. More  of them are showed in Table \ref{table:dataset}.
\begin{table}[h!]
	\centering
	\small
	\begin{tabular}{@{}lrrr@{}}
		\toprule[1.5pt]
		\textbf{}                				& \textbf{train} & \textbf{development} & \textbf{test} \\ 
		\hline
		\# Pair of Speakers      		  &   541     &  75     &   78  \\
		\# Sessions             			 &     5,037   &    653    &  610  \\
		\# Turns                			   &   42,564     &    5,210    &  5,352  \\
		Sessions per pair (mean)  	  &    9.31    &   8.71     &  7.82  \\
		Sessions per pair (std)        &    8.18    &   6.35     & 5.96   \\
		Turns per session (mean)    &    8.45     &   7.98     &   8.77 \\
		Turns per session (std)      &    6.96     &   5.60     &  7.93  \\
		\bottomrule[1.5pt]
		
	\end{tabular}
	\caption{Statistics on the splitted datasets.}\label{table:dataset}
\end{table}

The distribution of the whole dataset on 13 relation types are shown in Table \ref{table:relationtypes}. {\em Lovers}, {\em Friends} and {\em Colleague/Partners} are the three largest classes and take up about half of the dataset, while the smallest relation type {\em Neighbor} only have 2 pairs of interlocutors with 21 dialogue sessions. The proportion of different relation types are unbalanced, aggravating the difficulty of these classification tasks.





