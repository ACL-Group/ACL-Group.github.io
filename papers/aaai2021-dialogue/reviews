
Submissions
Help Center
Select Your Role : 
Author 
AAAI2021 
Qi Jia 
 Print
View Reviews
Paper ID8834
Paper TitleDDRel: A new dataset for interpersonal relation classification in dyadic dialogues
Track NameAAAI2021
Reviewer #1
Questions
1. {Summary} Please summarize the main claims/contributions of the paper in your own words. (Do not provide any review in this box)
The study reported here presents a dataset for deducing the relationship between two participants in a dialog based on short sessions of dialogs between them. Defines the corresponding task. Describes the annotation process. Presents results using three neural models. The results indicate that the models' performance is at best in the range of 0.20 - 0.52 F measure. They identify an opportunity for improvement.
2. {Novelty} How novel is the paper?
Paper makes non-trivial advances over past work
3. {Soundness} Is the paper technically sound?
I have not checked all details, but the paper appears to be technically sound
4. {Impact} How important is the paper likely to be, considering both methodological contributions and impact on application areas?
The paper will have a broad and significant impact
5. {Clarity} Is the paper well-organized and clearly written?
Good: paper is well organized but language can be improved
6. {Evaluation} Are claims well supported by experimental results?
Good: Experimental results are sufficient, though more analysis would significantly add support to the claims
7. {Resources} How impactful will this work be via sharing datasets, code and/or other resources? (It may help to consult the paper’s reproducibility checklist.)
Good: shared resources are likely to significantly impact future work
8. (Reproducibility) Would the experiments in the paper be easy to reproduce? (It may help to consult the paper’s reproducibility checklist.)
Excellent: e.g., code/data available and paper comprehensively describes experimental settings
9. {Reasons to Accept} Please describe the paper’s key strengths.
1. A new dataset and a new task - which is of some significance
2. Rtandard neural models' performance was shown
3. Room for improvement was identified, which encourages new research
11. {Reasons to Reject} Please describe the paper’s key weaknesses.
It is not clear if this is a relation extraction task; I view this as a summarization with a specific point of view; hence, may not have been compared with relevant research in summarization; authors did not consider/discuss this aspect.
12. {Detailed Comments} Please provide other detailed comments and constructive feedback.
Good work, other than what I mentioned above, I have no other comments.

I think that the introduction can be significantly shortened, the discussion about the differences between sentence-level and inter-sentence relations is irrelevant to the topic on hand.
13. {QUESTIONS FOR THE AUTHORS} Please provide questions for authors to address during the author feedback period. (Please number them)
Why is this a relation extraction? Why is it not a summarization with a specific point of view?

How does the solution change if it were summarization?
15. (OVERALL SCORE)
9 - Accept: will fight to get it accepted
Reviewer #3
Questions
1. {Summary} Please summarize the main claims/contributions of the paper in your own words. (Do not provide any review in this box)
1) The paper proposed a task of dialogue relation classification between speakers, and constructed a dataset for the task. The evidence used for the classification may be from one or multiple sessions in dialogue.

2) The paper presented a set of baseline methods for the task, including CNN, LSTM and BERT.
2. {Novelty} How novel is the paper?
Main ideas of the paper are known or incremental advances over past work
3. {Soundness} Is the paper technically sound?
I have not checked all details, but the paper appears to be technically sound
4. {Impact} How important is the paper likely to be, considering both methodological contributions and impact on application areas?
The paper will have low overall impact
5. {Clarity} Is the paper well-organized and clearly written?
Good: paper is well organized but language can be improved
6. {Evaluation} Are claims well supported by experimental results?
Good: Experimental results are sufficient, though more analysis would significantly add support to the claims
7. {Resources} How impactful will this work be via sharing datasets, code and/or other resources? (It may help to consult the paper’s reproducibility checklist.)
Fair: some may find shared resources useful in future work
8. (Reproducibility) Would the experiments in the paper be easy to reproduce? (It may help to consult the paper’s reproducibility checklist.)
Good: e.g., code/data available, but some details of experimental settings are missing/unclear
9. {Reasons to Accept} Please describe the paper’s key strengths.
The strength lines in the task proposed for relation classification between speakers, and constructed a dataset for the task.
11. {Reasons to Reject} Please describe the paper’s key weaknesses.
The weakness lies in the methods, and the baselines are the standard models, and have no novelty,.
12. {Detailed Comments} Please provide other detailed comments and constructive feedback.
1) The tasks is something news, considering the difficulties in contrast to the standard relation classification ones, it may involve multiple sessions in a dialogue. Normally, the texts of the sessions are longer.

2) The problem is that the paper doesn't analyze the difficulties faced by the task, and doesn't give any models to address the challenges. For example, how to captue the deep meaning represented in the multiple sessions?
13. {QUESTIONS FOR THE AUTHORS} Please provide questions for authors to address during the author feedback period. (Please number them)

1) What are the challenges brought by the task?

2) How to expand the standard models to address the challenge?
15. (OVERALL SCORE)
5 - Below threshold of acceptance
Reviewer #4
Questions
1. {Summary} Please summarize the main claims/contributions of the paper in your own words. (Do not provide any review in this box)
The paper proposes a new task, corresponding novel dataset, and baseline evaluation. The authors also present comparison with human performance. The task is to predict nature of relation between speakers in dyadic dialogues.
2. {Novelty} How novel is the paper?
Paper contributes some new ideas
3. {Soundness} Is the paper technically sound?
I have not checked all details, but the paper appears to be technically sound
4. {Impact} How important is the paper likely to be, considering both methodological contributions and impact on application areas?
The paper will impact a moderate number of researchers
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: paper is well organized and clearly written
6. {Evaluation} Are claims well supported by experimental results?
Good: Experimental results are sufficient, though more analysis would significantly add support to the claims
7. {Resources} How impactful will this work be via sharing datasets, code and/or other resources? (It may help to consult the paper’s reproducibility checklist.)
Fair: some may find shared resources useful in future work
8. (Reproducibility) Would the experiments in the paper be easy to reproduce? (It may help to consult the paper’s reproducibility checklist.)
Excellent: e.g., code/data available and paper comprehensively describes experimental settings
9. {Reasons to Accept} Please describe the paper’s key strengths.
The key strength of the paper is the introduction of a novel task. The problem formulation is quite interesting as the target goal of extracting the relation between speakers may not be explicitly mentioned in the text.
Second point is the efforts from authors to establish the baseline. The current results clearly show that both humans as well as ML models perform quite weak for fine-grained relation detection.
11. {Reasons to Reject} Please describe the paper’s key weaknesses.
Both, the data annotation and human evaluation is based on only two persons. This is too low.

The model performance analysis is quite straight forward and coarse-grained. The authors should analyze more fine-grained aspects of model performance. For example, what are the patterns in the model failure, which examples are impossible to classify for models as well as humans etc. Such detailed analysis will add value to the paper and will guide others who will advance this task further.
12. {Detailed Comments} Please provide other detailed comments and constructive feedback.
I am quite happy with the problem formulation. However, my main reservation about the paper is the lack of rigorous analysis of performance of models and weak labeling of data. The authors should should overcome both these weaknesses.
13. {QUESTIONS FOR THE AUTHORS} Please provide questions for authors to address during the author feedback period. (Please number them)
While doing the pair level classification, do your models use the session level classification results explicitly?

While doing the pair level classification, have you observed if the humans find it sufficient to read only a few conversations between the pair? For example if I establish that the pair is father-son after reading the first conversation, then I need not read rest of the conversations for the pair.
15. (OVERALL SCORE)
5 - Below threshold of acceptance
Reviewer #5
Questions
1. {Summary} Please summarize the main claims/contributions of the paper in your own words. (Do not provide any review in this box)
This paper presents a dialogue relation extraction dataset, which focuses on interpersonal relationships between speakers crossing dialogue sessions. The empirical result shows that this is a hard task for current models as there is a large gap between the performance of current models and human performance.
2. {Novelty} How novel is the paper?
Main ideas of the paper are known or incremental advances over past work
3. {Soundness} Is the paper technically sound?
I have not checked all details, but the paper appears to be technically sound
4. {Impact} How important is the paper likely to be, considering both methodological contributions and impact on application areas?
The paper will have low overall impact
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: paper is well organized and clearly written
6. {Evaluation} Are claims well supported by experimental results?
Moderate: Experimental results are weak: important baselines are missing, or improvements are not significant
7. {Resources} How impactful will this work be via sharing datasets, code and/or other resources? (It may help to consult the paper’s reproducibility checklist.)
Fair: some may find shared resources useful in future work
8. (Reproducibility) Would the experiments in the paper be easy to reproduce? (It may help to consult the paper’s reproducibility checklist.)
Excellent: e.g., code/data available and paper comprehensively describes experimental settings
9. {Reasons to Accept} Please describe the paper’s key strengths.
1. The idea of predicting the interpersonal relationships between speakers in the dialogue is interesting.
2. The readability is great.
11. {Reasons to Reject} Please describe the paper’s key weaknesses.
1. The baselines are relatively simple. The authors leveraged the BERT model as a baseline by only adding the turn embedding. It will be great if the author can propose some competitive baselines, which address the uniqueness of this dataset.
2. Inter-annoatator agreement score changes. In Section 4.2, the authors mentioned that this dataset is annotated by only one annotator due to the excessive cost, and the quality of the dataset is verified by having a second person annotating the 100 sampled pairs, and kappa score for 13-classes is 82.3%. However, in Section 6.3, the kapper score for 13-classes in huaman evaluation is only 61.4%, so I am wondering why the kapper scores change so much between the annotation process and human evaluation.
3. Confusing experimental result. In Table 3, for the 13-class, the BERT baseline performs better than the human upper bound marginally on accuracy but much worse on F1-macro score. One reason might be BERT model does not work very well on long-tailed classes, which undermines its advantage on dominant classes and leads to a low F1-macro score. It will be great if the authors can explain this result clearly during the rebuttal.
12. {Detailed Comments} Please provide other detailed comments and constructive feedback.
N/A
13. {QUESTIONS FOR THE AUTHORS} Please provide questions for authors to address during the author feedback period. (Please number them)
1. Why is the human score so low on session-level 13-class classification (even worse than BERT on accuracy)?
2. Why don't you use F1-micro as the evaluation metric?
15. (OVERALL SCORE)
5 - Below threshold of acceptance
Reviewer #6
Questions
1. {Summary} Please summarize the main claims/contributions of the paper in your own words. (Do not provide any review in this box)
The authors proposed the dyadic dialogues interpersonal relationship classification task and constructed a dialogue dataset. The task includes two sub-tasks: the session-level relation classification task and the pair-level relation classification task. Their raw-dataset is extracted from the IMDb dataset and they introduce a second-annotator verification. Experimental results show that there is a large gap between DNN methods such as BERT and human.
2. {Novelty} How novel is the paper?
Paper makes non-trivial advances over past work
3. {Soundness} Is the paper technically sound?
I have not checked all details, but the paper appears to be technically sound
4. {Impact} How important is the paper likely to be, considering both methodological contributions and impact on application areas?
The paper will impact a moderate number of researchers
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: paper is well organized and clearly written
6. {Evaluation} Are claims well supported by experimental results?
Good: Experimental results are sufficient, though more analysis would significantly add support to the claims
7. {Resources} How impactful will this work be via sharing datasets, code and/or other resources? (It may help to consult the paper’s reproducibility checklist.)
Good: shared resources are likely to significantly impact future work
8. (Reproducibility) Would the experiments in the paper be easy to reproduce? (It may help to consult the paper’s reproducibility checklist.)
Good: e.g., code/data available, but some details of experimental settings are missing/unclear
9. {Reasons to Accept} Please describe the paper’s key strengths.
1. The dyadic dialogues interpersonal relationship classification task is novel and creative.
2. The construction of the dataset is rigorous and introduce a second-annotator verification. I check some items of the dataset, the quality of the dataset seems good.
3. The authors implement baseline models and show that the task is challenging for DNN and even for human.
11. {Reasons to Reject} Please describe the paper’s key weaknesses.
1. The work of the dataset construction itself is solid. However, the session-level classification task seems too hard even to human. Maybe a more proper metric can be considered, such as the mean rank. Hit@3 in 13 classes.
2. It may be better to discuss the potential influence to downstream tasks.
12. {Detailed Comments} Please provide other detailed comments and constructive feedback.
I have some concern about the task and dataset, see QUESTIONS FOR THE AUTHORS section.
13. {QUESTIONS FOR THE AUTHORS} Please provide questions for authors to address during the author feedback period. (Please number them)
1. The session-level classification task seems too hard even to human: only 60% Acc for 4-class classification. Does the authors condider other metrices such as mean ranks? Maybe it is hard for human to choose the correct class but the second candidate he or she choosed is correct.
2. Can downstream dialogue system benefit from interpersonal relationship learning model?
15. (OVERALL SCORE)
7 - Accept
Go Back
© 2020 Microsoft Corporation About CMT | Help | Terms of Use | Privacy & Cookies | Request CMT SiteTwitter CMT Support