\documentclass[11pt,a4paper]{article}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[linesnumbered, boxed, ruled]{algorithm2e}


\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\renewcommand\arraystretch{1.2}
\setlength\parskip{0.1\baselineskip}
\setlength{\textfloatsep}{0.5cm}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}

\usepackage{tikz}
\usepackage{geometry}
\usetikzlibrary{automata,positioning}
\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}


\title{xxxx}

\author{Xiwen Chen, Kenny Q. Zhu \\
	Advanced Data and Programming Technologies Lab \\
	Shanghai Jiao Tong University \\
	\texttt{\{victoria-x@sjtu, kzhu@cs.sjtu\}.edu.cn} \\
}
\date{}


\begin{document}

\appendix
\section{Experimental Results Using Sentence-BERT}
\label{ap:datastats}
We have also experimented with using sentence embeddings output by Sentence-BERT~\citep{reimers2019sentencebert} instead of LDA in the CSG component. Specifically, we used bert-large-nli-stsb-mean-tokens from official repo\footnote{\url{https://github.com/UKPLab/sentence-transformers}}, which is a BERT-based model fine-tune on NLI and STS tasks. The experimental results of ranking metrics are listed below in Table \ref{table:instantiations}:
\begin{table}[h]
	\small
	\centering
	\begin{tabular}{cc c c c c c c c}
		\toprule
		\multicolumn{2}{c}{\textbf{Instantiation}} &\multirow{2}{*}{F1@3} &\multirow{2}{*}{P@1} &\multirow{2}{*}{P@3} &\multirow{2}{*}{R@3} &\multirow{2}{*}{MRR} &\multirow{2}{*}{NDCG@10} &\multirow{2}{*}{Semantic Sim.@3} \\
		\\ [-1.8ex]
		\cline{1-2}
		\\ [-1.8ex]
		CSG &DS & & & & & & &\\
		\midrule
		\multirow{4}{*}{WordNet} 
		&point-wise ranker &7.41 &9.26 &5.41 &13.13 &13.33 &14.78 &0.33 \\
		&pair-wise ranker &6.58 &8.49 &4.76 &11.71  &12.47 &14.09 &0.32 \\
		&list-wise ranker &7.34 &8.11 &5.15 &13.51 &12.21 &14.31 &0.32 \\
		\midrule
		\multirow{4}{*}{Probase} 
		&point-wise ranker &6.16 &6.56 &4.63 &10.29  &11.95 &12.56 &0.36 \\	
		&pair-wise ranker &6.18 &6.18 &4.77 &10.03  &12.64 &13.63 &0.35 \\
		&list-wise ranker &6.21 &8.89 &4.89 &9.84 &14.26 &13.98 &0.35 \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of combinations of different choices of CSG and DS.}
	\label{table:instantiations}
\end{table}
%Table \ref{tb:datastats} shows the vocabulary size, average sentence length, number of adjectives and Flesch readability of our two datasets. For each dataset, we only selected a specific pair of styles, which are Michael R. Katz/Richard Pevear for LT, and simple/standard Wikipedia for GSD. (Flesch readability ranges from 0 to 100, with higher meaning easier to read.)
%
%\begin{table*}[th]\footnotesize
%	\centering
%	\begin{tabular}{c|cccc}
%		& Vocab Size & Avg. Length & \#. of Adj. & Flesch \\
%		\hline
%		LT & 12589 / 13643 & 19.8 / 21.3 & 15338 / 13623 & 65.1 / 66.2 \\
%		GSD & 18124 / 20112 & 11.1 / 11.8 & 10692 / 12070 & 58.5 / 52.3
%	\end{tabular}
%	\caption{Dataset characteristics.}\label{tb:datastats}
%\end{table*}

%\section{More Generated Samples}
%
%Table \ref{tb:qualmore} lists some more samples that are randomly selected from the generated sentences of all models.
%
%\begin{table*}[th]\footnotesize
%	\centering
%	\begin{tabular}{c|c}
%		\hline
%		\textbf{Original Sentence} (Yelp positive) & \emph{the staff is welcoming and professional .} \\
%		\hline
%		Template & \emph{the staff is welcoming and professional .} \\
%		CrossAlign & \emph{glad glad glad} \\
%		CrossAlign (pretrained) & \emph{the staff is welcoming and professional .} \\
%		DeleteRetrieve & \emph{the staff is a time .} \\
%		DualRL & \emph{less expensive have working .} \\
%		VAE & \emph{the staff is rude and rude} \\
%		VAE (pretrained) & \emph{the staff is extremely welcoming and professional .} \\
%		\hline
%		ST$^2$-CrossAlign (ours) & \emph{the staff is friendly and unprofessional} \\
%		ST$^2$-VAE (ours) & \emph{the staff are rude and unprofessional .} \\
%		\hline
%		\hline
%		\textbf{Original Sentence} (Yelp negative) & \emph{these people do not care about patients at all !} \\
%		\hline
%		Template & \emph{these people wonderful about patients at all !} \\
%		CrossAlign & \emph{glad glad glad} \\
%		CrossAlign (pretrained) & \emph{these people do not care about patients at all !} \\
%		DeleteRetrieve & \emph{i was n't be a a appointment and i have .} \\
%		DualRL & \emph{and just like that it was over and i was .} \\
%		VAE & \emph{these people do not care about patients or doctors} \\
%		VAE (pretrained) & \emph{these guys do n't care about the patients at time} \\
%		\hline
%		ST$^2$-CrossAlign (ours) & \emph{these people do not satisfied at all !} \\
%		ST$^2$-VAE (ours) & \emph{i was so happy and i did n't consent} \\
%		\hline
%	\end{tabular}
%	\caption{Randomly selected sample outputs for the Yelp positive/negative review dataset.}\label{tb:qualmore}
%\end{table*}
\section{Application}
%\KZ{Add another pic side-by-side that shows the front page of iWen and also a pic that
%shows the context of this question, to convince ppl that this is a real app.
%Also tell them how long this app has been deployed, where they can download it and what is the
%lastest version.}
\begin{figure*}[ht!]
	\centering
	\scalebox{0.8}{\includegraphics[width=1.0\columnwidth]{figure/iwen_new_new.jpg}}
	\caption{The mobile app in which the proposed distractor generation framework is 
		deployed. 
		The right-most screenshot shows one generated MCQ given the news context in 
		the previous screenshot.}
	\label{fig:app}
\end{figure*}
In this section, we demonstrate how the proposed framework is deployed in a real-world application. The  most suitable scenario to implement our distractor generation system is AI-assisted education, especially in language learning. Our framework has been successfully deployed in an English learning and reading app. At the end of each chapter of book or news, it will generate 
several multiple-choice questions to assess how well users comprehend the content. 
In the implementation, after locating question-worthy context as question body and keyword as 
correct answer, our CSG+DS distractor generation framework is then applied to 
automatically generate distractive options that will be put together with correct answer 
to form a complete multiple-choice question.

An overview of the app and application of our 
distractor generation framework is shown in \figref{fig:app}, where the user 
selects \textit{signed} as the answer after reading the news:

The ideal distractors in MCQs are required to be plausible and reliable to fully test 
the users, which is to a large extent satisfied by our proposed framework. Recent statistics from the app shows that a user may take the quiz from either books or news 9.4 times per day on average and the chances of users making an incorrect
answer is about 42\%, which validates the effectiveness of our CSG+DS 
framework in practical use.

\bibliography{distractor}
\bibliographystyle{aaai21}
\end{document}
