We sincerely appreciate all the reviewers' efforts and reviews.


Detailed responses:
- "... such labor is substantial ... would not be completely open-domain": Due to the domain-specific training paradigm of previous distractor generation methods, one has to carefully identify the possible domain of an incoming cloze sentence either through manual inspection or heuristically designed matching approaches and then apply corresponding distractor generator. The former will cause non-trivial human labor and the latter will lead to sub-optimal performance. Importantly, many real-world scenarios requiring distractor generation are actually open-domain, particularly for language learning. It will be extremely desirable if a distractor generation system is able to seamlessly plugin.
- "... enough statistical power for me to trust it.": Please refer to the line 257-272 and line 397-405, where we stated that we employed two types of human evaluation and the details of them. The two types of human evaluation assess distractor generation in different settings and aspects to provide a comprehensive evaluation. The kappa inter-judge agreement scores are 0.658 and 0.743 respectively for plausibility and reliability, indicating strong reliability of our human evaluation.


In response to Reviewer #3: Thanks for your valuable review.
Detailed responses:
- "improvement does not look impactful.": Because we use our newly compiled distractor generation benchmark, direct comparisons with prior distractor approaches are not straightforward. To fairly test the capabilities of previous approaches on our open-domain setting, we accommodate all baselines into our framework by providing them distractor candidates pool composed of CSG component. Based on the unified candidate pool, our DS component also outperforms all baselines both in automatic and human evaluation. For details please refer to the first piece of responses to Reviewer #1.
- In line 134, we apologize for the potentially misleading example of "shore". Our goal is to show the necessity of disambiguating the sense of the key.
- For the web-search score, we will make it more clear and illustrative in a later version, and we give a brief explanation here: it measures the reliability of distractor candidate d by maximal semantic similarity between structured triple (arg1, relation, arg2) extracted from the synthesized sentence and search results from Bing. A high similarity score indicates low reliability.
- The kappa inter-judge agreement scores are 0.658 and 0.743 respectively for plausibility and reliability.
- We will include more diverse and comprehensive examples in the revised appendix. And we have already included the application of our proposed framework in a mobile educational English APP. Our framework is able to successfully generate distractors in various domains for reading materials like news and books.
- We will fix the typo in a later version.

Reviewer #4: 
Thanks for your detailed review and we do our best to resolve your questions here.
Detailed responses:
- "... The contribution there is incremental.": Our contribution is not the learning-to-rank method and it is indeed not a novel technique. Detailed explanation please refer to the first piece of responses to Reviewer #1 since Reviewer #4 and Reviewer #1 have similar concerns.
- "... requiring domain knowledge is not well supported": Prior work like Liang et al. 2018 is domain-dependent in that the distractor candidates pool is usually simply formed by domain-specific(science domain for Liang's case) vocabulary of the dataset it is trained on, hence the task is simplified and their model is particularly biased by the specific domain both in the training and inference phase. We cannot expect a sole ranking model trained on the scientific domain to produce reasonable distractors when used out of the domain. The creation of the benchmark dataset is also motivated by this reason in order to cover a broader range of domains to reduce biases in both training and testing.
- "...comparison with prior published results on component datasets is unavailable.": In addition to the results over the whole dataset, we also reported results of our framework broken down by domain(over 80% of instances in the science domain are from SciQ and MCQL) in Figure 5 to demonstrate the challenges of distractor generation in different domains, the trend of which is commonly shared by all baselines. We only report the overall performance considering all domains to more faithfully reflect the practical applicability of distractor generation systems in the real-world.
- "... in a more realistic control-test manner": We conducted two types of human evaluations as stated in line 257-272, the first one is to explicitly ask annotators to score the plausibility and reliability distractors and the second one(line 397-405) is performed in a more realistic control setting. Our second type of human evaluation poses quizzes to three senior English speakers and the result showcases our framework performs the best.
- "how is it helping assessing language learning skills?": Cloze-style multiple choice question is a widely adopted form of quizzes in examinations in a lot of countries, more particularly to test non-native speakers. Vocabulary learning is presumably the most straightforward a foreign language and can be seamlessly incorporated in cloze multiple-choice questions.
Go Back