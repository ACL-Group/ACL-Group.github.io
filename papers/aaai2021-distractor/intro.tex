\section{Introduction}
\label{sec:intro}
%\KZ{I find .tex files contains \string^M at the end of lines Remove them (using a tool)?}
% We focus on word+entity+verb distractors
Cloze-style multiple choice question (MCQ) is a common form of exercise used to evaluate the proficiency of language learners, frequently showing up
in homework and online testings.
Figure \ref{fig:mcq} shows a cloze-style MCQ, which typically 
consists of: a question stem with a blank to be filled in, 
the correct answer and multiple wrong answers used to distract testees.
Despite the high demand, manual crafting of such MCQs is highly time-consuming 
for educators, which calls for the automatic generation of 
as much practice material as possible from readily available plain texts so that formally usable quizzes can be generated after light-weight human calibration.

\begin{figure}[!htb]
	\centering
	\scalebox{1.0}{\includegraphics[width=1.0\columnwidth]{figure/mcq.eps}}
	\caption{A cloze-style MCQ} \label{fig:mcq}
	\end{figure}

Distractor generation, which aims to generate distractive alternatives~(i.e., distractors) of the correct answer given the question stem, is a critical part of cloze-style MCQ construction. However, it is not only time-consuming but also non-trivial to produce appropriate distractors without rich experience in language education.

Literature in language pedagogy~\cite{haladyna2002review,pho2014multiple} generally recommends 
two criteria for designing distractors: 
{\em plausibility} and {\em reliability}. 
By plausibility, it means distractors should be semantically related to 
the key and grammatically consistent with the context given by stem 
to adequately discriminate learners' proficiency. 
By reliability, it means the distractor, when filled into the blank of
the stem, results in a logically incorrect or inconsistent statement.

Automatically generating distractors has been previously explored as part of cloze-style MCQ construction in a few studies. However, those methods generally assume prior knowledge of a specific domain~(e.g., science) of 
the given question and then use corresponding domain-specific vocabulary 
as candidate distractor set, ranked by various unsupervised 
similarity heuristics~\cite{sumita2005measuring,Kumar2015RevUP,jiang2017distractor,conceptEmb} or supervised machine learning model~\cite{sakaguchi2013discriminative,welbl2017crowdsourcing,liang2018distractor}.
Since identifying the concrete domain of each question and 
preparing large-scale domain-specific vocabulary require 
substantial human labor, such corpus-based methods cannot be easily 
applied in real-world scenarios.

Another issue is that previous approaches mainly focus on selecting plausible distractors 
while rarely adopt reliability checks to ensure that the generated 
distractors are logically incorrect. 
Despite some attempts in early approaches~\cite{sumita2005measuring,jiang2017distractor}, 
they both used it in the post-processing step to filter out candidate 
distractors rejected by diverse predefined filters such as syntactic feature~(e.g., role in the dependency parse tree), 
which may exclude useful distractors like \textit{DNA} in 
Figure \ref{fig:mcq}.

% Moreover, another issue is that many previous work only focus on how to select plausible distractors, and adopt no or weak reliability checking, which is a critical part to ensure there is only one correct answer in each item.
% Although Sumita et al.~\shortcite{sumita2005measuring} conduct reliable checking based on retrieval from the Web, their hypothesis that an instance is a correct answer if it appears in search results of the stem is too strict. For the example in Figure \ref{fig:mcq}, \textit{DNA} is rejected by their reliable checking filter since it often appears together with \textit{genes} in web pages, but it is a very good distractor.

%简要介绍方法的亮点，需要和contribution对应
In this paper, we propose a configurable distractor generation framework for English cloze-style MCQ in the open domain, whose design is motivated by the shortcomings identified above. It mainly consists of two components: 
(1) a \textit{context-dependent} candidate set generator, which constructs a small set of 
candidate distractors from a general-purpose knowledge base, 
based on contextual information formed by the stem and the key; 
(2) a learning-to-rank model that takes both reliability checking and plausibility measures into consideration. By incorporating structured, human-curated
general-purpose knowledge base and conducting context-dependent 
conceptualization on the answer, we are able to effectively extract 
semantically-related candidate distractors without the need of 
domain-specific vocabulary. These candidate distractors are further re-ordered 
by a ranking model, trained with elaborately designed features to control the trade-off between plausibility and reliability.

%To overcome these shortcomings, we propose an open-domain, data-driven framework for generating word distractors that are both plausible and reliable with contextual fit. Based on contextual information, We construct a small-size candidate set from a large-scale general-purpose knowledge source, Probase, which is constructed from billions of web pages. Since this candidate source is generalizable and diverse enough, our method is adoptable in different domains. While generating distractors, we are actually making a trade-off between plausibility and reliability, so we can not consider than them individually. Instead of conducting candidate filtering~\cite{jiang2017distractor}, we integrate new reliable checking features with other plausibility measures into distractor selector. 
% Our reliable checking features evaluates the correctness of (argument1, relation, argument2) triples extracted from the full sentence formed by the stem and a candidate and the collocations involving the candidate. 

%With these methods, our framework generates \textit{antigens, DNA, viruses} for the example in Figure \ref{fig:mcq} whose quality is comparable with \textit{DNA, eggs, genomes} designed by human experts. 
Previous DG methods~\cite{Kumar2015RevUP,liang2017distractor,liang2018distractor} are evaluated either with sole human annotation or on ad hoc datasets that are often narrow in domain.
To the best of our knowledge, there is no open-source benchmark dataset for DG that is diverse enough to comprehensively evaluate the model performance. We compile a cross-domain cloze-style MCQ dataset covering science, trivia, vocabulary and common sense, which can be used as a benchmark for future research in DG. We further
investigate various instantiations of the framework. 
%Experimental results of both automatic and human evaluation show that our proposed framework achieves significant and 
%consistent improvement compared to several competing baselines.

The contributions of this paper are three-folds:
\begin{itemize}
	\setlength{\itemsep}{1pt}
	\setlength{\parsep}{1pt}
	\setlength{\parskip}{1pt}
	\item we compile and open-source a diverse and comprehensive benchmark dataset for training and evaluating distractor generation model~(\secref{sec:data}).
	\item we propose a configurable distractor generation framework for open-domain cloze-style MCQ, which requires no domain-specific vocabulary and jointly evaluates the plausibility and reliability of distractors (\secref{sec:method}).
	\item we conduct comprehensive experiments to evaluate and analyze various instantiations of our framework and show that it consistently outperforms previous methods in both automatic ranking measures~(about 2\% F1 score) and human evaluation~(\secref{sec:endtoend}).
\end{itemize}
