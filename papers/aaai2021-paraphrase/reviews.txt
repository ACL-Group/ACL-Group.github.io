Review 1
1. {Summary} Please summarize the main claims/contributions of the paper in your own words. (Do not provide any review in this box)
This paper presents a novel framework for paraphrase generation without parallel data. The key idea here is to convert a sentence into a set and give the perturbed set as input to reconstruct the original sentence. The perturbation is as simple as replacing some of the words with its synonyms. Further, the paper also proposes to use a back-translation model during inference jointly with the set-to-sentence generation model for generating the paraphrases. The proposed framework achieves significant improvements over the previous state-of-the-art in many datasets.
2. {Novelty} How novel is the paper?
Main ideas of the paper are creative and distinctive
3. {Soundness} Is the paper technically sound?
I have carefully checked all details and did not find any technical flaw
4. {Impact} How important is the paper likely to be, considering both methodological contributions and impact on application areas?
The paper will impact a moderate number of researchers
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: paper is well organized and clearly written
6. {Evaluation} Are claims well supported by experimental results?
Excellent: Experimental results are comprehensive and strongly support the claims
7. {Resources} How impactful will this work be via sharing datasets, code and/or other resources? (It may help to consult the paper’s reproducibility checklist.)
Good: shared resources are likely to significantly impact future work
8. (Reproducibility) Would the experiments in the paper be easy to reproduce? (It may help to consult the paper’s reproducibility checklist.)
Meets Minimum Standard: e.g., code/data unavailable, but paper is clear enough that an expert could confidently reproduce
9. {Reasons to Accept} Please describe the paper’s key strengths.
1) The proposed framework is very novel and an interesting idea to apply for paraphrase generation.
2) The empirical evaluation is very thorough and all the presented ablations are useful to know which components are important.
3) The proposed framework achieves state-of-the-art results in many datasets.
11. {Reasons to Reject} Please describe the paper’s key weaknesses.
1) During the decoding of set2seq+BT model, a constant hyperparameter (lambda) is set to weight the two vocabulary distributions from set2seq and BT models. However, it might be more appropriate to dynamically weight the two distributions at each time step by some means? I don’t think it’s a major issue but good to try some ways to achieve dynamic weighting, as it might further improve the results.
12. {Detailed Comments} Please provide other detailed comments and constructive feedback.
1) I would suggest reporting the performances on the METEOR metric as well since it considers synonyms while calculating the matching score.
2) Please provide more details on the training. Unfortunately, I couldn’t open the supplementary folder as it is showing some error, so if I missed any details of the training from it, I am sorry.
13. {QUESTIONS FOR THE AUTHORS} Please provide questions for authors to address during the author feedback period. (Please number them)
1) What is the effect of various values of the lambda hyperparameter over the model’s performance? Is there a way to replace the constant weight (lambda) with dynamic weight at each time step.
2) Please report statistical significance scores wherever possible. These are completely missing for all the experiments
15. (OVERALL SCORE)
9 - Accept: will fight to get it accepted

=============================================================================================================
Review 2
1. {Summary} Please summarize the main claims/contributions of the paper in your own words. (Do not provide any review in this box)
This paper proposes a novel paraphrasing method that uses set-to-sequence generation and back-translation jointly.
The two technologies would work in a complementary way and demonstrate some improvements in automatic and human evaluation.
2. {Novelty} How novel is the paper?
Paper contributes some new ideas
3. {Soundness} Is the paper technically sound?
I have not checked all details, but the paper appears to be technically sound
4. {Impact} How important is the paper likely to be, considering both methodological contributions and impact on application areas?
The paper will have low overall impact
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: paper is well organized and clearly written
6. {Evaluation} Are claims well supported by experimental results?
Good: Experimental results are sufficient, though more analysis would significantly add support to the claims
7. {Resources} How impactful will this work be via sharing datasets, code and/or other resources? (It may help to consult the paper’s reproducibility checklist.)
Fair: some may find shared resources useful in future work
8. (Reproducibility) Would the experiments in the paper be easy to reproduce? (It may help to consult the paper’s reproducibility checklist.)
Good: e.g., code/data available, but some details of experimental settings are missing/unclear
9. {Reasons to Accept} Please describe the paper’s key strengths.
- Reasonable combination of the two techniques
- Performance improvements in paraphrasing generation
11. {Reasons to Reject} Please describe the paper’s key weaknesses.
- Not fully convincing comparisons in the machine translation experiments
12. {Detailed Comments} Please provide other detailed comments and constructive feedback.
The proposes approach seems reasonable for balancing lexical and sentence meanings by the two different models.
My concern is on the machine translation experiments.
The authors could provide experimental results with the back-translation-based paraphrasing to reveal the application-oriented advantages of the proposed method.
13. {QUESTIONS FOR THE AUTHORS} Please provide questions for authors to address during the author feedback period. (Please number them)
- Can you evaluate the paraphrases in a semantic-oriented way like BERTScore?
15. (OVERALL SCORE)
6 - Above threshold of acceptance