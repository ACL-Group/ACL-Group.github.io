\section{Conclusion}
\label{sec:conclude}

In this paper, we extend the concept of text style to general writing styles with limited size of data. To tackle this new challenging problem, we propose a multi-task style transfer (ST$^2$) framework, which is the first of its kind to apply meta-learning to small-data text style transfer. We use the literature translation dataset and the grouped standard dataset to evaluate the state-of-the-art models and our proposed framework. 

Both quantitative and qualitative results show that ST$^2$ outperforms the state-of-the-art baselines. Unlike previous state-of-the-art models that are resource-demanding to impart rich knowledge into the networks, ST$^2$ is able to effectively utilize off-domain information to improve both language fluency and style transfer accuracy.

Since baseline models might not be able to learn an effective language models from small datasets, which is a possible reason for their bad performances, we further eliminate this bias by pretraining the base models using data from all tasks. From the results, we ascertain that the enhancement of meta-learning framework is substantial.
