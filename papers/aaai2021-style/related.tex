\section{Related Work}
\label{sec:related}
%\KZ{Should be related work...}
%disentangling style and content
The first line of work aim at disentangling style and semantic content in the learned latent space. \citet{fu2018style} devised a multi-encoder and multi-embedding scheme to learn a style vector via adversarial training. Adapting a similar idea, \citet{zhang2018shaped} built a shared private encoder-decoder model to control the latent content and style space. Also based on a seq2seq model, \citet{shen2017style} proposed a cross-align algorithm to align the hidden states with a latent style vector from the target domain using teacher-forcing.  \citet{john2018disentangled} used well-defined style-oriented and content-oriented losses based on a variational autoencoder to separate style and content in latent space. More recently, \citet{Yi2019} incorporated generative flow to model the target style space using instances from the target corpus. \citet{he2020probabilistic} proposed a probabilistic formulation of style transfer task using a deep latent variable model. \citet{Xu2019} identified the latent space vacancy problem within VAE-based models and proposed CP-VAE to constrain the latent space to be a smaller probability simplex.

%direct modification
The second line of work seek to disentangle style and content in the surface text form. \citet{li2018delete} proposed the DRG framework to directly remove style attribute words based on TF-IDF weights and trained a generative model that takes the remaining content words to construct the transferred sentence. \citet{Sudhakar2020} extended DRG with pretrained Transformers and obtained significant improvement. Inspired by the recent achievements of masked language models, \citet{wu2019mask} used an attribute marker identifier to mask out the style words in the source domain, and trained a ``infill'' model to generate sentences in the target domain.

%reinforcement learning
To improve on metrics used to evaluate a style transfer system, based on reinforcement learning, \citet{xu2018unpaired} proposed a cycled-RL scheme with two modules, one for removing emotional words (neutralization), and the other for adding sentiment words (emotionalization). \citet{wu2019hierarchical} devised a hierarchical reinforced sequence operation method using a \emph{point-then-operate} framework, with a high-level agent proposing the position in a sentence to operate on, and a low-level agent altering the proposed positions. \citet{luo2019dual} proposed a dual reinforcement learning model to jointly train the transfer functions using explicit evaluations for style and content. Although their methods work well in large datasets such as Yelp~\citep{asghar2016yelp} and GYAFC~\citep{rao2018dear}, it fails in our few-shot scheme.

%Revision based
Instead of separating style and content in the latent space, \citet{Liu2019} employed an edit-based model which directly modifies the entangled sentence embedding using gradient provided by a set of attribute classifier. A similar idea was also adopted by \citet{wang2019}, but Transformers were used instead of a recurrent network.

%meta-learning
There are also meta-learning applications on text generation tasks. \citet{qian2019domain} used the model agnostic meta-learning algorithm for domain adaptive dialogue generation, which requires paired data for training and is different from our task. To enhance the content-preservation abilities, \citet{li2019domain} proposed to first train an autoencoder on both source and target domain. But in addition to utilizing off-domain data, we are applying meta-learning method to enhance models' performance both in terms of language modeling and transfer abilities.

