1. Section 2 MAML应该算Preliminary ST2作为2.2单独讲 篇幅比重preliminary占比有点多
    roy: 把Literature translation construction放到Approach section，突出contribution.
2. Classifier怎么提升准确率（尤其GSD)
    replaced with roberta-large.
3. 为什么ST2比pretrain要好?解释一下
    roy: pretrain是为了获得更好的language model.
4. Table4改进一下？现在ST2比baseline好在哪里不太明显？
    roy: 添加了overall performance metrics(G3 and H3)
5. Template为啥这么好？
    roy: template的方法决定了它的perplexity和content preservation很高，但transfer acc不高
6. Table 5 为啥不把template等baseline也pretrain之后比？（那么为啥不把ST2也加到template等baseline上？是因为不好加吗？）
    roy: Template方法没有用到language model作为generator，所以不pretrain
7. Move dataset creation into approach..
    roy: done
8. add bi-dir VAE, latent space baselines.
    roy: another two recent state-of-the-art style transfer methods(B-GST(EMNLP2019) and CP-VAE(ICML2020))
