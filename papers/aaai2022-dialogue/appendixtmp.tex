\appendix

\section{QA2D dataset}
More details about QA2D dataset are shown in Table \ref{tab:qa2ddata}.
\begin{table}[h]
	\centering
	\begin{tabular}{lrrr}
		\toprule[1pt]
		\textbf{} & {Train}& {Val}& {Test} \\ 
		\midrule[1pt]
		{\#samples} & 60,710 &5,172&5,172\\
		{\#$[Q,A]$W} & 14.59 & 14.84&14.76\\
		{\#$D'$W} &12.91 &13.04&12.95\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Statistics of the QA2D dataset. \#*W represents the number of words in the corresponding text.}
	\label{tab:qa2ddata}
\end{table}

\section{Data Collection for DCD Dataset}

The number of ($D$, $D'$) pairs transformed from three dialogue reading comprehension datasets are shown in Table \ref{tab:qa2dstatistics}.

\begin{table}[h]
	\centering
	\begin{tabular}{lrrr}
		\toprule[1pt]
		\textbf{} & {Train}& {Validation}& {Test} \\ 
		\midrule[1pt]
		{DREAM} & 6,116 &2,040&2,041\\
		{FriendsQA}& 9,874 & 1,201&1,182\\
		{FriendsRC} &10,785&1,349&1,353\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Statistics of transformed ($D$,$D'$) pairs.}
	\label{tab:qa2dstatistics}
\end{table}

\section{Dial2Text Performance on CRD3 Dataset}

We also did experiment on CRD3 dataset~\cite{rameshkumar2020storytelling}, which contains role-playing dialogues between multiple participants. The number of train/dev/test dialgoues are 26232/3470/4541. There are around 478.97 words in each dialogue. The compression ratio of this dataset is 17.35\%, similar to DialSumm. The abstractiveness of CRD3 is much lower than other datasets especially reflected by Rouge-2 with 23.45\%. 

Results on the CRD3 test set are shown in Table \ref{tab:crd3results}. Ext and Abs represent the extractive and abstractive baselines proposed by Rameshkumar and Bailey~\shortcite{rameshkumar2020storytelling}, which are derived from Fast-Abs~\cite{chen2018fast}.
The extractive baseline performs much better than the abstractive one, also reflecting the high extractiveness of this dataset. 
BART and our proposed Dial2Text exceeds both baselines while performances on BART and Dial2Text are similar to each other.
It shows that our proposed approach are more suitable for more abstractive dialogue summarization tasks. At the same time, our post-training process didn't hurt the ability of the model on extractive datasets.

\begin{table}[h]
	\centering
	\small
	\begin{tabular}{lccc}
		\toprule[1pt]
		\textbf{Models} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} \\
		\midrule[1pt]
		%\multicolumn{4}{l}{\textit{Average among References}}\\
		%\midrule[1pt]
		{Ext} &25.20 &\textbf{9.23} &22.20 \\
		{Abs} & 23.25&4.91 &21.41\\
		{BART} & \textbf{27.04}&8.38 &\textbf{23.99} \\
		{Dial2Text} &26.95 &8.37 &23.97  \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Results(\%) on CRD3 test set.}
	\label{tab:crd3results}
\end{table}



\section{Dial2Text Fine-tuning with Additional Features}

We fine-tune our post-trained BART, i.e. Dial2Text, with input dialogues labeled with different features as shown in Table \ref{tab:adddial2text}.
Discrete, Topic and Stage are borrowed from Multi-view. DialoBART w/o key represents DialoGPT labelled data without ``\#KEY\#" parts since its data format is quite different. Original, representing dialogues without any labels, ranks first on all of Rouge scores.

\begin{table}[h]
	\centering
	\small
	\begin{tabular}{lccc}
		\toprule[1pt]
		\textbf{Models} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} \\
		\midrule[1pt]
		%\multicolumn{4}{l}{\textit{Average among References}}\\
		%\midrule[1pt]
		{Original} &\textbf{48.39} &\textbf{25.92} &\textbf{47.21} \\
		{Discrete} &47.77&25.34& 46.51\\
		{Topic} &48.36&25.09&46.96 \\
		{Stage} &47.32&24.67&45.83  \\
		{DialoBART} &47.37&24.72&45.92 \\
		{DialoBART w/o Key} &48.37&25.35&46.93  \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Results(\%) of Dial2Text fine-tuning with additional features on SAMSum test set.}
	\label{tab:adddial2text}
\end{table}

\section{Human Evaluation Analysis}

We show the overall score for each method from different annotators in Table \ref{tab:annotator}. The divergence among three annotators may caused by different degree of strictness, which varies from person to person. However, the overall trends are the same: Reference is the best, Bart is the worst, and Dial2Text performs slightly better than other two competitive methods.

\begin{table}[h]
	\centering
	\small
	\begin{tabular}{lccc}
		\toprule[1pt]
		\textbf{Models} & \textbf{Annotator 1} & \textbf{Annotator 2} & \textbf{Annotator 3} \\
		\midrule[1pt]
		BART & 0.15& -0.3&0.45\\
		Multi-view & 0.19& 0.05&0.67 \\
		DialoBART & 0.2& 0.09& 0.63\\
		Dial2Text &\textbf{0.25} &\textbf{0.11} & \textbf{0.68}\\
		\midrule[1pt]
		Reference & 1.67& 1.17&1.79 \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{The overall scores from different annotators.}
	\label{tab:annotator}
\end{table}

%To further compare the performance of Dial2Text with Multi-view and DialoBART, we transform the scores into ``Better-Equal-Worse" by comparing original scores within each sample. The Fleiss Kappa is also 0.34 with fair agreement. The results in Table \ref{tab:betterworse} shown the competitive results of Dial2Text with the state-of-the-art methods. It should be noted that Dial2Text can be easily applied to other dialogue scenarios while others can't.

%\begin{table}[h]
%	\centering
%	\small
%	\begin{tabular}{lcc}
%		\toprule[1pt]
%		\textbf{Annotator} & \textbf{Comparison} & \textbf{Better-Equal-Worse} \\
%		\midrule[1pt]
%		\multirow{2}{1pt}{1} & Dial2Text v.s. Multi-view & 51-107-42 \\
%		& Dial2Text v.s. DialoBART & 51-101-48 \\
%		\multirow{2}{1pt}{2} & Dial2Text v.s. Multi-view & 32-137-31\\
%		& Dial2Text v.s. DialoBART &42-119-39 \\
%		\multirow{2}{1pt}{3} & Dial2Text v.s. Multi-view & 27-147-26\\
%		& Dial2Text v.s. DialoBART &34-136-30 \\
%		\bottomrule[1pt]
%	\end{tabular}
%	\caption{The comparisons from different annotators.}
%	\label{tab:betterworse}
%\end{table}

\begin{table}[h]
	\centering
	\small
	\begin{tabular}{l}
		\toprule[1pt]
		\textbf{Multi-view} \\
		\hline
		\textit{Topic View:} William: hey im making spaghetti William: could\\ you please buy some fresh tomatoes \textbf{$\mid$} William: pretty please :) \\Olivia: no problem dear :) William: and Beth? it wouldn't hurt\\ to have some chocolate for after the dinner :D Beth: I'm on it\\ :D \textbf{$\mid$}\\
		\textit{Stage View:} \textbf{$\mid$} William: hey im making spaghetti William: could\\you please buy some fresh tomatoes \textbf{$\mid$} William: pretty please :) \\Olivia: no problem dear :) \textbf{$\mid$} William: and Beth? it wouldn't hurt\\to have some chocolate for after the dinner :D \textbf{$\mid$} Beth: I'm on it\\:D\\
		\hline
		\textbf{DialoBART} \\
		\hline
		William : hey im making spaghetti \textbf{$\mid$} William : could you please\\buy some fresh tomatoes \textbf{$\mid$} William : pretty please \textbf{$\mid$} Olivia : no\\problem dear \textbf{$\mid$} William : and Beth ? it wouldn't hurt to have\\ some chocolate for after the dinner :D \textbf{$\mid$} Beth : I'm on it :D \\\textbf{\#KEY\#} Olivia Beth William hey im making spaghetti Beth it :D\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{The annotated dialogue by Multi-view and DialoBART.}
	\label{tab:annodial}
\end{table}



The labeled dialogue, which are directly extracted from Multi-view's and DialoBART's released datasets are shown in Table \ref{tab:annodial}. ``$\mid$'' label for Multi-view refers to the topic transitions and state transitions for the same dialogue respectively. Both suffers from some wrong labels, such as the topic transition after the second utterance and the stage transitions before the last utterance from Beth.  ``$\mid$'' in DialoBART just refers to the end of each utterances. It actually fails to identify any topic segments or redundancies in this dialogue. There are also some keywords making no sense, such as ``hey'' and ``:D''.

They failed on such simple dialogues due to following reasons. Firstly, these designed features are more focus on content selection by segment the dialogue into pieces, annotate the redundant piece or highlight the key word. Secondly, it's doubt that whether the model can learn such labelling schemas well. Thirdly, these features are labeled with other labeling tools and human designed rules, which may not suitable for all cases, leading to error propagation.