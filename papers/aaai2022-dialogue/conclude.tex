\section{Conclusion}
\label{sec:conclusion}

In this work, we propose a new approach for dialogue summarization by post-training with cross-format data. The results on datasets covering different scenarios and granularities show the advantages of our approach over the pre-trained language model BART and other state-of-the-art baselines. Creating self-supervised tasks for cross-format post-training and designing compatible features for downstream fine-tuning are valuable future directions, which may not only work for dialogue summarization but also other unexploited scenarios.