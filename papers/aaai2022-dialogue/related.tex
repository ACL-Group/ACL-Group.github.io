\section{Related Work}
\label{sec:relatedwork}

%Consideration of our approach over previous approaches on 
Dialogue summarization and pretrained language models are discussed as follows.

%\subsection{Dialogue Summarization}

\textbf{Dialogue Summarization:} A growing number of works have been proposed for dialogue summarization in recent years~\cite{feng2021survey}. They widely explore explicit features by using pre-trained tools and labeling the input dialogue with corresponding labels, and may also modify the vanilla pretrained encoder-decoder model to utilize their additional labels.
Multi-view~\cite{chen2020multi} proposes to add separators between input utterances by topic segments and conversational stages with a double-encoder design.
Structure-BART~\cite{chen2021structure} constructs inter-utterance discourse graphs and intra-utterance action graphs with a graph encoder layer between the encoder and decoder structure.
ConvoSumm~\cite{fabbri2021convosumm} improves summarization by constructing an argument graph for each dialogue session and linearizing the graph as the input for models.
DialoBART~\cite{feng2021language} takes advantage of the pretrained response generator model DialoGPT\cite{zhang2020dialogpt} to label features with special tokens including keywords extraction, redundancy detection and topic segmentation.
Coref-Attention~\cite{liu2021coreference} utilizes coreference resolution tools accompanied with human-designed rules to get coreference links between words in dialogues.

Different from previous approaches, we firstly regard the dialogue summarization task into rephrasing from dialogue to narrative text and content selection. We propose to do post-training before downstream dialogue summarization tasks, exploiting the vanilla model's representation ability without additional labels and modifications. 

%\subsection{Pretrained Language Models}

\textbf{Pretrained Language Models:} Previous pretrained seq-to-seq models can be divided into two categories by training data formats.
One is models pretrained on narrative text, such as BART~\cite{lewis2020bart}, PEGASUS~\cite{zhang2020pegasus}, and T5~\cite{raffel2020exploring}. They use training data from Wikipedia, BookCorpus~\cite{zhu2015aligning} and C4~\cite{raffel2020exploring}. These models show great potentials on tasks such as translation and story ending generation, where both the input and output are in narrative text format.
The other is models pretrained on dialogue, such as ConveRT~\cite{henderson2020convert} and PLATO~\cite{bao2020plato}. Their training data are general-domain dialogues, such as Reddit~\cite{henderson2019repository} and Twitter~\cite{cho2014learning}. These models work for dialogue response selection and generation tasks, aiming at finding or generating the most suitable utterance given the dialogue history.
All of the above models are trained to exploit language modeling features within the same data format, with pre-training tasks such as masked token/sentence prediction and utterance permutation.
%on tasks including masked language modeling, next utterance or sentence prediction, and utterance or sentence permutation, exploiting the language modeling features within the same data format.

Pretraining with cross-format data hasn't been researched so far. 
%With the rapid growth of single format data understanding and generation mentioned above, cross-format tasks become new hot points, such as dialogue summarization. 
Different from the previous language modeling training target, 
such cross-format pretraining focuses on rephrasing between a third-person point of view and multi-speaker views.
In this paper, we only focus on learning to rephrase unidirectionally from  dialogue to narratives.




