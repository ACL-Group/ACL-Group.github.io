\section{Results and Discussions}
\label{sec:results}
We present more details of the DCD post-training dataset and results on summarization tasks including automatic evaluation, human evaluation and ablations in this section.
\subsection{Quality Control of the DCD Dataset}\label{sec:dpdresults}

%\begin{table}
%	\centering
%	\begin{tabular}{lrrr}
%		\toprule[1pt]
%		\textbf{} & {Train}& {Val}& {Test} \\ 
%		\midrule[1pt]
%		{\#samples} & 60,710 &5,172&5,172\\
%		{\#$[Q,A]$W} & 14.59 & 14.84&14.76\\
%		{\#$D'$W} &12.91 &13.04&12.95\\
%		\bottomrule[1pt]
%	\end{tabular}
%	\caption{Statistics of the QA2D dataset.}
%	\label{tab:qa2ddata}
%\end{table}

We split the original QA2D validation set into validation set and test set, and stop fine-tuning with early-stop at $3$ on the new validation set. The number of train/valid/test samples are $60,710/5,172/5,172$. The number of words in the concatenation of QA pairs and declarations are around $14.59$ and $12.91$ respectively.
%More statistics of QA2D dataset are shown in Table \ref{tab:qa2ddata}.
The results evaluated by BLEU~\cite{papineni2002bleu} on QA2D test set are shown in Table \ref{tab:qa2dresults}. We can see that QA2D is a rather simple task that the rule-based approach is competitive with the SOTA sequence generation model BART, and our simple hybrid method further enhanced the performance.

\begin{table}
	\small
	\centering
	\begin{tabular}{lccc}
		\toprule[1pt]
		\textbf{Models} & \textbf{BLEU-1}& \textbf{BLEU-2}& \textbf{BLEU-3} \\ 
		\midrule[1pt]
		{Rule-based} & 87.74 &74.59&66.29\\
		{BART}& 86.79 & 75.83&68.76\\
		{Hybrid} &\textbf{88.90}&\textbf{79.49}&\textbf{71.52}\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Performance of QA2D on its test set.}
	\label{tab:qa2dresults}
\end{table}


We use this hybrid method to transform the multiple-choice dataset DREAM~\cite{sun2019dream} where we neglect the false candidates and span-based dataset FriendsQA~\cite{yang2019friendsqa}. Another dataset FriendsRC~\cite{ma2018challenging} is also included. We merge all of the training sets and validation sets as a new training set, and merge all of the test sets as a new validation set. Finally, our newly proposed DCD dataset consists of 31,208 training samples and 4,540 validation samples.
%The number of ($D$, $D'$) pairs transformed from these three dataset are shown in Table \ref{tab:qa2dstatistics}.


%\begin{table}
%	\centering
%	\begin{tabular}{lrrr}
%		\toprule[1pt]
%		\textbf{} & {Train}& {Validation}& {Test} \\ 
%		\midrule[1pt]
%		{DREAM} & 6,116 &2,040&2,041\\
%		{FriendsQA}& 9,874 & 1,201&1,182\\
%		{FriendsRC} &10,785&1,349&1,353\\
%		\bottomrule[1pt]
%	\end{tabular}
%	\caption{Statistics of transformed ($D$,$D'$) pairs.}
%	\label{tab:qa2dstatistics}
%\end{table}





\subsection{Automatic Evaluation}\label{sec:automaticevaluation}


\begin{table}
	\centering
	\small
	\begin{tabular}{lccc}
		\toprule[1pt]
		%\textbf{Models} & \multicolumn{3}{c}{Rouge-1}& \multicolumn{3}{c}{Rouge-2}&\multicolumn{3}{c}{Rouge-L}\\ 
		%{} & F & P & R & F & P & R & F & P & R \\
		\textbf{Models} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} \\
		\midrule[1pt]
		{Lead-3} &31.40&8.68&29.42\\
		{Longest-3} &32.46&10.27&29.92 \\
		{PGN} &38.55&14.14&34.85 \\
		{Fast-Abs} &40.96&17.18&39.05\\
		{HRED} &40.39&16.13&37.65\\
		{Transformer} &36.62&11.18&33.06 \\
		{S-BART} &46.07 &22.60 &45.00 \\
		{Multi-view} &49.30&25.60&47.70\\
		{DialoBART} &49.13 &25.78 &\textbf{47.84} \\
		{Coref} & \textbf{50.30} & 25.10 & 46.20 \\
		\midrule[1pt]
		\multicolumn{4}{l}{\textit{Our Implementations}} \\
		{BART} &47.78 &24.82 & 46.21\\
		{Dial2Text} & 48.49&\underline{\textbf{25.92}} &\underline{47.21} \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Rouge scores(\%) on SAMSum test set. The best scores are in \textbf{bold}. \underline{Underlined} scores are statistically significantly better than BART with $p<0.05$ according to t-test.}
	\label{tab:samsumresults}
\end{table}


The performances of dialogue summarization on four datasets are compared with the state-of-the-art models and baselines respectively.

The results for SAMSum are shown in Table \ref{tab:samsumresults}. \textbf{Lead-3} and \textbf{Longest-3} are simple rule-based baselines which extract the first or the longest $3$ utterances in a dialogue as the summary respectively. \textbf{PGN}~\cite{see2017get}, \textbf{Fast-Abs}~\cite{chen2018fast}, \textbf{HRED}~\cite{serban2016building} and \textbf{Transformer}~\cite{vaswani2017attention} are well-known abstractive summarization models designed for news summarization. 
They perform poorly on dialogue summarization compared with other approaches.
\textbf{S-BART}~\cite{chen2021structure}, \textbf{Multi-view}~\cite{chen2020multi}, \textbf{DialoBART}~\cite{feng2021language} and \textbf{Coref}~\cite{liu2021coreference} are state-of-the-art models specially designed for dialogue summarization. All of the scores are borrowed from previous papers, except DialoBART which is recalculated with the unified Rouge evaluation tool. \textbf{BART} is the pretrained language model with the fine-tuning process and \textbf{Dial2Text} represents our full approach. 
Results show that Dial2Text performs statistically significantly better than BART and is also compatible to Multi-view and DialoBART. Although there isn't improvements on Rouge-1\&L, the highest Rouge-2 with 25.92\% shows that our approach can generate summaries with more accurate 2-grams. This is due to more accurate co-reference or reasoning abilities, which is supported by error analysis in Sec \ref{sec:humaneval}.

\begin{table}
	\small
	\centering
	\begin{tabular}{lccc}
		\toprule[1pt]
		\textbf{Models} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} \\
		\midrule[1pt]
		\multicolumn{4}{l}{\textit{Average among References}}\\
		%\midrule[1pt]
		{Lead-3} &15.65&4.81&15.80\\
		{Longest-3} &15.55&4.66&16.15 \\
		{Transformer} &26.91&5.66&25.25 \\
		{DialoBART} &39.02 &15.73 &36.91 \\
		{Multi-view} &39.65&16.63&37.42\\
		{BART} &39.90 &17.12 &37.48 \\
		{Dial2Text} &\underline{\textbf{41.24}} &\underline{\textbf{17.61}} &\underline{\textbf{39.02}} \\
		\midrule[1pt]
		\multicolumn{4}{l}{\textit{Max among References}}\\
		{Lead-3} &20.26&7.75&20.66\\
		{Longest-3} &20.15&7.68&21.03 \\
		{Transformer} &33.21&10.04&31.63 \\
		{DialoBART} & 47.36& 24.21& 45.54\\
		{Multi-view} &47.59&24.63&45.54\\
		{BART} &48.87 &26.23 &47.00 \\
		{Dial2Text} &\textbf{49.87} &\textbf{26.74} &\textbf{48.27} \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Rouge scores on DialSumm test set.}
	\label{tab:dialsummresults}
\end{table}



For DialSum, we compare Dial2Text with Lead-3, Longest-3, Transformer, Multi-view and BART in Table \ref{tab:dialsummresults}. Transformer is BART without pre-trained weights. Multi-view\footnote{\url{https://github.com/GT-SALT/Multi-View-Seq2Seq}.} and DialoBART\footnote{\url{https://github.com/xcfcode/PLM_annotator}.} were re-implemented on this new dataset. Since there are three reference summaries for this dataset, we show the average and maximum rouge scores among three references for each sample. Previous state-of-the-art model Multi-view and DialoBART doesn't outperform the strong baseline BART.
The reason is mainly due to error propagation from the wrong labeled features. % The reason is that their labeled features are not the same in spoken daily dialogues and online dialogues. 
%\JQ{as shown in Figure example?!!appendix?}. 
The features or hyper-parameters for these two methods are not suitable for different kinds of dialogues, and need greedy search or redesigning by human labors. 
Our Dial2Text is statistically significant better than BART on all of the scores with 1.34\%, 0.49\% and 1.54\% absolute increments on Rouge-1\&2\&L respectively.  
The highest Rouge score among three references means that the generated summary doesn't need to be similar to every reference due to the diversity of natural language expressions. It also reflects the quality of generated sample. Dial2Text achieves the highest scores.


\begin{table}
	\small
	\centering
	\begin{tabular}{llccc}
		\toprule[1pt]
		\textbf{Dataset}&\textbf{Models} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} \\
		\midrule[1pt]
		%\multicolumn{4}{l}{\textit{Email}}\\
		\multirow{5}{0.9pt}{Email}&{Lead-3} &15.10&3.71&16.62 \\
		&{Longest-3} &13.41 &3.27 &15.54  \\
		&{Transformer} &22.83 &1.82 &18.58 \\
		&{BART} &31.10 &8.25 &28.68  \\
		&{Dial2Text} &\underline{\textbf{32.91}} &\underline{\textbf{8.96}} &\underline{\textbf{30.20}} \\
		\midrule[1pt]
		%\multicolumn{4}{l}{\textit{TopicAMI}}\\
		\multirow{7}{0.9pt}{TopicAMI}&{Att Seq2Seq} &34.74& 25.15&34.70 \\
		&{PGN} &31.21 &26.35 &31.21  \\
		&{SG Full-Att} &38.62 &29.46 &40.39 \\
		&{SG Summary} &41.17 &30.85 &43.62 \\
		&{Transformer}&38.81 &31.49 &38.88 \\
		&{BART} &41.89 &33.18 &41.94  \\
		&{Dial2Text} &\underline{\textbf{46.45}} &\underline{\textbf{37.63}} &\underline{\textbf{46.50}} \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Rouge scores(\%) on Email and TopicAMI test sets.}
	\label{tab:emailresults}
\end{table}

Baselines for Email are the same except Multi-view and DialoBART, since the structure of email threads is quite distinct from online and spoken dialogues. Baselines for TopicAMI are borrowed from the original paper~\cite{goo2018abstractive}\footnote{https://github.com/MiuLab/DialSum. We re-implement since they mentioned that the scores of their methods in the original paper are incorrect according to this link.}. 
Our model outperforms corresponding baselines and BART on these two datasets as shown in Table \ref{tab:emailresults}.

Comparing the absolute improvements of Dial2Text over BART across datasets, we have the following observations:
\begin{itemize}
	\item Dial2Text can enhance dialogue summarization abilities across different dialogue scenarios and levels of summarization granularities. Application scenarios mentioned in Section \ref{sec:datasets} are various and compression ratios in Table \ref{tab:sumdataset} are divergent reflecting different levels of granularity.
	\item Dial2Text performance better on more abstractive dialogue summarization tasks. According to Table \ref{tab:sumdataset}, the abstractiveness of these four datasets are TopicAMI $\gg$ Emai l$\textgreater$ SAMSum $\textgreater$ DialSumm. The absolute improvement of Rouge-2 are TopicAMI(4.45\%) $\gg$ SAMSum(1.1\%)
	$\textgreater$ Email(0.71\%) $\textgreater$ DialSumm(0.49\%)\footnote{Results on a more extractive dataset are shown in Appendix.}.
\end{itemize}





%\begin{table}
%	\centering
%	\begin{tabular}{lccc}
%		\toprule[1pt]
%		\textbf{Models} & \textbf{Rouge-1} & \textbf{Rouge-2} & \textbf{Rouge-L} \\
%		\midrule[1pt]
		%\multicolumn{4}{l}{\textit{Average among References}}\\
		%\midrule[1pt]
%		{Att Seq2Seq} &34.74& 25.15&34.70 \\
%		{PGN} &31.21 &26.35 &31.21  \\
%		{SG Full-Att} &38.62 &29.46 &40.39 \\
%		{SG Summary} &41.17 &30.85 &43.62 \\
%		{BART} &41.89 &33.18 &41.94  \\
%		{Dial2Text} &\underline{\textbf{46.45}} &\underline{\textbf{37.63}} &\underline{\textbf{46.50}} \\
%		\bottomrule[1pt]
%	\end{tabular}
%	\caption{Rouge scores(\%) on TopicAMI test set.}
%	\label{tab:amiresults}
%\end{table}


\subsection{Human Evaluation}\label{sec:humaneval}




To compare with state-of-the-art models, we did human evaluations on the SAMSum test set according to Sec \ref{sec:humansetup}. 

\textbf{Overall Quality:} 
We annotated the reference summary together with generated summaries from BART, Multi-view, DialoBART and Dial2Text.
The reference summary achieves the highest score with $1.54$, while BART is the lowest with $0.1$. Multi-view and DialoBART get $0.30$ and $0.31$ respectively. Our method Dial2Text performs slightly better with $0.35$.
The Fleiss Kappa between three annotators is $0.34$\footnote{The agreement score is a bit lower than the values in \citet{feng2021language}, while our human evaluation setting is more complicated.}, indicating fair agreement. We further do error analysis for the three comparable methods to show their differences.
\begin{table}
	\centering
	\small
	\begin{tabular}{lcccc}
		\toprule[1pt]
		\textbf{Model} & \textbf{Multi-view} & \textbf{DialoBART} & \textbf{Dial2Text} & \textbf{Agr} \\
		\midrule[1pt]
		Rea & 21 & 22 & 11 & 0.53\\
		Cor & 13 & 13 & 11 & 0.42\\
		Rea$\mid$Cor & 31 & 33 & 21 & 0.77\\
		\midrule[1pt]
		Mis & 64 & 63 & 69 & 0.89\\
		Red& 56 & 53 & 58 & 0.80\\
		Mis$\mid$Red & 77 & 72 & 78 & 0.93\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Error analysis. Rea, Cor, Mis and Red are short for reasoning, coreference, missing and redundancy. ``$\mid$'' refers to ``OR'' logical 
operation between error types. Numbers in the columns of Multi-view, 
DialoBART and Dial2Text represent the number of incorrect samples agreed 
by both annotators. Agreement(Agr) is measured by Jaccard coefficient.}
	\label{tab:error}
\end{table}


\textbf{Error Analysis:} The results are shown in Table \ref{tab:error}. 
Error types are correlative thus indistinguishable in some cases. For example, 
mismatching of person and event among multiple utterances can be both 
a reasoning error or a coreference error. Besides, content redundancy will 
always lead to content missing. So, we divide them into two groups and 
merge the error types in each group with the ``OR'' logical operation. 
The agreement on the merged ones is much higher with $0.77$ and $0.93$ respectively. The increase on the agreement between Rea$\mid$Cor is much higher than that between Mis$\mid$Red, showing there are more indistinguishable cases for Rea and Cor. For Mis and Red, we ask annotators to strictly compare with references. It should be noted that references are not the only correct answer. So, the high error counts here don't mean that the generated summary is totally unacceptable.

We can find that our Dial2Text performs slightly worse on selecting correct content, while
performs much better on reasoning and coreference than Multi-view and DialoBART. This is consistent with the overall scores that DialoBART achieves $0.04$ higher scores than the other two methods.

\begin{table}
	\centering
	\small
	\begin{tabular}{p{1.3cm}p{6.3cm}}
		\toprule[1pt]
		\textbf{Model} & \multicolumn{1}{c}{\textbf{Summary}} \\
		\midrule[1pt]
		Zero-shot BART& \textbf{WilliamWilliam}: hey im making spaghetti \textbf{William}: could you please buy some fresh tomatoes \textbf{William}: \textbf{Olivia}: no problem dear :) (29.41/12.50/33.33)\\
		\hline
		Zero-shot Dial2Text& \textbf{William} wanted to buy some fresh tomatoes. (36.36/10.00/42.11)\\
		\midrule[1pt]
		%BART&Olivia will buy some fresh tomatoes for the spaghetti. Beth will buy chocolate for after the dinner.\\
		%\hline
		Multi-view&\textbf{William} is making spaghetti. \textit{\textbf{Olivia}} will buy fresh tomatoes and \textit{chocolate for after the dinner}. (73.33/57.14/81.48)\\
		\hline
		DialoBART&\textbf{William} is making spaghetti . \textit{\textbf{Olivia}} will buy some fresh tomatoes and \textit{some chocolate for after the dinner}. (68.75/46.67/78.57)\\
		\hline
		Dial2Text&\textbf{William} is making spaghetti. \textbf{Olivia} will buy fresh tomatoes and \textbf{Beth} will buy chocolate for after the dinner. (84.85/70.97/85.71)\\		
		\hline
		Reference&\textbf{William} is making spaghetti. \textbf{Olivia} will buy fresh tomatoes for \textbf{William}. \textbf{Beth} will buy chocolate.\\
		\bottomrule[1pt]
	\end{tabular}
	
	\caption{Generated summaries from different methods. \textbf{Names} are in bold and \textit{unfaithful contents} are italic. Rouge-1/2/L scores(\%) are in parentheses.}
	\label{tab:example}  
\end{table}


We also show a case study in Table \ref{tab:example} with summaries generated by different methods for the dialogue in Figure \ref{fig:example}. Both Multi-view and DialoBART failed to recognize that ``Beth will buy the chocolate instead of Olivia'' and generate unfaithful summaries~\cite{maynez2020faithfulness,cao2018faithful}. This mainly due to the original dialogue are wrongly labeled or the features are not suitable. As a result, the errors propagate to the final summary. Dial2Text doesn't rely on any labelers, and learns required features directly by the model with post-training and fine-tuning, which generates an accepted summary. More details please refer to Appendix.



%\begin{table*}
%	\centering
%	\small
%	\begin{tabular}{lcccc}
%		\toprule[1pt]
%		\textbf{Models} & \textbf{SAMSum} & \textbf{DialSumm} & \textbf{Email} & \textbf{TopicAMI}\\
%		\midrule[1pt]
%		%\multicolumn{5}{l}{\textit{Dial2Text ablations}}\\
%		{ Dial2Text w/o post-training}& -0.71/-1.10/-1.00&-1.34/-0.49/-1.54 &-1.81/-0.71/-1.54 &-4.56/-4.45/-4.59\\
%		{ Dial2Text w/o prefix-guided}& +0.15/-0.27/-0.04&-0.36/-0.24/-0.37 &-1.31/-0.56/-0.75 &-0.50/-0.58/-0.50 \\
%		%{ - w/o post-training} &47.78/24.82/46.21&39.90/17.12/37.48&31.10/8.25/28.68&41.89/33.18/41.94 \\
%		%{ - w/o prefix-guided} &\textbf{48.64}/25.65/47.17&40.88/17.37/38.65&31.60/8.40/29.45&45.95/37.05/46.00 \\
%		%{ - full} &48.49/\textbf{25.92}/\textbf{47.21}&\textbf{41.24}/\textbf{17.61}/\textbf{39.02}&\textbf{32.91}/\textbf{8.96}/\textbf{30.20}&\textbf{46.45}/\textbf{37.63}/\textbf{46.50}\\
%		\bottomrule[1pt]
%	\end{tabular}
%	\caption{Rouge-1/2/L F1 scores(\%) for ablations compared with the full model. ``w/o" is short for ``without".}
%	\label{tab:ablation}
%\end{table*}

\begin{table}
	\centering
	\small
	\begin{tabular}{lcccccc}
	\toprule[1pt]
	 \multirow{2}{0.7pt}{\textbf{Datasets}}& \multicolumn{3}{c}{\textbf{w/o post-training}}& \multicolumn{3}{c}{\textbf{w/o prefix-guided} }\\

	& R-1 & R-2 & R-L& R-1 & R-2 & R-L \\
	\midrule[1pt]
	SAMSum & -0.71&-1.10&-1.00 & +0.15&-0.27&-0.04 \\
	DialSumm & -1.34&-0.49&-1.54 & -0.36&-0.24&-0.37 \\
	Email & -1.81&-0.71&-1.54 & -1.31&-0.56&-0.75 \\
	TopicAMI & -4.56&-4.45&-4.59 & -0.50&-0.58&-0.50\\
	\bottomrule[1pt]
	\end{tabular}
	\caption{Rouge-1/2/L F1 scores(\%) for ablations compared with the full model. ``w/o'' is short for ``without''.}
	\label{tab:ablation}
\end{table}


\subsection{Ablations}\label{sec:ablations}



We verify the effectiveness of our post-training method by comparing the full method with two ablations. One is without post-training, i.e. BART. The other using the normal sequence generation strategy without the guiding prefix.% In other words, it only use post-training dataset. 
The results in Table \ref{tab:ablation} are consistent across datasets, indicating that our proposed dataset and the post-training task both enhanced the results, making significant improvements.
%enhanced the dialogue summarization performances.

 We do zero-shot to test if the cross-format rephrasing ability is learned during post-training. The results on SAMSum test set are $24.25\%/9.65\%/ 23.88\%$ for Rouge 1/2/L F1-scores on the post-trained BART, compared with $15.22\%/4.80\%/15.55\%$ on the original BART respectively. Our post-trained BART, i.e. Zero-shot Dial2Text in Table \ref{tab:example}, can successfully transcribe the dialogue into narrative text while BART can't. %according to Zero-shot BART and Zero-shot Dial2Text in Table \ref{tab:example}. 
The results also reflect that this post-training task hasn't learned to select important contents and only generate part of the dialogue focuses in a third-person point of view by comparing Zero-shot Dial2Text and Dial2Text.

%During the fine-tuning process, our approach doesn't add any additional labels to the original dialogue. 
We also fine-tune our post-trained BART with input dialogues labeled with different features (More details in Appendix) to investigate whether our post-trained model can further take advantage of previous proposed features during fine-tuning.
%explanation of different feature
Dialogues without any labels, rank first on all of Rouge scores, showing that previous labeling schemas are not compatible to our approach. The competitive scores with the SOTA models in Table \ref{tab:samsumresults} show that our fine-tuning process based on post-trained BART has already learned such dialogue structure features and content selection features for dialogue summarization more or less. Previous labels are not totally correct while our fine-tuning process can learn these features directly from the data.
% conclusion



