\section{Experimental Setup}
\label{sec:experiment}
%\KZ{Give a little preamble here.}
This section describes the chatbots that we experiment with, the
set of baseline approaches that are compared to ChatMatch, and some
implementation details used in the following experiments.

\subsection{Description of Seven Bots}
%\KZ{Put these and the next section into two tables to save space?} 
We pick seven chitchat chatbots trained or fine-tuned on ConvAI2 \citep{dinan2019second} to be evaluated in our experiments 
as \tabref{tab:bots} shows.
All chatbots are running and evaluated on a Intel (R) Xeon (R) CPU 
E5-2678 v3 @ 2.50GHz with NVIDIA GeForce RTX 2080 and a 12GB RAM. 

\begin{table}[th]
\centering
\scriptsize
\begin{tabular}{p{0.05\columnwidth}|p{0.85\columnwidth}}
%\hline
\toprule
\textbf{Bot} & \textbf{Description} \\ \midrule
BB & Blender Bot \citep{roller2020recipes} is a
90M-parameter generative model following the training
of \citet{shuster2020dialogue} and then finetuned on
blended skill talk tasks \cite{smith2020together}.\\ \hline
 PL & PLATO-2 \citep{bao2020plato} is a high-quality open-domain chatbot trained via curriculum learning.\\ \hline
 CS & A Seq2Seq model with Control.  \citep{see2019makes} Here, we use their
specificity-controlled WD model (with WD repetition control).\\ \hline
%provided in this paper.
CR & The response-relatedness WD model
(with WD repetition control) provided in the paper about
Controllable Seq2Seq. \citep{see2019makes} \\ \hline
UG & A large pre-trained seq2seq Transformer
with vocab unlikelihood which sets parameter
$\alpha = 100$ \citep{li2020dont} \\ \hline
DG & DialoGPT medium. \citep{zhang2020dialogpt}\\ \hline
 DD & Image Seq2Seq model. \citep{shuster2020dialogue} \\
\bottomrule
\end{tabular}
\caption{Seven bots participating in the tournament.}
\label{tab:bots}
\end{table}


%\begin{itemize}
%\item BB: Blender Bot \citep{roller2020recipes} which is a 
%90M-parameter generative model following the training 
%of \citet{shuster2020dialogue} and then finetuned on 
%blended skill talk tasks \cite{smith2020together}.
%\item PL: PLATO-2 \citep{bao2020plato}, a high-quality open-domain chatbot trained via curriculum learning.
%\item CS: A Seq2Seq model with Control.  \citep{see2019makes} Here, we use their 
%specificity-controlled WD model (with WD repetition control). 
%provided in this paper.
%\item CR: The response-relatedness WD model 
%(with WD repetition control) provided in the paper about 
%Controllable Seq2Seq. \citep{see2019makes}
%\item UG: A large pre-trained seq2seq Transformer
%with vocab unlikelihood which sets parameter 
%$\alpha = 100$ \citep{li2020dont}
%\item DG: DialoGPT medium. \citep{zhang2020dialogpt}
%\item DD: Image Seq2Seq model. \citep{shuster2020dialogue}
%\end{itemize}
 
\subsection{Baseline Evaluation Approaches}
We choose four automatic evaluation methods and one manual evaluation 
method to compete with CM. 
%Before applying these evaluation approaches on our 7 bots,
First,
we make our seven bots 
generate responses for following
evaluation
using
the test set of DailyDialog \citep{li2017dailydialog} 
as static scripts. 
%\KZ{Add how you apply these baselines on our 7 bots to get the eval
%results $\tau$. The first 4 are using the same test framework which is
%generate a response from a static script context. STB is human-bot. Ours is
%bot-bot.}
Then we apply the baselines as follows:
\begin{itemize}
\item PPL: Perplexity \\
Lower perplexity means that the generated sequence is 
the more likely to be close to a human sentence.
\item TA: Token Accuracy \\
Token Accuracy is used to measure the generation accuracy 
of each token, which refers to the ratio of the number of 
correctly predicted tokens to the total number of predicted tokens.
\item BS: BERTScore  \\
We use a commonly used automatic evaluation metric for text generation, 
BERTScore from \citet{zhang2019bertscore}.
BERTScore
will return a similarity score between referenced answer and generated response
from the bot.
%computes a similarity score 
\item HAE: Holistic and Automatic Evaluation  \\
We combine the four separate metrics which measures Fluency, Context Coherence, Logical Self Consistency and Diversity from \citet{pang-etal-2020-towards} by distributing each bot a per-metric score(1-7)first and then summing them up to get a final score. 
\item STB: \textit{Spot The Bot}  \\
The recently proposed interactive manual evaluation metric \textit{Spot The Bot} \citep{deriu-etal-2020-spot} asks human judges to decide whether the speaker is human or bot with a mix of human-bot and bot-bot chat logs.   
%We cut the dialogue logs into segments for every 2, 3, 5 exchanges, 
%and distribute them to human judges. It is up to human judges to decide 
%whether the chat is between humans, bots or unsure for each exchange. 
%The scores for being recognised as human is larger than that as unsure and then as a bot. Finally, the dominant round will 
%accumulate scores for the corresponding chatbot. 

\end{itemize}

%We also choose the recent proposed interactive manual evaluation metric  \textit{Spot The Bot} \citep{deriu-etal-2020-spot} as our baseline. In order to reproduce their evaluation %approach in a more simple way, we ask three college students who are skilled in English to be our annotators. We mix up our generated bot2bot chat logs with some human-bot %conversations and then cut them into short segments. Next, our annotators will give each turn a label which tells if the speaker is a bot from their prospectives. 


\subsection{Ground Truth for Rankings}
 In order to obtain rankings that can be reliably used as ground truth, 
we asked 10 college students who are fluent in English to chat 
with each of the seven bots and then manually assess the ability of 
the bots. Seven dimensions, namely fluency (flu), 
knowledge (kno), proactivity (pro), specificity (spf), 
diversity (div), consistency (con), and relevance (rel), 
are used to help them complete the ranking task 
which are rated on a 5-point Likert-scale. They can decide to 
stop the conversation whenever they feel confident enough to score
on these seven dimensions. We set the minimum number of exchanges
to be 20. After finishing the chats with all seven bots,  
each judge will provide their score for each dimension
 and also a general ranking  (known as HR) based on the overall impression.
More details are shown in Appendix.
%\KZ{How do u aggregate the scores from each judge?}

We use Kendall ranking correlation ($\tau$) to evaluate the agreement 
among human judges and also between evaluation approaches and 
general human judgement. 
In the rest of the paper,
we use $\tau_{i}$ and $\tau_{g}$ to denote inter-judge agreement and
 agreement between ranking come from evaluation approaches and ground-truth
ranking respectively.  
\tabref{tab:inter} shows $\tau_i$ on individual dimension and overall ranking. 
We observe that $\tau_i$ for Consistency and Knowledge are lower than 
that for other dimensions.
 We find from their
chat logs 
 that they tend to test these two abilities
 by asking questions. 
Specifically, when evaluating bots' knowledgeability, 
human judges come up with some factual questions 
related to their conversation topic at the time.
Due to the differences among their questions and 
the limited coverage of the questions, 
human judges score the bots differently on this dimension. 
The same is true for consistency, which is tested by asking the same question twice and then checking if the answers are contradictory.     
% \KZ{Here we need to explain why some of the
%$\tau$ scores are not very high, e.g., Con, Kno and Spf. 
%Anything under 0.33 is consider not very correlated? Also why Overall
%is higher than most other $\tau$'s.}

\begin{table}[ht!]
\centering
\scriptsize
%\small
\begin{tabular}{lrrrrrrrr}
%\hline
\toprule
& Flu& Kno& Pro& Spf& Div& Con & Rel & Overall \\ \midrule
$\tau_i$ & 0.39 &0.28 &0.50 &0.35 &0.66 &0.21 &0.48 & 0.53  \\
\bottomrule
\end{tabular}
\caption{Inter-judge agreement among human judges calculated by Kendall's $\tau$.}
\label{tab:inter}
\end{table}
 
Later, we will analyze different $\tau_g$ 
considering human overall rankings as ground-truth ranking.  


\subsection{Parameters Settings for CM}
These settings are determined by empirics.
\begin{itemize}
\item  Each game contains 100 exchanges (200 turns) of conversation
to ensure a sufficient length to evaluate the bots.
\item  The starting utterance is always set to
a daily routing sentence
since the players of our tournament are chitchat bots.
\item For each game, the weight for individual dimension to be equal. 
\item For each match, a bot will gain 3, 1 or 0 points for a win, tie or lose.
\item Each tournment has 42 games (21 matches) 
in total. 
%\item As match level, a bot will gain 3, 1 and 0 point for a win, tie and lose.
\end{itemize}

%\begin{figure*}[th]
%\centering
%\includegraphics[width=0.7\textwidth]{macro.eps}
%\caption{Scoreboard (Macro) and Rankings of 6-bot Tournament by ChatMatch.}
%\label{fig:macro}
%\end{figure*}

\section{Results and Analysis}
\label{sec:main}
%\KZ{Rephrase: In this subsection, we will first present the evaluation results from our automatic metrics, other baselines and human evaluation. 
%Then we will analyze the agreement between all automatic evaluation results and human judgments. }
In this section, we first present the evaluation results from our automatic framework 
and other baselines.
Then to test the generalizability of CM, we randomly choose four bots out of seven, implement CM on these four bots and check if the $\tau_g$ is still high.
 Finally, we analyze the design of the framework with ablation tests in detail.  
%\subsubsection*{ChatMatch Final results}
%As we can see,  BB scores the top in our scoring framework (both for micro and macro). 
%BB is a competitive bot as it aims at performing well in multiple chitchat test sets in different domains. 
%CS and CR come in second and third respectively. They score quite close to each other as they use the same fine-tuned baseline seq2seq model, which only differ in the 
%choice of specific parameters for some improvement in different aspects. 
%DG and DD score close to each other. UG is always losing the match and ranks at the bottom. 
%We can find an obvious difference between the scores of  
%''the loser'' and the other five bots.

\subsection{End-to-end Evaluation of 7 Bots}
We deploy the four baseline methods and our framework CM on the 7 chatbots.
For simplicity, we convert the raw scores to rankings from 1 to 7 (the lower
the better) by
each of these methods. \figref{fig:botranks} depicts these rankings,
along with human ranking (HR) as a reference.
%\textcolor{red}{add some explanation after adding BERTScore}
BB ranks the top among all by CM and HR. 
This is not surprising because BB is a well-known competitive bot 
that performs well in many chitchat test sets and in different domains. 
The general trends exhibited by CM and STB track the human 
evaluation more
closely, whereas the trends of TA and BS are almost the reverse of human.  
PPL can only evaluate whether the sequence generated by the model is 
close to human language without considering the context. 
%TA reflects whether the generated response matches the ground truth of the dataset. As reasonable responses are not limited to ground truth itself,
% it is difficult to correctly evaluate the ability of the chatbot with static scripts. 
We also find that HAE tends to give a higher score to the shorter response, 
so BB and other bots that tend to generate longer sequences are not evaluated
properly.

%CS comes in second. CS and CR are two controllable neural text 
%generation models using conditional training and weighted decoding to control
%different attributes.
%Here, CS is designed for controlling specifity while CR aims at controlling response-relatedness.
%%CS performs quite well in our automatic assesment based on seven dimensions. 
%PL, CR, DD and UG score close to each other. 
%DG always loses the match and ranks at the bottom.
%  
%We can see that BB ranks the best in Human Rankings. As for two common automatic metrics PPL and TA, CR ranks the best in PPL while UG and DD rank the best in TA. 
%BB and PL perform badly according to TA, which is inconsistent with our human evaluation results. 
%\KZ{Need more explanations about why we got these automatic results.
%And compared to human, discuss a bit...}

%\begin{table}[ht!]
%\centering
%\scriptsize
%%\small
%\begin{tabular}{lrrrrrrr}
%%\hline
%\toprule
%& BB &PL& CS & CR & UG & DG & DD  \\ \midrule
%PPL & 13.89 &11.64 &10.83 &\textbf{10.74} &16.35 & 20.19 & 14.88  \\
%TA & 0.12 & 0.05& 0.12& 0.10 & \textbf{0.23} & 0.19& \textbf{0.23}  \\
%HAE& 20 &\textbf{10} &11  &15 &11  & 24 & 21  \\ 
%STB & 2 &\textbf{1} &3 & 7& 6 & 5 & 4\\
%CM &\textbf{16}  &8&  15&7 &6& 0& 7\\\midrule
%%Micro & \textbf{58}&33 &42 &37 &39 & 25 & 34\\ 
%%CMR&\textbf{1} &3&2 &4 &6 &7 &4 \\ 
%Human& \textbf{1} &2& 4& 5& 7&6 & 3\\
%\bottomrule
%\end{tabular}
%\caption{Raw scores given by competing evaluation methods including
%human evaluation (best is
%bold in each row) \KZ{Include raw scores for STB and HR}.}
%\label{tab:baseline-scoring}
%\end{table}

\begin{figure}[th]
\centering
\includegraphics[width=0.9\columnwidth]{rank.eps}
\caption{Bot rankings by methods and human (HR)}
\label{fig:botranks}
\end{figure}


\begin{table}[th]
\centering
%\small
\scriptsize
\begin{tabular}{lccc}
\toprule
 & $\tau_{g}$ & Evaluation Time \\ \midrule
PPL  &0.14 & \textasciitilde 30 secs \\
TA&-0.42 & \textasciitilde 10 secs \\
BS& -0.43 & \textasciitilde 10secs\\
HAE&0.10 &\textasciitilde 2 min\\
STB &0.62 & \textasciitilde 60 min/human\\
Human & -  & \textasciitilde 90 min/human \\
CM&\textbf{ 0.68} & 2 min 57 secs\\
 \bottomrule
\end{tabular}
\caption{$\tau_g$ for different approaches
 and their respective evaluating time.}
\label{tab:main}
\end{table}

We further compute $\tau_g$ of the baselines and 
ours against the human rankings and include them in \tabref{tab:main}. 
%However, common automatic evaluation metrics such as PPL and HAE present poor agreements 
%with human judgements as their ranking agreement coefficient is close to zero, 
%which indicates a weak agreement. 
%TA is even negatively correlated with human judges. It  reflects whether the generated response matches the ground truth of the dataset. As reasonable responses are not limited to ground truth itself,
% it is difficult to correctly evaluate the ability of the chatbot with static scripts. 
%\textit{Spot The Bot} correlates well with human judgements as they 
%mainly depend on human annotations themselves.
The agreement between our metric and human judgment is 0.68,
 which is even better than the agreement among human judges ( $\tau_i = 0.53$ ). 
This indicates CM's evaluation results are very close to the average 
judgment made by ten different human judges. 

\textit{Spot The Bot} correlates well with human judgments as they
mainly depend on human annotations themselves. 
However, common automatic evaluation metrics such as PPL and HAE 
present poor agreements with human judgments. 
TA is even negatively correlated with human judges. 
TA assesses whether the generated response matches the ground truth 
of the dataset. The same goes for BS, 
which computes token similarity with contextual embeddings.
Since there may be many other plausible responses than
the reference response itself, it is difficult to correctly evaluate 
the ability of the chatbot with static scripts.

\subsection{Time Efficiency}
\label{sec:time}
The efficiency of the competing methods is also assessed in \tabref{tab:main}. 
Though slower than other automatic methods,
CM is much faster than methods requiring human efforts. 
It takes a human judge 90 minutes on average to complete
the conversations with all 7 bots, decide ratings on seven dimensions and 
then give their overall ranking.
To put it in perspective, we ask three human judges to evaluate the same
7 bots by \textit{Spot The Bot} framework. It takes on average one hour for
each human judge to complete the ranking.

%We also notice a good correlation between ChatMatch's micro scoring metric
%and human judges on Non-repetitiveness dimension as 
%well. However, neither of our two scores fare very well with
%consistency, probably because it is harder for us to 
%detect implicit inconsistencies using our rules while human can better pick up such inconsistencies.

\subsection{Generalizability of ChatMatch Framework}
\label{sec:four}
To test the generalizability of CM,
we construct $\tbinom{7}{4} = 35$ test groups each of which consists of four randomly
chosen bots from our bot pool. 
We first develop the ground truth ranking for each combination 
of bots from human judgment. 
To ensure the reliability of human judges,
we only keep the groups whose inter-judge agreement is 
higher than the average of all 35 test groups, which is 
0.53.
Then we have 17 four-bot groups left
and we implement the double-round CM on
these test groups.
As \tabref{tab:four} shows,
 the average $\tau_g$ equals
0.60. 
Besides, we calculate the average agreement
 of these bot groups by using relative four-bot rankings deduced from original CM 
results, which equals 0.67.
As the values of the two average agreements are quite close, 
it is convincing to believe that this evaluation framework
presents good  generalizability.
   
%\textcolor{red}{explain 0.48, add one column for human inter-agreement}
%Then we implement CM on this test set and the rankings are shown in \tabref{tab:four}.
\begin{table}[ht!]
\centering
\scriptsize
%\small
\begin{tabular}{lrr}
%\hline
\toprule
Combination of bots & CM's ($\tau_{g}$) & Inter-judge agreement($\tau_{i}$) \\ \midrule
BB, CS, CR, UG & 1.00 &0.84   \\
BB, CS, CR, DG & 1.00 &0.72   \\
BB, CR, PL, UG & 0.71 &0.73     \\
BB, CR, DD, DG & 0.67 & 0.64 \\
%BB, DD, PL, UG & 0.33 & 0.59 \\
%CD, CR, DD, DG & 0.33 \\
... & ... & ...\\  \midrule
Average & 0.60 & 0.67 \\
\bottomrule
\end{tabular}
%\caption{Ranking for four randomly chosen bots 
%with CM. CM7 denotes the ranking of these four bots 
%through our original tournament. CM4 denotes the result
%from CM only
%using chat logs among the four chosen bots.
%Human denotes the human evaluation results. 
%}
\caption{CM's Agreement($\tau_{g}$) with ground-truth ranking
and Inter-agreement among judges ($\tau_{i}$) for 
each group of four randomly chosen bots.
Here, average denotes the average $\tau$ of 17 test groups.} 
%\KZ{Maybe delete the last row with tau=0.33?}}
\label{tab:four}
\end{table}

%Even if we only used the chat logs of four bots for the experiment, 
%the ranking does not change, which indicates that CM's output is 
%stable enough.


\subsection{Ablation Studies}
\label{sec:ablation}
The ChatMatch framework essentially consists of two main components: i) the 
bot-to-bot chat tournament set-up, and ii) the scoring metrics. 
Given its success in the end-to-end experiments,
one natural question to ask is whether the high correlation with the human
evaluation comes from the bot-to-bot set-up or the seven scoring metrics.
If the scoring metrics are significant, which ones are more useful?
In this subsection, we seek to answer these questions 
and also explore other factors or parameters in
CM that might contribute to its effectiveness, 
such as the number of exchanges, similarity functions, 
starting utterance.


\subsubsection{Effect of Different Chatting Setups}
We design two alternative settings to compete with our bot-bot tournament 
framework:
\begin{itemize}
\item Human-bot conversations: we use our seven metrics to evaluate
the human-bot chat logs
collected 
from our ten human judges who
chat with bots directly. 
For each bot, we obtain the per-metric rankings (1-7) 
first and then we sum up the 7 per-metric rankings
for each bot
 to get their overall scores.
The final ranking is decided by the overall scores.
%Bots are ranked by this overall score in the end.
\item Self-chat conversations: 
we use our seven metrics to evaluate
the generated
 100-exchange self-chat logs 
and obtain the final rankings. 
The scoring process is similar 
to what we have done for the human-bot chat logs above.
\end{itemize} 

As the results in \tabref{tab:frame} show, CM gets the highest agreement
 among the three frameworks while
implementing seven metrics 
on self-chat logs 
correlates weakly
to human judgments.
To figure out why our seven metrics do not 
work well with Human-bot setup and Self-chat setup, 
we calculate the average number of inconsistencies and relevant concepts
caught by our metrics, regardless of the speaker.
 We can tell from \tabref{tab:frame} that
Inconsistency is hardly
detected in Self-chat logs
as bots tend to chat about the same things
within this setup.
This can also explain why revelant 
concepts are often popular in self chat logs.
Under this 
circumstance, 
these two metrics are not 
capable of distinguishing bots' real
abilities.
 
Evaluating relevance with our metric
is difficult on human-bot logs. Human judges
are often switching topics by raising 
some questions to shorten the evaluation time.
That is why we are not able to evaluate bots'
ability for memorizing some long-distance concepts.
 
%Self-chat conversations do not seem natural 
%as we observe 70 repetitive turns
% for one 100-exchange self-chat conversation
%on average.
%Human-bot conversations are stronger than self-chat conversations
%as the quality of self-chat chat logs are poorer. 
%Human-bot conversations are chat logs from our ten human judges who
%chat with bots directly.
% which contains 21 exchanges on average per conversation.
We normalize the number of inconsistencies and relevance to one hundred rounds
to make a more reasonable comparison.
    
%\KZ{Any other take-ways?}
\begin{table}[ht!]
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrrrrrr}
\begin{tabular}{lrrrr}
%\hline
\toprule
Setup& $\tau_{g}$  & Inconsistency & Relevance &Length   \\ \midrule
Human& 0.39   &4.59& 0.88 &21   \\
Self-chat & 0.30  & 0.14 & \textbf{31.42} & 100\\
CM& \textbf{0.68}  & \textbf{7.10} & 12.60&100 \\
%Top bot &BB &PL &BB &BB &BB &UG &UG\\
\bottomrule
\end{tabular}
\caption{
%Average score on
%individual dimension
%and 
%agreement (kendall's $\tau$) with human judges 
%for different setup.
$\tau_{g}$ and average number of 
inconsistency and relevance
per hundred rounds detected with 
different setups. 
Human denotes Human-bot setup. 
%Here, human-bot conversations are chat logs from our ten human judges who
%chat with bots directly.
Length refers to the number of exchanges.
 %\KZ{This table is problematic.
%the negative correlations are hard to explain. The explainable
%in the text below is not convincing.}
}
\label{tab:frame}
\end{table} 

 
%\begin{table}[th!]
%\centering
%\small
%\begin{tabular}{lr}
%\hline
%\toprule
%Chat Setup& Agreement ($\tau$)\\ \midrule
%Human-bot   & 0.39  \\
%Self-chat  & 0.30  \\
%Bot-bot tounament(CM)  & \textbf{0.68}  \\
%\bottomrule
%\end{tabular}
%\caption{
%Agreement (kendall's $\tau$) with human judges for different framework 
%}
%\label{tab:frame}
%\end{table}


\subsubsection{Effect of different scoring metrics}
To understand the role and effects of each
scoring dimension at game level, 
each time we set one dimension coefficient to zero
and others remain one. We call these experiments
``minus $x$" experiments where $x$ is one of the 
metrics under testing.  
\tabref{tab:minus} shows the agreement with human judges 
that comes from each ``minus $x$" experiment 

%\KZ{I think we need the results of all - x, where x is one of the
%7 metrics.}

\begin{table}[ht!]
\centering
\scriptsize
%\small
\begin{tabular}{lrrrrrrrr}
%\hline
\toprule
&- Flu& -Kno& -Pro& -Spf& -Div& -Con & -Rel &Overall \\ \midrule
$\tau_{g}$ &\textbf{0.68} &0.48 &0.43&0.62 &0.05 &\textbf{0.68}& 0.20&0.68   \\
%Top bot &BB &PL &BB &BB &BB &UG &UG\\
\bottomrule
\end{tabular}
\caption{Minus-$x$ tests of dimension. 
%\KZ{This table is problematic.
%the negative correlations are hard to explain. The explainable
%in the text below is not convincing.}
}
\label{tab:minus}
\end{table}


%\begin{table}[ht!]
%\centering
%\scriptsize
%\small
%\begin{tabular}{lrrrrrrrr}
%\hline
%\toprule
%&Flu& Kno& Pro& Spf& Div& Con & Rel  \\ \midrule
%Human-bot &9.27 &0.29 &16.37&0.72 &2.37 &0.96& 0.19   \\
%Self-chat & 6.4 & 0.29 & 31.29 & 0.59 & 69.86 & 0.14 & 31.42\\
%CM & 10.35 & 0.33 & 30.69 & 0.64 & 68.15 & 7.10 & 12.60 \\
%Top bot &BB &PL &BB &BB &BB &UG &UG\\
%\bottomrule
%\end{tabular}
%\caption{

 %\KZ{This table is problematic.
%the negative correlations are hard to explain. The explainable
%in the text below is not convincing.}
%}
%\label{tab:count}
%\end{table}


%\begin{table}[ht!]
%\centering
%\small
%\begin{tabular}{lrrrr}
%\toprule
%& Average& STD& Min&Max \\ \midrule
%Flu & 4.12&0.62&3.39&4.76   \\
%Spf &0.65 &0.04 &0.58 &0.70\\
%\bottomrule
%\end{tabular}
%\caption{Average, Standard Deviation, Minimum and Maximum of Score for Fluency and 
%Specificity per turn among seven bots. 
%STD, Min and Max denote 
%the standard deviation, 
%minimum and maximum score
% for two dimensions respectively.
%}
%\label{tab:average}
%\end{table}

Eliminating Diversity and Relevance presented a strong effect on evaluation as $\tau_{g}$ has dropped from 0.68 to 0.05 and 0.20 respectively. 
These two dimensions are usually the first two things that come to human judges' minds while completing the evaluation. Bots that generate highly context-related and diverse responses always rank highly in human evaluation.

Removing Fluency, Specificity or Consistency has little effect on $\tau_{g}$. As for Consistency, we calculate the average number of inconsistency that we have caught 
with our algorithm described in \algoref{algo:inconsist}, which is 7 per conversation with standard deviation equals 16. 
Out of 42 conversations, no inconsistency was detected in 28 conversations. 
%However, this does not mean there is no inconsistency in these conversations.
%We ask two human annotators to annotate the inconsistency 
%for all chatlogs. 
%They finally find \textcolor{red}{insert a number} in total.
Currently, CM is not able to detect implicit inconsistency which does not follow a question. 
%We also train a BERT \citep{devlin2018bert} classifier on Dialogue NLI dataset \citep{welleck2018dialogue}.
% Inconsistency detection is proceeding by considering contradiction predictions from the BERT NLI model to be inconsistency cases in a dialogue. 
%However, the average number of detected inconsistency is only 
%\textcolor{red}{insert a number}
%per conversation.  
Developing a more potent inconsistency detection method
that can make bots more likely to 
expose their weakness on this dimension
 will be one of our future goals.


As for Fluency and Specificity, 
we calculate the average and 
standard deviation 
of these two scores
among seven bots.
Seven bots score 
closely 
at these two dimensions: 
4.12 per turn on average for Fluency with standard deviation equals 0.62 
and 0.65 for Specificity with standard deviation equals 0.04. This indicates that their ability at these two dimensions is quite close to each other so that canceling them will not have much impact on the final results.
%\begin{table}[ht!]
%\centering
%\small
%\scriptsize
%\begin{tabular}{lrrrr}
%\toprule
%& Average& STD& Min&Max \\ \midrule
%Flu & 4.12&0.62&3.39&4.76   \\
%Spf &0.65 &0.04 &0.58 &0.70\\
%\bottomrule
%\end{tabular}
%\caption{Average, Standard Deviation, Minimum and Maximum of Score for Fluency and
%Specificity per turn among seven bots.
%STD, Min and Max denote
%the standard deviation,
%minimum and maximum score
% for two dimensions respectively.
%}
%\label{tab:average}
%\end{table}
%Agreements between Specificity, Fluency, Proactivity and 
%human judgments are quite weak since 
%they have less influence on human evaluation 
%\KZ{I'm not sure about this. Any proof? The inter-judge agreement on Pro is quite high at 0.5!}, 
%even though they are important abilities for bots to improve.
%Knowledge and Consistency are weakly negatively correlated to human judgments. The first main reason for this is that they are often ignored by humans while ranking the bots. \KZ{That's why the low
%correlations among humans in \tabref{tab:inter}?} 
%Second, our automatic rule-based algorithms can only
%capture limited instances of inconsistency and ignorance. 
%This kind of inconsistency and ignorance are always common in less capable bots. \KZ{GIve some numbers here?} 
%More potent detection rules will be introduced in the future.

%\KZ{Add a table to show the average number of times each dimension
%is captured within 100 exchanges. This can explain some of the negative
%correlations or small $\tau$ numbers in other tables.}

Additionally, to compare our scoring metrics with other 
unsupervised metrics, we try to combine the four metrics from
 HAE (Fluency, Context Coherence, Logic Self Consistency, and Diversity) 
into CM, because these 4 metrics can be computed without reference utterances. 
%Authors of \citet{pang-etal-2020-towards} does not provide a way to combine 
%the four metrics mentioned in the paper. Since these four metrics are  
%all reference-independent, we can test them with our bot-bot tournament framework. 
Results shown in \tabref{tab:metrics} indicate that
the four metrics do not work well with bot-to-bot settings,
also worse than the HAE result from \tabref{tab:main} (0.1).
This is because, in \tabref{tab:main}, the four metrics are trained on 
DailyDialog and tested on static scripts from DailyDialog. 
Therefore, while CM is a flexible framework that can accommodate more
metrics, they need to be carefully designed to suit bot-to-bot chatting.
%HAE with bot-bot chat setup obtains a weaker agreement
%as every separate metric from HAE is trained on 
%DailyDialog.
% Consequently, we doubt that even if 
%HAE itself does not need
%any reference while implementing the 
%evaluation, it still relies a lot 
%on testing with DailyDialog.
%\KZ{Don't understand this table. Elaborate some more? HAE was an
%eval method, why is it a metric now?}
%\textcolor{red}{Need explain more?}

\begin{table}[th!]
\centering
\scriptsize
%\small
\begin{tabular}{lr}
%\hline
\toprule
Metric  & ($\tau_{g}$)\\ \midrule
CM (with HAE's metrics) & -0.05  \\
CM (with our metrics)  & \textbf{0.68}  \\
\bottomrule
\end{tabular}
\caption{
Agreement with ground-truth ranking using different metrics
}
\label{tab:metrics}
\end{table}


\subsubsection{Effect of the Starting Utterance}

As most of the chitchat bots we test in the competition 
are for open domain, we use three types of starting utterances, 
namely {\em greetings}, {\em declarative statements}, and 
{\em questions}. 
We show one example for each type in 
\tabref{tab:multi-context}.

\begin{table}[th!]
\centering
%\small
\scriptsize
\begin{tabular}{llr}
%\hline
\toprule
 & Example  & Avg ($\tau_{g}$)\\ \midrule
Greetings  & ``Hi! How are you?'' & 0.59  \\
Declarative  & ``I'm not feeling well this morning.'' & 0.62  \\
Question  & ``What did you do last week? '' & \textbf{0.68}  \\
%Average  & & 0.35  \\
\bottomrule
\end{tabular}
\caption{
Agreement with ground truth ranking using different starting utterances.} 
%Average denotes agreement after summing up scores with 
%three different starts.}
\label{tab:multi-context}
\end{table}

As \tabref{tab:multi-context} shows,
 all three starting utterances lead to a 
good correlation with human. 
The model that starts with a question performs the best since 
raising a question is always a good way to start a conversation 
and make the bots to start talking about it.
%However, we do not find a significant difference with different 
%starting utterance. 
We also find that when two 
bots are free to talk without human intervention, they prefer 
to steer the chat to their ``comfort zones'' (e.g., talking about 
their basic personal information) rather than stick to the ball 
``started'' by CM. 
Hence, we decide to use the question in other experiments.
%\KZ{In the end, do we use the questions only or a combination of
%all three? This is not said. Also, I think we should also
%test combination? If we use only questions to achieve 0.68 in the
%end to end tests, we should have said this in the experiment setup
%or approach?} 

\subsubsection{Different Number of Exchanges in a Game} 

We also test CM with a different number (e.g., 5, 10, 25, 50, 100, 150 and 200) of exchanges in a game.
A game of 100 exchanges reaches the best agreement between 
CM and human judgments, which we believe is long enough to make the bots
 expose their flaws
and show their strengths.


\subsubsection{Similarity Functions} 
 We provide a set of simple similarity functions 
to score diversity(non-repetitiveness) and consistency in chat logs which are: 
tf-idf cosine similarity, mean of Word2Vec sentence cosine similarity 
and Bag-of-Word Jaccard coefficient. $\tau_{g}$ using these three functions are 0.68, 0.65 and 0.62 respectively. We choose tf-idf cosine similarity in
the rest of the experiments.

%\begin{table}
%\scriptsize
%\centering
%\begin{tabular}{lcccc}
%\toprule
%&tf-idf & Word2Vec & Jaccard \\ \midrule
%$\tau$ &\textbf{0.68} &  0.65 & 0.62\\
%Time &\textbf{ 2m 57s} &  3m 1s & 5m 52s\\ \bottomrule
%\end{tabular}
%\caption{Comparison of three similarity functions using $\tau$ agreement.}
%\KZ{Run a few times to get average timem then update the number in time efficiency section.}
%\label{tab:similarity}
%\end{table}  



%\subsubsection{Weights of Scoring Dimensions}
%\KZ{remove this subsubsection.}
%Here we test two sets of weights which are used for summing up 
%individual dimensions at game-level:
%\begin{itemize}
%\item Equal weights: each dimension is worth one point.
%\item Linear-combination weights: we fine-tune seven 
%coefficients with a linear regression model
% according to the scores on these seven dimensions 
%and the overall rankings provided by human judges, 
%we set $fluency = knowledge = diversity = 0.33$, others$=0$.
%\KZ{Why are other coefficients zero?}
%\end{itemize}

%The final results with alternative choices  are: 
%$\tau^i = 0.68, \tau^f = 0.14 $ respectively. 
%It is not surprising to find weak agreement between our framework with fine-tuned weights and human judgments as 
%humans' standards for individual dimension rating are usually rather ambiguous according to \tabref{tab:inter}. As a result, inter-agreement among human judges on individual dimension is quite weak and the parameters fine-tuned based on this relationship are no longer reliable.

%Since equal weight performs better, this setting is adopted in
%other parts of the experiments. 

 
\subsubsection{Points for a Win, Tie and Lose at Match Level Scoring}
We have tried three sets of points a bot will gain aften a win, tie or lose at match-level:
\begin{itemize}
\item win = 3, tie = 1, lose = 0
\item win = 1, tie = 0, lose = -1
\item win = 1, tie = 0.5, lose = 0
\end{itemize}

With these three sets of parameters, 
 $\tau_g$ are 0.68, 0.62 and 0.62.
This indicates that the first setting is the best and used throughout
the experiments. 

%\subsection{Time Efficiency}
%\label{sec:time}
%As for timing, a human judge takes more than 90 minutes on average to complete 
%the conversations with all 7 bots, decide ratings on seven dimensions and then rank the bots. 
%Our system only requires about 2 min 57 secs on average to 
%complete a game, 
%which is an 100-exchange conversation. As all 42 games can be processed in 
%parallel, the total time cost for evaluating all the 6 bots is less than
%3 minutes. To put it in perspective, when we evaluate the same
%6 bots by \textit{Spot The Bot} framework, it takes on average one hour for
%each human judge to complete the ranking.


\subsection{Response Diversity of Bot-to-bot Chat}
\label{sec:diversity}
While going through bot-to-bot chat logs, 
we find that sometimes 
conversations between bots 
are even more informative than that between human and bot. 
To demonstrate this finding, 
we present the average specificity score
in \tabref{tab:div} which indicates the variety of the use of words
in dialogue.  

% \KZ{Why do we pick CR to evaluate this?
%Why chat with PL? These choices must be justified. Otherwise it's
%`` ad hoc''!} 
We can see that bots tend to generate longer 
responses while chatting with other bots. 
The diversity of words in bot-bot and human-bot conversations
 are quite close.
More examples extracted from our collected
bot-bot chat logs are shown in Appendix.    

\begin{table}[th]
\scriptsize
%\small
\centering
\begin{tabular}{lccccc}
\toprule
& D-1 & D-2 & avg(D-1, D-2)&Avg Len\\ \midrule
human-bot & 0.44  & \textbf{0.89}   &\textbf{0.67}  &12.8 \\ \midrule
bot-bot & \textbf{0.49}  &0.83    &0.66 &\textbf{15.7} \\ \bottomrule
\end{tabular}
\caption{Average Distinct-1/2 and average length of response for bot-bot chat logs and human-bot chat logs. }
\label{tab:div}
\end{table}

%\subsection{Discussion}
%In our experiments, we only test chatbots used for daily chatting. 
%It would also be useful to test chatbots from other domains 
%(e.g., domain-specific bots with specialized knowledge, 
%empathetic bots used for entertainment, etc.) in the future.  
%In order to detect more subtle weakness in bots, we probably 
%need a more comprehensive setup for future design of the system . 

