
R1:
Thanks for your review with helpful suggestions for writing and organizing the paper.

R2:
Thanks for your review with valuable questions and constructive suggestions.

Above all, we have carefully read your reasons to reject. 

1.    Doubt about evaluating "bot-bot" conversations: 
i)      First, we admit that the ultimate goal of chatbot task is to build a human-like bot which is capable of conversing freely with humans. However, this does not mean that their chat partners are limited to humans. During the conversation, the chatbot does not know whether the other party is a robot or a human. Therefore, it is not necessary to require the other party to be a human, as long as we ensure that we can detect the strength and weakness of the bot in this interactive chatting process. 

ii)    What's more, evaluation with human participation has many drawbacks. Traditional human evaluation falls into two categories: static and interactive evaluation. As for static evaluation, human judges are often asked to score the last utterance generated by the bot with static script. It is difficult to evaluate some sequential aspect about chatbot with static evaluation approach. Interactive evaluation asks human judges to talk directly with chatbots which is extremely demanding in terms of time and human effort. Human judges may often get bored easily. As a result, their conversation will not last for long, which sometimes leads to an insufficient evaluation. 

iii)   The main reason for choosing bot-bot conversations to evaluate is that it is much more time efficient and can provide more objective results. The number of rounds of a bot-bot chat can be hundreds or thousands. After chatting with a group of different bots, they can fully express their strengths and expose their shortcomings.

2.    Whether the evaluation can be generalized to other rounds of evaluation or not?

Here is our understanding of your question: as we have introduced some parameters in our framework, will the selection of these parameters still be correct and the correlation with human judges remains high if we change a group of chatbots for evaluation? According to the ablation studies(section 3.4), we do not observe an obvious difference while changing some of the parameters(e.g., number of exchanges and choice of starting utterance). Therefore, we didn't consider using test vs. dev set to choose the parameters rather than using a group of parameters that could provide reasonable results. We believe that this evaluation does generalize to other rounds of evaluation for open-domain evaluation bots.

3.    Choice of baselines.
We explain this part in the "general response to reviewers" part below.

4.    Questions that haven't been discussed before:

Q1: Difference between our approach and spot the bot
It's true that Spot the Bot also used bot-bot conversations. However, in the mean time, their work includes human-bot conversations and requires human annotation. Hence, we are the first to propose an interactive evaluation framework for chatbots which is based solely on bot-bot conversations.

Q2: Details about human evaluation
Yes, actually we provide human judges with a file which includes the definition (as Table1 shows), one good example and one poor example for individual dimension. Further description is provided in Section3.1 "Ground Truth for Rankings" part.

Q3: ChatMatch scoring methods 
In our paper, we use seven evaluation metrics. Evaluation for diversity, consistency and relevance is realized using algorithms (look at algorithm1,2,3 in the paper) based on some rules which are designed by ourselves. As for knowledge, we simply count the time a bot shows that it does not know what is talking about and as for proactivity, we count the number of questions. 

Q4: Explanation for "Bonus"
Bonus in in Line 177 means that we will add one point for the bot which shows its ability to memorize some important concepts that have shown up before. There's no difference for other points. We will give up using word "bonus" in the final version so as not to cause any understanding problem.

Q5: Bots trained/finetuned on different data
As ChatMatch is a blackbox framework, our approach doesn't care about the specific training details of chatbot (e.g., training/finetuned dataset). Therefore, as long as you think that two bots can be compared together, our framework will work.


Q6: Explanation for "Daily routing sentence"
Daily routing sentence refers to utterances appear frequently in our daily life. As all of the bots are open-domain chat bots which are not task oriented, so we believe providing them with a sentence from daily life is easy and natural to start the conversation. To ensure the fairness, two bots in one match take turns to be the first to respond to the starting utterance.

Q7: Explanation for "empirics"
Here empirics refers to what we have done for ablation test (in section3.4), where we observe similar results from different setup of parameters, so we decide to pick one group of parameters from these parameter groups.


R3:

Thanks for your review with valuable questions and constructive suggestions.

1.    Complaint about baseline:
We explain this part in the "general response to reviewers" part below.

2.    Importance of competition-inspired evaluation:
As for ChatMatch, the framework and the seven metrics are not independent from each other. On contrary, these seven metrics are designed based on the competition-inspired framework. Specifically, this double-loop competition between bots provides us with enough bot-bot chat logs for evaluation with our metrics. Meanwhile, some of the metrics such as Relevance and Consistency won't work with traditional script-based evaluation approaches. However, we have tried to use the combination of these seven metrics in static human-bot conversations. In this way, their correlation (kendall's tau) is 0.39, which is much lower than CM's result. 


Q1: Omit of coherence
Coherence is a quite comprehensive notion which includes consistency and relevance of a response. In fact, it's always considered as a dimension that represents the overall ability of a bot. However, in our paper, we decide to decompose the ability of a bot to seven dimensions which have little overlap.

Q2: Evaluation time 

Yes, you are right that the evaluation time will increase as the number of bots augment.  When we  say that the framework is efficient we mean that ChatMatch requires much lower time compared with other interactive human evaluation approaches as Table 5 shows.

General Response to Reviewers:
We found some common complaints about our paper. As for the motivation for baseline selection, we made the following reply:
1. Perplexity is a commonly used metric to evaluate the quality of generated responses. Although it is relatively simple, it is still used frequently as a baseline in many NLG tasks. 
2. Token Accuracy is another common automatic metric which needs ground truth. 
Context Coherence (Pang et al., 2020) is a latest automatic evaluation metric based on GPT-2 language model, which is automatic. 
3. Spot The Bot (Deriu et al., 2020) differs from other baselines in that it is a manual evaluation framework, which also makes use of bot-bot conversations. 

