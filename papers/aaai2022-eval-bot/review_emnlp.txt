  EMNLP 2021 
START Conference Manager 
Ruolan Yang (ruolan77)    


  

 
  

Loading... 
    
 Usr 

      
The 2021 Conference on Empirical Methods in Natural Language Processing
EMNLP 2021
Author Response

Title: 
ChatMatch: Evaluating Chatbots by Autonomous Chat Tournaments
Authors: 
Ruolan Yang, Zitong Li and Kenny Zhu 

Instructions
The author response period has begun. The reviews for your submission are displayed on this page. If you want to respond to the points raised in the reviews, you may do so in the boxes provided below. 
Please note: you are not obligated to respond to the reviews. 

For reference, you may see the review form that reviewers used to evaluate your submission. If you do not see some of the filled-in fields in the reviews below, it means that they were intended to be seen only by the committee. See the review form HERE. 

Review #1 
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposed an evaluation method for chat-bots. The method is interactive and tournament-like.
I think the basic idea is interesting. In practice, many weights, threshold, points are involved in the score calculation, which makes the setting and discussion complex.
[strength] The idea is interesting.
[weakness] The composition of the paper seems not refined. For example, the sec. 3.4 contains too many detailed issues. Perhaps this method should be simplified.
Reasons to accept
The idea is good, and it can be further developed.
Reasons to reject
The presentation of the paper can be further improved.
Reproducibility:
4
Overall Recommendation - Long Paper:
3.5

Review #2 
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes a tournament-style, automatic evaluation for chatbots. A set of chatbots chat with every other chatbot twice and are scored with 7 automatic metrics (mapped to 7 text quality dimensions). The paper describes the evaluation approach and shows that it correlates better with human chatbot evaluation than several baseline evaluation approaches.
Strengths: The paper is clear and well-structured. It addresses the issue of collecting high-quality evaluations for dialog systems in an inexpensive, fast, and somewhat interpretable way. The experiments include both automatic and human evaluation baselines, and there are additional ablation studies examining alternative setups.
Weaknesses: Given the evaluation setup was selected based on its correlation with the human evaluation scores and that there was no "held-out" evaluation set, it is unclear how well this setup generalizes outside of this specific round of ChatMatch. The motivation in the choice of baselines/evaluation dimensions and their connections to previous work are not always clear. Evaluating bot-bot conversations (as opposed to human-bot) is not aligned with the bot's intended use, which is not addressed.
Reasons to accept
It provides a direction for automatic evaluations of dialog systems and enables researchers to consider a larger set of baseline bots than they might be willing to use with an expensive, time-consuming human evaluation. While I point out some issues with the evaluation methodology, the correlation with human evaluations is still high, and the multi-dimensional and tournament-style aspects of the evaluation help with understanding the evaluation results.
Reasons to reject
While the results indicate high correlation with the human evaluations for this set of chatbots, it's unclear whether the evaluation will generalize to other rounds of evaluation. It is also unclear how this approach compares to current automatic evaluation approaches.
Questions for the Author(s)
-I was confused by the argument that bot-bot evaluation is better because bots produce more "natural and interesting conversation" than people do. Isn't the end task of the chatbots to produce natural and interesting conversations with people? Do you think there are any concerns with optimizing bots for bot-bot conversations if the goal is human-bot conversations?
-You say in your first contribution that this is the first interactive evaluation based on bot-to-bot conversations, but isn't Spot the Bot an interactive, bot-to-bot evaluation method too?
-Could you include a few more details about the human evaluation, especially about any instructions and/or training the evaluators received (if any)?
-I had trouble understanding where the ChatMatch scoring methods fit into previous work. Are the scoring methods novel or are they methods already used in chatbot work? Other than Distinct-1 and Distinct-2, there are no citations, nor are there any novelty claims that I saw.
-What does it mean in Line 177 that relevance is a "bonus" reward? How is the bonus different from the other points a bot receives?
-Does it matter that all of the chitchat bots were trained or finetuned on the same dataset? Do you think this bot-bot approach would still work well in cases where the bots might not have had access to the same data before testing?
-Line 295 says "These settings are determined by empirics." I think this needs some more explanation. Which settings did you consider and how did you determine which ones to use?
-Line 299: What do you mean by a "daily routing sentence"? I had trouble understanding how the chats were started, other than each bot took turns starting the conversation. Could you provide more details about this?
-How well does your choice of baseline evaluation metrics reflect current practices? A collection of automatic metrics seems to be more common than only using perplexity or context coherence to evaluate a system. Are the baselines you list here realistic alternatives? If so, it would be helpful to point to chatbot work that currently uses these metrics. You also mention several evaluation metrics in the related work that you don't consider in the experiments (Zhao et al, Mehri and Eskenazi, etc.)--did you consider comparing against these metrics?
-Was there any notion of a dev vs. test set in this work? It seems like you tried a lot of possible versions of ChatMatch and chose the one that correlated best with a particular set of human judgments. While it seems like many settings of ChatMatch had a high correlation with the human judgments, it makes it less obvious how well it will work for another set of bots, especially given how many different setups you experimented with.
Missing References
-There are several claims in the paper that should be cited, either by external work or by pointing to a specific result in this work e.g.,:
Line 062: "[Human annotators] tend to ask common and goal-directed questions."
Line 438: "[Knowledge and consistency] are often ignored by humans while ranking the bots."
-ChatEval (Sedoc et al., 2019) seems relevant.
Typos, Grammar, Style, and Presentation Improvements
-An example tournament diagram could be helpful for explaining Section 2.1.
Typos:
Line 024: "[A limited number …] are not sufficient" -> "is not sufficient"
Lines 107 & 156: missing period at the end
Line 186: "diviersty" -> "diversity"
Line 199: "of soccer tournament" -> "of a soccer tournament"
Reproducibility:
5
Overall Recommendation - Long Paper:
2.5

Review #3 
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes an interactive and time-efficient evaluation method called ChatMatch for chatbots. The paper claims several advantages over human-based evaluation, namely evaluating by ranking arbitrary number chatbots. The proposed framework uses several different criteria for evaluating chatbots such as consistency, fluency, knowledge, specificity, diversity, relevance, and proactivity (Summarized in Table 1). The paper further experiments with their proposed framework using seven chatbots and compared them against four baseline evaluation approaches (three without humans, and one with humans).
The strength of this paper is the idea inspired by sports competitions and applying it to evaluating chatbots. The main weakness of this paper is the baseline. Three out of four baselines use only one metric (Perplexity, Token Accuracy, or Context Coherence) whereas the proposed method uses seven different metrics. Therefore, I cannot conclude from the paper whether the competition among bots is the key, or simply including multiple metrics is important.
Reasons to accept
    • A new paradigm of evaluating chatbots (as best of my knowledge) inspired by sports competitions.
    • Evaluation on newly annotated data from 10 annotators, further reporting Kendall's tau.
Reasons to reject
    • Baselines (perplexity, token accuracy, and context coherence) are relatively simple, whereas, the proposed method uses seven metrics and using the competition-inspired evaluation.
    • Related to the above point, the conclusion of using the competition-inspired evaluation being the key is not persuasive and requires evaluation against the stronger baseline evaluation approaches.
Questions for the Author(s)
    • Why are some metrics e.g., coherence, omitted in the proposed framework?
    • In the abstract (Line 9-11), it states "This framework can efficiently rank any number of bots [...]" but doesn't the evaluation time is proportional to the number of bots to evaluate upon?
Reproducibility:
4
Overall Recommendation - Long Paper:
3


Submit Response to Reviewers
Use the following boxes to enter your response to the reviews. Please limit the total amount of words in your comments to 900 words (longer responses will not be accepted by the system). 
Response to Review #1:
 
Response to Review #2:
 
Response to Review #3:
 
General Response to Reviewers:
 

Response to Chairs
Use this textbox to contact the chairs directly only when there are serious issues regarding the reviews. Such issues can include reviewers who grossly misunderstood the submission, or have made unfair comparisons or requests in their reviews. Most submissions should not need to use this facility. 
 
 

START Conference Manager (V2.61.0 - Rev. 6350) 
  
