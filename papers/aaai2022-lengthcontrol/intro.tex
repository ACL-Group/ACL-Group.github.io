\section{Introduction}
\label{sec:intro}
Abstractive summarization aims at reproducing the semantics and 
topics of original text in a concise and fluent summary by paraphrasing.
In order to display the summary on different mobile devices 
or websites with space limitations, we have to produce summaries in different lengths. 
The existing length-controllable summarization based on encoder-decoder models, which performs well in general abstractive summarization~\cite{NallapatiZSGX16,SeeLM17,CelikyilmazBHC18,UniLM19,BART19,setlevel21,GSum21}, can be divided into two categories:
%(1) \textit{stoping decoding}, and (2) deciding which information to be summarized according to desired lengths.
(1) \textit{early-stop during decoding} and (2) \textit{information selection 
before encoding}.

\begin{table}[th!]
	\centering
	\scriptsize
	\begin{tabular}{|p{7.8cm}|}
		\hline \bf Reference summary (10 tokens) \\
		\hline
		iranians celebrate the deal online and in the streets . \\
		\hline 
		\bf BART~\cite{BART19} (56 tokens)\\
		\hline 
		iranians erupted in celebration as young people waved flags from their sunroofs , blasted music from stereos and chatted online with the hashtag $\#$irantalks . the excitement came after a breakthrough nuclear deal with the united states and other world powers . the agreement was struck on the final day of persian new year festivities . \\
		\hline 
		\bf LenAtten~\cite{lenatten21} (12 tokens) \\
		\hline  
		the agreement on the final day of persian new year festivities , \\
		\hline 
		\bf  LPAS~\cite{Proto20} (22 tokens) \\
		\hline 
		iranians erupted in celebration . the excitement came after a breakthrough nuclear deal with the united states and other world powers . \\
		\hline
		\bf  LAAM (ours) (10 tokens) \\
		\hline
		iranians erupted in celebration after the breakthrough nuclear deal .\\
		\hline 
		\bf  LAAM with pretraining (ours) (10 tokens)\\
		\hline 
		iranians celebrate online and in the streets after deal . \\
		\hline
	\end{tabular}
	\caption{\label{tab:intro} Example summaries generated by
		different models with a desired length 10. 
%BART~\cite{BART19} is the SOTA model in abstractive summarization.
		%\KZ{These acronyms are confusing. I have no idea what they are. if they are existing models, give citations. If it's our stuff, u need to say so in the caption at least.}
	}
\end{table}

%\KZ{The following 2 paras can be simplified. Just talk about the area in general,
%don't say the specifics of each paper/method.}
%The length-controllable methods based on 
{\em Early-stop during decoding} methods~\cite{KikuchiNSTO16,LiuLZ18,GOLC19,lenatten21} 
learn the relationship between length and the decoder state that determines when
to output {\em eos} (end of sequence), indicating the end of the summary.
%Both LenEmb and LenInit~\cite{KikuchiNSTO16} convert the length to embeddings.
%LenEmb adds the remaining length embedding at each decoding step.
%LenInit inputs desired length embedding to initial memory cell of LSTM decoder.
%LC~\cite{LiuLZ18} uses the desired length as an additional input to the first layer of the CNN decoder.
%GOLC~\cite{GOLC19} %considers length information by %
%adds overlength penalty to loss.
%LenAtten~\cite{lenatten21} proposes a length attention unit,
%which computes attention between decoder state and length embeddings within the desired length.
%%\cut{%%%%
%\JQ{don't understand these two sentences}
%The models without length information for training control the summary length 
%by assigning the score of {\em eos} at decoding
%during test.
%The {\em eos} score of  -$\infty$ prevents generating {\em eos} and {\em eos} score of +$\infty$ generates {\em eos}.
%Suppose the desired length is $M$, the score of EOS is -$\infty$ until the first $M$ tokens are generated and +$\infty$ after the $M^th$ token.
%This approach can be applied in seq2seq models.
%}%%%%%
However,
these methods simply add length information to the decoder
%cannot select different content from source document for summaries with different desired length.
%only pay more attention to the change of length information during decoding but 
and ignore that the content of summaries 
%selected from the source document 
should also {\bf vary according to length constraints}.
Thus, some generated summaries with short desired length are likely to be incomplete,
similar to the truncated version of summaries generated by models without length constraint.
The summary of LenAttenin in \tabref{tab:intro} is not complete and 
loses the information about ``iranians''.

Methods based on {\em information selection} 
are {\em two-stage} methods~\cite{Compress20,SeeLM17,Proto20}. 
%and then input the generated summaries to generate summaries with length constraints.
%MLS~\cite{Compress20} first generates an abstractive summary by extended PG~\cite{SeeLM17} as prototype summary. 
%Then, it expands or copies the prototype summary depending on the 
%compression budget to get final summary.
For example, LPAS~\cite{Proto20} first extracts top $N$ most important tokens from source document as prototype summary by an extractor and then summarize the source document encoded with prototype summary by a dual-encoder abstractor.
$N$ is the desired length.
%by the probability of selecting between the copy and expand operation for a sentence in the prototype summary.
%Unfortunately, at test time, the output of first stage should be input
t%o the second stage, 
%which 
On the one hand, such two-stages approaches bring noise due to the error in intermediate results.
On the other hand, the second stages of above methods weaken the length information, decreasing their length control ability.
%which makes their ability to contro length more dependent on the summary lengths in training dataset. 
As shown in \tabref{tab:intro}, LPAS contains redundant information about ``deal'' and its length is much longer than reference summary.


%the length-controllable models suffer from generating  summaries whose lengths are similart to the summary length of training set.
Inspired by LPAS, we propose a {\bf length-aware attention mechanism} (LAAM) 
which extends a transformer seq2seq model with ability to select information in 
the context according to length constraints. 
LAAM re-normalizes the attention between encoder and decoder to boost the tokens with higher attention scores based on the desired length, helping with selecting length-aware information from source document.
%The tokens of source document with higher probabilities in re-normalized attention are more likely to be selected based on current remaining length.  
The number of boosted tokens decreases step by step until $eos$ gets 
the highest attention score, which is helpful in stopping the decoding process 
at desired length.
LAAM can be thought of as a hybrid approach between the two types of previous approaches.

We observe that there is great difference in the number of summaries within different length ranges in an original training set. 
The shorter reference summaries are especially rare.
As shown in \tabref{tab:intro},
given a short desired length, the summaries of
the previous methods and LAAM still select redundant information.
%previous length-controllable methods don't perform well in length controlling or content selection.
To balance the distribution of summaries in different length ranges, we create a {\bf length-balanced dataset} (LBD) by pre-predefining the length ranges and constructing extractive summaries within different length ranges.

%which first extracts summaries within variant length ranges for each source document in orginal dataset and then sample
%consisting of source documents from original dataset and their extractive summaries with more various lengths.
In our approach, we first train LAAM on LBD to enhance the ability of LAAM on information selection with length constraints.
Then we fine-tune the pretrained LAAM on the
original dataset to learn to paraphrase
the selected information as abstractive summaries in different lengths.
The task of generating short summaries by the models finetuned on datasets without short reference summaries can be seen as a {\em zero-shot} problem.
Benefiting from the pre-training with LBD, our approach can also solve zero-shot length control problem.
Our contributions are as follows:

\begin{enumerate}
\item We propose a new length-aware attention mechnism (LAAM) to generate high-quality summaries with desired length. (\secref{sec:model})
%to help decoder to attend to more important information of source document under the length limitation.
\item We create length-balance datasets (LBD) with the lengths of summaries evenly distributed in different length ranges. (\secref{sec:lbd})
\item Our approach outperforms the state-of-the-art length-controllable methods 
on CNN/Daily Mail and XSUM. (\secref{sec:general})
Besides, our approach is effective on zero-shot length control problem.
(\secref{sec:zeroshot})

\end{enumerate}

