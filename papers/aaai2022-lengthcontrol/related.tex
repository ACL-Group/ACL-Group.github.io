\section{Related Work}
\label{sec:related}

%\subsection{Length-controllable Abstractive Summarization}
%Previous methods on length-controllable abstractive summarization
%can be divided in two aspects: 
%1) stoping decoding at a particular time, 
%and 2) deciding the information to be summarized according to desired lengths. 
Previously, most length-controllable approaches in abstractive summarization
focused on stoping decoding at a particular time.
Ad-hoc~\cite{RushCW15} generated the $eos$ token by 
assigning a score of -$\infty$ to the tokens in vocabulary and 
generate a fixed number of words. 
LenEmb and LenInit~\cite{KikuchiNSTO16} input different length embeddings to decoder respectively.
LenEmb input remaining length embeddings at each step.
LenInit added the desired length embedding into initial state. 
%This model use Gigawords as dataset and focus on 
%the abstractive summarization in sentence level 
%which generates one sentence as the summary.
LC~\cite{LiuLZ18} added the desired length into the first layer of CNN-based encoder, which can learn the probability of generating $eos$ with length information attenuation.
\citet{FanGA18} predefined some special markers to denote different length ranges and prepended the input with such markers during training and testing. 
\citet{pos19} extended the sinusoidal positional encoding to take account of stepwise remaining length.
GOLC~\cite{GOLC19} optimized LenEmb and LC by formalizing loss with overlength penalty.
\citet{RLLC19} took LenInit and LenEmb as an agent and adjusted the reward incorporating with desired length.
LenAtten~\cite{lenatten21} added length attention unit to exploit proper length information based on the stepwise remaining length. 

Other length-controllable approaches decided the content to be summarized by generating length-aware intermediate summaries.
%and then input the generated summaries to generate summaries with length constraints.
LPAS~\cite{Proto20} first extracted a word sequence with desired length from source document and then generated summary by non-length-controllable model with document and extracted summary as input.
MLS~\cite{Compress20} generated a general summary
and then input it to the length-controllable model.

Compared with previous length-controllable methods, 
our approach can effectively control the length of generated summaries
by pretrainig the length-controllable information selection model on length-balanced dataset. 
Meanwhile, it can generate summaries with length approximate
to the desired length in zero-shot controlling length problem. 

%\subsection{Pretrained Abstractive Summarization}
Recently, the approaches fine-tune the pretrained language models, such as BART~\cite{BART19}, PEGASUS~\cite{PEGASUS20} and GSum~\cite{GSum21},  on summarization datasets. They achieve outstanding performances on summarization tasks. In this paper, we also develop our approach based on a pretrained transformer model BART, which is a seq2seq model consisting of bidirectional encoder and auto-regressive decoder.

%perform best on summarization tasks.
%BERT~\cite{Bert19} was a bidirectional transformer, which was used as the pretrained encoder in summarization.
%UniLM~\cite{UniLM19} was a multi-layer transformer network,
%which included pretrained seq2seq model.
%BART~\cite{BART19} was a pretrained seq2seq model consisting of bidirectional transformer encoder and auto-regressive transformer decoder .
%ProphetNet~\cite{ProNet20} was a transformer seq2seq model to predict future n-gram.
%PEGASUS~\cite{PEGASUS20} trained the transformer seq2seq model for masked sentences generation.
%used self-supervised objective Gap Sentences Generation to train a transformer seq2seq model, which masks sentences rather than smaller continuous text spans.
%GSum~\cite{GSum21}  was pretrained model with dual encoder, which added a content guidance encoder to BART.
%and generated summaries of the specified content.
%We choose BART as a basic model 
%because it performs well in summary and requires less memory during training.
%Since these pretrained model are transformer-based encoder-decoder models, they will enjoy similar boost in controlling length after applying our approach.
%




