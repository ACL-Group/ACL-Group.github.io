Paper ID4723
Paper TitleLength Control in Abstractive Summarization by Pretraining Information Selection
Track NameMain Track
Reviewer #1
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper "Length Control in Abstractive Summarization by Pretraining Information Selection" presents work on creating summaries with varying length while retaining the relevant information. It presents results on standard datasets from the news domain. The results based on a quantitative evaluation using ROUGE shows improved results as compared to previous work in this domain.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: The paper is well-organized and clearly written.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be moderately useful to other AI researchers.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
The paper presents interesting experiments and results on creating abstractive summaries of varying lengths.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
It is unclear if the created resources will be made available to the research community. Also the evaluation using ROUGE on summaries of varying length which is not part of the reference summaries is questionable.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
1) Which version of ROUGE did you use?
2) What parameters were used for ROUGE?
3) ROUGE is very sensitive to the length of the summaries it is being compared to. The paper claims that the presented approach creates reasonable summaries even for summary lengths that have not been seen in the training data. How were these evaluated using ROUGE? If the respective summaries were truncated, the results are not very telling. The same is true, if the summaries were not truncated and are considerably longer than the reference summaries. A clarification on this would be very important.
4) Are you planning to publish the research artefacts?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
* For the tables the semantics of the bold figures is unclear. Maybe this could be indicated in the caption.
* The early DUC summarization scenarios had summaries of varying lengths on the same document and/or document cluster. This might be interesting to compare the approach to and strengthen the claims of the paper. In case it was a deliberate decision to not use the DUC data, this could be discussed in the paper.
* Some more details on the human annotators would be useful. How were they recruited and how were they payed?
Some minor remarks:
* It would be nice if links were actually clickable. It would make following the links easier.
* There are instances where ROUGE is capitalized and where it is not. It might be reasonable to make this more consistent.
* Qi et al (2020) is missing information about the publication venue.
* Shleifer & Rush (2020) is missing information about the publication venue.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Weak Accept: Technically solid, moderate to high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.
Reviewer #2
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposes a length-aware attention mechanism (LAAM) to adjust the information in the source document for a summary, based on a length constraint.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Fair: The paper has minor, easily fixable, technical flaws that do not impact the validity of the main results.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Fair: The paper is somewhat clear, but some important details are missing or unclear.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Good: key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Good: The paper adequately addresses most, but not all, of the applicable ethical considerations.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
This paper proposes a length-aware attention mechanism (LAAM) to adjust the information in the source document for a summary, based on a length constraint. The idea is good and reasonable.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. First, the paper is poorly written. It should receive a native check. The authors should carefully and thoroughly check it.
2. The method for LBD creation seems ad hoc. It was neither discussed nor verified experimentally why it should be extractive rather than compressive or abstractive.
3. For the zero-shot setting, I wonder the test datasets are too small. Why not using all the data in the ranges?
4. As for summary evaluations, fair comparisons might be difficult with different base neural models.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
The beam size can be a kind of parameters. The authors should describe how they fixed it in the paper.
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
When the authors use these small test datasets for the zero-shot setting, they should compare the cases with and without the training data for the ranges to show whether the model really works well for the setting.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility, mostly unaddressed ethical considerations.
Reviewer #3
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposes to achieve length-controllable abstractive summarization by both a length-aware attention mechanism and a length-balanced extractive summarization dataset. They achieve better length controlling ability than some other length-controllable baselines and get overall better performance.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas.
3. {Soundness} Is the paper technically sound?
Fair: The paper has minor, easily fixable, technical flaws that do not impact the validity of the main results.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Good: key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. Length control for abstractive summarization is rather important in real-world applications.

2. The idea of letting the model be aware of length capacity within the cross-attention mechanism sounds like a smart choice, and the proposed method is mostly reasonable to me.

3. The way they build the length-balanced dataset and pertain the model on it is a reasonable approach to overcome the length bias in the training set.

4. The paper is easy to follow and the methods are straightforward.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. The comparison between their model and the baselines are not fair enough and thus makes the results not strong enough to support their claims.

2. In Figure1, the authors well illustrate how their length-aware attention can help control length. It is a pity that I don't see actual examples shown in the paper or supplementary. Because I think this is a good way to show whether the method actually helps in the way we expect it does.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
1. Did you use the pretrained BART or not? It confuses me when you say ``our implementation of BART''. I assume you did because training from scratch might not get this good performance.

2. I assume Prot is the same as LPAS. You used two different model names for the same citation.

3. It bugs me to understand the intuition behind reducing the weights as length decreases (Eq. 5). Wouldn't we want the model to be more concentrated towards the end of generation and we want to give it more flexibility at the beginning?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
1. One thing I feel unsure about the proposed length-aware attention is that attention is not only about weights but also about the norms of the hidden vectors (https://arxiv.org/abs/2004.10102). From my experience, when you check the tokens with high attention weights, they are usually unimportant words (like punctuations). But if you check the tokens with high attention weights*vector norms, it is a different picture. So, I'm not sure if increasing the high-attention-weight tokens' weights is a good idea here. That's why I hope to see some illustrations of real examples.

2. I don't think the comparison with baselines are fair enough. For example, BART has no gold length as input, so intrinsically, the proposed model has an advantage. And if I understand correctly, the Prot baseline has no LM pertaining, so it is also not fair to compare with it, if the proposed method is built upon pretraiend BART. So, I think BART+Exact is a good baseline, which indeed works worse than LAAM or PtLAAM in terms of ROUGE but only slightly worse, and it has 0 length variance.

3. Human evaluation with only 50 examples and 2 evaluators may not be enough to draw statistically confident conclusions.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Borderline reject: Technically solid paper where reasons to reject, e.g., lack of novelty, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.
Reviewer #4
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposes a method for controlling generation length in text summarization. The method is based on renormalizing cross attention scores according to the remaining length in the decoder.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Poor: The main ideas of the paper are not novel or represent incremental advances.
3. {Soundness} Is the paper technically sound?
Poor: The paper has major technical flaws.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Poor: The paper is likely to have minimal impact on AI.
5. {Clarity} Is the paper well-organized and clearly written?
Poor: The paper is unclear and very hard to understand.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Poor: The experimental evaluation is flawed or the results fail to adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Poor: key details (e.g., proof sketches, experimental setup) are incomplete/unclear, or key resources (e.g., proofs, code, data) are unavailable.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
Length control is useful and interesting in summarization.

11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
The writing is not clear and many details are missing or misleading.

Some technical details are problematic. For example, in section 3.3, the initial learning rate is 3e-5 and “We terminate the training process when the learning rate drops below 10e-5”. If the above is true, the training should stop before it even starts.

The proposed method is based on BART. However, results of BART on CNN/DM and XSUM are much lower than reported in other papers (Lewis et al., 2020; Liu et al., 2021; Dou et al., 2021). Therefore, it is difficult to compare this work with other text summarization work.

To demonstrate the ability of controlling length. In addition to results to Figure 3, the authors should report (ROUGE) results of the same document with different control lengths.


References:
RefSum: Refactoring Neural Summarization Yixin Liu, Zi-Yi Dou Pengfei Liu. NAACL 2021

GSum: A General Framework for Guided Neural Abstractive Summarization. Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig. NAACL 2021.

12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
see above
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
see above
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility, mostly unaddressed ethical considerations.
