\section{Evaluation}
\label{sec:eval}
In this section, we introduce the dataset and experimental setup.
%We compare our proposed model
We analyze the experimental results
%with existing opinion summarizaiton models 
and demonstrate the advantages of our approach~\footnote{ Data and source code:\url{http://anonymized.for.blind.review}.}.

\subsection{Datasets}
In this experiment, we use the following two datasets.

\textbf{Yelp} 
\footnote{https://www.yelp.com/dataset}
contains a large number of reviews of businesses.
%in the area of business. 
%For summarization purpose, 
\citet{MeanSum19} released the development and test set with 100 samples each
containing human-written summaries on 8 reviews for the same business.

\textbf{Amazon} 
\footnote{https://cseweb.ucsd.edu/~jmcauley/datasets.html}
is a similar dataset~\cite{HeM16} made up of product reviews. % from four different categories. %Similary, a set of summaries are created by Amazon Mechanical Turk (AMT) workers in~\cite{Copycat20} . 
The development set including 28 samples and test set including 32 samples.
Each sample in development and test set contains 8 reviews and 3 human-written summaries~\cite{Copycat20}. 

For the training of our proposed approaches, we construct the synthetic semi-structured training set with 100k and 90k samples for Yelp and Amazon respectively.
Other approaches use their own synthetic data for training. 
%The human-annotated multi-review and summary pairs
%are used as development set and testing set for all compared approaches.  
%Details are shown in \tabref{tab:datasets}.

%\begin{table}[th]
%	\scriptsize
%	\centering
%	\begin{tabular}{|l|c|c|c|}
%		\hline
%		\textbf{} & \textbf{Training} &\textbf{Development} & \textbf{Testing}\\
%		\hline
%		Yelp & 100k & 100 & 100 \\
%		Amazon & 90k & $28\times3$ & $32\times3$ \\
%		\hline
%	\end{tabular}
%	\caption{%Data statistics show 
%		The number of  train/dev/test pairs. 
%		Training set is our synthetic semi-structured data. 
%		$\times3$ means three human-written  summaries per multi-review.
		%\KZ{So we are not using the original training set of the these datasets at all?}
%		}
%	\label{tab:datasets}
%\end{table}
%\KZ{What are the units of these numbers in the table? Are they reviews or multi-reviews?
%what about the lengths of the those reviews (number of words) etc. More detailed
%stats which are related to our method can be released here. Cos people might be thinking
%is it appropriate to use your methods under the characteristics of these two 
%datasets or are you taking advantage of some peculiar features in the dataset?}

\subsection{Models under Comparison}

We compared different methods trained on their own synthetic training data.
The brief description is in \tabref{tab:baselines}.
%\JQ{put the ablations to discussions, only show our best model(MB--T and MB-B) here. The definition of the our best model should be better added to the of approach }
%\begin{itemize}
%	\item \textbf{MeanSum}~\cite{MeanSum19}. An unsupervised auto-encoder model decodes the summary based on the mean representation of input reviews.
%	\item \textbf{CopyCat}~\cite{Copycat20}. A hierarchical variational autoencoder model with controlling of novelty between inputs and the generated new review.
%	\item \textbf{OpinionDigest}~\cite{SuharaWAT20}. A framework consists of opinion extraction, opinion selection and summary generation.
%	\item \textbf{Denoise}~\cite{Denoise20}. A denoising summarization model with linguistically motivated noising datasets.
%	\item \textbf{FewSum}~\cite{Fewshot20}. A conditional transformer model with specially design on consideration of review properties, including content coverage, writing style and length deviations.
%	\item \textbf{PlanSum}~\cite{abs-2012-07808}. A method explicitly take the form of aspect and sentiment distributions for synthetic dataset construction and summary generation.		
%\end{itemize}
\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{|m{1cm}<{\raggedleft}|p{6cm}|}
		\hline
		\textbf{Abbrev.} & \textbf{Description} \\ 
		%\hline
		%\multicolumn{2}{|c|}{\bf Previous Approaches} \\
		\hline
		MeanSum &Mean representations~\cite{MeanSum19}  \\
		\hline
		Copycat & Copycat-Review~\cite{Copycat20}\\
		\hline
		OpiDig & OpinionDigest~\cite{OpiDig20}\\
		\hline
		Denoise& Nosing \& denoising ~\cite{Denoise20} \\
		\hline
		FewSum & Few-shot learning~\cite{Fewshot20}\\
		\hline
		PlanSum & Content planning~\cite{Plansum20}\\
		\hline
		TransSum & Translating embedding~\cite{transsum21}\\
		\hline
		\cut{%%%
			\multicolumn{2}{|c|}{\bf Our Proposed Approaches} \\
			\hline
			BAG & \tabincell{l}{Single encoder with only noisy OAs as input.} \\
			\hline
			BAI & \tabincell{l}{Single encoder with the concatenation of noisy OAs and ISs \\ as input.} \\
			\hline
			MAI & \tabincell{l}{Dual encoder. OA encoder with noisy OAs as input and IS \\ encoder with noisy ISs as input.} \\
			\hline
			MB & \tabincell{l}{Training MAI based on pretrained BAG.} \\
			\hline
			MB-T & \tabincell{l}{Applying MAI and BAG on Transformer \cite{Transformer17} } \\
			\hline
			MB-B & \tabincell{l}{Applying MAI and BAG on BART \cite{BART20} \\} \\
			\hline
		}%%%%
	\end{tabular}
	\caption{The abbreviation and description of methods.}
	\label{tab:baselines}
\end{table}



\subsection{Implementation Details}
In this experiment,
we apply aspect-guided models on transformer seq2seq model~\cite{Transformer17}
and pretrained BART~\cite{BART20}
\footnote{https://github.com/pytorch/fairseq}.
%\protect\footnotemark

\begin{table*}[ht]
	\centering
	\scriptsize
	\begin{tabular}{|r|l|ccccc|ccccc|}
		\hline
		&\multirow{2}{*}{\bf Approach} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\ \cline{3-12}
		& & R-1 & R-2 & R-L& AC &Div & R-1 & R-2 & R-L& AC & Div\\
		\hline
		\multirow{6}{*}{Non-Pretrained}&Meansum & 28.86 & 3.66 & 15.19 & 0.34 & 0.38 &  29.20 &4.70 & 18.15& 0.17& 0.40\\
		&Copycat & 29.47 & 5.26 &18.09& 0.38 & 0.34 & 31.97 & 5.81 &20.16  & 0.18 & 0.43 \\
		&OpiDig & 29.96 &5.00 & 17.33& 0.39 & 0.33 & 29.02 & 5.14 & 17.73 & 0.23 & 0.32 \\
		&Denoise & 30.14 & 4.99 & 17.65& 0.39 & 0.27 &31.76 & 5.85 & 19.87 & 0.22 & 0.27 \\
		&FewSum\protect\footnotemark
		%\tablefootnote{In this experiment, the summary is generated by FewSum without finetuned on human-annotated data, to be fair.} 
		& 31.96 & 5.64 & 17.77 & 0.38 & 0.28 & 32.04 & 5.93 & 20.03 & 0.20 & 0.30 \\
		&MB-T & 33.43 & 6.27 & 18.37& 0.41 & 0.22 & 32.57 & 6.19 & 20.18 & 0.33 & 0.26\\
		\hline
		\multirow{3}{*}{Pretrained}&PlanSum & 34.79&7.01 &19.74 &0.38 & 0.27 & 32.87 &6.12 & 19.05 & 0.23 & 0.32\\
		&TransSum & 36.62&8.41 &20.31 &0.38 & 0.27 & 34.23& 7.24 & 20.49 & 0.23 & 0.32\\
		&MB-B & \underline{\bf 37.58} & \underline{\bf 8.76} & \underline{\bf 20.92} & \bf 0.44 & \bf 0.20 & \underline{\bf 35.30} & \underline{\bf 7.84} & \underline{\bf 21.33} & \bf 0.34 & \bf 0.25 \\
		\hline
	\end{tabular}
	\caption{Automatic evaluation. 
		MB-T and MB-B are MBs based on Transformer \cite{Transformer17} and BART \cite{BART20} respectively.
		The scores underlined are statistically
		significantly better than TransSum  with p$<$0.05 according to t-test.\protect\footnotemark
		%\footnotesize{The ROUGE scores of OpiDig and FewSum are different from the published version because their test sets are different from other approach. To be fair, we select the test sets used in most approaches.} 
	}\label{tab:all}  
\end{table*}

For creating synthetic data, 
we set $N_r$ (\secref{sec:data}) as $8$ 
since every summary in the human-annotated development and test set has $8$ reviews.
For transformer seq2seq model, we follow \citet{Transformer17} and use SGD as optimizer and set the initial learning rate as $0.1$, momentum $\beta=0.1$ 
decay $\gamma=0.1$, and a batch size of 8. 
For decoding, we used Beam Search with a beam size of $5$.
For BART, we use the {\em bart.large} model with its 
default settings and follow~\citet{BART20} in fine-tuning BART with
$lr=3e$-$05$ and warmup $=500$. We used a one-layer LSTM for encoding sections. 
The temperature is $0.2$.
At decoding, we set beam size equaling $5$.
We train our models on one RTX 2080Ti GPU with 11G RAM. 
The average training time of our approaches is about $10$ hours.
%Previous methods using different versions of ROUGE,
%the ROUGE scores shown in papers cannot be compared.
%In our experiment, we use {\em pyrouge} \footnote{https://github.com/andersjo/pyrouge} to compute the ROUGE scores.

\addtocounter{footnote}{-1}
\footnotetext{In this experiment, the summary is generated by FewSum without finetuned on human-annotated data, to be fair.}
\addtocounter{footnote}{1}
\footnotetext{The ROUGE scores of OpiDig and FewSum are different from the published version because their test sets are different from other approach. To be fair, we select the test sets of most approaches.}

\subsection{Evaluation Metrics}
We evaluate the performance of our methods by {\em automatic metrics}
and {\em human evaluation}.

%\textbf{Automatic Metrics.}

%\begin{itemize} 
%\item 
\textbf{ROUGE}
\footnote{https://github.com/andersjo/pyrouge} 
scores (F1) include
%is the standard evaluation metric in summarization task,
ROUGE-1 (R-1), ROUGE-2 (R-2) and
ROUGE-L(R-L)~\cite{rouge}.


\textbf{Diversity} (Div). It uses self-BLEU~\cite{SelfBleu18}
which measures BLEU scores of each generated sentence by considering others as reference. 
The lower value means more diversity.

\textbf{Aspect Coverage} (AC).
It measures the overlapping of aspects in gold summary and generated summary.
We extract aspects from summaries 
by a rule-based method~\cite{aspect14},
which is different from MIN-MINER in our approach
\footnote{This is to eliminate the bias caused by using the same aspect extraction tool in the approach and evaluation.}.
We take aspects of gold summary as reference,
and compute R-1 recall between reference  
and aspects of generated summaries.
AC is the average of these R-1 recall scores.
%\end{itemize}

\textbf{Human Evaluation.}
We randomly select 32 samples (the same as Amazon) from Yelp and all samples from Amazon.
We ask three 
human annotators,
%\footnote{The Cohen's Kappa coefficient among annotators are $0.6$, indicating substantial agreement.}
who are native or proficient English speakers to 
rank gold summary and summaries 
generated by our best model and TransSum considering the consistency with multi-review and informativeness together.
%\KZ{This part is not clear. How did judges score the gold summaries and the generated summaries? One by one or give them rankings? Also, since you ask the judges to rank based on consistency and informativeness, that sounds like two scores. Then how do u merge these two?}
\citet{bestworst16} show that Best-Worst Scaling~\cite{bestworst} is more reliable.
We use Best-Worst Scaling as our human evaluation, which computes the percentage of times a model was selected as the best minus the percentage of times it was selected as the worst.
The score of a model in Best-Worst Scaling ranges from -1 (unanimously worst) to +1 (unanimously best). 
%\KZ{When u say percent of times, what's the denominator? 3? This doesn't really make sense. Why don't u just ask human judges rank a set of summaries produced for the same multi-review, one of which is the gold, the rest are generated by various methods. Then then you just sum up all the rankings for each methods and compare in a table?}
\cut{%%
PlanSum according to the consistency with multi-review and informativeness,
i.e., best (1.0), average (2.0) and worst (3.0).
%For a generated summary, we average the scores by
For a generated summary, we average the scores by
three annotators.
The human evaluation score of a model is the average scores of its all generated summaries.
}%%%%



\subsection{Results}
\label{sec:results}
In this section, we analyze our proposed synthetic training data  and 
aspect-guided models.

\subsubsection{Generation.}
As shown in \tabref{tab:all}, we compare our best model MB with previous models.
The MB-T based on non-pretrained model and performs best among all non-pretrained models.
Compared with TransSum and PlanSum, the ROUGE scores of MB-T is lower
because TransSum and PlanSum are based on the pretrained BERT~\cite{BERT19}.
We also convert MB-T models to MB-B on pretrained BART.
MB-B achieves the best scores in terms of ROUGE, AC and Div,
showing that our MB-B model 
fully utilizes our created synthetic training data.
MB trained on our semi-structured synthetic training data
learns how to expand important OAs from input to generate some general
sentences and abstract ISs to describe implicit information 
in specific sentences.
The higher AC score of MB-B means
that the model benefits from directly taking OAs as input.
The lower Div scores show the model captures the information of
ISs.

The improvement of our best model on Yelp is less than Amazon
because each sample in Amazon has 3 reference summaries and only has one reference summary in Yelp.
%There are more than one correct abstractive summaries for a multi-revew.
A single reference summary may not cover all important information in multiple reviews.
Thus, even if the generated summary covers
more important information,
the ROUGE scores based on one reference summary may not be changed much.
%The multiple reference summaries are friendly to various abstractive summaries.
The improved generated summaries are more likely to match tokens
in multiple reference summaries. 


\begin{table*}[th]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|r|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}{}&\multicolumn{2}{c|}{\bf Approach} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\ \cline{2-13}	
			&\textbf{Synthetic data} & \textbf{Model} & R-1 & R-2 & R-L & AC & Div & R-1 & R-2 & R-L & AC & Div \\ 
			\hline
			\multirow{7}{*}{Non-Pretrained}&OpiDig & OpiDig & 29.96 &5.00 & 17.33& 0.39 & 0.33 & 29.02 & 5.14 & 17.73 & 0.23 & 0.32 \\
			\cline{2-13}	 
			&semi-OpiDig& MB-T & 30.62 & 5.38 & 18.00 & 0.41 & 0.21 &28.87 & 5.33& 17.84 & 0.26 & 0.28\\
			\cline{2-13}
			&FewSum & FewSum & 31.96 & 5.64 & 17.77 & 0.38 & 0.28 & 32.04 & 5.93 & 20.03 & 0.20 & 0.30 \\
			\cline{2-13}
			&semi-FewSum & MB-T & 32.15 & 6.04 & 17.78 & 0.39 & 0.24 &32.36 & 6.12& 20.21 & 0.28 & 0.27\\
			\cline{2-13}	 
			&Denoise & Denoise& 30.14 & 4.99 & 17.65& 0.39 & 0.27 &31.76 & 5.85 & 19.87 & 0.22 & 0.27  \\
			\cline{2-13} 
			&semi-Denoise & MB-T & 30.77 & 5.46& 17.94 & 0.41 & 0.24 & 32.13& 6.35& 20.17 & 0.20 & 0.38 \\
			\cline{2-13} 
			&OURs & MB-T & 33.43 & 6.27 & 18.37& 0.41 & 0.22 & 32.57 & 6.19 & 20.18 & 0.33 & 0.26 \\ 
			\hline
			\multirow{5}{*}{Pretrained} & PlanSum & PlanSum & 34.79&7.01 &19.74 &0.38 & 0.27 & 32.87 &6.12 & 19.05 & 0.23 & 0.32\\  
			\cline{2-13}
			&semi-PlanSum& MB-B & 35.28& 7.28& 20.34& 0.41 & 0.23 & 33.14 & 6.42 &  19.27& 0.28 & 0.28 \\
			\cline{2-13}
			&TransSum & TransSum & 36.62&8.41 &20.31 &0.38 & 0.27 & 34.23& 7.24 & 20.49 & 0.23 & 0.32\\  
			\cline{2-13}
			&semi-TransSum& MB-B & 36.96& 8.38 & 20.62 & 0.41 & 0.23 & 34.87 & 7.38 &  20.67& 0.28 & 0.28 \\
			\cline{2-13}
			&OURs& MB-B & \bf 37.58 & \bf 8.76 & \bf 21.17 & \bf 0.44 & \bf 0.20 & \bf 35.30 & \bf 7.84 & \bf 21.33 & \bf 0.34 & \bf 0.25 \\ 
			\hline
		\end{tabular}
	\end{center}
	\caption{The different synthetic datasets in semi-structured version
		are trained on our model MB-T and MB-B because of the characteristics of semi-structured data. ``semi-'' means the semi-structured version. The data of TransSum is from {\em leave-one-out}. OURs is our created synthetic training data.}
	\label{tab:traindata}  
\end{table*}


%As the abstractive summary is various,
Besides, we use human evaluation
to help automatic evaluation.
As shown in  \tabref{tab:human}, 
the human evaluation score of Gold is the best
since the Gold summaries are written by human.
%The score of our model lower than PlanSum
%denotes that 
%human-annotators thinks that
The summaries generated by our model 
are more consistent with multi-reviews than TransSum with higher score.
\begin{table}[th]
	\centering
	\scriptsize
	\begin{tabular}{|l|c|c|c|}
		\hline 
		& Gold & TransSum & MB-B\\
		\hline
		Yelp & \bf{0.41} &  -0.63 & 0.22 \\
		Amazon &\bf{0.32} & -0.48 & 0.16 \\
		\hline
	\end{tabular}
	\caption{Human evaluation. The Fleiss' Kappa coefficient is 0.62, indicating substantial agreement.
		%The average pair-wise Kappa coefficient among 3 judges is $0.6$, 
		%indicating substantial agreement.
		%\KZ{How do you compute the kappa coefficient?} 
	}\label{tab:human}  
\end{table}

%\KZ{why do u have to temper with the footnote count?}




\textbf{Synthetic training data.}
%\JQ{put this sub-section after Generation}
%\KZ{There is a wrong wording in this section: you are not training datasets on a model;
%you are training a model USING some dataset. Check everywhere to correct this wording.}
%\cut{Existing synthetic training datasets
%either takes textual review as input or only takes opinion-aspect pair as input.}
%The synthetic data of the state-of-the-art model, PlanSum,
%consists of multi-review and summary pairs.
To evaluate the effectiveness of semi-structure training data versus
other kinds of synthetic data,
we convert the previous synthetic data into semi-structured version,
and compare the original model trained on original synthetic data 
and the model MB trained on semi-structured verison of different synthetic datasets.
The results are listed in \tabref{tab:traindata}.



For the datasets with textual multi-review as input (FewSum, Denoise, PlanSum and TransSum), 
we convert them into semi-structured version by extracting 
opinion-aspect pairs (OAs) and implicit sentences (ISs)
from their multi-reviews as input.
For the synthetic data with strucutured input (OpiDig),
it takes a review as output and the OAs of this review as input.
%During generation, OpiDig clusters the OAs from multi-review and
%takes centroids of clusters as input.
We construct the semi-structured version of such data
by generating the noisy OAs and noisy ISs for each output in original dataset as input.
To be fair, as shown in \tabref{tab:traindata}, we train MB-T (non-pretrained) and MB-B (pretrained) on the datasets %and its semi-structured version 
in non-pretrained approaches and pretrained approaches respectively.
%, which should use MB for training.
%that synthetic datasets are specific and 
%the models designed for datasets can make full use of data.
%As shown in \tabref{tab:traindata}, 
We train different models on different original synthetic datasets because
the synthetic dataset is compatible with its model.
The MB trained on semi-structured version of previous textual synthetic datasets
is better than previous approaches in terms of ROUGE, AC and Div scores,
showing that semi-structured data
is helpful in highlighting the aspects and opinions.



Compared with the structured datasets OpiDig, the semi-OpiDig get better ROUGE scores and Div scores as the ISs in semi-structured data help the model capture implicit information.
As shown in \tabref{tab:OAIS}, we calculate the distribution of the sentences (OA-sent) that can extract OAs and the sentences (ISs) that cannot extract OAs in multi-reviews and reference summaries in human-annotated test sets.
Only using OAs as input will miss out the much information of multi-review
as the proportion of ISs in multi-review is more than $40\%$.
More than $30\%$ of sentences in reference summary are ISs, which should be summarized from the ISs in multi-review.
Thus, adding ISs is important and necessary.
%and the ISs in semi-structured data help model capture implicit information.

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\multirow{2}{*}{Dataset}&\multicolumn{2}{c|}{\textbf{Multi-review}}&\multicolumn{2}{c|}{\textbf{Reference summary}} \\ \cline{2-5}
		&OA-sent & ISs & OA-sent & ISs \\
		\hline
		Yelp & 0.54& 0.46&0.68 &0.32\\
		\hline
		Amazon &  0.55&0.45&0.63&0.37\\
		\hline
	\end{tabular}
	\caption{The proportion of OA-sent and IS in the test sets.}
	\label{tab:OAIS}
\end{table}



Our semi-structured synthetic dataset
achieves the best scores in \tabref{tab:traindata}. This shows that
%usefulness of representing review by semi-structured OAs and ISs. 
%And 
the way of getting noisy OAs and ISs according to sampled summary 
is optimal.



\textbf{Our proposed models.}
As shown in \tabref{tab:abla},
we compare different models designed for semi-structured training data.
%The performance of
%BAG is worst because
%of only using OAs as input.
%The generated summary of BAG in \tabref{tab:exp} 
%shows that BAG with only OAs as input always generate 
%sentences in general and cannot capture
%the implicit information in multi-review.
BAG with only the noisy OAs as input performs worst,
as BAG %training on noisy OAs and textual summary 
only learns to make sentence from OAs. At test, the BAG can only generate summaries
from the OAs in multi-review and cannot capture the information of sentence without OAs.
As shown in \tabref{tab:exp}, the sentences in the summary of BAG 
are in general and have no specific information.

BAI 
%uses the same model structure as BAG but 
takes the concatenation of noisy OAs and ISs as input.
It 
%introduce implicit information to model by ISs and 
performs better than BAG on ROUGE and Div 
because of introducing ISs.
With noisy OAs and ISs as input, the model should learn to expand important OAs as sentences and summarize ISs in a better way.
But BAI equally deals with OAs and ISs,
which interferes with the model's understanding of OAs and ISs.
In \tabref{tab:abla} and \tabref{tab:exp},
BAI gets lower AC and fails to filter some noisy aspects, such as ``atmosphere'' and ``experience''.

MAI deals with OAs and ISs by different encoders, which performs better than BAI on all evaluation metrics. 
As shown in \tabref{tab:exp}, MAI makes sentences from OAs better and generates more sentences summaried from ISs.
The AC scores models in \tabref{tab:abla} are similar
because all of them directly use OAs as input.

To make full use of semi-structured data,
MB is the model first trained on BAG and then finetuned on MAI.
%which first gets explicit opinion about OAs and
%then gets implicit opinion from ISs.  
The pretraining of MB on BAG enables the model better to select OAs. 
The OAs and ISs of MB in \tabref{tab:exp} are most similar to Gold.
MB obtains the best results on both datasets, which shows
that the way of training the semi-structured data on 
the model with dual encoder is effective. % for opinion summarization.

\begin{table}[th]
	\centering
	\scriptsize
	\subtable[Aspect Coverage and Diversity scores]{
		%\label{tab:abla}  
	
	\begin{tabular}{|l|l|c|c|c|c|}
	\hline
	\multicolumn{2}{|c|}{\multirow{2}{*}{\bf Model}} & \multicolumn{2}{c|}{\bf Yelp} &  \multicolumn{2}{c|}{\bf Amazon} \\ \cline{3-6}
	\multicolumn{2}{|c|}{} & AC & Div & AC & Div \\
	\hline
	\multirow{4}{*}{Trans} & BAG & 0.35 & 0.26  & 0.26& 0.28 \\
	& BAI & 0.33 & 0.24 &0.24 & 0.27 \\
	& MAI & 0.39&  0.23 & 0.30 & 0.26 \\
	& MB & \bf 0.41 & \bf 0.22 & \bf 0.33 & \bf 0.26\\
	\hline
	\multirow{4}{*}{BART} & BAG &0.40 & 0.25 &0.28 & 0.27 \\
	& BAI & 0.38 & 0.22 & 0.28 & 0.26 \\
	& MAI & 0.43 & 0.22 & 0.32 & 0.26 \\
	& MB & \bf 0.44 & \bf 0.20 &\bf 0.34 & \bf 0.25 \\
	\hline
\end{tabular}
	}
\qquad
\subtable[ROUGE scores]{
	%\label{tab:acdv}
\begin{tabular}{|l|m{0.7cm}<{\raggedright}|m{0.6cm}<{\centering}|c|c|c|c|m{0.6cm}<{\centering}|}
	\hline
	\multicolumn{2}{|c|}{\multirow{2}{*}{\bf Model}} & \multicolumn{3}{c|}{\bf Yelp} &  \multicolumn{3}{c|}{\bf Amazon} \\ \cline{3-8}
	\multicolumn{2}{|c|}{}& R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
	\hline
	\multirow{4}{*}{Trans}&BAG & 30.78 & 5.37 & 17.38 & 29.20 & 5.42 & 17.96 \\
	&BAI & 32.09 & 5.55 & 17.60 & 30.27 & 5.73& 18.39 \\
	&MAI & 32.21 & 5.71 & 18.06 & 31.94 & 6.00& 19.69\\
	&MB & \bf 33.43 & \bf 6.07  & \bf 18.37 & \bf 32.57 &\bf 6.19 & \bf 20.18 \\
	\hline
	\multirow{4}{*}{BART}&BAG & 34.79 & 6.83 & 18.42 & 31.27 & 6.31 & 19.14  \\
	&BAI & 36.27 & 7.94  & 20.60 & 33.96 & 7.23 &20.77 \\
	&MAI & 36.93 & 8.40  & 21.11 & 34.47& 7.57& 21.03\\
	&MB & \bf 37.58 & \bf 8.76 & \bf 21.17 & \bf 35.30 & \bf 7.84 & \bf 21.33  \\
	\hline
\end{tabular}
}
\caption{The results of summaries generated by transformer (Trans) and BART training on our synthetic training data.}	\label{tab:abla}
\end{table}

\cut{%%%
\begin{table*}[ht]
	\centering
	\scriptsize
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{}&\multirow{2}{*}{\bf Model} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\ \cline{3-12}
		& & R-1 & R-2 & R-L & AC & Div & R-1 & R-2 & R-L& AC & Div\\
		\hline
		\multirow{4}{*}{Trans}&BAG & 28.04 & 5.37 & 25.60 & 0.35 & 0.26 & 29.20 & 5.42 & 26.47&0.26&0.28 \\
		&BAI & 28.99 & 5.55 & 26.40 & 0.33 & 0.24 & 30.27 & 5.73& 27.39 &0.24 & 0.27 \\
		&MAI & 29.49 & 5.71 & 26.86 & 0.39 & 0.23 & 29.94 & 5.88& 26.69 & 0.30 & 0.26 \\
		&MB & \bf 30.43 & \bf 6.37  & \bf 28.09 & \bf 0.41 & \bf 0.22& \bf 31.79 &\bf 6.08 & \bf 28.52 & \bf 0.33 & \bf 0.26 \\
		\hline
		\multirow{4}{*}{Trans}&BAG & 29.79 & 5.83 & 27.07 & 0.40 & 0.25 & 30.01 & 5.77 & 26.54 & 0.28 & 0.27 \\
		&BAI & 30.27 & 5.94  & 26.90 &0.38 & 0.22 & 30.18 & 5.90 &27.14 & 0.28 & 0.26 \\
		&MAI & 30.93 & 6.40  & 28.11 & 0.43 & 0.22 & 31.47& 6.27& 28.03 & 0.32 & 0.26 \\
		&MB & \bf 32.20 & \bf 7.06 & \bf 29.15 & \bf 0.44 & \bf 0.20 & \bf 32.65 & \bf 6.78 & \bf 29.14&  \bf 0.34 & \bf 0.25 \\
		\hline
	\end{tabular}
	\caption{The ROUGE scores of summaries generated by models training on our created synthetic data. Trans means transformer.
	}\label{tab:abla}  
\end{table*}
}%%%%%

\cut{%%%
\begin{table}[th]
	\centering
	\scriptsize
		\begin{tabular}{|l|m{0.7cm}<{\raggedright}|m{0.6cm}<{\centering}|c|c|c|c|m{0.6cm}<{\centering}|}
			\hline
			\multirow{2}{*}{}&\multirow{2}{*}{\bf Model} & \multicolumn{3}{c|}{\bf Yelp} &  \multicolumn{3}{c|}{\bf Amazon} \\ \cline{3-8}
			& & R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
			\hline
			\multirow{4}{*}{Trans}&BAG & 28.04 & 5.37 & 25.60 & 29.20 & 5.42 & 26.47 \\
			&BAI & 28.99 & 5.55 & 26.40 & 30.27 & 5.73& 27.39 \\
			&MAI & 29.49 & 5.71 & 26.86 & 29.94 & 5.88& 26.69\\
			&MB & \bf 30.43 & \bf 6.37  & \bf 28.09 & \bf 31.79 &\bf 6.08 & \bf 28.52 \\
			\hline
			\multirow{4}{*}{Trans}&BAG & 29.79 & 5.83 & 27.07 & 30.01 & 5.77 & 26.54  \\
			&BAI & 30.27 & 5.94  & 26.90 & 30.18 & 5.90 &27.14 \\
			&MAI & 30.93 & 6.40  & 28.11 & 31.47& 6.27& 28.03\\
			&MB & \bf 32.20 & \bf 7.06 & \bf 29.15 & \bf 32.65 & \bf 6.78 & \bf 29.14  \\
			\hline
		\end{tabular}
	\caption{The ROUGE scores of summaries generated by models training on our created synthetic data. Trans means transformer.
	}\label{tab:abla}  
\end{table}
}%%%%

\begin{table}[th]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|m{6.2cm}|}	
			%\hline \bf{BAG} \\
			\hline
			Gold & 
			the \textbf{servers} are \textbf{kind} and \textbf{knowledgeable} . \textit{they will} 
			\textit{patiently answer your questions .} \textit{they offer patio  seating .} 
			%\textit{seating if you ' d prefer to sit outside .} 
			the \textbf{free chips} and \textbf{salsa} are always a plus , and the \textbf{margaritas} are \textbf{amazing} too . the menu is \textbf{full tasty authentic mexican food .} \\
			\hline
			BAG & the \textbf{servers} are always \textbf{friendly} and the \textbf{food} is \textbf{great} . it is a \textbf{good mexican restaurant .} \\
			%\hline \bf{BAI} \\
			\hline
			BAI & \textbf{great food} , \textbf{great service} , \textbf{great atmosphere} , and \textbf{great prices} . \underline{i have been there a few times and have \textbf{never} had a} \underline{\textbf{bad experience} . }
			\vspace{0.15em}
			\\
			%\hline \bf{MAI} \\
			\hline
			MAI & \textit{i love this place}. the \textbf{food} is \textbf{good} and the \textbf{service} is \textbf{great} . the \textbf{atmosphere} is \textbf{great} too. 
			\textit{\underline{the only thing is} \underline{that it 's a little pricey for what you get .}}
			\vspace{0.15em}
			\\
			% i will definitely be coming back.}  \\
			%\hline \bf{MB} \\
			\hline
			MB & it 's one of the \textbf{authentic mexican restaurant} in the area. the \textbf{food} is \textbf{great} . the \textbf{servers} are \textbf{very friendly} and \textbf{knowledgeable} . \textit{they took the order patiently . } the \textbf{chips} and \textbf{salsa} are \textbf{good} too . \textit{it is huge and has patio seating .}  
			%\color{gray}{you can get a lot of food for the price you pay . }
			\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Summaries generated by BART-based models. Bolded words belong to OAs and Italicized sentences are ISs. The underlined sentences don't match Gold sumamry.
	}\label{tab:exp}  
\end{table}

\cut{%%%%
\begin{table}[th]
	\centering
	\scriptsize
	\begin{tabular}{|l|l|c|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{\multirow{2}{*}{\bf Model}} & \multicolumn{2}{c|}{\bf Yelp} &  \multicolumn{2}{c|}{\bf Amazon} \\ \cline{3-6}
		\multicolumn{2}{|c|}{} & AC & Div & AC & Div \\
		\hline
		\multirow{4}{*}{Trans} & BAG & 0.35 & 0.26  & 0.26& 0.28 \\
		& BAI & 0.33 & 0.24 &0.24 & 0.27 \\
		& MAI & 0.39&  0.23 & 0.30 & 0.26 \\
		& MB & \bf 0.41 & \bf 0.22 & \bf 0.33 & \bf 0.26\\
		\hline
		\multirow{4}{*}{BART} & BAG &0.40 & 0.25 &0.28 & 0.27 \\
		& BAI & 0.38 & 0.22 & 0.28 & 0.26 \\
		& MAI & 0.43 & 0.22 & 0.32 & 0.26 \\
		& MB & \bf 0.44 & \bf 0.20 &\bf 0.34 & \bf 0.25 \\
		\hline
	\end{tabular}
	\caption{The aspect coverage and diversity scores of summaries generated by transformer (Trans) and BART trained on our created synthetic data.}\label{tab:acdv}  
\end{table}
}%%%%





