\section{Related Work}
\label{sec:related}

%Multi-document summarization~\cite{abs-2011-04843} is used for generating an informative summary of multiple topic-related texts, such as news~\cite{FabbriLSLR19} and emails~\cite{ZajicDL08}.
% Wikipedia articles ~\cite{LiuSPGSKS18} and so on. 
Opinion summarization~\cite{GeraniMCNN14,rtopi21} is a typical multi-document summarization proble~\cite{abs-2011-04843,FabbriLSLR19,ZajicDL08},
which can generate a summary covering the salient opinions
of multiple reviews. It inherently has a special focus on aspects of the product or service, 
making it different from other tasks.

Opinion summarization suffers from a lack of training pairs. 
Some work~\cite{MeanSum19, Copycat20,tree21} used auto-encoder to train the model by 
reconstructing loss or sentence embeddings. 
%\citet{TianY019} classified words into three types, including aspect, opinion and context and predicted the work type as a first step. 
Others construct synthetic datasets for supervised training. 
The input format of the synthetic datasets can be divided into 
textual input and structured input.  
For the textual input,
%the most intuitive way 
%proposed by some approaches
some approaches~\cite{Fewshot20,transsum21}
regarded one review for a product as a summary 
and took all or part of the rest as input. 
\citet{transsum21} computed the distance between the summary and all remaining reviews as weights of review embeddings.
%So, further works~
\citet{Plansum20} took the nearest neighbors as inputs based on review representations.
%Adding noise to the sampled summary and taking the disturbed ones as the input reivews is another way to generate synthetic datsets. 
\citet{Denoise20} added noise to the sampled summary from two aspects: the segment noising at token and chunk level, and document noising by replacing the whole review by a similar one. 
\citet{prefix21} labeled input reviews and sampled summaries with control tokens as additional input and took control tokens as prefixes at decoding,
which cannot be compared fairly with the methods without external labels.
%Different from their simple replacing, removing and inserting operations, 
However, the quality of such datasets is limited by the biased reviews, 
which cannot be summarized from other reviews. 


%Although above mentioned work mainly focused on data construction and ignored the characteristics of reviews, aspects and opinions are quite important for opinion summarization~\cite{MukherjeePVGBG20}. 
The aspects and opinions are quite important for opinion summarization.
Some approaches~\cite{AngelidisL18,MukherjeePVGBG20} classified the sentences of reviews into different aspects and collected the most salient sentence of each class as the summary.
\citet{TianY019} %is a single input opinion summarization, which 
classified words into three types (i.e. aspect, opinion and context) and predicted words by the probability distribution on these types.
Inspired by these works,
\citet{OpiDig20} extracted opinion-aspect phrases from each review for filtering information and transformed the task into single document summarization. 
%Besides, previous work~\cite{Plansum20} shows that project each review into aspect and sentiment distributions can also help.
However, all of these works are limited by the accuracy of 
the opinion-aspect extractor and also
neglect that some detailed other information is left in the sentences without typical opinion-aspect pairs. 

Thus, we propose a method to create a semi-structured synthetic dataset consisting of
opinion-aspect pairs and implicit sentences. 
We construct the noisy opinion-aspect pairs noising and noisy implicit sentences as input, which leads to more comprehensive summaries.  
