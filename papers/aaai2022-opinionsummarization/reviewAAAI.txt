Reviewer #1
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper proposes a method to summarize opinions in an abstractive manner. To facilitate training without the availability of gold-standard summaries, they create a synthetic dataset with semi-structured inputs. Results show that training a neural model with the proposed dataset creation method outperforms existing methods.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Fair: The paper has minor, easily fixable, technical flaws that do not impact the validity of the main results.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR moderate impact across more than one subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Poor: The paper is unclear and very hard to understand.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Good: The shared resources are likely to be very useful to other AI researchers.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Good: key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Good: The paper adequately addresses most, but not all, of the applicable ethical considerations.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. The use of implicit sentences is well-motivated.
2. The proposed model performs better than the previous state-of-the-art.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. The paper is very hard to read, lacks important figures and examples, and contains grammatical errors.
2. Parts of the evaluation are not very convincing.
3. The contributions of each proposed modules are unclear.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
see below for details.
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
1. Multiple parts of the paper are very hard to read. This is perhaps because there are many notations and abbreviations used. For example, OAs and ISs are also referred to as P^e and S^e, which are not very intuitive.
2. It would be much easier for readers to understand the paper if there are illustrations of the following: (a) a running example that transforms review sets into review-summary training pairs to explain the creation dataset method, (b) illustrations of the different architectures proposed in this paper (BAG, BAI, MAI)
3. A single human evaluation score for each of the models is not very informative. It is better to divide this score into multiple criteria (informativeness, coherence, fluency, etc.) so that readers would know why humans voted the proposed model as the best. Also, please provide a description of the instructions given to the annotators (e.g., what does it mean to be a "better" summary? what are the given information to judge the summaries?)
4. I find it hard to trust the aspect coverage metric. The authors use a rule-based method to extract aspects from system summaries. Is this method good enough to be used for evaluation? How good is the method on extractive aspects from different domains (Yelp vs Amazon)? These questions are especially important since the method was published in 2014 and may not be the current state-of-the-art for aspect extraction.
5. There are multiple components proposed in the paper (OAs, ISs, noisy versions, EM OAs, MM OAs, etc.). It is unclear how much of these components contributed to the overall performance of the model. Please provide an ablation study on these components.
6. PlanSum and TransSum do not fine-tune pretrained LM weights, as is done in this paper. I don't think they can be classified similarly as shown in Table 3.
7. Please describe in detail the rule-based MIN-MINER and the syntactic rules used to extract OAs. These are important details that the readers need to know to understand how the method works.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility, incompletely addressed ethical considerations.
Reviewer #2
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposes a novel data creation method for opinion summarization. It generates semi-structured synthetic training data from review texts, which contains ﻿opinion-aspect pairs (OAs) and implicit sentences (ISs). ﻿Then it regards OAs and ISs as input and textual summary as output, and design three aspect-guided encoder-decoder models. ﻿The results showed that the proposed model can achieve better performance compared with other pre-trained or non-pre-trained methods.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be moderately useful to other AI researchers.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Good: key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Good: The paper adequately addresses most, but not all, of the applicable ethical considerations.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
A novel data creation method for the pretraining of opinion summarization

A dual-encoding summarizer with opinion-aspect pairs and implicit sentences as inputs

The proposed method achieves SOTA results on two opinion summarization datasets
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
The automatic evaluation table is problematic. Some baselines may use google rouge rather than pyrouge

Human evaluation is weak compared with other works. Human evaluation should be performed under different measures (informativeness, fluency, conciseness, etc). Copycat paper or some other papers provide good examples.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
Currently, OAs and ISs are sampled based on token overlapping and bow embeddings. Is it possible to use some deeper semantic-level similarities during sampling to make the summary more abstractive?

To further fine-tune MAI based on the pre-trained BAG, how to initialize the parameters that are in MAI but not in BAG? Will two encoders share the same parameters during training?

﻿Why aggregate the information of OAs using LSTM rather than a special initial token (like <cls> in BERT or <s> in BART)?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
See {Reasons to Reject}
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Accept: Technically solid paper, with high impact on at least one sub-area of AI or moderate to high impact on more than one area of AI, with good to excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.
Reviewer #3
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The authors present a weakly-supervised aspect-guided multi-doc (reviews) abstractive opinion summarization model.
By aspect guided, it creates a synthetic aspect-guided dataset that it breaks down an opinion into components of Opinion-Aspect (OA) pairs and Implicit Statements (IS). The latter are sentences that cannot be extracted for OA’s; they are designated as ISs. This is because reviews are short. Thus all sentences have value is some way.

The paper presents 4 variations of the model:
* basic transformer seq2seq trained on OAs only (BAG)
* basic transformer seq2seq trained on concatenated OAs and ISs (BAI)
* Modified encoder-decoder (original Transformer or BART) model with an LSTM to aggregate the separately encoded OAs and ISs, taking into account the proper distribution of each (MAI)
* MAI but trained on BAG first before trained on MAI (MB)
 Testing (metrics: ROGUE, diversity, aspect-coverage, human evaluation):
* Performance: best among systems, and closest to human as per human evaluation
* MB performance among the presented model is best. It separately treats OAs and ISs. The two-stage training of BAG then MAI also improves performance.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR moderate impact across more than one subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Fair: The paper is somewhat clear, but some important details are missing or unclear.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Good: key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
* Good idea to create better synthetic dataset for training
* The dual stage training with BAG (less complex, it seems), before with MAI (more difficult) goes in step with a number of observations in multi-task learning, etc.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
* did not thoroughly present all facets of the model's performance
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
Notes:
* What new information does the section 4 (Related Work) add? It seems just a rephrasing some of the paragraphs in the Introduction
* Table 8 for anecdote is a good addition. However, there should be more and more analysis on specific cases needs to be added to show which parts need to be improved to go closer to human (Gold) standard. What are the current typical shortcomings?
* It would be nice to talk about the mechanisms to appropriately adding an IS after a particular OA. For example, in table 8, the MB output last sentence goes: “the chips and salsa are good too. It is huge and has patio seating” Clearly “It” refers to the restaurant space, not the size of the chips and salsa. I suppose this would have been the “entailment” modeling of ordering sentences.

13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
Section 2 is quite dense and needs to be read slowly.

Grammars, typos, etc.:
pg.7, col.1, par.1: proble —> problem
pg.7, col.1, par.2: reivews —> reviews
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Borderline accept: Technically solid paper where reasons to accept, e.g., novelty, outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.
Reviewer #4
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper is an interesting work done on abstractive opinion summarization of multiple reviews. It provides a new data creation method to generate a semi-structured synthetic training data for opinion summarization. In addition, the authors design an aspect-guided model with opinion-aspect pair encoder and implicit sentence encoder. The experimental results on two benchmark datasets show that the proposed model can make full use of semi-structured data and generate high quality summaries.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Good: The shared resources are likely to be very useful to other AI researchers.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Good: key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Fair: The paper addresses some but not all of the applicable ethical considerations.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
The paper is generally well written and the proposed method is easy to follow and understand.
(1) The paper proposed to use semi-structured data as the input of synthetic training data. It can express the opinion explicitly and implicitly.
(2) The proposed synthetic training data seems to work better than other baselines.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
The main contribution of this paper is the method of constructing a semi-structured dataset, but it is slightly lacking in novelty and adequacy of experimental evaluation.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
(1) Regarding opinion summarization, I would like to know how the authors handle the problem of conflicting views in the reviews. For example, some reviews in table1 say the food is not good, and a few reviews say the food is not bad. At the extreme, it is not reasonable to randomly sample reviews with views contrary to those of the majority of reviews as summary y.
(2) The authors divide the input data sentences into OAs and ISs based on whether opinion-aspect pairs can be explicitly extracted from the sentence. However, a sentence from which opinion-aspect pairs can be explicitly extracted may also contain implicit information. Therefore, some information may be lost.
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
The test samples of the two datasets used are relatively small, and the experimental results may be accidental. In order to be more convincing, it is recommended to add other datasets, such as “Rotten Tomatoes”.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Borderline reject: Technically solid paper where reasons to reject, e.g., lack of novelty, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

