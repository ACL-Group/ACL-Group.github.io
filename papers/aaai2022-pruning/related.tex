\section{Related Work}
Since the emergence of large pretrained language models, 
much work has focused on understanding internal contextual representations 
produced by such models. Most prior work~\citep{shi-etal-2016-string,belinkov-etal-2017-neural} pays special attention to either using extraneous 
probing tasks to examine whether certain linguistic properties 
can be identified from those representations or ablating the models 
to observe how behavior changes. More recently, some studies~\citep{DBLP:journals/corr/abs-1901-05287,DBLP:journals/corr/abs-1905-06316} have shown the existence 
of linguistic knowledge~(e.g., syntax) in various but 
generally lower layers of pretrained transformers.

It is worthy of shedding more light on how PLMs 
memorize abstract knowledge rather than trivial statistical co-occurrence 
patterns.  We extend previous probe~\citep{Petroni2020} on relational 
knowledge. Specifically, we are concerned with commonsense knowledge 
that is grounded on ConceptNet relations. Our work differs in that 
we focus on not only probing but also bringing potentially more 
implicit commonsense knowledge to the surface and unleashing more 
potential in knowledge-intensive applications.

Another relevant line of researches is network 
pruning~\citep{liu2018rethinking,Lin2020Dynamic} and lottery ticket 
hypothesis~\citep{conf/iclr/FrankleC19,Prasanna2020,Chen2020}. 
The former aims at reducing the size of model parameters without 
compromising accuracy and the latter reveals subnetworks whose 
initializations made them capable of being trained effectively comparable 
to the original model. In contrast, we seek to uncover subnetworks in 
over-parametrized PLMs that specializes on commonsense knowledge 
tailored for downstream tasks rather than focusing on good global
initialization, and achieve good results. 
