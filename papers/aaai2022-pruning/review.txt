Reviewer #1
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The authors propose two pruning-based methods, stochastic and deterministic, to filter weights in PLMs for relational commonsense knowledge. The goal of the paper is to answer whether we can identify the part of a general-purpose PLM that contains relational commonsense knowledge. The authors first illustrate that their pruning method can outperform the traditional fineturning method for LMs. Then, they evaluate their method based on CKBC under more challenging settings, such as zero-shot. The results indicate that their pruning-based method can outperform finetuning-based method for LM, under different scenarios, including challenging setup like zero-shot. As a result, the paper concludes that PLM possesses vivid relational commonsense knowledge and suggests an efficient way to leverage it.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Excellent: I am confident that the paper is technically sound, and I have carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Excellent: The paper is likely to have high impact across more than one subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: The paper is well-organized and clearly written.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Excellent: key resources (e.g., proofs, code, data) are available and key details (e.g., proof sketches, experimental setup) are comprehensively described for competent researchers to confidently and easily reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. The paper is well-written and easy to follow.
2. The evaluation results show consistent and significant improvement over other baselines on different settings. The superiority of the proposed method is convincing.
3. The paper shows that PLM contains vivid relational commonsense knowledge and suggests an effective way to leverage it, which is inspiring and worth sharing with the community.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1. I would expect more qualitative analyses beyond embedding visualization, such as human evaluation, case studies and error analysis.
2. A couple of questions require clarifications. See my questions below.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
1. For pruning layer selection, is there a reason for not examining lower layers (l_b < 6)?
2. In Table 2, why RoBERTa with fine-tuning have a performance drop, which doesn’t make sense?
3. From Figure 2 (right), l_b = 6 seems to have way stronger performance than l_b=7. Why is “l_b=6” not applied to the experiments in Table 2?
4. In Table 3, it is surprising that unsupervisedly pruned PLM can outperform supervised methods. Have you tried to combine the two ways, i.e., initialized with the pruned PLM and run supervised finetuning on top?
5. In Table 3, It is also surprising that DistilBERT is the best candidate among other generally stronger PLMs for pruning on KBC, while this is not the case for QA benchmarks in Table 5. Is there any justification for this? It will be good to have a discussion on what types of PLMs are suitable for pruning-based methods.
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
1. In Table 2, it’s interesting to see the number of parameters saved by pruning. A potential direction is to optimize this saved number of parameters without impairing the performance too much.
2. In Section 3.2, “link prediction” might not be a good terminology for your setting, because it sounds like the problem is “given a triplet, predict 1/0”, while the KBC setting is “given (subj, rel), predict obj”.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Strong Accept: Technically strong paper with, with novel ideas, excellent impact on at least one area of AI or high to excellent impact on multiple areas of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.
Reviewer #2
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper carry out an in-depth and comprehensive study regarding disentangling pretrained language models (PLMs) into relational commonsense models, and found that a pruned subnetworks could be more generalizable than original unpruned PLMs.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Poor: The main ideas of the paper are not novel or represent incremental advances.
3. {Soundness} Is the paper technically sound?
Fair: The paper has minor, easily fixable, technical flaws that do not impact the validity of the main results.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Poor: The shared resources are unlikely to be useful to other AI researchers.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Good: The paper adequately addresses most, but not all, of the applicable ethical considerations.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
- Carry out an in-depth and comprehensive study regarding disentangling pretrained language models (PLMs) into relational commonsense models
- Experimental results show different pruning mechanisms of PLMs could be more generalizable that unpruned PLMs in terms of relational commonsense reasoning
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
The paper presents interesting findings regarding PLMs and relational commonsense knowledge, however, the novelty is not so enough
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
N/A
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
N/A
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Reject: For instance, a paper with technical flaws, weak evaluation, inadequate reproducibility, incompletely addressed ethical considerations.
Reviewer #3
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This work studies the relational commonsense knowledge encoded in pretrained language models (PLMs) via a network pruning framework. In particular, the question addressed is can PLMs be disentangled into dedicated relation-specific knowledge models. The results suggest that it is possible to find latent sparse subnetworks capable of representing commonsense relations. The experiments are conducted using four PLMs and multiple commonsense question answering datasets in single-relation and multi-relation settings as well as zero-shot learning scenario.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR moderate impact across more than one subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: The paper is well-organized and clearly written.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be moderately useful to other AI researchers.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
Very well written paper.
Comprehensive and convincing set of experiments.
Multiple relevant baselines considered in the evaluation.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
It would be helpful to see some qualitative examples of where the pruned networks outperform the original networks, are there any particular set of commonsense relations that are more consistently modeled than others (sections 3.2 and 3.3)?

12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
(It is mentioned that the code and pruned networks have been made open-sourced at anonymous.4open.science but the exact url doesn't seem to have been mentioned).
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
Please see above.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Accept: Technically solid paper, with high impact on at least one sub-area of AI or moderate to high impact on more than one area of AI, with good to excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.
Reviewer #4
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper proposes to prune BERT-like models for relational commonsense datasets.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas.
3. {Soundness} Is the paper technically sound?
Fair: The paper has minor, easily fixable, technical flaws that do not impact the validity of the main results.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
- Experiments are conducted for both single- and multi-relation settings.

- The paper is easy to follow.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
- The claim of "disentangling" is misleading. The method is more like a way of task-specific compression.

- It's non-surprising that the general LM can be compressed or fine-tuned for a specific task.

- The comparisons between "original" and "pruned" in Figure 6 and Table 6 are not very meaningful, if we regard pruning as one way of fine-tuning [1].

[1] Masking as an Efficient Alternative to Finetuning for Pretrained Language Models. https://aclanthology.org/2020.emnlp-main.174.pdf
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
- How about the comparisons between pruning(-fine-tuning) and other parameter-efficient-fine-tuning methods (such as adapter)?

- What is the gap between dev set and test set in Table 3?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
- The comparisons between "original" and "pruned" in Figure 6 and Table 6 are not very meaningful, if we regard pruning as one way of fine-tuning [1].


- The improvements in Table 5 are marginal.

- The claim of "disentangling" is misleading. The method is more like a way of task-specific compression or another way of fine-tuning fine-tuning [1].


[1] Masking as an Efficient Alternative to Finetuning for Pretrained Language Models. https://aclanthology.org/2020.emnlp-main.174.pdf
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper. Ideally, we should have: - No more than 25% of the submitted papers in (Accept + Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 20% of the submitted papers in (Strong Accept + Very Strong Accept + Award Quality) categories; - No more than 10% of the submitted papers in (Very Strong Accept + Award Quality) categories - No more than 1% of the submitted papers in the Award Quality category
Borderline reject: Technically solid paper where reasons to reject, e.g., lack of novelty, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.