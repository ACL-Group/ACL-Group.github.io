\section{Conclusion}

This paper defines a new kind of curriculum learning strategy for NLG tasks called in-sample curriculum learning (ICL) by manipulating the difficulty of training within a training sample instead of ranking among samples. We propose the ICL algorithm with the sequence completion curriculum which boosts the performance of strong baselines on a wide range of tasks, showing the effectiveness and strong generalization ability of our approach. More training strategies under ICL digging the inherent difficulties of generating a language sequence are expected in the future.


%Previous state-of-the-art models for different NLG tasks shows further significant improvements when armed with the propotional ICL, including dialogue summarization, style transfer, reading comprehension and question generation. Comprehensive ablations and analysis show that ICL not only generalizes well to different tasks, but also alleviates the exposure bias to some extent.

