We profoundly appreciate the hard work and the precious suggestions and comments in the reviews. Responses to the "questions for us" section in the reviews are as follows.

R#6
1. The cutting point c is controlled by manipulating p during training, where p is updated if there's no improvements on the validation set (Algorithm 1). p_prev and p_new represent p before and after the update operation.

2. We tried to consider different backbone models (BART, UniLM, GPT-2) to show that our approach is compatible to different pre-trained language models.

3. Most hyperparams are the same as the vanilla model(Sec 3.1) except the newly introduced ones which are selected empirically as discussed in Sec4.2.

4. We will reorganize the contents according to your suggestion.

5. We distinguish the baseline and our methods in two aspects. One is the curriculum criterion represented by SG or SC. The other is the training algorithm represented by TCL or ICL. TCL-SG is the original approach by Liang et al.(2021). ICL-SC is our proposed method in Sec2. TCL-SC and ICL-SG are two ablations for comparisons. The differences between them are at the end of Sec 3.1.

7. In style transfer, TCL-SG hurts both metrics mainly due to that doing sub-sequence generation hurts the integrity of sentences. We conclude that such in-sample curriculum shows more benefits in tasks with single sentence outputs. For scenarios with longer outputs, inter-sentence relations are expected to be considered as future work.

8. The results of ICL-SC are in Table 2, we will add it in Table 8 as well.

9. "Better generalization" means that in-sample curriculum has better generalization ability compared to sample-wise curriculum as mentioned in Sec 1. In-sample curriculum doesn't need to produce a numeric score for each training sample using domain knowledge which varies from task to task, and can be directly applied to different tasks. We also tried with some sample-wise curriculum in Sec 4.3, while our ICL-SG in Table 2 outperforms them.

R#7
1. The losses are gathered from different tokens, resulting in different fine-tuned models. In Figure 1, taking the fist row as an example, 1(b) calculate the loss with only the first two tokens in the output, while 1(c) calculate the loss with the last two tokens given the first four as known prefix. The information of each sample for 1(c) is always complete, and the loss is supposed to be more accurate for updating the model. 

2. The results are averaged over three runs with differen random seed. We only do significant test by comparing our ICL-SC with two baselines(w/o CL and TCL-SG). We apologize that the underline of 9.18 vs 8.86 is a mistake. 9.18 is not siginifantly better than 8.86.  However, for 17.43 vs 16.38, T-test is done between their three runs [17.55, 17.59, 17.16] vs [16.08, 16.59, 16.47], and p-value=0.043, which means the improvement is significant.

3. All of the models are trained with the same max training epochs with the same early-stop. The details of w/o CL are in Sec3.1. The curriculum learning approaches are added on top of these baselines without changing the vanilla hyperprameters. A comparisons of the training time is in Appendix C due to space limitation. 

R#10

There are mainly two reasons. First, in-sample curriculum approaches are orthogonal to sample-wise CL. It can be applied with sample-wise CL with further improvements as shown in Sec 4.3. Second, designing appropriate sample-wise curriculum is not easy, which can be reflected by the results in Sec 4.3 that didn't outpreform our ICL-SC in Table 2, and is different among tasks. So, we mainly compared our approach with TCL.

