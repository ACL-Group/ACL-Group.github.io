Reviewer #6
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposed a new curriculum learning strategy for NLG tasks called in-sample curriculum learning (ICL). The idea is to manipulate the difficulty of training within a training sample instead of ranking among samples. To this end, the author perform sequence completion given varying (decreasing) length input prefix. The intuition is that completing a longer prefix would be an easier task (though this is debatable!). The propose method is tested on multiple NLG tasks showing improvements over previous token-base (TCL) baselines. The author additionally included analysis and ablations which show more insight into the working of the method.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strength:
- Proposing a new CL strategy
- improved performance over several NLG tasks
Weakness:
- the results look close and needs significant testing to make sure if they are truly significant.
- details of the baselines are missing
- overclaiming better generalization while there is no experiment to support this.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
1- what’s p_prev. The part on explaining how to dynamically change c during training is not clear.
2- Why not using BART for the QG task as well? There are prev. works who used BART for QG (Esin Durmus et al. ACL 2020) . Not that UniLM is bad, but BART is more recent and popular and the fact that you are using it for almost all other tasks makes is amore natural choice.
3- How are the hyperpara selected for different experiments/tasks?
4- While the author spend quite sometime explaining different well-known tasks, they didn’t do a good job at explaining the baselines which seems to be important for evaluating the efficiency of their proposed method. I’d suggets the author to rewrite this section and include more details. While they can move some content from the task definitions to appendix.
5- I realized the ICL is different from the TCL in that it is proposing to do sequence completion rather than sub-sequence generation. Then how the TCL-SC baseline different from the propose ICL? these are some of the important details missing from the paper.
6 - The author claimed significant improvement in several places but hasn’t done any significant testing.
7- Any explanations on why the TCL hurt the fluency and semantic sim in style trasfer? It’s also seen that ICL has the least improvement over the vanilla baseline on this task. It’s interesting to include a discussion over which tasks benfits the most from ICL.
8- In Table 7, it’s worth adding the original ICL-SC to understand how does it compared with the traditional CL methods.
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have modest impact within a subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be of some use to other AI researchers.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Borderline accept: Technically solid paper where reasons to accept, e.g., good novelty, outweigh reasons to reject, e.g., fair quality. Please use sparingly.
13. (CONFIDENCE) How confident are you in your evaluation?
Somewhat confident, but there's a chance I missed some aspects. I did not carefully check some of the details, e.g., novelty, proof of a theorem, experimental design, or statistical validity of conclusions.
14. (EXPERTISE) How well does this paper align with your expertise?
Mostly Knowledgeable: This paper has little overlap with my current work. My past work was focused on related topics and I am knowledgeable or somewhat knowledgeable about most of the topics covered by the paper.
Reviewer #7
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This work proposes a denoising objective for text generation. Unlike existing works, the method gradually decreases the context available to the model. This mimics a setting where the model is solving increasingly difficult generation problems. The idea is intuitive and well-motivated. Experiments on five language generation tasks show marginal improvements over the baselines.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strengths: The idea is well-motivated and is aligned (in spirit) with the recent progress in developing new denoising objectives for training language and vision models.

Weaknesses:

There are important issues with conceptual differences from the related work, the efficacy of the approach, and evaluation.

1. For an autoregressive generation, why does it matter if the outputs are truncated at the right? The generation is left-to-right, so both Figure 1(b) and 1(c) are identical for a generation model.


2. Comparisons in Table 2 include 52.69 vs. 53.07, 33.44 vs. 33.99. Are these differences not just caused by random noise? The Table caption mentions that statistical tests were done. Can the authors please share the details of one of the tests? For example, Table 1(a), B3 8.86 vs. 9.18 – 9.18 is underlined so that it will be useful to the details of the t-test.


3. Can the authors share some more details about the w/o CL baseline? Critically, were all the models trained for the *exact same number of training steps*? If not, the differences could just be due to longer training time. This is a typical issue with many proposed denoising objectives variations, so it'd be great if the authors could mention training time for each row.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
Please see weaknesses. I look forward to reading the authors' response and will adjust my evaluation for all points accordingly.
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Poor: The paper has major technical flaws. For example, the proof of the main theorem is incorrect or the experimental evaluation is flawed and fails to adequately support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have modest impact within a subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable and/or some key details (e.g., proof sketches, experimental setup) are unavailable which make it difficult to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Poor: The shared resources are unlikely to be useful to other AI researchers.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Borderline reject: Technically solid paper where reasons to reject, e.g., poor novelty, outweigh reasons to accept, e.g. good quality. Please use sparingly.
13. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Very Knowledgeable: This paper significantly overlaps with my current work and I am very knowledgeable about most of the topics covered by the paper.
Reviewer #10
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper proposes in-sample curriculum learning. A model in training is asked to generate the last few words in the beginning of the training. At the end of training, the model is asked to predict the whole sequence. The authors test the proposed idea on reading comprehension, dialogue summarization, style transfer, question generation, and news summarization. The proposed idea generally outperforms the baselines.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strength
1. The authors validate the idea on 5 NLG tasks
2. The proposed idea outperforms the baselines
3. The authors thoroughly test the proposed idea with ablation study

Weakness
1. The core weakness of the paper is missing baselines. There are only two baselines in this study: a model without curriculum learning (CL), and TCL. For example, sample-wise CL is missing for comparison.
2. The improvement is marginal. For example, on summarization tasks, the model outperforms the baselines with a small margin. The authors argue the improvement is significant, but I personally think the improvement is marginal.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
1. Is there any special reason why only TCL is compared?
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have modest impact within a subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Good: The shared resources are likely to be very useful to other AI researchers.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Good: The paper adequately addresses most, but not all, of the applicable ethical considerations.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Borderline reject: Technically solid paper where reasons to reject, e.g., poor novelty, outweigh reasons to accept, e.g. good quality. Please use sparingly.
13. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.