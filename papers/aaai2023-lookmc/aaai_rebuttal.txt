Reviewer #1
Questions

Q1: More empirical analysis on augmented data...

A1: Thanks for your suggestion. We have done some empirical analysis of augmented data 
in Section 3.1 and show that 98% and 97% are the accurate datapoints for crossover and mutation 
with human annotation as you mentioned.  

Reviewer #2
Questions

Weaknesses:

W1. It is better to use a bar chart instead...
A1: Thanks for your suggestion. We will change Figure 5 with a bar chart in the revised version to make it more 
clear. 

W2: Indeed, the proposed method (C+M) still performs...
A2: Mostly, the proposed method (C+M) takes several points drop compared with the vanilla model in Figure 5. 
It indicates that crossover and mutation can reduce short circuits in models. 
The C+M performance which is better than majority voting (e.g. 60% vs 50% for BERT on ROC) shows 
that we can not eliminate all short circuits and we will try other methods in our future work.

W3: I would suggest adopting more carefully... 
A3: As our title illustrates, our method is focused on reducing short circuits for models. 
Mostly, the short circuits are caused by data bias. If the data is balanced enough, the model can be more 
robust. We try our best to make the training data more balance (with fewer bias features) to enhance model 
performance. Thus carefully constructed datasets that cost a lot and 
contain little biased data will not be considered in this paper. 

Q1. How should we interpret the improvements...
A1: Despite the incorrect choices from Crossover and Mutation might be too easy for humans, 
it can be hard for models which only pay attention to biased information. 
For example, if a model believes that the word "apple" always appears in right choices, 
it can choose the wrong choice when "apple" appears in the wrong choice with an unrelated premise. 
Thus we use these two operators to encourage models not to pay attention to biased information only. 

Q2. Is the final hidden vector from the CLS token...
A2: Thanks for your suggestion. We have done this experiment by calculating the 
ratio of the weight of CLS to the premise and the choice. 
The higher the ratio is, the better the model considers the information 
between the premise and the choice at the same time.
We find that the result is consistent with our ending-only 
test in Figure 5. We will give the detailed result in our revised version.  

Reviewer #6

Weaknesses:
W1: Results in the choice-only setting are quite mixed...
A1: Thanks for your suggestion. We will use chart bars to show our results which have been 
included in our Appendix file.

W2. The paper is missing comparison with some related work...
A2: While there exist promising other model-oriented debiasing techniques, we only focus on data augmentation 
methods in this paper. It is known that model instance is combined with training data and model structure. 
Mostly the fragility of models is caused by training data bias. Thus, we try on different data augmentation 
methods to reduce the bias in the data set and enhance the performance of model instances. 

W3: The motivation behind the mutation operator is slightly contestable...
A3: Please refer to A1 (Q1) of Reviewer 2.

W4: Since this topic has been extensively studied in NLI and RC... 
A4: Thanks for your suggestion, we will describe the terminology more carefully. 
In fact, "annotation artifacts" belong to datasets, and "short circuits" 
belong to model instances. Mostly, "annotation artifacts" are the cause of model short-circuiting. 

Q1. Are all experiments carried out with base or large...
A1: Though models with large versions are more robust, 
many researchers with limited resources prefer to use small models. 

Q2: Were the ROC dataset experiments carried out with v1?
A2: It is carried with v1. v1.5 for ROC dataset is not maintained now.

Q3: Would it be possible to include current SOTA ...
A3: Thanks for your suggestion. We will include them in our revised version. 

Reviewer #7


Q1: Firstly, there is not a lot of detail about...
A1: We have described the details of augmented datasets in Section 3.1. 
The size of training datasets is illustrated in the 
last paragraph of Section 3.1.

Q2: Secondly, the results should be reported...
A2: We trained models with 3 different seeds and calculate their average score as the result of each test. 
We have illustrated it in the first paragraph of Section 3.2.

Q3: Thirdly, the choice of baselines...
A3: 
While there exist promising data augmentation methods (Qu et al. 2020; Chen et al. 2021) 
that are based on dynamic perturbation of hidden states, 
back-translation is by far the most effective data 
augmentation method that operates on the input level (Kumar, Choudhary, and Cho 2020)

Q4: Finally, while section 3.4 provides a nice illustration...
A4: Thanks for your suggestion, we have done some quantified analysis and found that 
the results are consistent with our ending-only test. We will give our detailed results in the
revised version.

