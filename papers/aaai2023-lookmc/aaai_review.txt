View Reviews
Paper ID9453
Paper TitleReducing Short Circuits in Multiple-Choice Natural Language Reasoning Models with Data Augmentation
Track NameMain Track
Reviewer #1
Questions

1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper proposes two data augmentation methods -- crossover and mutation -- that reduces a phenomenon that model processes only the choices without the premise, and lessen the statistical bias in multiple-choice reasoning tasks.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strengths
- an interesting idea that regulates the augmentation method with certain logical rules to improve model performance.
- explained the uniqueness of the augmentation methods well.
- performance seems strong.

Weaknesses
- slightly hard to read or follow the content though I understand that there are a lot of information that needs to grasp at once.
- more empirical analysis on augmented data can be available beyond the overall model performance -- e.g., whether the two augmentation methods produce more accurate datapoints or noisy datapoints to generalize the model.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
I wonder if the proposed augmentation methods produces more accurate datapoints or even more noisy examples to improve the model performance in the end.
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR modest impact across more than one subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Borderline accept: Technically solid paper where reasons to accept, e.g., good novelty, outweigh reasons to reject, e.g., fair quality. Please use sparingly.
13. (CONFIDENCE) How confident are you in your evaluation?
Somewhat confident, but there's a chance I missed some aspects. I did not carefully check some of the details, e.g., novelty, proof of a theorem, experimental design, or statistical validity of conclusions.
14. (EXPERTISE) How well does this paper align with your expertise?
Very Knowledgeable: This paper significantly overlaps with my current work and I am very knowledgeable about most of the topics covered by the paper.
Reviewer #2
Questions

1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper aims to reduce the behavior of shortcut learning in neural models for multi-choice reasoning tasks. The proposed methods are based on data augmentation, which yields incorrect choices that are either correct in other questions (Crossover) or only differ from the correct (incorrect) choices by the order of a few words (Mutation). Experiments show that Crossover and Mutation together lead to three Transformer-based models which 1) perform comparable or even better on the original testset, 2) are more sensitive to perturbed data, and 3) take into account the task premise more often.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strengths:

1. The studied problem, namely shortcut learning is important and data augmentation is proved to be an effective approach overall.
2. The paper is well-organized and easy to follow.
3. The experiments are comprehensive and provide evidence of the effectiveness of the proposed methods.

Weaknesses:

1. The analysis of the attention weights is not convincing. I suggest using the attention from the CLS token to other tokens to check whether the model jointly considers both the premise and choice. Aggregating the outcome from each case study to show the distribution of such attention over the whole dataset would also be more helpful for drawing the conclusion.
2. Despite the empirical improvement, both methods, i.e., Crossover and Mutation, exhibit some problems respectively. The incorrect choices from Crossover might be too easy for the model as they do not even fit the context. For Mutation, changing the order of a few words may not necessarily yield an absolutely incorrect choice.
3. It is hard to tell apart the performance of different models in Figure 5. It is better to use a bar chart instead. Indeed, the proposed method (C+M) still performs surprisingly well on several data points (with over 60% in accuracy). This exposes a severe annotation bias in the selected datasets. I would suggest adopting more carefully constructed datasets such as HellaSWAG for experiments.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
1. How should we interpret the improvements brought by Crossover and Mutation respectively? It seems both easy negative samples (as in Crossover) and hard negative samples (as in Mutation) are both helpful. What reasoning skills/inductive biases are picked up by the models exactly?
2. Is the final hidden vector from the CLS token? If yes, why should we look at the attention weights from the choice tokens to the premise tokens? We can instead analyze the attention from the CLS token to the whole input sequence.
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR modest impact across more than one subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Reject: For instance, a paper with poor quality, inadequate reproducibility, incompletely addressed ethical considerations.
13. (CONFIDENCE) How confident are you in your evaluation?
Very confident. I have checked all points of the paper carefully. I am certain I did not miss any aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Expert: This paper is within my current core research focus and I am deeply knowledgeable about all of the topics covered by the paper.
Reviewer #6
Questions

1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper tackles the task of training natural language reasoning models to be more robust to dataset biases/artifacts present in the data via data augmentation strategies. Focusing on multiple choice tasks, the main goal of this work is to make models robust to “short circuits”, a phenomenon in which models are able to choose the correct answer even when the context (question or premise) is not provided, due to the presence of features in the answer choices that spuriously correlate with true labels. To achieve robustness, the authors propose two biologically inspired data augmentation operations: (i) crossover, and (ii) mutation. The crossover operation substitutes incorrect answer choices for one MCQ with the correct answer choice from another. Such swapping disrupts features present in the correct answers from spuriously correlating with the true label. The mutation operator swaps two consecutive words either in the correct/incorrect answer choice, mutating right choices into choices that are “less right”. This operation also forces the model to be more cognizant of word order. Additional training data is generated using these two operations and transformer-based baselines such as BERT, RoBERTa and XLNet are trained using a combination of both original and augmented data. The robustness of these trained models is evaluated on 4 reasoning tasks by testing their performance on the original data, as well as in a choice-only setting in which the model is asked to predict the answer given only the choices (and no context/premise). Additionally, linguistically-motivated stress tests are created for each task, which test model robustness to various features such as negation, voice, etc., and model performance is also evaluated on these tests. Experimental results show that training on this augmented data improves model performance on the original test and stress tests, with some mixed results in the choice-only setting.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strengths:
1. The proposed augmentation operations are simple to extend and broadly applicable to other NLP tasks. It is likely that they can be extended to other formats (beyond multiple choice) such as reading comprehension as well.
2. Training on augmented data generated using the operations proposed in this work provides good improvements on stress test evaluation
3. The stress tests constructed for all natural language reasoning tasks included in the paper could be useful resources for the NLU community upon release.

Weaknesses:
1. Results in the choice-only setting are quite mixed. Sometimes, there is no drop in performance even after training on augmented data (e.g., XLNet on COPA), which indicates that this augmentation strategy does not always improve robustness to “short circuits”, as defined in the introduction. Additionally, performance of the augmented models also stays much higher than random, especially on ROC, indicating that this bias is not completely unlearned.
2. The paper is missing comparison with some related work. While there are comparisons with another data augmentation technique (back translation), it would also help to include comparisons with model-oriented debiasing techniques, such as adversarial training (https://aclanthology.org/S19-1028/) and self-debiasing (https://aclanthology.org/2020.emnlp-main.613.pdf), which are also agnostic to specific types/categories of bias. Presently, from this work, it is unclear whether and why data augmentation strategies would be preferable over such model-oriented debiasing for these reasoning tasks.
3. The motivation behind the mutation operator is slightly contestable since it performs very local (consecutive words) perturbations and labels perturbations in the correct answer as “less correct”. However, such perturbations are often naturally present in datasets, especially large crowdsourced ones (due to inputting mistakes), and are not typically considered incorrect. It would help to discuss this point more, and provide further support for why the mutation operator is a reasonable modification.
4. Since this topic has been extensively studied in NLI and RC, it would be helpful to include more related work from those areas, and borrow terminology from them/make connections to those studies. In particular the following additions might be useful:
-Connecting “short circuits” to “annotation artifacts”, detected using “partial-input baselines” (e.g., here: https://arxiv.org/pdf/1905.05778.pdf ). 
-Some early work on annotation artifacts in NLI: https://aclanthology.org/N18-2017/, https://aclanthology.org/S18-2023/
-Annotation artifacts in other tasks: RC (https://aclanthology.org/D18-1546.pdf), Rocstories (https://aclanthology.org/P17-2097/)
-Prior exploration of swapping-based data augmentation for bias mitigation in NLI (https://ojs.aaai.org/index.php/AAAI/article/view/4696)
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
1. Are all experiments carried out with base or large versions of the corresponding pretrained models? If you are using the base versions, it might be helpful to also conduct the same set of experiments with large versions since they are often more robust.
2. Were the ROC dataset experiments carried out with v1 (which contained lots of spurious biases), or the improved v1.5 (https://aclanthology.org/P18-2119.pdf)?
3. Would it be possible to include current SOTA scores for every dataset in table 5?

Minor suggestions for presentation:
1. There are some mistakes among the citations. For example, the Naik et al (2018) paper on stress testing for NLI was published at COLING (), not ICLR. There are question marks present in the citation for XLNet.
2. It would really help to show scores on individual stress tests as bar charts instead of the current line graphs, since it mistakenly implies links/trends between results on different stress tests which is not the case.
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Fair: The paper has minor technical flaws. For example, the proof of a theorem has some fixable errors or the experimental evaluation is weak.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have modest impact within a subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable and/or some key details (e.g., proof sketches, experimental setup) are unavailable which make it difficult to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be of some use to other AI researchers.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Borderline reject: Technically solid paper where reasons to reject, e.g., poor novelty, outweigh reasons to accept, e.g. good quality. Please use sparingly.
13. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.
Reviewer #7
Questions

1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposes a new data augmentation technique for natural language inference and MCQA tasks in an effort to reduce the reliance on spurious correlations. This is done by performing in-example and in-dataset perturbations of training examples. The effectiveness of the technique is demonstrated in an empirical study on "stress-test" datasets that test for various hard cases aimed to fool models that learn to rely on spurious correlations.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
The proposed method is simple yet effective. The evaluation is representative (though more recent models such as electra, t5, deberta could have been considered), seems robust enough and supports the conclusions broadly.

I have some concerns with the paper: Firstly, there is not a lot of detail about how the stress-test data was generated. It is also not immediately clear, how big the final training sets are, and whether the w/o training set is as big as the augmented ones. Secondly, the results should be reported with appropriate error margins, and ideally averaged over multiple trained models initialised from different random seeds, because the test (and also train) set sizes are small. 
Thirdly, the choice of baselines seems a bit thin with back-translation only and I am not particularly convinced by the motivation of the choice.
Finally, while section 3.4 provides a nice illustration, I believe it doesn't hold a lot of value beyond that. However, the observations made in that section can easily be quantified, for example by averaging the premise attention scores of models optimised on vanilla, +M+C and +B version of the training data.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
How big are the training sets?
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have modest impact within a subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Good: The shared resources are likely to be very useful to other AI researchers.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Weak Accept: Technically solid, modest-to-high impact paper, with no major concerns with respect to quality, reproducibility, and if applicable, resources, ethical considerations.
13. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.
