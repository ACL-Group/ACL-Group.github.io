Official Review of Paper27 by Reviewer 4jRq  
Paper Summary:
Summary Of Weaknesses:
Q1: The main weakness I see with the paper is that the proposed techniques (crossover and mutation) seem fairly obvious and tailored to the choices of stress tests and if I'm being honest, at the end of reading the paper, I didn't feel like I learned something substantially new. I really appreciate the authors putting in considerable effort into benchmarking their methods on several datasets, but I don't feel confident that the proposed techniques are substantially novel/generalize to other types.
A1:  Though our proposed techniques (crossover and mutation) seem fairly easy, they are not tailored to the choices of stress tests. 
Crossover and Mutation are totally different with stress test operators. 
Besides, we are the first to augment multiple choice reasoning tasks with these techniques. 
You may think that Mutation(randomly swapping tokens) in one sentence 
is a widely used method for data augmentation in lots of work 
(Artetxe et al., 2018; Lample et al., 2018; Wei and Zou, 2019; Miao et al., 2020). 
These work believes that the semantics of natural language
is sensitive to text order information, while slight order change is still readable for humans. 
Therefore, the mutation 
within a reasonable range can be used as a data augmentation method. 
However we treat the choices with mutation operation as 
a wrong choice which is more strict than previous work. 
Because we design mutaion to encourage the
model to look into the premise due to its two very similar
choices (same set of tokens)
Besides, it can also make the model more
sensitive to find differences in word orders and enhances the
model’s prior grammatical knowledge (described in Sec 2.3). 
We ensure the correctness of augmented data with this strict operation. 


Q2: The above could potentially also be because the related work section does not do a great job on placing this paper in the context of the vast literature on data augmentation in NLP. The authors only mention 2-3 papers for data aug in NLP and say that it is underexplored, but even with a quick google search (I don't work in this area), I found at least 2 survey papers on various data aug techniques in NLP.

A2: In fact, our work is focus on multiple choice reasoning tasks which can not easily be augmented by usual augmented methods which you have mentioned with extra links. 
Thus we show related works which can augment multiple choice reasoning tasks and we compare with a strong baseline (back-translation).

Q3: Finally, while I appreciate the ablations, attention maps and choice-only test analysis, the paper could use some deeper analysis that can inform future work on other ways to augment data. e.g. An error analysis could reveal which concepts are still hard for models. Table 6 in the appendix does contain a breakdown of the stress tests, but right now it is just a dump of numbers without any insights. Adding such insights could improve the value of the paper to the community.
A3: Thanks for your suggestions, we will consider to explain models and find more weaknesses of models in our future work.
Comments, Suggestions And Typos:
Q4: The experiments try a pairwise combo of C+M, but did you also try combining C or M with B, or even having a combination of all 3 (B+C+M). This seems like a gap in the paper that could be resolved fairly easily (even if not done on all datasets).
A4: Thanks for your suggestions, our experiments are designed to show the effectiveness of our data augmentation methods to reduce short circuits. 
If given more space, we will complete the results with all combinations. 

Limitations And Societal Impact:
The authors could add some discussion about negative social impacts - would the choice of data augmentation introduce any biases into the models? etc.


Official Review of Paper27 by Reviewer 8NN4  
ACL ARR 2022 June Paper27 Reviewer 8NN4
09 Jul 2022ACL ARR 2022 June Paper27 Official ReviewReaders: Program Chairs, Paper27 Senior Area Chairs, Paper27 Area Chairs, Paper27 Reviewers Submitted, Paper27 Authors
Paper Summary:
The paper introduces two data augmentation strategies for multiple-choice tasks: crossover, where an example is augmented with the correct choice from another random example, and mutation, where a random consecutive pair of words is swapped within either the correct choice (making it incorrect) or an incorrect choice. Using BERT, XLNet, and RoBERTa models on four datasets: ROC, COPA, ARCT, and RECLOR, the paper demonstrates that crossover and/or mutation augmented training significantly improves performance on “stress test” modifications of test examples, while not hurting performance much on the original test examples (or even helping in some cases). The paper then uses “choice-only tests” (removing the premises from each example) and attention maps in a case study to argue that the crossover and mutation augmentations result in less “short-circuit” behavior in the models, encouraging them to base predictions on the premise and not just the choices.

Summary Of Strengths:
The proposed augmentations are simple and general enough to apply to any multiple-choice natural language reasoning dataset. They show significant gains for the stress test metrics, outperforming the back-translation baseline, and they appear to reduce “short circuit” behavior. The methodology and analysis are well-described, and experiments cover a range of models and datasets.

Summary Of Weaknesses:
Q1: It’s unfortunate that the augmentations don’t conclusively improve original test set performance. The stress tests are helpful but seem a bit artificial. It’d be very nice if gains were demonstrated on more “natural” challenge sets, indicating better generalization “in the wild”. It’s understandable that these may be hard to come by though.
A1: Thanks for your suggestion. We propose data augmented methods for reducing short circuits in models in this paper. And we eager to make model more robust on "wild" environment in our future work. 

Q2: The attention map case study is interesting to see, but I wasn’t sure which layer(s) are represented in the map (is it just the top layer?) and whether that’s sufficient to definitively indicate “short circuit” behavior. Perhaps aggregating attention from all layers, or additionally using token-based salience visualizations would provide a deeper indication (e.g. https://github.com/PAIR-code/lit/wiki/components.md#token-based-salience).
A2. In section 1, we have note that we task attention map between the words in the 
full question from the final encoder 
layer of the model. This method is used as a white-box test for finding short-circuits in models. 
Perhaps it's not the best way to visualize models. However, visualization and interpretation for models are still difficult problems to solve. 
Thanks for your suggestions, and we will consider to try on token-based salience visualizations for a deeper indication. 

Comments, Suggestions And Typos:
Q3: For mutation, I’m curious how important it is to mutate the wrong choice 50% of the time. Is this as helpful as mutating the right choice? Could be interesting to see this ablated too.


Official Review of Paper27 by Reviewer tsrx  
Summary Of Weaknesses:
Q1: This paper tries to make two different contributions. The first is proposing a new stress test for 4 different tasks, and the second is the data augmentation method. This ambitious goal makes it miss important points on both goals, and I wonder if it had not been better to break this down into two different projects and give each the full attention (see below).
A1: We are sorry that we include redandunt grammar testdata in appendix which is not mentioned in this paper. 
For negation add problems, we sampled 100 quesions from 4 datasets, and find the new cases are perfectly right. 
There may be a few cases that we haven't covered.
We will sample more data to confirm the results.

Q2: Moreover, I have concerns regarding the evaluation setup, as many of the datasets experimented with do not have supervised training sets.
A2: In fact, the lack of training data is caused by hard labeling for these reasoning tasks. 
Thus, previous work try to use little data provided as validation data, like ROC. 
Our data augmentation methods can reduce short circuits without extra human labeling work.

Q3: Finally, some of the claims in the paper feel unjustified to me.

See more details below.

Comments, Suggestions And Typos:
Major comments:
a. The authors propose a new stress test, and base the validity of their method on excelling on it. However, they only spend two halves of columns (lines 123-143 and 261-271) describing it. Creating new datasets usually requires significant efforts in design, implementation and validation. Here the authors say that the correctness of the stress test is 100% (#267). A quick glance at the data provided along with the submission indicates that this is not necessarily the case. Consider the following examples from the COPA grammar test for instance (which is in the provided data but not described in the paper):

501,The item was packaged in bubble wrap .,It was fragile .,right

501,The item was packaged in bubble wrap .,was It fragile .,wrong

The wrong example here could be read as a question, and it thus not wrong. I found 10 such cases (out of 500)

Another example is the negation_add case. Here many examples are not wrong. E.g.,

502,I emptied my pockets .,I retrieved a ticket stub .,right

502,I emptied my pockets .,I didn't retrieve a ticket stub .,wrong

Both are equally plausible

512,The man hit his head .,He got a concussion .,right

512,The man hit his head .,He didn't get a concussion .,wrong

Again, both are equally plausible.

To conclude, I think the proposed stress tests are very interesting and potentially valuable, but I am not fully convinced that they have been properly validated and analyzed, which casts some doubts on the results in this paper.

b. The ROC and COPA datasets don't have supervised training sets. Table 2 indicates that the validation set was used as training data. This is not unusual, but typically some of the data is reserved for validation (e.g., https://aclanthology.org/K17-1004.pdf). In this case, given that no validation data is mentioned, I wonder how the authors performed hyperparameter search, model development and such.

c. Some of the claims in the paper are somewhat inaccurate. For instance, the end of the intro mentions up to 10% improvement on the original test sets, which is technically true, but in the majority of cases the improvement was much smaller, and in particular almost never higher than the back-translation improvement, so this claim is somewhat misleading. Moreover, the interpretation of the choice-only test (Section 3.3) seems surprising to me. The plots on Fig. 5 seem very close to each other, and table 7 in the appendix shows a similar trend, where the data augmentation never reduced the choice-only results by more than a few points (here lower is better), and sometimes even improved performance (e.g., COPA with BERT and XLNET). I would recommend toning down these arguments.

More comments
a. The examples in Figure 1 and Section 3.4 might be misleading: the value of attention as explanation has been questioned in the past (https://arxiv.org/abs/1902.10186, https://arxiv.org/abs/1906.03731), thus the statement (#056) "clearly shows that there’s virtually no connection between the first choice and the premise" is somewhat exaggerated.

b. Statements such as "We use back-translation as our baseline because it is popularly used in NLU tasks" (#290) and " back-translation is by far the most effective data augmentation method that operates on the input level" (#294) require references.

c. mBART (#300) requires a reference.

Minor
a. The COPA paper citation is wrong, please use this one https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF

b. There seem to be a problem with the latex style file, as links (e.g., to references) are not working.

Overall Assessment: 2.5 
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Limitations And Societal Impact:
See my limitations above.

Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 3 = Potentially useful: Someone might find the new datasets useful for their work.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
