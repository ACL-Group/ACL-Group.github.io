Official Review of Paper27 by Reviewer 4jRq  
ACL ARR 2022 June Paper27 Reviewer 4jRq
16 Jul 2022ACL ARR 2022 June Paper27 Official ReviewReaders: Program Chairs, Paper27 Senior Area Chairs, Paper27 Area Chairs, Paper27 Reviewers Submitted, Paper27 Authors
Paper Summary:
This paper proposes two ways (crossover and mutation) of producing augmented data to solve the problem of short circuits in NLP models for multiple-choice questions. The authors perform a wide variety of stress tests on their proposed method and several baselines, across four different datasets. I like the extensive experimentation performed (barring some small gaps), but overall I feel the paper is lacking novelty since the proposed methods are fairly pedestrian and have some overlap with prior work (e.g. 1 in list below).

Summary Of Strengths:
Paper is well written and easy to read
Fairly detailed empirical evaluation
Solving an important problem in modern NLP
Summary Of Weaknesses:
The main weakness I see with the paper is that the proposed techniques (crossover and mutation) seem fairly obvious and tailored to the choices of stress tests and if I'm being honest, at the end of reading the paper, I didn't feel like I learned something substantially new. I really appreciate the authors putting in considerable effort into benchmarking their methods on several datasets, but I don't feel confident that the proposed techniques are substantially novel/generalize to other types of stress tests and don't know how to place it in context of all the other data augmentation work.
The above could potentially also be because the related work section does not do a great job on placing this paper in the context of the vast literature on data augmentation in NLP. The authors only mention 2-3 papers for data aug in NLP and say that it is underexplored, but even with a quick google search (I don't work in this area), I found at least 2 survey papers on various data aug techniques in NLP.
Finally, while I appreciate the ablations, attention maps and choice-only test analysis, the paper could use some deeper analysis that can inform future work on other ways to augment data. e.g. An error analysis could reveal which concepts are still hard for models. Table 6 in the appendix does contain a breakdown of the stress tests, but right now it is just a dump of numbers without any insights. Adding such insights could improve the value of the paper to the community.
Related work that I feel should be discussed and compared to (I don't closely follow this area but these at least seem quite related from a quick skim):

https://arxiv.org/abs/2204.08198
https://arxiv.org/abs/2106.07499
https://arxiv.org/pdf/2105.03075.pdf
Comments, Suggestions And Typos:
The experiments try a pairwise combo of C+M, but did you also try combining C or M with B, or even having a combination of all 3 (B+C+M). This seems like a gap in the paper that could be resolved fairly easily (even if not done on all datasets).
Overall Assessment: 2.5 
Confidence: 2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.
Best Paper: No
Limitations And Societal Impact:
The authors could add some discussion about negative social impacts - would the choice of data augmentation introduce any biases into the models? etc.

Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper27 by Reviewer 8NN4  
ACL ARR 2022 June Paper27 Reviewer 8NN4
09 Jul 2022ACL ARR 2022 June Paper27 Official ReviewReaders: Program Chairs, Paper27 Senior Area Chairs, Paper27 Area Chairs, Paper27 Reviewers Submitted, Paper27 Authors
Paper Summary:
The paper introduces two data augmentation strategies for multiple-choice tasks: crossover, where an example is augmented with the correct choice from another random example, and mutation, where a random consecutive pair of words is swapped within either the correct choice (making it incorrect) or an incorrect choice. Using BERT, XLNet, and RoBERTa models on four datasets: ROC, COPA, ARCT, and RECLOR, the paper demonstrates that crossover and/or mutation augmented training significantly improves performance on “stress test” modifications of test examples, while not hurting performance much on the original test examples (or even helping in some cases). The paper then uses “choice-only tests” (removing the premises from each example) and attention maps in a case study to argue that the crossover and mutation augmentations result in less “short-circuit” behavior in the models, encouraging them to base predictions on the premise and not just the choices.

Summary Of Strengths:
The proposed augmentations are simple and general enough to apply to any multiple-choice natural language reasoning dataset. They show significant gains for the stress test metrics, outperforming the back-translation baseline, and they appear to reduce “short circuit” behavior. The methodology and analysis are well-described, and experiments cover a range of models and datasets.

Summary Of Weaknesses:
It’s unfortunate that the augmentations don’t conclusively improve original test set performance. The stress tests are helpful but seem a bit artificial. It’d be very nice if gains were demonstrated on more “natural” challenge sets, indicating better generalization “in the wild”. It’s understandable that these may be hard to come by though.

The attention map case study is interesting to see, but I wasn’t sure which layer(s) are represented in the map (is it just the top layer?) and whether that’s sufficient to definitively indicate “short circuit” behavior. Perhaps aggregating attention from all layers, or additionally using token-based salience visualizations would provide a deeper indication (e.g. https://github.com/PAIR-code/lit/wiki/components.md#token-based-salience).

Comments, Suggestions And Typos:
For mutation, I’m curious how important it is to mutate the wrong choice 50% of the time. Is this as helpful as mutating the right choice? Could be interesting to see this ablated too.

Figure 5 is a bit hard to read, partially because of the high variance between datasets. Perhaps plotting the choice-only accuracy reduction (delta from original test performance) for each method would be clearer.

Minor typos: Line 141 looks like it should refer to Table 1 instead of Table 3. In Figure 4, the last column of charts have no label on the x-axis after “Voice”.

Overall Assessment: 3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.
Confidence: 2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work.
Best Paper: No
Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper27 by Reviewer tsrx  
ACL ARR 2022 June Paper27 Reviewer tsrx
27 Jun 2022ACL ARR 2022 June Paper27 Official ReviewReaders: Program Chairs, Paper27 Senior Area Chairs, Paper27 Area Chairs, Paper27 Reviewers Submitted, Paper27 Authors
Paper Summary:
This paper studies the robustness of NLP models by considering partial inputs. The authors first propose a set of stress tests, similar in spirit to the NLP CheckLists (Ribeiro et al., 2020), and show that they challenge standard models. They then propose a data augmentation method that allows models to substantially improve on these stress tests.

Summary Of Strengths:
Overall this paper proposes a simple idea that works well on the studied tasks: perform simple perturbations on the training instances, and augment it as additional training samples. This method is intuitive, and seems to work well. The paper is also generally well written.

Summary Of Weaknesses:
This paper tries to make two different contributions. The first is proposing a new stress test for 4 different tasks, and the second is the data augmentation method. This ambitious goal makes it miss important points on both goals, and I wonder if it had not been better to break this down into two different projects and give each the full attention (see below).

Moreover, I have concerns regarding the evaluation setup, as many of the datasets experimented with do not have supervised training sets.

Finally, some of the claims in the paper feel unjustified to me.

See more details below.

Comments, Suggestions And Typos:
Major comments:
a. The authors propose a new stress test, and base the validity of their method on excelling on it. However, they only spend two halves of columns (lines 123-143 and 261-271) describing it. Creating new datasets usually requires significant efforts in design, implementation and validation. Here the authors say that the correctness of the stress test is 100% (#267). A quick glance at the data provided along with the submission indicates that this is not necessarily the case. Consider the following examples from the COPA grammar test for instance (which is in the provided data but not described in the paper):

501,The item was packaged in bubble wrap .,It was fragile .,right

501,The item was packaged in bubble wrap .,was It fragile .,wrong

The wrong example here could be read as a question, and it thus not wrong. I found 10 such cases (out of 500)

Another example is the negation_add case. Here many examples are not wrong. E.g.,

502,I emptied my pockets .,I retrieved a ticket stub .,right

502,I emptied my pockets .,I didn't retrieve a ticket stub .,wrong

Both are equally plausible

512,The man hit his head .,He got a concussion .,right

512,The man hit his head .,He didn't get a concussion .,wrong

Again, both are equally plausible.

To conclude, I think the proposed stress tests are very interesting and potentially valuable, but I am not fully convinced that they have been properly validated and analyzed, which casts some doubts on the results in this paper.

b. The ROC and COPA datasets don't have supervised training sets. Table 2 indicates that the validation set was used as training data. This is not unusual, but typically some of the data is reserved for validation (e.g., https://aclanthology.org/K17-1004.pdf). In this case, given that no validation data is mentioned, I wonder how the authors performed hyperparameter search, model development and such.

c. Some of the claims in the paper are somewhat inaccurate. For instance, the end of the intro mentions up to 10% improvement on the original test sets, which is technically true, but in the majority of cases the improvement was much smaller, and in particular almost never higher than the back-translation improvement, so this claim is somewhat misleading. Moreover, the interpretation of the choice-only test (Section 3.3) seems surprising to me. The plots on Fig. 5 seem very close to each other, and table 7 in the appendix shows a similar trend, where the data augmentation never reduced the choice-only results by more than a few points (here lower is better), and sometimes even improved performance (e.g., COPA with BERT and XLNET). I would recommend toning down these arguments.

More comments
a. The examples in Figure 1 and Section 3.4 might be misleading: the value of attention as explanation has been questioned in the past (https://arxiv.org/abs/1902.10186, https://arxiv.org/abs/1906.03731), thus the statement (#056) "clearly shows that there’s virtually no connection between the first choice and the premise" is somewhat exaggerated.

b. Statements such as "We use back-translation as our baseline because it is popularly used in NLU tasks" (#290) and " back-translation is by far the most effective data augmentation method that operates on the input level" (#294) require references.

c. mBART (#300) requires a reference.

Minor
a. The COPA paper citation is wrong, please use this one https://people.ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF

b. There seem to be a problem with the latex style file, as links (e.g., to references) are not working.

Overall Assessment: 2.5 
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Limitations And Societal Impact:
See my limitations above.

Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 3 = Potentially useful: Someone might find the new datasets useful for their work.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
