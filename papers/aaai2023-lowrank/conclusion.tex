\section{Conclusion}
In this work, we first conduct a series of exploratory experiments on common weights 
pruning and low-rank factorization for language model compression. 
We identify a unique low-rank sparsity phenomenon in models produced by first-order pruning, 
which in turn motivates us to propose LPAF, a generic framework for task-specific language model compression. 
Augmented with the proposed sparsity-aware SVD and mixed-rank fine-tuning as two optimizations, our framework exhibits 
substantially better compression-performance trade-off on diverse natural language processing tasks.

