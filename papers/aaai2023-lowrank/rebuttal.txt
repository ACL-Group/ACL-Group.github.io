To Reviewer #1:

Q1: "Why not report results on the entire GLUE datasets?":
R1: The reason we didn't report results on these datasets is that we already report the results of datasets of the same type as RTE and MRPC, i.e., natural language inference and paraphrase identification. Nonetheless, we would be happy to add the results on the remaining datasets in the final version given an additional page.

Q2: "Would LPAF outperform existing methods with 50% reduced parameters?"
R2: Though our original intention is to explore a relatively heavy compression rate(<=25%), LPAF also works under a moderate compression rate such as 50%. For example, on the SQuAD v2.0 dataset, the F1 score of TinyBERT-6L-768D and BERT-of-Theseus are 72.2 and 71.2 respectively, while LPAF-280(~50% parameter and FLOPs reduction) achieves 77.8 F1. With your permission, we are willing to add these results to the final version.

Q3: "While the paper claims that existing pruning-based methods do not ..."
R3: We would like to clarify that we are not claiming that all pruning methods do not reduce latency. The pruning methods discussed in Section 3 are all unstructured pruning methods, i.e., each parameter is pruned individually. For these types of pruning methods, memory and FLOPs reduction are not straightforward to be obtained without specialized hardware. However, we found that the low rankness induced by first-order unstructured pruning opens up the possibility of low-rank matrix decomposition to achieve real memory/FLOPs reduction on regular GPU. We do not report latency because it is hardware-dependent, so we opt for FLOPs, a general hardware-agnostic metric instead.

Q4: "Is this paper the first to make the observation in Section 3 ..."
R4: To the best of our knowledge, we are the first to discover such a phenomenon and exploit it for effective compression under high compression rates.



To Reviewer #6:
Q1: "Why only validate the method on the BERT"
R1: In section 5.4, we also evaluate MiniLMv2 which is another PLM that is already four times smaller compared to BERT. Table 10 shows that LPAF consistently outperforms strong baselines under such extreme compression rate.

Q2: "Why don't you conduct experiments on all the GLUE datasets?"
R2: Please see our R1 to Q1 of Reviewer #1.



To Reviewer #7:
Q1: "It would be good to clarify how your method is different..."
R1: The paper "On Compressing Deep Models by Low Rank and Sparse Decomposition" assumes each weight matrix can be represented as the sum of a low-rank matrix and a sparse matrix and explores complex mathematical algorithms to find such reparametrization. Without special hardware, the sparse matrix still takes up the same memory and FLOPs as a normal matrix. In contrast, models compressed by LPAF only consist of dense low-rank matrices that are much smaller than before. "Trained Rank Pruning for Efficient Deep Neural Networks" use the gradient of the decomposed matrix to update the original matrix, while in LPAF the decomposed matrix is not updated if it is replaced by its original matrix at that optimization step.

Q2: "I think it would be good to focus on other pruning baselines"
R2: Unstructured pruning doesn't reduce the storage/memory/FLOPs and we report the result of one such method(SMvP) in Table 4 for reference. We reported the results of structured pruning in the Appendix due to space constraints.



To Reviewer #8:
Q1: " The joint low-rank and sparse compression method was already published on existing literature [R1]."
R1: LPAF is fundamentally different from the method proposed in the referred paper. In the referred paper, each weight matrix is represented as the sum of a low-rank matrix and a sparse matrix. Without special hardware, the sparse matrix still takes up the same memory and FLOPs as a normal matrix. In contrast, models compressed by LPAF only consist of dense low-rank matrices that are much smaller than before.

Q2: "The sparsity-aware SVD is truncated SVD in Eq. 8. ..."
R2: Truncated SVD provide the optimal solution to Eq. 5, while in LPAF we want to solve the optimization problem in Eq. 6, not Eq. 5. Because Eq.6 has no close-form solution, we provide an approximation via Eq.7, which can be analytically solved by Eq.8.

Q3: "The proposed compression may not benefit from practical speedup"
R3: Though the reduction on FLOPs does not necessarily translate to equal speedup, LPAF-120 is still able to provide a 1.5x speedup as measured on an RTX 2080Ti GPU.
