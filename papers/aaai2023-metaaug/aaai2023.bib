@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{t5,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {CoRR},
  volume    = {abs/1910.10683},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.10683},
  eprinttype = {arXiv},
  eprint    = {1910.10683},
  timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{transformer,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{overpara,
  author    = {Preetum Nakkiran and
               Gal Kaplun and
               Yamini Bansal and
               Tristan Yang and
               Boaz Barak and
               Ilya Sutskever},
  title     = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  journal   = {CoRR},
  volume    = {abs/1912.02292},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.02292},
  eprinttype = {arXiv},
  eprint    = {1912.02292},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-02292.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mag,
  author    = {Song Han and
               Jeff Pool and
               John Tran and
               William J. Dally},
  title     = {Learning both Weights and Connections for Efficient Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02626},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02626},
  eprinttype = {arXiv},
  eprint    = {1506.02626},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/HanPTD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{svd,
    title = "Compressing Pre-trained Language Models by Matrix Decomposition",
    author = "Ben Noach, Matan",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.88",
    pages = "884--889",
    abstract = "Large pre-trained language models reach state-of-the-art results on many different NLP tasks when fine-tuned individually; They also come with a significant memory and computational requirements, calling for methods to reduce model sizes (green AI). We propose a two-stage model-compression method to reduce a model{'}s inference time cost. We first decompose the matrices in the model into smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of reducing the number of parameters while preserving much of the information within the model. We experimented on BERT-base model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4x, and increase inference speed by a factor of 1.45x, while maintaining a minimal loss in metric performance.",
}

@article{movement,
  author    = {Victor Sanh and
               Thomas Wolf and
               Alexander M. Rush},
  title     = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  journal   = {CoRR},
  volume    = {abs/2005.07683},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.07683},
  eprinttype = {arXiv},
  eprint    = {2005.07683},
  timestamp = {Tue, 02 Jun 2020 12:48:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-07683.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cao,
author = {Cao, Shijie and Zhang, Chen and Yao, Zhuliang and Xiao, Wencong and Nie, Lanshun and Zhan, Dechen and Liu, Yunxin and Wu, Ming and Zhang, Lintao},
title = {Efficient and Effective Sparse LSTM on FPGA with Bank-Balanced Sparsity},
year = {2019},
isbn = {9781450361378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289602.3293898},
doi = {10.1145/3289602.3293898},
abstract = {Neural networks based on Long Short-Term Memory (LSTM) are widely deployed in latency-sensitive language and speech applications. To speed up LSTM inference, previous research proposes weight pruning techniques to reduce computational cost. Unfortunately, irregular computation and memory accesses in unrestricted sparse LSTM limit the realizable parallelism, especially when implemented on FPGA. To address this issue, some researchers propose block-based sparsity patterns to increase the regularity of sparse weight matrices, but these approaches suffer from deteriorated prediction accuracy. This work presents Bank-Balanced Sparsity (BBS), a novel sparsity pattern that can maintain model accuracy at a high sparsity level while still enable an efficient FPGA implementation. BBS partitions each weight matrix row into banks for parallel computing, while adopts fine-grained pruning inside each bank to maintain model accuracy. We develop a 3-step software-hardware co-optimization approach to apply BBS in real FPGA hardware. First, we propose a bank-balanced pruning method to induce the BBS pattern on weight matrices. Then we introduce a decoding-free sparse matrix format, Compressed Sparse Banks (CSB), that transparently exposes inter-bank parallelism in BBS to hardware. Finally, we design an FPGA accelerator that takes advantage of BBS to eliminate irregular computation and memory accesses. Implemented on Intel Arria-10 FPGA, the BBS accelerator can achieve 750.9 GOPs on sparse LSTM networks with a batch size of 1. Compared to state-of-the-art FPGA accelerators for LSTM with different compression techniques, the BBS accelerator achieves 2.3 ~ 3.7x improvement on energy efficiency and 7.0 ~ 34.4x reduction on latency with negligible loss of model accuracy.},
booktitle = {Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {63–72},
numpages = {10},
keywords = {deep neural networks, inference, bank-balanced sparsity, lstm, weight pruning, fpga},
location = {Seaside, CA, USA},
series = {FPGA '19}
}

@article{yao,
  author    = {Zhuliang Yao and
               Shijie Cao and
               Wencong Xiao and
               Chen Zhang and
               Lanshun Nie},
  title     = {Balanced Sparsity for Efficient {DNN} Inference on {GPU}},
  journal   = {CoRR},
  volume    = {abs/1811.00206},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.00206},
  eprinttype = {arXiv},
  eprint    = {1811.00206},
  timestamp = {Sun, 21 Mar 2021 17:13:43 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-00206.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{group,
  author    = {Patrick H. Chen and
               Si Si and
               Yang Li and
               Ciprian Chelba and
               Cho{-}Jui Hsieh},
  title     = {GroupReduce: Block-Wise Low-Rank Approximation for Neural Language
               Model Shrinking},
  journal   = {CoRR},
  volume    = {abs/1806.06950},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.06950},
  eprinttype = {arXiv},
  eprint    = {1806.06950},
  timestamp = {Mon, 06 Jan 2020 13:40:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-06950.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{chen2020lottery,
    title={The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
    author={Tianlong Chen and Jonathan Frankle and Shiyu Chang and Sijia Liu and Yang Zhang and Zhangyang Wang and Michael Carbin},
    year={2020},
    eprint={2007.12223},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{cap,
  author    = {Runxin Xu and
               Fuli Luo and
               Chengyu Wang and
               Baobao Chang and
               Jun Huang and
               Songfang Huang and
               Fei Huang},
  title     = {From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language
               Model Compression},
  journal   = {CoRR},
  volume    = {abs/2112.07198},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.07198},
  eprinttype = {arXiv},
  eprint    = {2112.07198},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-07198.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{st,
  author    = {Yoshua Bengio and
               Nicholas L{\'{e}}onard and
               Aaron C. Courville},
  title     = {Estimating or Propagating Gradients Through Stochastic Neurons for
               Conditional Computation},
  journal   = {CoRR},
  volume    = {abs/1308.3432},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.3432},
  eprinttype = {arXiv},
  eprint    = {1308.3432},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BengioLC13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@techreport{bestsvd,
  title={Perturbation theory for the singular value decomposition},
  author={Stewart, Gilbert W},
  year={1998}
}

@inproceedings{weightedsvd,
  title={Weighted low-rank approximations},
  author={Srebro, Nathan and Jaakkola, Tommi},
  booktitle={Proceedings of the 20th international conference on machine learning (ICML-03)},
  pages={720--727},
  year={2003}
}

@inproceedings{hsu2021language,
  title={Language model compression with weighted low-rank factorization},
  author={Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{rdrop,
  author    = {Xiaobo Liang and
               Lijun Wu and
               Juntao Li and
               Yue Wang and
               Qi Meng and
               Tao Qin and
               Wei Chen and
               Min Zhang and
               Tie{-}Yan Liu},
  title     = {R-Drop: Regularized Dropout for Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2106.14448},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.14448},
  eprinttype = {arXiv},
  eprint    = {2106.14448},
  timestamp = {Wed, 20 Apr 2022 16:41:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-14448.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sst2,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}

@article{mnli,
  author    = {Adina Williams and
               Nikita Nangia and
               Samuel R. Bowman},
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through
               Inference},
  journal   = {CoRR},
  volume    = {abs/1704.05426},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.05426},
  eprinttype = {arXiv},
  eprint    = {1704.05426},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WilliamsNB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{qnliandsquad,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  journal   = {CoRR},
  volume    = {abs/1606.05250},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.05250},
  eprinttype = {arXiv},
  eprint    = {1606.05250},
  timestamp = {Mon, 24 Aug 2020 14:01:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RajpurkarZLL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{theseus,
  author    = {Canwen Xu and
               Wangchunshu Zhou and
               Tao Ge and
               Furu Wei and
               Ming Zhou},
  title     = {BERT-of-Theseus: Compressing {BERT} by Progressive Module Replacing},
  journal   = {CoRR},
  volume    = {abs/2002.02925},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.02925},
  eprinttype = {arXiv},
  eprint    = {2002.02925},
  timestamp = {Wed, 19 Feb 2020 17:11:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-02925.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pkd,
  author    = {Siqi Sun and
               Yu Cheng and
               Zhe Gan and
               Jingjing Liu},
  title     = {Patient Knowledge Distillation for {BERT} Model Compression},
  journal   = {CoRR},
  volume    = {abs/1908.09355},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09355},
  eprinttype = {arXiv},
  eprint    = {1908.09355},
  timestamp = {Fri, 04 Sep 2020 16:10:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09355.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{distilbert,
  author    = {Victor Sanh and
               Lysandre Debut and
               Julien Chaumond and
               Thomas Wolf},
  title     = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
               and lighter},
  journal   = {CoRR},
  volume    = {abs/1910.01108},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.01108},
  eprinttype = {arXiv},
  eprint    = {1910.01108},
  timestamp = {Tue, 02 Jun 2020 12:48:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pdbert,
  author    = {Iulia Turc and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {Well-Read Students Learn Better: The Impact of Student Initialization
               on Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/1908.08962},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.08962},
  eprinttype = {arXiv},
  eprint    = {1908.08962},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
},
  author    = {Iulia Turc and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {Well-Read Students Learn Better: The Impact of Student Initialization
               on Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/1908.08962},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.08962},
  eprinttype = {arXiv},
  eprint    = {1908.08962},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-08962.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kd,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  eprinttype = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}BLP:journals/corr/abs-1711-05101,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  eprinttype = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@inproceedings{bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@Article{lstm,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@article{CKD,
  author    = {Geondo Park and
               Eunho Yang},
  title     = {Distilling Linguistic Context for Language Model Compression},
  journal   = {CoRR},
  volume    = {abs/2109.08359},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.08359},
  eprinttype = {arXiv},
  eprint    = {2109.08359},
  timestamp = {Wed, 22 Sep 2021 14:16:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-08359.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tinybert,
    title = "{T}iny{BERT}: Distilling {BERT} for Natural Language Understanding",
    author = "Jiao, Xiaoqi  and
      Yin, Yichun  and
      Shang, Lifeng  and
      Jiang, Xin  and
      Chen, Xiao  and
      Li, Linlin  and
      Wang, Fang  and
      Liu, Qun",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.372",
    doi = "10.18653/v1/2020.findings-emnlp.372",
    pages = "4163--4174",
    abstract = "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large {``}teacher{''} BERT can be effectively transferred to a small {``}student{''} TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8{\%} the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only {\textasciitilde}28{\%} parameters and {\textasciitilde}31{\%} inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base.",
}

@article{rail,
  author    = {Md. Akmal Haidar and
               Nithin Anchuri and
               Mehdi Rezagholizadeh and
               Abbas Ghaddar and
               Philippe Langlais and
               Pascal Poupart},
  title     = {{RAIL-KD:} RAndom Intermediate Layer Mapping for Knowledge Distillation},
  journal   = {CoRR},
  volume    = {abs/2109.10164},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.10164},
  eprinttype = {arXiv},
  eprint    = {2109.10164},
  timestamp = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-10164.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{isp,
  author    = {Pavlo Molchanov and
               Stephen Tyree and
               Tero Karras and
               Timo Aila and
               Jan Kautz},
  title     = {Pruning Convolutional Neural Networks for Resource Efficient Transfer
               Learning},
  journal   = {CoRR},
  volume    = {abs/1611.06440},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.06440},
  eprinttype = {arXiv},
  eprint    = {1611.06440},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MolchanovTKAK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{l0,
  title={Learning sparse neural networks through $ L\_0 $ regularization},
  author={Louizos},
  journal={arXiv preprint arXiv:1712.01312},
  year={2018}
}

@inproceedings{metadistil,
    title = "{BERT} Learns to Teach: Knowledge Distillation with Meta Learning",
    author = "Zhou, Wangchunshu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.485",
    pages = "7037--7049",
    abstract = "We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., \textit{learning to teach}) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models.",
}

@article{minilm,
  title={Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5776--5788},
  year={2020}
}

@article{mobilebert,
  author    = {Zhiqing Sun and
               Hongkun Yu and
               Xiaodan Song and
               Renjie Liu and
               Yiming Yang and
               Denny Zhou},
  title     = {MobileBERT: a Compact Task-Agnostic {BERT} for Resource-Limited Devices},
  journal   = {CoRR},
  volume    = {abs/2004.02984},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.02984},
  eprinttype = {arXiv},
  eprint    = {2004.02984},
  timestamp = {Wed, 08 Apr 2020 17:08:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-02984.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{matekd,
    title = "{MATE}-{KD}: Masked Adversarial {TE}xt, a Companion to Knowledge Distillation",
    author = "Rashid, Ahmad  and
      Lioutas, Vasileios  and
      Rezagholizadeh, Mehdi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.86",
    doi = "10.18653/v1/2021.acl-long.86",
    pages = "1062--1071",
    abstract = "The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP). While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical. We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits. Then using knowledge distillation a student is trained on both the original and the perturbed training samples. We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines. On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large.",
}

@inproceedings{ad,
  title={Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers},
  author={Zhang, Minjia and Naresh, Niranjan Uma and He, Yuxiong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11685--11693},
  year={2022}
}

@article{mixkd,
  title={MixKD: Towards efficient distillation of large-scale language models},
  author={Liang, Kevin J and Hao, Weituo and Shen, Dinghan and Zhou, Yufan and Chen, Weizhu and Chen, Changyou and Carin, Lawrence},
  journal={arXiv preprint arXiv:2011.00593},
  year={2020}
}


@InProceedings{maml,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author =       {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}

@article{glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1804.07461},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.07461},
  eprinttype = {arXiv},
  eprint    = {1804.07461},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}