\section{Introduction}

In recent years, the predominent paradigm for deep learning has been shifted from training-from-scrach to transfer learning. In the field of Natural Language Processing~(NLP), large pre-trained language models~(PLMs), such as BERT~\cite{bert} and RoBERTa~\cite{roberta}, are first trained on massive general-domain corpus and adapted to different downstream tasks with additional data via fine-tuning. This paradigm has achieved remarkable success across various NLP tasks, making PLMs indispensable component in modern intelligent systems. However, large PLMs often contain hundreds of millions of parameters, which brings challenges when deploying real-world applications due to memory and inference latency constraints.

To address such issue, model compression techeniques are gaining increasing attention for facilitating more compute-efficient and resource-friendly deployment of PLMs. One effective approach for compressing PLMs is Knowledge Distillation~(KD), where a large model is first trained as the teacher and a smaller model as the student is trained to mimic the behavior of the teacher. In NLP domain, KD has been extensively applied based on various definition of knowledge that is supposed to be transferred from the teacher to the student, including output logits~\cite{kd}, intermediate hidden states~\cite{distilbert,minilm}, and attention distributions~\cite{tinybert,mobilebert}. Unfortunately, the performance gap between the teacher and the student can still be large even with the help of advanced distillation objectives, especially when the downstream task data is scarce. 

To fully tap the potential of KD, existing works have explored the use of data augmentation~(DA) with the aim of allowing more 
comprehensive teacher-student knowledge transfer. These methods can be divided into two categories depending on the awareness of the learning progress of student model: (1)~\textit{model-independent} augmentation methods enlarge the training set via pre-define static transformations over the input data, such as word-level replacement using masked language model~\cite{bert} or sentence-level linear interpolation as in MixKD~\cite{mixkd}. 
(2) \textit{model-aware} augmentation methods generate additional data via learnable data augmentor trained with objectives that condition on the student model's learning status, such as adversarial perturbation~\cite{matekd,ad}.
Though demonstratring improments over plain KD, existing data augmentation methods are not optimized for distillation and therefore brings limited benefit to student's generalization ability.

In this paper,we propose Learning to Augment~(LTA), a new data augmentation approach which formulates teacher-student knowledge distillation as a meta-learning problem. 
As in previous works on model-aware augmentation, LTA also employ a learnable data augmentor to dynamically generate additional data points besides the original training set. Since our goal is to obtain a student model with good generalization performance, we design the optimization objective of the data augmentor in LTA to be the student's generalization error on a ``quiz set'', which is a separately reserved data split from the original training set. 
Specifically, at each training iteration, the data augmentor first generate a batch of experimental samples which are used to temporarily update the student model via common KD loss. Then we calculate the validation loss on the quiz set as a proxy of student model's expected generalization error and use this loss as the feedback signal to update the data augmentor to refine its generation policy. Finally, the updated data augmentor is used to generate real samples for knowledge distillation. The above process naturally fits into the bi-level optimization based meta-learning framework~\cite{maml}, which permits adaptive adjustment of the data augmentor based on the feedback from the student in a 
differentiable way.

To evaluate the effectiveness of LTA, we conduct extensive experiments on low-resource and full-data natural language understanding tasks, i.e., GLUE~\cite{glue} benchmark. Experimental results show that LTA outperforms previous DA methods for knowledge distillation, especially when the fine-tuning data is limited.

