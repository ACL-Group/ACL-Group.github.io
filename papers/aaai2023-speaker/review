Reviewer #2
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The paper claims the first investigation of the speaker name fairness in close-form dialogue understanding and generation tasks and proposes Insensitivity Loss to reduce such sensitivity which is verified in extensive experiments. The insensitivity loss penalizes the differences between each pair of attentions from different augmented samples.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
1. The experiments show the improvement of model generalization power by augmenting samples with different speaker names and reducing its sensitivity
2. Overall the paper is easy to follow and narration is straightforward to illustrate the key idea of introducing techniques.
3. The experiments are abundant and well-designed to evaluate the unfairness among different model choices.
4. The problem seems to remain unexplored yet important, while the augmentation and loss technique have been adopted in other related works and could be further improved.
5. Some results may require further statistical test to demonstrate its significance improvement in Table 2 and 3. Seems the performance of vanilla augmentation and insensitivity loss are close to each other.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
1. Why is the specific design choice of comparison between cross attention? Wonder how will be the performance like by directly comparing encoder states between different augmented samples with disparate speaker names?
2. What is the Score({d^t_i, o^t_i})mentioned in equation 3,4?
3. What is the empirical value of alpha? Is the model sensitive to the choice of alpha? Wonder whether two losses are at the same scale since the only difference between augmented samples is the replacement of the speaker names and could insensitivity loss be negligibly small?
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR modest impact across more than one subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Excellent: The paper is well-organized and clearly written.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Excellent: key resources (e.g., proofs, code, data) are available and key details (e.g., proof sketches, experimental setup) are comprehensively described such that competent researchers will be able to easily reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Excellent: The shared resources are likely to have a broad impact on one or more sub-areas of AI.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Good: The paper adequately addresses most, but not all, of the applicable ethical considerations.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Weak Accept: Technically solid, modest-to-high impact paper, with no major concerns with respect to quality, reproducibility, and if applicable, resources, ethical considerations.
13. (CONFIDENCE) How confident are you in your evaluation?
Somewhat confident, but there's a chance I missed some aspects. I did not carefully check some of the details, e.g., novelty, proof of a theorem, experimental design, or statistical validity of conclusions.
14. (EXPERTISE) How well does this paper align with your expertise?
Very Knowledgeable: This paper significantly overlaps with my current work and I am very knowledgeable about most of the topics covered by the paper.
Reviewer #7
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
* The paper studies the speaker name fairness in dialogue summarization and question generation with quantitative analysis.
* The paper proposes a loss called Insensitivity Loss and claims that compared with the baseline approaches the proposed loss can better help reduce the sensitivity in dialogue summarization and question generation.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
* The paper is generally well-written and easy to follow, though there are a few presentation issues needed to be addressed (e.g., p2: the definition of p_i^j is missing; p7: grammar issue in "Some work […] relations")
* The my knowledge, the paper presents the first quantitative study towards the speaker name fairness in dialogue summarization and question generation. However, given that the paper only studies two tasks (i.e., summarization and question generation), I think the current version of the paper overclaims its impact, and the contribution claimed by the paper should be specified more clearly throughout the paper. For example, most appearances of the term "dialogue understanding and generation" should be replaced by "dialogue summarization and question generation", including the one in the title. Alternatively, the paper should carry out experiments on more close-form dialogue understanding and generation tasks. It is unclear whether the conclusion drawn from experiments on the two studied tasks can generalize to most close-form dialogue understanding and generation tasks.
* The paper claims significant gains several times but does not offer tests of significance. Tests of significance should be added, or the claims should be removed.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
n/a.
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have modest impact within a subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Weak Accept: Technically solid, modest-to-high impact paper, with no major concerns with respect to quality, reproducibility, and if applicable, resources, ethical considerations.
13. (CONFIDENCE) How confident are you in your evaluation?
Not very confident. I am able to defend my evaluation of some aspects of the paper, but it is quite likely that I missed or did not understand some key details, or can't be sure about the novelty of the work.
14. (EXPERTISE) How well does this paper align with your expertise?
Somewhat Knowledgeable: This paper has little overlap with my current work. I am somewhat knowledgeable about some of the topics covered by the paper.
Reviewer #9
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This work proposes a measure for model’s sensitivity to speaker names, and evaluate both offline and online methods for reducing speaker name sensitivity. They also propose their own method Insensitivity Loss that helps reduce sensitivity by reducing attention distances of the same dialogue with different speaker names. They perform experiments on multiple datasets and show that their approach works well over other baselines for multiple dialogue tasks.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strengths
* Problem setting is important and well-motivated.
* The effects of methods on the downstream performance of two tasks are analyzed.
* The proposed method performs well and results are discussed in detail.
Analysis in section 5.3 provides good insights.

Weaknesses
- Most of the methods analyzed in this work are from prior work, however, comparing them together is beneficial.
- Some details in the paper are hard to figure out. For example, why are online approaches needed? How is the Insensititivy loss calculated and backpropagated across samples (since they are two different data points)
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
How is the Insensititivy loss calculated and backpropagated across samples (since they are two different data points)
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR modest impact across more than one subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation has minor details that could be improved.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Good: The shared resources are likely to be very useful to other AI researchers.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Good: The paper adequately addresses most, but not all, of the applicable ethical considerations.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Accept: Technically solid paper, with high impact on at least one sub-area of AI or modest-to-high impact on more than one area of AI, with good to excellent quality, reproducibility, and if applicable, resources, and no unaddressed ethical considerations. Top 60% of accepted papers.
13. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.
Reviewer #10
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper presents a way to reduce the sensitivity of name bias in conversation models. The main idea is that generate multiple data argumentation by replacing speaker names in the dialogue corpus and set up the difference from the original sample and augmented samples as a loss function. The authors suggest new evaluation metrics for sensitivity. They chose two conversation-related tasks – dialogue summarization and question generation. Experiments show that the suggested method outperforms other baselines in both tasks and reduces the sensitivity among changed names in the test data.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
The main strength of this paper is suggesting an interesting and well-performed methodology for reducing the bias of speaker names. Even if the idea is simple, it outperforms a baseline, and other researchers can apply and extend the idea to their models.

What concerns me most is the lack of human evaluation. Automatic evaluations on natural language generation tasks such as conversation models have limitations. So human evaluation is required to verify whether suggested models outperform baseline models.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
1. Are the augmented samples in Aug and Ins the same? I am curious about the impact of K.
2. What are the 100 males’ and 100 females’ names? And other major groups’ names?
3. What is the meaning of y-axis in Figure 4?
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
5. {Evaluation: Quality} Is the paper technically sound?
Good: The paper appears to be technically sound. The proofs, if applicable, appear to be correct, but I have not carefully checked the details. The experimental evaluation, if applicable, is adequate, and the results convincingly support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR modest impact across more than one subfield of AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Fair: The paper is somewhat clear, but some important details are missing or unclear.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Fair: The paper addresses some applicable ethical considerations but fails to address some important ones.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Borderline accept: Technically solid paper where reasons to accept, e.g., good novelty, outweigh reasons to reject, e.g., fair quality. Please use sparingly.
13. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Very Knowledgeable: This paper significantly overlaps with my current work and I am very knowledgeable about most of the topics covered by the paper.
Reviewer #11
Questions
1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This work studies speaker name fairness in dialogue summarization and question generation. To aid models in addressing speaker name fairness, the authors propose to use data augmentation by replacing speaker names in a variety of ways, as well as proposing a novel loss function, insensitivity loss, to reduce sensitivity with the augmented data.
2. {Strengths and Weaknesses} Please provide a thorough assessment of the strengths and weaknesses of the paper, touching on each of the following dimensions: novelty, quality, clarity, and significance.
Strengths:
- The proposed area of study (speaker name sensitivity) seems to be fairly novel, and of interest to the dialogue systems community.


Weaknesses:
- While the area of study seems to be of interest, I am not convinced that this work presents a meaningful study of speaker name sensitivity. For example, the study utilizes 2 datasets in experimentation, one with "mostly real" names, and the other uses "nicknames online made up of random characters". Presumably, experiments on a dataset that does not use actual names (I am assuming that random characters would mean a string such as "pnvazxjara") would not lead to a meaningful results. Indeed, the authors find that trends are opposite from one dataset to the other.
- The methodology seems to be somewhat arbitrary and confusing. The authors propose speaker name augmentation as an offline approach, but they classify replacing names with either speaker IDs or frequent names as online approaches. It seems their suggestion is that speaker name augmentation does not alter the validation or test sets, but then how have they run the experiments where they "change-all-names" or "change-one-name", shown in tables 3 and 4? The decision to split some augmentations into "online" vs. "offline" is arbitrary, as well as the use of augmentation at test time.
- While I appreciate that this work is trying to address challenges in fairness, the experimental setup did not receive enough ethical considerations. The authors give no mention of what a "male" or "female" name is, yet they include this as part of their experimentation.
3. {Questions for the Authors} Please carefully describe questions that you would like the authors to answer during the author feedback period. Think of the things where a response from the author may change your opinion, clarify a confusion or address a limitation. Please number your questions.
1) You mention that "genders should be consistent in each mapping", but it isn't clear why that is important. For example, in figure 1, exchanging a traditionally male or female name shouldn't change the outcome.
2) What is the intuition behind padding 0's to the end of cross-attention in the "unification functions"? Since you are using a mean squared error between tokens, won't padding lead to very high loss for names which tokenize to different lengths from the original name?
3) The use of "Online" and "Offline" is very confusing. Why are approaches that require dataset pre-processing defined as being "online"? Additionally, it's unclear to me why the 2 "online" approaches are not simply subsets of the augmentation method. It seems that the augmentation method is only performed during training time, but then how have you run the experiments in tables 3 and 4 with the augmentation method? What is the difference between augmentation and the in-distribution testing setting?
4. {Evaluation: Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas or represents incremental advances.
5. {Evaluation: Quality} Is the paper technically sound?
Poor: The paper has major technical flaws. For example, the proof of the main theorem is incorrect or the experimental evaluation is flawed and fails to adequately support the main claims.
6. {Evaluation: Significance} How do you rate the likely impact of the paper on the AI research community?
Poor: The paper is likely to have minimal impact on AI.
7. {Evaluation: Clarity} Is the paper well-organized and clearly written?
Fair: The paper is somewhat clear, but some important details are missing or unclear.
8. (Evaluation: Reproducibility) Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)checklist.)
Good: key resources (e.g., proofs, code, data) are available and sufficient details (e.g., proofs, experimental setup) are described such that an expert should be able to reproduce the main results.
9. {Evaluation: Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be of some use to other AI researchers.
10. {Evaluation: Ethical considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Poor: The paper fails to address obvious ethical considerations.
11. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Reject: For instance, a paper with poor quality, inadequate reproducibility, incompletely addressed ethical considerations.
13. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
14. (EXPERTISE) How well does this paper align with your expertise?
Very Knowledgeable: This paper significantly overlaps with my current work and I am very knowledgeable about most of the topics covered by the paper.