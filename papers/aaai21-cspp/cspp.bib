@inproceedings{kneser1995improved,
  title={Improved backing-off for m-gram language modeling},
  author={Kneser, Reinhard and Ney, Hermann},
  booktitle={1995 International Conference on Acoustics, Speech, and Signal Processing},
  volume={1},
  pages={181--184},
  year={1995},
  organization={IEEE}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@inproceedings{wallach2006topic,
  title={Topic modeling: beyond bag-of-words},
  author={Wallach, Hanna M},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={977--984},
  year={2006},
  organization={ACM}
}

@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@inproceedings{hwang2015aligning,
  title={Aligning sentences from standard wikipedia to simple wikipedia},
  author={Hwang, William and Hajishirzi, Hannaneh and Ostendorf, Mari and Wu, Wei},
  booktitle={Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={211--217},
  year={2015}
}


@article{asghar2016yelp,
  title={Yelp dataset challenge: Review rating prediction},
  author={Asghar, Nabiha},
  journal={arXiv preprint arXiv:1605.05362},
  year={2016}
}


@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{shen2017style,
  title={Style transfer from non-parallel text by cross-alignment},
  author={Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Advances in neural information processing systems},
  pages={6830--6841},
  year={2017}
}

@article{li2018delete,
  title={Delete, retrieve, generate: A simple approach to sentiment and style transfer},
  author={Li, Juncen and Jia, Robin and He, He and Liang, Percy},
  journal={arXiv preprint arXiv:1804.06437},
  year={2018}
}

@article{john2018disentangled,
  title={Disentangled representation learning for text style transfer},
  author={John, Vineet and Mou, Lili and Bahuleyan, Hareesh and Vechtomova, Olga},
  journal={arXiv preprint arXiv:1808.04339},
  year={2018}
}

@article{zhang2018shaped,
  title={Shaped: Shared-private encoder-decoder for text style adaptation},
  author={Zhang, Ye and Ding, Nan and Soricut, Radu},
  journal={arXiv preprint arXiv:1804.04093},
  year={2018}
}

@article{xu2018unpaired,
  title={Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach},
  author={Xu, Jingjing and Sun, Xu and Zeng, Qi and Ren, Xuancheng and Zhang, Xiaodong and Wang, Houfeng and Li, Wenjie},
  journal={arXiv preprint arXiv:1805.05181},
  year={2018}
}

@article{rao2018dear,
  title={Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer},
  author={Rao, Sudha and Tetreault, Joel},
  journal={arXiv preprint arXiv:1803.06535},
  year={2018}
}

@article{prabhumoye2018style,
  title={Style transfer through back-translation},
  author={Prabhumoye, Shrimai and Tsvetkov, Yulia and Salakhutdinov, Ruslan and Black, Alan W},
  journal={arXiv preprint arXiv:1804.09000},
  year={2018}
}

@inproceedings{fu2018style,
  title={Style transfer in text: Exploration and evaluation},
  author={Fu, Zhenxin and Tan, Xiaoye and Peng, Nanyun and Zhao, Dongyan and Yan, Rui},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}



@inproceedings{chen2019align,
  title={Aligning Sentences between Comparable Texts of Different Styles},
  author={Chen, Xiwen and Zhu, Kenny Q. and Zhang, Mengxue},
  booktitle={The 9th Joint International Semantic Technology Conference},
  year={2019}
}


@article{wu2019mask,
  title={" Mask and Infill": Applying Masked Language Model to Sentiment Transfer},
  author={Wu, Xing and Zhang, Tao and Zang, Liangjun and Han, Jizhong and Hu, Songlin},
  journal={arXiv preprint arXiv:1908.08039},
  year={2019}
}

@article{li2019domain,
  title={Domain Adaptive Text Style Transfer},
  author={Li, Dianqi and Zhang, Yizhe and Gan, Zhe and Cheng, Yu and Brockett, Chris and Sun, Ming-Ting and Dolan, Bill},
  journal={arXiv preprint arXiv:1908.09395},
  year={2019}
}


@article{qian2019domain,
  title={Domain Adaptive Dialog Generation via Meta Learning},
  author={Qian, Kun and Yu, Zhou},
  journal={arXiv preprint arXiv:1906.03520},
  year={2019}
}

@article{wu2019hierarchical,
  title={A Hierarchical Reinforced Sequence Operation Method for Unsupervised Text Style Transfer},
  author={Wu, Chen and Ren, Xuancheng and Luo, Fuli and Sun, Xu},
  journal={arXiv preprint arXiv:1906.01833},
  year={2019}
}


@article{luo2019dual,
  title={A Dual Reinforcement Learning Framework for Unsupervised Text Style Transfer},
  author={Luo, Fuli and Li, Peng and Zhou, Jie and Yang, Pengcheng and Chang, Baobao and Sui, Zhifang and Sun, Xu},
  journal={arXiv preprint arXiv:1905.10060},
  year={2019}
}

@inproceedings{lee-etal-2019-neural,
    title = "Neural Text Style Transfer via Denoising and Reranking",
    author = "Lee, Joseph  and
      Xie, Ziang  and
      Wang, Cindy  and
      Drach, Max  and
      Jurafsky, Dan  and
      Ng, Andrew",
    booktitle = "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation",
    month = jun,
    year = "2019",
    pages = "74--81"
}

@misc{liu2019roberta,
	title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
	author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year={2019},
	eprint={1907.11692},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{Sudhakar2020,
	author = {Sudhakar, Akhilesh and Upadhyay, Bhargav and Maheswaran, Arjun},
	journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
	title = {{Transforming delete, retrieve, generate approach for controlled text style transfer}},
	year = {2020}
}

@article{Xu2019,
	abstract = {The variational autoencoder (VAE) can learn the manifold of natural images on certain datasets, as evidenced by meaningful interpolation or extrapolation in the continuous latent space. However, on discrete data such as text, it is unclear if unsupervised learning can discover a similar latent space that allows controllable manipulation. In this work, we find that sequence VAEs trained on text fail to properly decode when the latent codes are manipulated, because the modified codes often land in holes or vacant regions in the aggregated posterior latent space, where the decoding network fails to generalize. Both as a validation of the explanation and as a fix to the problem, we propose to constrain the posterior mean to a learned probability simplex, and perform manipulation within this simplex. Our proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, our method outperforms unsupervised baselines and strong supervised approaches on text style transfer. On automatic evaluation metrics used in text style transfer, even with the decoding network trained from scratch, our method achieves comparable results with state-of-the-art supervised approaches leveraging large-scale pre-trained models for generation. Furthermore, it is capable of performing more flexible fine-grained control over text generation than existing methods.},
	archivePrefix = {arXiv},
	arxivId = {1905.11975},
	author = {Xu, Peng and Cheung, Jackie Chi Kit and Cao, Yanshuai},
	eprint = {1905.11975},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Cheung, Cao - 2019 - On Variational Learning of Controllable Representations for Text without Supervision.pdf:pdf},
	title = {{On Variational Learning of Controllable Representations for Text without Supervision}},
	url = {http://arxiv.org/abs/1905.11975},
	year = {2019}
}

@article{Yi2019,
	author = {Yi, Xiaoyuan and Liu, Zhenghao and Li, Wenhao and Sun, Maosong},
	file = {:E$\backslash$:/phd/mendely{\_}papers/0526.pdf:pdf},
	keywords = {Natural Language Processing: Natural Language Gene,Natural Language Processing: Natural Language Proc},
	pages = {3801--3807},
	title = {{Text Style Transfer via Learning Style Instance Supported Latent Space}},
	year = {2019}
}

@article{Liu2019,
	abstract = {Typical methods for unsupervised text style transfer often rely on two key ingredients: 1) seeking for the disentanglement of the content and the attributes, and 2) troublesome adversarial learning. In this paper, we show that neither of these components is indispensable. We propose a new framework without them and instead consists of three key components: a variational auto-encoder (VAE), some attribute predictors (one for each attribute), and a content predictor. The VAE and the two types of predictors enable us to perform gradient-based optimization in the continuous space, which is mapped from sentences in a discrete space, to find the representation of a target sentence with the desired attributes and preserved content. Moreover, the proposed method can, for the first time, simultaneously manipulate multiple fine-grained attributes, such as sentence length and the presence of specific words, in synergy when performing text style transfer tasks. Extensive experimental studies on three popular text style transfer tasks show that the proposed method significantly outperforms five state-of-the-art methods.},
	annote = {Superiority over previous methods:
	1. avoid the vulnerable/brittle training process of adversarial learning that is used to pose different types of constrints(disentanglement of content and style、attribute comformation) to the model.
	2. similar to the idea of editing entangled latent representation based on the gradient from binary style classifier, but have the capability of controling multiple fine-grained attributes by using gradients obtained from multiple attribute predictors(categorical or real-valued). 
	3. Also introduce the Bag-of-Word based content predictor to explicitly enhance content preservation of transfered sentence which is not considered in the previous similar work.This can further control the presence of specific words in the generated sentence.
	
	4. attribute predictors and content predictor are trained in two stages:they are firstly trained jointly with VAEs using ground-truth attribute labels and BOW feature of each input sentence, then they are trained individually by sampling latent code z from prior distribution and generating decoder output sentence.
	Then use the predicted attribute label as ground-truth label for re-training predictors.},
	archivePrefix = {arXiv},
	arxivId = {1905.12304},
	author = {Liu, Dayiheng and Fu, Jie and Zhang, Yidan and Pal, Chris and Lv, Jiancheng},
	eprint = {1905.12304},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2019 - Revision in Continuous Space Fine-Grained Control of Text Style Transfer.pdf:pdf},
	month = {may},
	title = {{Revision in Continuous Space: Fine-Grained Control of Text Style Transfer}},
	url = {http://arxiv.org/abs/1905.12304},
	year = {2019}
}

@article{Wang2019,
	abstract = {Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data, while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately, e.g., learning different attributes' representations or using multiple attribute-specific decoders. However, it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems, we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically, we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text, then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore, we also show that our model can not only control the degree of transfer freely but also allow to transfer over multiple aspects at the same time.},
	annote = {1. The model proposed in this paper is not a probabilistic model.It consist of one Transformer-based autoencoder and an binary atteribute classifier.Both autoencoder and attribute classifier are trained using non-parallel corpus.
	2. the modified entangled latent representation z{\_}prime is classified under the target attribute category and it should be closet to the original latent representation in terms of content information.In this paper this is implicitly achieved by apply different modification weights as the step size for gradient-based optimization.
	
	
	Draw back:
	1. no explicit control over content preservation.
	2. can only control single binary attribute(e.g. pos/neg sentiment) using single binary attribute classifier.(Can be extended to multiple attributes' case by using multiple attribute predictors)
	3. the revised/edited latent representation z{\_}prime is not guaranteed to be able to generate plausible sentence in terms of semantic content info.Because the author use a deterministic auto-encoder，the latent code act as isolated point in the feature space.(Addressed by VAEs)},
	archivePrefix = {arXiv},
	arxivId = {1905.12926},
	author = {Wang, Ke and Hua, Hang and Wan, Xiaojun},
	eprint = {1905.12926},
	file = {:C$\backslash$:/Users/Admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Hua, Wan - 2019 - Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation.pdf:pdf},
	month = {may},
	title = {{Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation}},
	url = {http://arxiv.org/abs/1905.12926},
	year = {2019}
}

@misc{he2020probabilistic,
    title={A Probabilistic Formulation of Unsupervised Text Style Transfer},
    author={Junxian He and Xinyi Wang and Graham Neubig and Taylor Berg-Kirkpatrick},
    year={2020},
    eprint={2002.03912},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{rao-tetreault-2018-dear,
    title = "Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer",
    author = "Rao, Sudha  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1012",
    doi = "10.18653/v1/N18-1012",
    pages = "129--140",
    abstract = "Style transfer is the task of automatically transforming a piece of text in one particular style into another. A major barrier to progress in this field has been a lack of training and evaluation datasets, as well as benchmarks and automatic metrics. In this work, we create the largest corpus for a particular stylistic transfer (formality) and show that techniques from the machine translation community can serve as strong baselines for future work. We also discuss challenges of using automatic metrics.",
}