\section{Methodology}
\label{sec:method}
%\KZ{replace all barking/barks with vocal or vocalization}
To verify our assumptions that dogs from different human language environments have different vocals and the difference is related to their host's language, we conduct classification-based experiments and analyze the Shapley value to find which features are important in distinguishing their vocals. %\KZ{What do you mean by interpret prominent features? Is it more like to use the Shapley value to measure how important an acoustic feature is? Pls be specific.} 

\subsection{To Answer the First Question: Pairwise Classification}


%Unfortunately, due to the large number of combinations among the scenes, locations and activities, our initial investigation shows that there are not many clips to classify under the same context. Thus classification accuracy with such grouping is not statistically significant. Therefore, 
To better control context and language environment, we adopt a 4-way classification for pairwise comparison. Specifically, we pair the dog vocalizations from the same context but under different language environments in 4 ways: En-En, Ja-Ja, En-Ja, and Ja-En. Two clips are considered to be from the same context if they have the same scene category, the same location, and their activity vectors with a cosine similarity of 0.95 or above. The problem is then defined as the classification of these pairs into any of the above four classes. Our classification models include xgboost, KNN, Logistic Regression, and Random Forest. Multiple acoustic features including spectral features (filterbank~\cite{strang1996wavelets}, PLP~\cite{hermansky1990perceptual}, MFCC~\cite{davis1980comparison}) and handcrafted feature-sets (eGeMAPs, GeMAPs~\cite{eyben2015geneva}, ComParE~\cite{schuller2013interspeech}) are utilized to feed into the model. 

Dogs may also differ by their
age and sex, but these attributes are hard to obtain from the YouTube data even manually.
Given the large size of our dataset, we believe a significantly higher than random classification
accuracy will show that we can distinguish English dogs with Japanese dogs just 
by acoustic features.  

An alternative method is to group all fine-grained  clips by the contexts, and then
do a two-way (English or Japanese environment) classification in each group using only 
acoustic features.  However given the complex combination of  scene, location and activity, few samples from both languages share exactly the same context hence we conducted the more difficult four-class experiment. 

\subsection{To Answer the Second Question: Correlation on Prominent Factors}
%\JY{Correlation Missed!}
To ascertain the influence of the host language on dog vocalizations, we analyze prominent factors 
that distinguish Japanese and English dogs' sounds. Shapley value is commonly adopted 
to explain feature importance for a given machine learning model, which can help 
determine the prominent features influencing dog vocalizations. %Python implements that in SHAP. 


%\JY{not talk GeMAPs here}
%\KZ{Given that you actually tested against a number of different audio feature
%sets in the experiment part 1, why do u pick GeMAPs here so early? Maybe here instead of
%fixing one feature set, just talk generically and only finalize the actual set after
%we have seen the result in Sec 4? Sec 4.1 will tell us which feature set is good,
%and then we we analyze that feature set in more details in 4.2? }
%GeMAPs\cite{eyben2015geneva} is a widely-used acoustic feature set consisting of statistical 
%computation on acoustic features, which are low-level descriptors that exhibit high 
%interpretability.

%For its universality and explainability, it is selected as the input features to compute %Shapley values and determine our prominent influencing acoustic features.

To compare the relationships between a dog vocalization and the host language, 
we also include features extracted from human speech (English and Japanese corpus). 
The speech sources include 8,000 clips from CommonVoice~\cite{ardila2019common}, 
which is an open source multilingual speech dataset contributed by volunteers around 
the world, and host speech from EJShibaVoice. We adopt two different sets of speech data 
because they provide distinctive features. 
Speech from EJShibaVoice has direct relation with vocalizations, 
so that we can conduct Pearson value analysis between them, 
while CommonVoice is purer and more common to help us find universal 
feature of human speech. Similar procedures are conducted on human language 
and the prominent factors are later compared with those inferred 
from dog vocalizations~(\secref{sec:prominentfactor}).
Furthermore, to ascertain the correlation between vocalizations and their host speech in a statistical way, we analyze the Pearson correlation between them. In the meantime, the Pearson correlation between vocalizations and random speech is shown to compare~(\secref{sec:prominentfactor1}).
