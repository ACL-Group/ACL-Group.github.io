\section{Image Annotation Instruction}
\label{sec:annotation_instruction}
% \KZ{Again, use colored fonts very sparingly!}
Figure \ref{fig:annotation} shows the image annotation instruction for annotators of CogBench.
% Annotators we hired are students in a university.
% Our annotators are mostly undergraduate or graduate students with normal cognitive functioning, aged between 18 and 28.

\begin{figure*}
    \begin{tcolorbox}[
      % title = {Image Annotation Instruction},
      % colframe = blue!50!black, 
      colframe = blue!30!white, 
      colback = blue!2!white,
      colbacktitle = blue!10!white,
      colupper = black, collower = yellow!75!red,
      coltitle = black!90!white
      ]
      \small
      You are going to see some pictures. Each picture tells a story and requires different kinds of reasoning to fully understand the story behind it. You will be first asked to identify the entities and reasoning processes in the picture. Then, you will need to describe the story of the picture based on your identified entities and reasoning processes. \\
  
      First, you will be asked to identify the entities in the picture. The annotation format is [A, B, C], where A, B, C are entities. \\
  
      [Entities]: Please list all entities appearing in the picture, including people, animals, objects etc. You are encouraged to list as many entities as possible. Note that these entities need to be in your picture description afterwards. For entities that are difficult to recognize, please do not list them here or describe them. \\
  
      Then, you will be asked to identify different reasoning processes in the picture. The annotation format should follows the structure [A1 + A2 -> B], where A1 and A2 are premises and B is the conclusion. Note that if you write a conclusion, there must be at least one premise. Do not write a conclusion only, like [B]. Please write one conclusion at a time, do not write a reasoning process like [A1->B->C], which should be split into two. Each picture does not necessarily requires all the reasoning. Please write None, if a picture does not involve a specific kind of reasoning or it is not important in the picture. \\
  
      [Special Time Reasoning] Please write your reasoning processes about the special time of the story in the picture, e.g. festivals, seasons etc. The special time is usually relevant to the story of the picture. For instance, if it is daytime in a picture, it is easily recognized and requires no reasoning and there is nothing special, you can write None. However, if there is a lamp on or a clock indicating a specific time, you can write down your reasoning about time. \\
  
      [Location Reasoning] Please write your reasoning processes about the location of the story in the picture, e.g. near the school. \\
  
      [Character Reasoning]: Please write your reasoning processes about the character of the subjects in the picture, e.g. teacher, doctor etc. \\
  
      [Character Relationship Reasoning]: Please write your reasoning processes about relationship between characters in the picture, e.g. friendship.  \\
  
      [Event Reasoning]: Please write your reasoning processes about events happened in the current moment and previous moments in the picture based on clues in the picture. Note that you only need to annotate those high-level events and you can ignore those low-level events. For instance, “the woman is looking at the man” is a low level event and you can ignore its reasoning process. Differently, the reasoning process [A mother is busy cooking. + A boy is fetching cookies behind the mom. + A girl is shushing the boy. -> The boy is stealing cookies.] is a reasoning about high-level event stealing and you should write it down. \\
  
      [Event Relationship Reasoning]: Please write your reasoning processes about relationship between different events in the picture. These events are usually linked through causal and temporal relations. Note that events in this part do not necessarily appears in the [Event Reasoning] part as some events here are low-level events. \\
  
      [Next Moment Event Reasoning]: Please write your reasoning processes about the event that will happen in the next moment. Note that you only need to write down events that have a very high probability of happening, instead of guessing what might happen next. \\
  
      [Mental State Reasoning]: Please write your reasoning processes about mental state of subjects in the picture, e.g. daydreaming, happy etc. You need to reason as best you can about the mental states of all the subjects in the picture, unless they are not showing obvious emotions. \\
  
      Finally, you will be asked to describe the picture in as much detail as you can.\\
  
      [Description]: Please describe all you see in the picture in a paragraph based on the entities and reasoning processes you identified above, ensuring that all of them are included in your description. Each picture has a story behind it and you need to tell that story through your description. \\
    \end{tcolorbox}
    \caption{Image annotation instruction for annotators.}
    \label{fig:annotation}
\end{figure*}

\section{Prompt of CoR-based GPT-assisted Question Generation}
\label{sec:qa_prompt}

Figure \ref{fig:cgqg_prompt} shows an example prompt of CoR-based GPT-assisted Question Generation for GPT-4. 
This prompt is used to generate questions based on [Event Reasoning] CoRs. 
Prompts for other CoR types are similar to this one.

\begin{figure*}[h]
    % \small
    \scriptsize
    \begin{tcolorbox}[
      % title = {CGQG Prompt},
      % colframe = blue!50!black, 
      colframe = blue!30!white, 
      colback = blue!2!white,
      colbacktitle = blue!10!white,
      colupper = black, collower = yellow!75!red,
      coltitle = black!90!white,
      % breakable
      ]
    We have a description of an image and the description tells a detailed story unfolding in the image. In the process of describing an image, it is often necessary to engage in reasoning about events based on the clues within the image, leading to certain conclusions. For example, when we see the wind is blowing outside, and a man is reading a newspaper in the telephone booth, we can infer that he is actually hiding from the wind in the telephone booth. Therefore, in this task, in addition to the image description, the reasoning processes about event within the image description have also been extracted. For each reasoning process, we use A1+A2+...->B to represent it, where A1, A2, ... are clues we observed in the picture and B represents the conclusion about event we inferred. \\

    Thus, given an image description and the reasoning processes about event, our task is: \\
    1) Generate a question based on reasoning processes about event. \\
    2) Generate four options: A, B, C, and D. There is only one correct answer among the four options, which is consistent with the description and reasoning processes provided. The correct answer option should be randomly chosen from A, B, C, and D. For those incorrect options (distractors), you are encouraged to hallucinate some clues that are highly relevant to the question and the description but do not actually consistent with the description. That is, you can distort the facts in the description and reasoning processes using elements related to the question to generate some easily selectable distractors. It would be better if you can generate some distractors that are similar to but different from the correct option. Please avoid situations where the correct option is significantly longer or shorter than the distractors. \\
    --- \\
    For example, if the description is "There are some snow on the ground and it is windy, ... We can see it is cold. Inside a phone booth, a man is smiling while looking at a newspaper. He is sheltering from the cold wind in the phone booth..." and the question is "Why can we tell that the man is seeking shelter for warmth?", you can use "newsstand", which is related to "seeking shelter for warmth" in the question, to distort the fact in description "in a phone booth." Then you can get "the man is in the newsstand." Similarly, you can hallucinate a question related distractor "it is raining and a man is smiling and reading a newspaper in a phone booth," which is similar to the correct option "it is windy and a man is smiling and reading a newspaper in a phone booth," but different from it and inconsistent with the description.  \\
    --- \\
    3) Generate the the letter corresponding to the correct answer, that is A, B, C, or D.\\

    Here are some examples: \\
    ---------- \\
    
    [Description]:  \\
    There are some snow on the ground and it is windy, indicating it is winter. There are two men and two women standing on the roadside. There is a sign that says "NO STANDING BUS STOP", indicating it is near a bus stop. A man is standing on the road side, wrapping his coat tightly around himself, and peering out onto the road. They are probably waiting for a bus here. We can see it is cold. Inside a phone booth, a man is smiling while looking at a newspaper. He is sheltering from the cold wind in the phone booth. He looks happy, because it is warm there. Two women are also wrapping their coats tightly and looking at the man in the phone booth. They are probably friends and standing together. They are unhappy with the man. There are some buildings by the road. \\

    [Event Reasoning]:  \\ 
    It is windy and cold. + A man is standing in a phone booth reading newspaper. -> The man is sheltering from the cold wind in the phone booth. \\

    [Generated Multiple-Choice Questions]:  \\
    What is the man doing in the phone booth? \\
    A. Making a phone call. \\
    B. Reading a book. \\
    C. Avoiding someone he doesn't want to see. \\
    D. Sheltering from the cold wind. \\
    Correct Answer: [D] \\

    Why can we tell that the man is seeking shelter for warmth? \\
    A. It is windy and a man is smiling and reading a newspaper in a newsstand. \\
    B. It is raining and a man is smiling and reading a newspaper in a newsstand. \\
    C. It is windy and a man is smiling and reading a newspaper in a phone booth. \\
    D. It is raining and a man is smiling and reading a newspaper in a phone booth. \\
    Correct Answer: [C] \\
    ---------- \\

    Please: \\
    1). Generate at least one question for each reasoning process. \\
    2). Generate more diverse questions, try to generat questions from different perspectives or angles and don't limit yourself to the question templates provided in the examples. \\
    3). Avoid generating repetitive questions with similar meanings. \\
    \end{tcolorbox}
    \caption{An example prompt of CoR-based GPT-assisted Question Generation for GPT-4 to generate questions based on [Event Reasoning].}
    \label{fig:cgqg_prompt}
  \end{figure*}

\section{Prompt of GPT-based Description Cognition Evaluation}
\label{sec:cogid_eval_prompt}

Figure \ref{fig:eval} and Figure \ref{fig:er_eval} shows prompts of ChatGPT or GPT-4 for cognition evaluation of Description task. 

For CoR types other than [Event Relationship Reasoning], we task GPT-4 with determining whether the conclusion in each CoR is mentioned in the description. 
The prompt is shown in Figure \ref{fig:eval}.
For [Event Relationship Reasoning], we task GPT-4 with determining whether the causal relationship between events (i.e. the whole CoR), as annotated, is present in the description.
The prompt is shown in Figure \ref{fig:er_eval}.

% Prompt in Figure \ref{fig:eval} is used to prompt ChatGPT or GPT-4 to determine whether CoRs of reasoning types other than [Event Relationship Reasoning] are mentioned in the description, and Prompt in Figure \ref{fig:er_eval} is used to prompt ChatGPT or GPT-4 to determine whether CoRs of [Event Relationship Reasoning] are mentioned in the description.

\begin{figure}[h]
  \begin{tcolorbox}
      % [title = {Reasoning Eval},
      [
      colframe = blue!30!white, 
      colback = blue!2!white,
      colbacktitle = blue!10!white,
      colupper = black, collower = yellow!75!red,
      coltitle = black!90!white
      ]
      \noindent
      \small
      Given a <DESCRIPTION> and some <KEY POINT>s, please tell me if the <DESCRIPTION> explicitly presents the exact or similar semantics of each <KEY POINT>. The following points are required: \\

      1) Instead of reasoning about whether each <KEY POINT> is possibly correct based on the <DESCRIPTION>, you only need to determine whether the <DESCRIPTION> mentions the semantics in the <KEY POINT>. \\
      2) Do not overlook the semantics in the <DESCRIPTION> that are semantically equivalent to the <KEY POINT> but expressed in different ways. For instance, if the <DESCRIPTION> mentions "The woman is playing with her son...", we can tell it successfully includes semantics in the <KEY POINT> "The woman is the mother of the boy." \\
      3) If several possible scenarios are listed using 'or' at a <KEY POINT>, you only need to determine whether one of these scenarios is mentioned in the <DESCRIPTION>. \\

      Assign a score of 0 or 1 to each <KEY POINT>, where 0 represents NO and 1 represents YES. \\

      % \textcolor{blue}
      {<DESCRIPTION>:} \\
      \textcolor{blue}{\{Description generated by models.\}} \\

      % \textcolor{green}
      {<KEY POINT>:} \\
      \textcolor{c2}{1. \{Annotated key point 1.\}} \\
      \textcolor{c2}{2. \{Annotated key point 2.\}} \\
      \textcolor{c2}{...} \\
      \textcolor{c2}{N. \{Annotated key point N.\}} \\
      

      Please write your answers in ``[ ]'' with 0 or 1 in the following format (number + square brackets): \\
      
      1. [1]  2. [0] \\
      
      Your answers to the \textcolor{c2}{\{N\}} <KEY POINT>(s) above: \\
      \textcolor{c2}{1. [ ]  2. [ ] ... N. [ ]}  \\
  \end{tcolorbox}
  \caption{Evaluation prompt for GPT-4 of Reseasoning types other than [Event Relationship Reasoning].}
  \label{fig:eval}
\end{figure}

\begin{figure}[h!]
  \begin{tcolorbox}[
      % title = {Reasoning Eval},
      colframe = blue!30!white, 
      colback = blue!2!white,
      colbacktitle = blue!10!white,
      colupper = black, 
      collower = yellow!75!red,
      coltitle = black!90!white]
      \small
      % Given a <DESCRIPTION> and some <EVENT RELATIONSHIP>s, please tell me whether this description clearly depicts the cause-and-effect relationships between events. The format of event relationships follows the structure ``A1 + A2 + ... -> B'', where A1, A2, ...and B are some events. Events A1, A2, ... are the causes of event B, and event B is the result caused by events A1, A2, .... The criteria for judgment lie in whether the description mentions these events and clearly explains the causal relationships between them. Assign a score of 0 or 1 to each ``A1 + A2 + ... -> B'', where 0 represents NO and 1 represents YES.
      Given a <DESCRIPTION> and some <EVENT RELATIONSHIP>s, please tell me whether this <DESCRIPTION> clearly depicts the cause-and-effect relationships between events. \\

      The format of a <EVENT RELATIONSHIP> follows the structure "A1 + A2 + ... + An -> B", where A1, A2, ..., An and B are events. Events A1, A2, ..., An are the causes of event B, and event B is the result caused by events A1, A2, ..., An. The criteria for judgment lie in whether the <DESCRIPTION> mentions these events and clearly depicts the causal relationships between them. \\

      Assign a score of 0 or 1 to each <EVENT RELATIONSHIP>, where 0 represents NO and 1 represents YES. \\

      % \textcolor{blue}
      {<DESCRIPTION>:} \\
      \textcolor{blue}{\{Description generated by models.\}} \\

      % \textcolor{green}
      {<EVENT RELATIONSHIP>:} \\
      \textcolor{c2}{1. \{Annotated event relationship 1.\}} \\
      \textcolor{c2}{2. \{Annotated event relationship 2.\}} \\
      \textcolor{c2}{...} \\
      \textcolor{c2}{N. \{Annotated event relationship N.\}} \\

      Please write your answers in ``[ ]'' with 0 or 1 in the following format (number + square brackets): \\
      
      1. [1]  2. [0] \\
      
      Your answers to the \textcolor{c2}{\{N\}} <EVENT RELATIONSHIP>(s) above: \\
      % 1. [ ]  2. [ ]  3. [ ]  ... \\
      \textcolor{c2}{1. [ ]  2. [ ] ... N. [ ]}  \\

  \end{tcolorbox}
  \caption{Evaluation prompt for GPT-4 of [Event Relationship Reasoning].}
  \label{fig:er_eval}
\end{figure}


\section{Introduction to Selected LVLMs}
\label{sec:lvlms}

\begin{table}[h]
  \centering
  \small
  \setlength{\tabcolsep}{2.5pt}
  \caption{\label{tab:lvlms}
  LVLMs evaluated in this paper.
  }
  \begin{tabular}{lll}
  \hline
  \textbf{Model} & \thead{\textbf{Visual Encoder} } & \thead{\textbf{Language Model}}\\ % \\ \textbf{Accuracy}
  \hline
  InstructBLIP-7B  & EVA-CLIP ViT-g & Vicuna-7B \\
  Qwen-VL-Chat & OpenCLIP ViT-G & Qwen-7B \\
  LLaVA-v1.5-7B  & CLIP ViT-L & Vicuna-7B \\
  LLaVA-v1.5-13B & CLIP ViT-L & Vicuna-13B \\
  mPLUG-Owl-2 & CLIP ViT-L & LLaMA-7B \\
  % Otter  &  &   \\
  % CogVLM & 0 & & & & & & & & \\
  ShareGPT4V-7B & CLIP ViT-L & Vicuna-7B \\
  ShareGPT4V-13B & CLIP ViT-L & Vicuna-13B \\
  % Monkey &  &  \\
  GPT-4V & \multicolumn{1}{c}{--}  & \multicolumn{1}{c}{--} \\
  GPT-4o & \multicolumn{1}{c}{--}  & \multicolumn{1}{c}{--} \\
  \hline
  \end{tabular}

\end{table}

\begin{itemize}
    \item \textbf{InstructBLIP} \cite{dai2023instructblip} builds upon BLIP-2 \cite{li2023blip}. 
    % and restructures 26 publicly available datasets into an instructional tuning format and uses 13 held-in datasets for instruction tuning. 
    It consists of an image encoder, a LLM, and a Query Transformer (Q-Former). 
    During instruction tuning, only the Q-Former is updated. 
    We use ``blip2-vicuna-instruct'' for testing.
    \item \textbf{Qwen-VL-Chat} \cite{bai2023qwenvl} is a instruction-tuned VL chatbot based on Qwen-VL. 
    Its training process consists of two stages of pre-training and a final stage of instruction fine-tuning. 
    As for architecture, it consists of a vision encoder, a LLM, and position-aware vision-language adapter. We test ``Qwen-VL-Chat''.
    \item \textbf{LLaVA v1.5} \citep{liu2023improved} is an upgraded version of LLaVA  \cite{liu2023llava}. 
    LLaVA connects a vision encoder and LLM for general-purpose visual and language understanding. 
    It is instruction-tuned on the language-image instruction-following data generated by language-only GPT-4 \cite{openai2023gpt4}. 
    By using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, LLaVA v1.5 achieves better performance. 
    ``llava-v1.5-7b'' and ``llava-v1.5-13b'' are tested.
    \item \textbf{mPLUG-Owl-2} \cite{ye2023mplug} mainly comprises a fundamental vision encoder, a visual abstractor, and a language decoder. 
    It also adopts a two-stage training strategy, comprising pre-training and visual instruction tuning. 
    We test ``mplug-owl2-llama2-7b''. 
    \item \textbf{ShareGPT4V} \cite{chen2023sharegpt4v} follows the design of LLaVA v1.5.
    % 1.2 million
    % ShareGPT4V dataset
     They incorporate a large-scale resource featuring highly descriptive captions into both the pre-training and supervised fine-tuning phases of ShareGPT4V model.
    % The model shows remarkable performance across a majority of the multi-modal benchmarks.
    We test``ShareGPT4V-7B'' and ``ShareGPT4V-13B''.
    \item \textbf{GPT-4V} \citep{openai2023gpt4} is one of the most powerful LVLMs in the world developed by OpenAI. The version of ``gpt-4-vision-preview'' is tested.
    \item \textbf{GPT-4o} \citep{openai2023gpt4} is currently OpenAI's latest and most powerful multimodal model. It is a single model trained end-to-end across text, vision, and audio. The version of ``gpt-4o'' is tested.
\end{itemize}

Table \ref{tab:lvlms} shows an overview of the design of different LVLMs.



\section{Evaluation Results of Description Task based on Traditional Image Captioning Metrics}
\label{sec:cap_eval}

% based on 
\begin{table*}[h!]
  \centering
  \small
  \setlength{\tabcolsep}{2.5pt} 
  \caption{\label{tab:cap_eval}
  Model performance on Description task evaluated by traditional image captioning evaluation metrics.
  }
  \begin{tabular}{lcccccc}
  \hline
  \textbf{Model} & \thead{\textbf{METEOR} } & \thead{\textbf{CIDEr} } & \thead{\textbf{BLEU-1} } &  \thead{\textbf{BLEU-2} } & \thead{\textbf{BLEU-3} } & \thead{\textbf{BLEU-4}} \\ % \\ \textbf{Accuracy}
  \hline
  InstructBLIP-7B & 0.130 & 0.043 &  0.255 & 0.127 &  0.063 & 0.033 \\
  Qwen-VL-Chat & 0.128 & 0.036 & 0.232 & 0.121 & 0.059 & 0.031 \\
  LLaVA-V1.5-7B & 0.146 & 0.054 &  0.309 & 0.158 & 0.076 & 0.037 \\
  LLaVA-V1.5-13B & 0.146 & 0.051 &  0.312 & 0.160 & 0.076 & 0.037 \\
  % CogVLM & 0 & & & & & & & & \\
  mPLUG-Owl-2 & 0.132 & 0.035 &  0.260 & 0.126 & 0.057 & 0.027 \\
  ShareGPT4V-7B & 0.162 & 0.017 &  0.259 & 0.120 & 0.050 & 0.024 \\
  ShareGPT4V-13B & 0.165 & 0.024 &  0.278 & 0.129 & 0.055 &  0.026 \\
  GPT-4V & 0.208 & 0.001 &  0.234 & 0.115 & 0.052 & 0.025 \\
  GPT-4o & 0.210 & 0.008 &  0.254 &  0.128 & 0.061 &  0.030 \\
  % \hline
  % Human &  &  &  &  &  &  &  &  &  \\
  \hline
  \end{tabular}

\end{table*}

% the following metrics 
Table \ref{tab:cap_eval} shows the performance of models on traditional image captioning evaluation metrics.
Following \citet{krause2017hierarchical}, we use METEOR \cite{banerjee-lavie-2005-meteor}, CIDEr \cite{vedantam2015cider}, BLEU-1, BLEU-2, BLEU-3, and BLEU-4 \cite{papineni2002bleu} to evaluate the performance of models on Description task.
Similar to the findings of \citet{zhu2023chatgpt}, it can be observed that traditional image captioning evaluation metrics are not quite suitable for evaluating the Description task. 
There are two possible reasons. 
The first possible reason is that image descriptions are longer and more flexible than traditional image captions.
%and contains more details. 
The second possible reason is that Description task requires evalution metrics to consider high-level semantics in description, while traditional image captioning evaluation metrics only concerns low-level information.

% cognitive ability of models

% % based on 20240124_94
% \begin{table*}
%   \centering
%   \small
%   \begin{tabular}{lcccccc}
%   \hline
%   \textbf{Model} & \thead{\textbf{METEOR} } & \thead{\textbf{CIDEr} } & \thead{\textbf{BLEU-1} } &  \thead{\textbf{BLEU-2} } & \thead{\textbf{BLEU-3} } & \thead{\textbf{BLEU-4}} \\ % \\ \textbf{Accuracy}
%   \hline
%   InstructBLIP-7B & 0.127 & 0.032 & 0.245 & 0.123 & 0.062 & 0.032  \\
%   Qwen-VL-Chat & 0.131 & 0.042 &  0.239 & 0.126 & 0.060 & 0.031  \\
%   LLaVA-V1.5-7B & 0.140 & 0.039 & 0.285 & 0.146 & 0.071 &  0.036  \\
%   LLaVA-V1.5-13B &  0.144 & 0.037 &  0.300 & 0.156 & 0.074 & 0.037  \\
%   % CogVLM & 0 & & & & & & & & \\
%   mPLUG-Owl-2 & 0.130 & 0.022 & 0.255 & 0.123 & 0.055 & 0.025 \\
%   ShareGPT4V-7B & 0.161 & 0.023 & 0.285 & 0.135 & 0.06 &  0.028 \\
%   ShareGPT4V-13B & 0.163 & 0.035 & 0.287 & 0.135 & 0.0592 & 0.027  \\
%   % Monkey &  &  &  &  &  &  &  &  &  \\
%   GPT-4V & 0.206 & 0.007 & 0.238  & 0.117 & 0.052 & 0.024 \\
%   % \hline
%   % Human &  &  &  &  &  &  &  &  &  \\
%   \hline
%   \end{tabular}
%   \caption{\label{tab:cap_eval}
%   Model performance on Description task evaluated by traditional image captioning evaluation metrics.
%   }
% \end{table*}




% \section{Implementation of Non-GPT-based Evaluation Methods for Cognition Evaluation of Description}
% \label{sec:eval_method}

% The task of Cognition evaluation of Description is to determine whether a CoR is mentioned in the description. 
% To perform this task, different evaluation methods are implemented apart from ChatGPT or GPT-4, as shown in Table \ref{tab:eval_metrics}.

% For methods based on ROUGE \cite{lin2004rouge}, BERTScore \cite{bertscore}, and BLEURT \cite{sellam2020bleurt},
% we first split the description into sentences, then use a CoR as a reference to calculate the (recall) score for each sentence compared to the CoR. 
% % score (recall score for ROUGE and BERTScore)
% Then, the highest score among all calculated scores is taken as the score of the CoR corresponding to the description.
% Finally, the score is converted into 0/1 using a threshold. 

% We also tried Natural Language Inference (NLI) models to perform the task. 
% First, we use DeBERTa \cite{he2021deberta} to perform sentence-level NLI task similar to methods mentioned above. 
% If there is at least one ``Entailment'' for all the sentences, the score of the CoR will be 1.
% The model we adopted is \textit{mDeBERTa-v3-base-xnli-multilingual-nli-2mil7}.
% The second NLI model we tried is DocNLI \cite{yin-etal-2021-docnli}, which can directly take the description and CoR as input and do the classification task.

% \section{Effectiveness Analysis of GPT-based Cognition Evaluation of Description Task}
% \label{sec:eval_gpt_eval}

% % Considering the interpretability issues 

% To prove the effectiveness of GPT-based evaluation, we manually annotated a subset by assigning 0/1 to CoRs of 20 images and use the subset to evaluate the performance of different evluation methods.

\section{Implementation of Non-GPT-based Cognition Evaluation Methods of Description Task}
\label{sec:eval_gpt_eval}

The task of cognition evaluation of Description task is to determine whether a CoR is mentioned in the description. 
Apart from ChatGPT or GPT-4, some other evaluation methods are implemented to perform this classification task, as shown in Table \ref{tab:eval_metrics}.

For methods based on ROUGE \cite{lin2004rouge}, BERTScore \cite{bertscore}, and BLEURT \cite{sellam2020bleurt},
we first split the description into sentences, then use a CoR as a reference to calculate the (recall) score for each sentence compared to the CoR. 
% score (recall score for ROUGE and BERTScore)
Then, the highest score among all calculated scores is taken as the score of the CoR corresponding to the description.
Finally, the score is converted into 0/1 using a threshold. 

We also tried Natural Language Inference (NLI) models to perform the task. 
First, we use DeBERTa \cite{he2021deberta} to perform sentence-level NLI task similar to methods mentioned above. 
If there is at least one ``Entailment'' for all the sentences, the score of the CoR will be 1.
The model we adopted is \textit{mDeBERTa-v3-base-xnli-multilingual-nli-2mil7}.
The second NLI model we tried is DocNLI \cite{yin-etal-2021-docnli}, which can directly take the description and CoR as input and do the classification task. 
% \KZ{Do we show results of these eval methods? If not, why not?}
% \XJ{We show results of these eval methods in Table \ref{tab:eval_metrics}.}

% \subsection{Result Analysis}

% % after 20240204_95
% \begin{table}[h!]
%   \centering
%   \small
%   \begin{tabular}{lc}
%   \hline
%   \textbf{Model} & \textbf{Accuracy}\\ % \\ \textbf{Accuracy}
%   \hline
%   ROUGE  & 0.656 \\
%   % ROUGE-paragraph & 0.567  \\
%   BERTScore & 0.635 \\
%   % BERTScore-paragraph & 0.620  \\
%   BLEURT & 0.620 \\
%   DeBERTa & 0.693 \\
%   DocNLI  & 0.714 \\
%   GPT-3.5 & 0.807 \\
%   GPT-4 & 0.833 \\
%   \hline
%   \end{tabular}
%   \caption{\label{tab:eval_metrics}
%   Accuray of different evaluation methods.
%   }
% \end{table}

% Table \ref{tab:eval_metrics} shows the accuracy of different evaluation methods on the subset.
% It can be seen that GPT-4 achieves the best performance, which indicates that GPT-based evaluation is generally consistent with human evaluation and thus effective for evaluating the performance of LVLMs on Description task.
% % The implementation details of evaluation methods beyond ChatGPT and GPT-4 can be found in Appendix \ref{sec:eval_method}.




% Details are shown in Appendix.
% Besides, as the evaluation task is a fine-grained discriminative task, the interpretability problem of GPT can be alleviated to some extent.

% before 20240204_95
% \begin{table}
%     \centering
%     \small
%     \begin{tabular}{lc}
%     \hline
%     \textbf{Model} & \textbf{Accuracy}\\ % \\ \textbf{Accuracy}
%     \hline
%     ROUGE  & 0.656 \\
%     % ROUGE-paragraph & 0.567  \\
%     BERTScore & 0.641 \\
%     % BERTScore-paragraph & 0.620  \\
%     BLEURT & 0.620 \\
%     DeBERTa & 0.682 \\
%     DocNLI  & 0.703 \\
%     GPT-3.5 & 0.797 \\
%     GPT-4 & 0.83 \\
%     \hline
%     \end{tabular}
%     \caption{\label{tab:eval_metrics}
%     Accuray of different evaluation methods on human evaluation.
%     }
% \end{table}







% \end{itemize}

\section{Ethical Considerations}
\label{sec:ethical}

Most images in CogBench are manually collected from Pinterest. 
We follow the terms of service of Pinterest to collect these images.
% The dispaly of the images is transformative and
The images are used as fair use for research purposes only.
% They are used for research purposes only and are not used for commercial purposes.
We will share the data with other researchers who will follow the ethical considerations as established in this study.
We have compensated our annotators at a rate of \$0.60 per image, which aligns with the university's recruitment standards for labeling personnel. This amount is offered as an appropriate token of gratitude for their valuable time and efforts in assisting with our project.
We answer any questions our annotators have during their annotation process promptly. 
They are free to take breaks or quit the annotation task at any time.