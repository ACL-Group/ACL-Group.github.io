% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{liu2023llava,
    author      = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    title       = {Visual Instruction Tuning},
    booktitle   = {NeurIPS},
    year        = {2023}
}
@misc{openai2023gpt4,
      title={{GPT}-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{yu2023mm,
  title={{MM-V}et: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}
@inproceedings{zellers2019vcr,
  author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  title = {From Recognition to Cognition: Visual Commonsense Reasoning},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2019}
}
@InProceedings{park2020visualcomet,
  author = {Park, Jae Sung and Bhagavatula, Chandra and Mottaghi, Roozbeh and Farhadi, Ali and Choi, Yejin},
  title = {VisualCOMET: Reasoning about the Dynamic Context of a Still Image},
  booktitle = {In Proceedings of the European Conference on Computer Vision (ECCV)},
  year = {2020}
}
@inproceedings{hessel2022abduction,
  title={The abduction of sherlock holmes: A dataset for visual abductive reasoning},
  author={Hessel, Jack and Hwang, Jena D and Park, Jae Sung and Zellers, Rowan and Bhagavatula, Chandra and Rohrbach, Anna and Saenko, Kate and Choi, Yejin},
  booktitle={European Conference on Computer Vision},
  pages={558--575},
  year={2022},
  organization={Springer}
}
@article{chen2023measuring,
  title={Measuring and improving chain-of-thought reasoning in vision-language models},
  author={Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay},
  journal={arXiv preprint arXiv:2309.04461},
  year={2023}
}
@inproceedings{FZCVR22,
    author = {Xingyu Fu and Ben Zhou and Ishaan Preetam Chandratreya and Carl Vondrick and Dan Roth},
    title = {{Thereâ€™s a Time and Place for Reasoning Beyond the Image}},
    booktitle = {Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL)},
    year = {2022},
    url = "https://cogcomp.seas.upenn.edu/papers/paper-to-come.pdf",
    funding = {KAIROS},
}
@article{liu2023mmbench,
  title={Mmbench: Is your multi-modal model an all-around player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}
@article{zhu2023chatgpt,
  title={{ChatGPT} asks, blip-2 answers: Automatic questioning towards enriched visual descriptions},
  author={Zhu, Deyao and Chen, Jun and Haydarov, Kilichbek and Shen, Xiaoqian and Zhang, Wenxuan and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2303.06594},
  year={2023}
}
@inproceedings{cafagna2023hl,
  title={HL dataset: visually-grounded description of scenes, actions and rationales},
  author={Cafagna, Michele and van Deemter, Kees and Gatt, Albert},
  booktitle={Proceedings of the 16th International Natural Language Generation Conference},
  pages={293--312},
  year={2023}
}
@inproceedings{krause2017hierarchical,
  title={A hierarchical approach for generating descriptive image paragraphs},
  author={Krause, Jonathan and Johnson, Justin and Krishna, Ranjay and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={317--325},
  year={2017}
}
@article{chan2023ic,
  title={IC3: Image Captioning by Committee Consensus},
  author={Chan, David M and Myers, Austin and Vijayanarasimhan, Sudheendra and Ross, David A and Canny, John},
  journal={arXiv preprint arXiv:2302.01328},
  year={2023}
}
@misc{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{tasnim-etal-2022-depac,
    title = "{DEPAC}: a Corpus for Depression and Anxiety Detection from Speech",
    author = "Tasnim, Mashrura  and
      Ehghaghi, Malikeh  and
      Diep, Brian  and
      Novikova, Jekaterina",
    editor = "Zirikly, Ayah  and
      Atzil-Slonim, Dana  and
      Liakata, Maria  and
      Bedrick, Steven  and
      Desmet, Bart  and
      Ireland, Molly  and
      Lee, Andrew  and
      MacAvaney, Sean  and
      Purver, Matthew  and
      Resnik, Rebecca  and
      Yates, Andrew",
    booktitle = "Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology",
    month = jul,
    year = "2022",
    address = "Seattle, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.clpsych-1.1",
    doi = "10.18653/v1/2022.clpsych-1.1",
    pages = "1--16",
    abstract = "Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis system of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labelled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of baseline machine learning models built on this dataset with baseline models trained on other well-known depression corpora.",
}
@article{describe-ctp,
author = {Cummings, Louise},
year = {2019},
month = {03},
pages = {151-174},
title = {Describing the Cookie Theft picture: Sources of breakdown in Alzheimer's dementia},
volume = {10},
journal = {Pragmatics and Society},
doi = {10.1075/ps.17011.cum}
}
@misc{liu2023improved,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
      year={2023},
      eprint={2310.03744},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{bai2023qwenvl,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@BOOK{goodglass2001-ej,
  title     = "{BDAE}: The Boston Diagnostic Aphasia Examination",
  author    = "Goodglass, Harold and Kaplan, Edith and Weintraub, Sandra",
  publisher = "Lippincott Williams \& Wilkins",
  year      =  2001,
  address   = "Philadelphia, PA"
}
@article{li2023seed,
  title={Seed-bench: Benchmarking multimodal {LLMs} with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}
@article{bai2023touchstone,
  title={Touchstone: Evaluating vision-language models by language models},
  author={Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.16890},
  year={2023}
}

@inproceedings{densecap,
  title={DenseCap: Fully Convolutional Localization Networks for Dense Captioning},
  author={Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and 
             Pattern Recognition},
  year={2016}
}
@inproceedings{chan-etal-2023-clair,
    title = "{CLAIR}: Evaluating Image Captions with Large Language Models",
    author = "Chan, David  and
      Petryk, Suzanne  and
      Gonzalez, Joseph  and
      Darrell, Trevor  and
      Canny, John",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.841",
    doi = "10.18653/v1/2023.emnlp-main.841",
    pages = "13638--13646",
    abstract = "The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6{\%} and over image-augmented methods such as RefCLIP-S of 18.3{\%}. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score.",
}
@article{chen2023sharegpt4v,
  title={{ShareGPT4V}: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv preprint arXiv:2311.12793},
  year={2023}
}
@article{ye2023mplug,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04257},
  year={2023}
}

@article{xie2022visual,
  title={Visual clues: Bridging vision and language foundations for image paragraph captioning},
  author={Xie, Yujia and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Bach, Nguyen and Liu, Ce and Zeng, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17287--17300},
  year={2022}
}
@article{zhuge2023mindstorms,
  title={Mindstorms in Natural Language-Based Societies of Mind},
  author={Zhuge, Mingchen and Liu, Haozhe and Faccio, Francesco and Ashley, Dylan R and Csord{\'a}s, R{\'o}bert and Gopalakrishnan, Anand and Hamdi, Abdullah and Hammoud, Hasan and Herrmann, Vincent and Irie, Kazuki and Kirsch, Louis and Li, Bing and Li, Guohao and Liu, Shuming and Mai, Jinjie and Pi{\k{e}}kos, Piotr and Ramesh, Aditya and Schlag, Imanol and Shi, Weimin and Stani{\'c}, Aleksandar and Wang, Wenyi and Wang, Yuhui and Xu, Mengmeng and Fan, Deng-Ping and Ghanem, Bernard and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2305.17066},
  year={2023}
}
@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}
@article{fu2023mme,
  title={Mme: A comprehensive evaluation benchmark for multimodal large language models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}
@article{wu2023q,
  title={Q-bench: A benchmark for general-purpose foundation models on low-level vision},
  author={Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and others},
  journal={arXiv preprint arXiv:2309.14181},
  year={2023}
}
@article{JIANG201739,
title = {Investigation of different speech types and emotions for detecting depression using different classifiers},
journal = {Speech Communication},
volume = {90},
pages = {39-46},
year = {2017},
issn = {0167-6393},
doi = {https://doi.org/10.1016/j.specom.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167639316303053},
author = {Haihua Jiang and Bin Hu and Zhenyu Liu and Lihua Yan and Tianyang Wang and Fei Liu and Huanyu Kang and Xiaoyu Li},
keywords = {Acoustic features, Depression, Classifiers, Speech types, Speech emotions},
abstract = {Depression is one of the most common mental disorders. Early intervention is very important for reducing the burden of the disease, but current methods of diagnosis remain limited. Previously, acoustic features of speech have been identified as possible cues for depression, but there has been little research to link depression with speech types and emotions. This study investigated acoustic correlates of depression in a sample of 170 subjects (85 depressed patients and 85 healthy controls). We examined the discriminative power of three different types of speech (interview, picture description, and reading) and three speech emotions (positive, neutral, and negative) using different classifiers, with male and female subjects modeled separately. We observed that picture description speech rendered significantly better (pâ€‰<â€‰0.05) classification results than other speech types for males, and interview speech performed significantly better (pâ€‰<â€‰0.05) than other speech types for females. Based on speech types and emotions, a new computational methodology for detecting depression (STEDD) was developed and tested. This new approach showed a high accuracy level of 80.30% for males and 75.96% for females, with a desirable sensitivity/specificity ratio of 75.00%/85.29% for males and 77.36%/74.51% for females. These results are encouraging for detecting depression, and provide guidance for future research.}
}
@inproceedings{farzana-parde-2023-towards,
    title = "Towards Domain-Agnostic and Domain-Adaptive Dementia Detection from Spoken Language",
    author = "Farzana, Shahla  and
      Parde, Natalie",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.668",
    doi = "10.18653/v1/2023.acl-long.668",
    pages = "11965--11978",
    abstract = "Health-related speech datasets are often small and varied in focus. This makes it difficult to leverage them to effectively support healthcare goals. Robust transfer of linguistic features across different datasets orbiting the same goal carries potential to address this concern. To test this hypothesis, we experiment with domain adaptation (DA) techniques on heterogeneous spoken language data to evaluate generalizability across diverse datasets for a common task: dementia detection. We find that adapted models exhibit better performance across conversational and task-oriented datasets. The feature-augmented DA method achieves a 22{\%} increase in accuracy adapting from a conversational to task-specific dataset compared to a jointly trained baseline. This suggests promising capacity of these techniques to allow for productive use of disparate data for a complex spoken language healthcare task.",
}
@inproceedings{li-etal-2022-gpt,
    title = "{GPT}-{D}: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models",
    author = "Li, Changye  and
      Knopman, David  and
      Xu, Weizhe  and
      Cohen, Trevor  and
      Pakhomov, Serguei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.131",
    doi = "10.18653/v1/2022.acl-long.131",
    pages = "1866--1877",
    abstract = "Deep learning (DL) techniques involving fine-tuning large numbers of model parameters have delivered impressive performance on the task of discriminating between language produced by cognitively healthy individuals, and those with Alzheimer{'}s disease (AD). However, questions remain about their ability to generalize beyond the small reference sets that are publicly available for research. As an alternative to fitting model parameters directly, we propose a novel method by which a Transformer DL model (GPT-2) pre-trained on general English text is paired with an artificially degraded version of itself (GPT-D), to compute the ratio between these two models{'} \textit{perplexities} on language from cognitively healthy and impaired individuals. This technique approaches state-of-the-art performance on text data from a widely used {``}Cookie Theft{''} picture description task, and unlike established alternatives also generalizes well to spontaneous conversations. Furthermore, GPT-D generates text with characteristics known to be associated with AD, demonstrating the induction of dementia-related linguistic anomalies. Our study is a step toward better understanding of the relationships between the inner workings of generative neural language models, the language that they produce, and the deleterious effects of dementia on human speech and language characteristics.",
}
@inproceedings{duan-etal-2023-cda,
    title = "{CDA}: A Contrastive Data Augmentation Method for {A}lzheimer{'}s Disease Detection",
    author = "Duan, Junwen  and
      Wei, Fangyuan  and
      Liu, Jin  and
      Li, Hongdong  and
      Liu, Tianming  and
      Wang, Jianxin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.114",
    doi = "10.18653/v1/2023.findings-acl.114",
    pages = "1819--1826",
    abstract = "Alzheimer{'}s Disease (AD) is a neurodegenerative disorder that significantly impacts a patient{'}s ability to communicate and organize language. Traditional methods for detecting AD, such as physical screening or neurological testing, can be challenging and time-consuming. Recent research has explored the use of deep learning techniques to distinguish AD patients from non-AD patients by analysing the spontaneous speech. These models, however, are limited by the availability of data. To address this, we propose a novel contrastive data augmentation method, which simulates the cognitive impairment of a patient by randomly deleting a proportion of text from the transcript to create negative samples. The corrupted samples are expected to be in worse conditions than the original by a margin. Experimental results on the benchmark ADReSS Challenge dataset demonstrate that our model achieves the best performance among language-based models.",
}
@article{zhu2023minigpt,
  title={{MiniGPT}-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing {GPT}-4 with 90\%* {ChatGPT} quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}
@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}
@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}
@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}
@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}
@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{bertscore,
  author       = {Tianyi Zhang and
                  Varsha Kishore and
                  Felix Wu and
                  Kilian Q. Weinberger and
                  Yoav Artzi},
  title        = {BERTScore: Evaluating Text Generation with {BERT}},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=SkeHuCVFDr},
  timestamp    = {Wed, 03 Jun 2020 10:08:32 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhangKWWA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sellam2020bleurt,
  title = {BLEURT: Learning Robust Metrics for Text Generation},
  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},
  year = {2020},
  booktitle = {Proceedings of ACL}
}
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}
@inproceedings{yin-etal-2021-docnli,
    title = "{D}oc{NLI}: A Large-scale Dataset for Document-level Natural Language Inference",
    author = "Yin, Wenpeng  and
      Radev, Dragomir  and
      Xiong, Caiming",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.435",
    doi = "10.18653/v1/2021.findings-acl.435",
    pages = "4913--4922",
}
@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with {GPT}-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}
@article{zhuang2023efficiently,
  title={Efficiently Measuring the Cognitive Ability of {LLMs}: An Adaptive Testing Perspective},
  author={Zhuang, Yan and Liu, Qi and Ning, Yuting and Huang, Weizhe and Lv, Rui and Huang, Zhenya and Zhao, Guanhao and Zhang, Zheng and Mao, Qingyang and Wang, Shijin and others},
  journal={arXiv preprint arXiv:2306.10512},
  year={2023}
}
@article{yang2023dawn,
  title={The dawn of lmms: Preliminary explorations with {GPT}-4{V} (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  volume={9},
  number={1},
  pages={1},
  year={2023}
}
@article{mueller2018connected,
  title={Connected speech and language in mild cognitive impairment and Alzheimerâ€™s disease: A review of picture description tasks},
  author={Mueller, Kimberly D and Hermann, Bruce and Mecollari, Jonilda and Turkstra, Lyn S},
  journal={Journal of clinical and experimental neuropsychology},
  volume={40},
  number={9},
  pages={917--939},
  year={2018},
  publisher={Taylor \& Francis}
}
