\section{Related Work}

% We will introduce related work of our paper in this section. 
% Existing evaluation benchmarks for LVLMs, Image Captioning and Visual Reasoning are related to while differ from CogBench.


% \subsection{Large Vision Language Models}

% InstructBLIP \cite{dai2023instructblip} 
% LLaVA \citep{liu2023llava} and LLaVA v1.5 \cite{liu2023improved}
% Qwen-VL-Chat \cite{bai2023qwenvl}
% mPLUG-Owl-2 \cite{ye2023mplug}
% ShareGPT4V \cite{chen2023sharegpt4v}
% GPT-4V \citep{openai2023gpt4} is one of the most powerful LVLMs developed by OpenAI.

% \MY{you need a preamble between section title and subsections, saying that there are 3 working areas related while they differ from your work}

% \subsection{Evaluation Benchmark for LVLMs}
\paragraph{Evaluation Benchmark for LVLMs.}
% With the rapid advancement of LVLMs, they have emerged with new capabilities.
To better understand emerging capabilities of LVLMs, many scholars have proposed different evaluation benchmarks.
LVLM-eHub \cite{xu2023lvlm} evaluates LVLMs' multimodal capabilities across six categories using various publicly available computer vision datasets.
% uses different public available computer vision task datasets as evaluation samples to evaluate LVLMs' capability on 6 categories of multimodal capabilities.
MME \cite{fu2023mme}, MMBench \cite{liu2023mmbench} and SEED Bench \cite{li2023seed} use True/False Questions or Multiple-Choice Questions to evaluate different abilities of LVLMs.
% SEED Bench \cite{li2023seed} provide objective and comprehension evaluation of MLLMs, 
% contains more Multiple-Choice Questions covering 12 evaluation dimensions including both spatial and temporal understanding.
% They use True/False Questions or Multiple-Choice Questions to evaluate the capabilities of LVLMs.
% address the evaluation of generative comprehension in LVLMs. 
MM-VET \cite{yu2023mm} evaluates LVLMs in terms of their integrated VL capabilities via open-ended questions. 
% They defines 6 core VL capabilities for LVLMs in their benchmark. 
% Q-Bench \cite{wu2023q} evaluates the abilities of LVLMs on low-level visual perception and understanding.
Different from them, CogBench mainly focus on high-level cognition evaluation of LVLMs. 
Though some of them also consider cognition as one of the evaluation dimensions, most images they use can only be used to evaluate limited aspects of reasoning.
% TouchStone \cite{bai2023touchstone} 

% \subsection{Image Captioning}
\paragraph{Image Captioning.}
Image Captioning is a classical VL task. 
% As the VL capabilities of models improve, researchers are dedicated to enabling models to further recognize, understand and describe the content within images.
As model capabilities advance, researchers strive to enhance their ability to describe images in detail.
% \citet{densecap} propose Dense Captioning task, which requires models to both localize and describe salient regions in images in natural language.
\citet{krause2017hierarchical} propose Image Paragraph Captioning, tasking models with generating a descriptive paragraph for an image.
%  task and it requires models to generate a paragraph to describe the image.
% With the emergence of LLMs and LVLMs recently, 
Recently, some researchers \cite{xie2022visual, zhu2023chatgpt, zhuge2023mindstorms,chen2023sharegpt4v} are trying to leveraging the ability of LLMs to generate more detailed image descriptions.
None of these tasks consider to evaluate the high-level cognitive abilities of models through description.
% improve model performance on image description task.
% \citet{zhuge2023mindstorms} also use image description task to evaluate the ability of their method. 
%  \cite{FZCVR22} 
% However, images they use contains less kinds of reasoning and CogBench can be served as a dat.
HL dataset \cite{cafagna2023hl} requires models to generate high-level captions, but it only considers three aspects (scene, action, rationale).
The content of most images in existing datasets does not reach the level of a story.
This reveals the need for higher-level datasets like CogBench.

% Visual Clues \cite{xie2022visual}
% ChatGPT asks \cite{zhu2023chatgpt}
% IC3 \cite{chan2023ic}

% \subsection{Visual Reasoning}
\paragraph{Visual Reasoning.}
Visual Reasoning is closely related to the cognitive abilities of models.
% Visual Commonsense Reasoning (VCR) \cite{zellers2019vcr} requires models to answer visual questions related to commonsense reasoning and provide a rationale explaining why its answer is true. 
VCR \cite{zellers2019vcr} tasks models with answering visual questions using commonsense reasoning and justifying their answers.
% The questions are all Multiple-Choice Questions.
VisualCOMET \cite{park2020visualcomet} is a framework of visual commonsense reasoning tasks to predict past, future events, and present intents.
% a framework of visual commonsense reasoning tasks to predict events that might have happened before, events that might happen after, and the intents of the people at present.
\citet{hessel2022abduction} utilize images from VCR and Visual Genome \cite{krishna2017visual} to evaluate the ability of models to perform abductive reasoning.
% \citet{FZCVR22} propose a task aiming at identifying the time and location the given image was taken. 
\citet{FZCVR22} propose a task to identify the time and location of a given image.
CURE \cite{chen2023measuring} is proposed to measure both the zero-shot reasoning performance and consistency of VLMs. 
% benchmark
% builds on the Sherlock dataset \citet{hessel2022abduction} and
% It is also a Multiple-Choice Question Answering task.
Similarly, compared to CogBench, these tasks considers less kinds of reasoning and CogBench can be seen as the next step of these efforts.
% to images in 

% \MY{what's abductive reasoning?}
% \XJ{Their definition is: the process of making the most plausible inference in the face of incomplete information}