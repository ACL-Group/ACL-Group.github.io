@ARTICLE{ic9600,
  author={Feng, Tinglei and Zhai, Yingjie and Yang, Jufeng and Liang, Jie and Fan, Deng-Ping and Zhang, Jing and Shao, Ling and Tao, Dacheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={IC9600: A Benchmark Dataset for Automatic Image Complexity Assessment}, 
  year={2023},
  volume={45},
  number={7},
  pages={8577-8593},
  keywords={Integrated circuits;Complexity theory;Feature extraction;Integrated circuit modeling;Task analysis;Entropy;Visualization;Image complexity assessment;image attributes;large-scale well-annotated dataset},
  doi={10.1109/TPAMI.2022.3232328}}


@article{cummings2019describing,
  title={Describing the cookie theft picture: Sources of breakdown in Alzheimer’s dementia},
  author={Cummings, Louise},
  journal={Pragmatics and Society},
  volume={10},
  number={2},
  pages={153--176},
  year={2019},
  publisher={John Benjamins Publishing Company Amsterdam/Philadelphia}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{tasnim-etal-2022-depac,
    title = "{DEPAC}: a Corpus for Depression and Anxiety Detection from Speech",
    author = "Tasnim, Mashrura  and
      Ehghaghi, Malikeh  and
      Diep, Brian  and
      Novikova, Jekaterina",
    editor = "Zirikly, Ayah  and
      Atzil-Slonim, Dana  and
      Liakata, Maria  and
      Bedrick, Steven  and
      Desmet, Bart  and
      Ireland, Molly  and
      Lee, Andrew  and
      MacAvaney, Sean  and
      Purver, Matthew  and
      Resnik, Rebecca  and
      Yates, Andrew",
    booktitle = "Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology",
    month = jul,
    year = "2022",
    address = "Seattle, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.clpsych-1.1",
    doi = "10.18653/v1/2022.clpsych-1.1",
    pages = "1--16",
    abstract = "Mental distress like depression and anxiety contribute to the largest proportion of the global burden of diseases. Automated diagnosis system of such disorders, empowered by recent innovations in Artificial Intelligence, can pave the way to reduce the sufferings of the affected individuals. Development of such systems requires information-rich and balanced corpora. In this work, we introduce a novel mental distress analysis audio dataset DEPAC, labelled based on established thresholds on depression and anxiety standard screening tools. This large dataset comprises multiple speech tasks per individual, as well as relevant demographic information. Alongside, we present a feature set consisting of hand-curated acoustic and linguistic features, which were found effective in identifying signs of mental illnesses in human speech. Finally, we justify the quality and effectiveness of our proposed audio corpus and feature set in predicting depression severity by comparing the performance of baseline machine learning models built on this dataset with baseline models trained on other well-known depression corpora.",
}

@article{gpt4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/ARXIV.2303.08774},
  eprinttype    = {arXiv},
  eprint       = {2303.08774},
  timestamp    = {Mon, 28 Aug 2023 21:26:19 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Beltagy2020Longformer,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={arXiv:2004.05150},
  year={2020},
}

@BOOK{goodglass2001-ej,
  title     = "{BDAE}: The Boston Diagnostic Aphasia Examination",
  author    = "Goodglass, Harold and Kaplan, Edith and Weintraub, Sandra",
  publisher = "Lippincott Williams \& Wilkins",
  year      =  2001,
  address   = "Philadelphia, PA"
}

@article{song2024cognitive,
  title={A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models},
  author={Song, Xiujie and Wu, Mengyue and Zhu, Kenny Q and Zhang, Chunhao and Chen, Yanyi},
  journal={arXiv preprint arXiv:2402.18409},
  year={2024}
}

@inproceedings{vit,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kong2016photo,
  title={Photo aesthetics ranking network with attributes and content adaptation},
  author={Kong, Shu and Shen, Xiaohui and Lin, Zhe and Mech, Radomir and Fowlkes, Charless},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part I 14},
  pages={662--679},
  year={2016},
  organization={Springer}
}

@inproceedings{ying2020patches,
  title={From patches to pictures (PaQ-2-PiQ): Mapping the perceptual space of picture quality},
  author={Ying, Zhenqiang and Niu, Haoran and Gupta, Praful and Mahajan, Dhruv and Ghadiyaram, Deepti and Bovik, Alan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3575--3585},
  year={2020}
}

@article{saraee2020visual,
  title={Visual complexity analysis using deep intermediate-layer features},
  author={Saraee, Elham and Jalal, Mona and Betke, Margrit},
  journal={Computer Vision and Image Understanding},
  volume={195},
  pages={102949},
  year={2020},
  publisher={Elsevier}
}

@article{zhai2020perceptual,
  title={Perceptual image quality assessment: a survey},
  author={Zhai, Guangtao and Min, Xiongkuo},
  journal={Science China Information Sciences},
  volume={63},
  pages={1--52},
  year={2020},
  publisher={Springer}
}

@article{koniq10k,
author={V. {Hosu} and H. {Lin} and T. {Sziranyi} and D. 
{Saupe}},
journal={IEEE Transactions on Image Processing},
title={KonIQ-10k: An Ecologically Valid Database for Deep 
Learning of Blind Image Quality Assessment},
year={2020},
volume={29},
pages={4041-4056}}

@InProceedings{Fang_2020_CVPR,
author = {Fang, Yuming and Zhu, Hanwei and Zeng, Yan and Ma, Kede and Wang, Zhou},
title = {Perceptual Quality Assessment of Smartphone Photography},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@INPROCEEDINGS{tid2013,
  author={Ponomarenko, Nikolay and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Jin, Lina and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and Kuo, C.-C. Jay},
  booktitle={European Workshop on Visual Information Processing (EUVIP)}, 
  title={Color image database TID2013: Peculiarities and preliminary results}, 
  year={2013},
  volume={},
  number={},
  pages={106-111},
  keywords={Measurement;Databases;Image color analysis;Visualization;Noise;Color;Image coding;Image visual quality;color image database},
  doi={}}

@inproceedings{wu2024qbench,
    author = {Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and Lin, Weisi},
    title = {Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision},
    booktitle = {ICLR},
    year = {2024}
}

@article{deng2017image,
  title={Image aesthetic assessment: An experimental survey},
  author={Deng, Yubin and Loy, Chen Change and Tang, Xiaoou},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={80--106},
  year={2017},
  publisher={IEEE}
}

@InProceedings{Yi_2023_CVPR,
    author    = {Yi, Ran and Tian, Haoyuan and Gu, Zhihao and Lai, Yu-Kun and Rosin, Paul L.},
    title     = {Towards Artistic Image Aesthetics Assessment: A Large-Scale Dataset and a New Method},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {22388-22397}
}

@InProceedings{2015JenAesthetics,
author="Amirshahi, Seyed Ali
and Hayn-Leichsenring, Gregor Uwe
and Denzler, Joachim
and Redies, Christoph",
editor="Agapito, Lourdes
and Bronstein, Michael M.
and Rother, Carsten",
title="JenAesthetics Subjective Dataset: Analyzing Paintings by Subjective Scores",
booktitle="Computer Vision - ECCV 2014 Workshops",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="3--19",
abstract="Over the last few years, researchers from the computer vision and image processing community have joined other research groups in searching for the bases of aesthetic judgment of paintings and photographs. One of the most important issues, which has hampered research in the case of paintings compared to photographs, is the lack of subjective datasets available for public use. This issue has not only been mentioned in different publications, but was also widely discussed at different conferences and workshops. In the current work, we perform a subjective test on a recently released dataset of aesthetic paintings. The subjective test not only collects scores based on the subjective aesthetic quality, but also on other properties that have been linked to aesthetic judgment.",
isbn="978-3-319-16178-5"
}

@article{fekete2022vienna,
  title={The Vienna Art Picture System (VAPS): A data set of 999 paintings and subjective ratings for art and aesthetics research.},
  author={Fekete, Anna and Pelowski, Matthew and Specker, Eva and Brieber, David and Rosenberg, Raphael and Leder, Helmut},
  journal={Psychology of Aesthetics, Creativity, and the Arts},
  year={2022},
  publisher={Educational Publishing Foundation}
}

@ARTICLE{2011Photonet,
  author={Joshi, Dhiraj and Datta, Ritendra and Fedorovskaya, Elena and Luong, Quang-Tuan and Wang, James Z. and Li, Jia and Luo, Jiebo},
  journal={IEEE Signal Processing Magazine}, 
  title={Aesthetics and Emotions in Images}, 
  year={2011},
  volume={28},
  number={5},
  pages={94-115},
  keywords={Emotion recognition;Photography;Semantics;Data visualization;Painting;Human factors},
  doi={10.1109/MSP.2011.941851}}

@INPROCEEDINGS{2008DPChallenge,
  author={Datta, Ritendra and Jia Li and Wang, James Z.},
  booktitle={2008 15th IEEE International Conference on Image Processing}, 
  title={Algorithmic inferencing of aesthetics and emotion in natural images: An exposition}, 
  year={2008},
  volume={},
  number={},
  pages={105-108},
  keywords={Inference algorithms;Testing;Image quality;Image analysis;Mood;Statistical analysis;Emulation;Humans;Image processing;Photography;Aesthetics;emotion;learning;datasets},
  doi={10.1109/ICIP.2008.4711702}}

@inproceedings{ijcai2022p132,
  title     = {Rethinking Image Aesthetics Assessment: Models, Datasets and Benchmarks},
  author    = {He, Shuai and Zhang, Yongchang and Xie, Rui and Jiang, Dongxiang and Ming, Anlong},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {942--948},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/132},
  url       = {https://doi.org/10.24963/ijcai.2022/132},
}
@InProceedings{visc2009,
author="Forsythe, Alexandra",
editor="Harris, Don",
title="Visual Complexity: Is That All There Is?",
booktitle="Engineering Psychology and Cognitive Ergonomics",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="158--166",
abstract="Visual complexity is conventionally defined as the level of detail or intricacy contained within an image. This paper evaluates different measures of complexity and the extent to which they may be compromised by a familiarity bias. It considers the implications with reference to measures of visual complexity based on users' subjective judgments and explores other metrics which may provide a better basis for evaluating visual complexity in icons and displays. The interaction between shading and complexity is considered as a future direction for the empirical study of visual complexity.",
isbn="978-3-642-02728-4"
}
@article{Snodgrass1980ASS,
  title={A standardized set of 260 pictures: norms for name agreement, image agreement, familiarity, and visual complexity.},
  author={Joan Gay Snodgrass and Mary Louise Vanderwart},
  journal={Journal of experimental psychology. Human learning and memory},
  year={1980},
  volume={6 2},
  pages={
          174-215
        },
  url={https://api.semanticscholar.org/CorpusID:16871046}
}

@ARTICLE{Siahaan2016,
  author={Siahaan, Ernestasia and Hanjalic, Alan and Redi, Judith},
  journal={IEEE Transactions on Multimedia}, 
  title={A Reliable Methodology to Collect Ground Truth Data of Image Aesthetic Appeal}, 
  year={2016},
  volume={18},
  number={7},
  pages={1338-1350},
  keywords={Reliability;Crowdsourcing;Visualization;Quality assessment;Media;Multimedia systems;Image recognition;image aesthetic appeal;subjective quality assessment;Quality of Experience (QoE);crowdsourcing,;computational aesthetics;Computational aesthetics;crowdsourcing;image aesthetic appeal;quality of experience (QoE);subjective quality assessment},
  doi={10.1109/TMM.2016.2559942}}

@inproceedings{cot2024,
author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
title = {Chain-of-thought prompting elicits reasoning in large language models},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting.Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {1800},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{berube2019stealing,
  title={Stealing cookies in the twenty-first century: Measures of spoken narrative in healthy versus speakers with aphasia},
  author={Berube, Shauna and Nonnemacher, Jodi and Demsky, Cornelia and Glenn, Shenly and Saxena, Sadhvi and Wright, Amy and Tippett, Donna C and Hillis, Argye E},
  journal={American journal of speech-language pathology},
  volume={28},
  number={1S},
  pages={321--329},
  year={2019},
  publisher={ASHA}
}

@article{rethinkingct,
author = {Amy Steinberg  and Patrick D. Lyden  and Arielle P. Davis },
title = {Bias in Stroke Evaluation: Rethinking the Cookie Theft Picture},
journal = {Stroke},
volume = {53},
number = {6},
pages = {2123-2125},
year = {2022},
doi = {10.1161/STROKEAHA.121.038515},
URL = {https://www.ahajournals.org/doi/abs/10.1161/STROKEAHA.121.038515},
eprint = {https://www.ahajournals.org/doi/pdf/10.1161/STROKEAHA.121.038515},
abstract = {Despite a current emphasis on equity in stroke care, one of the most common stroke assessment tools that is used both nationally and internationally, includes an anachronistic image that projects cultural, linguistic, and socioeconomic bias. This image, titled The Cookie Theft picture, is included in the National Institutes of Health Stroke Scale and was originally developed in 1972. Now, 50 years later, it does not reflect our current diverse, linguistically rich, and multicultural patient population.}}

@article{HUSSEIN2015152,
title = {Arabic cross cultural adaptation and validation of the National Institutes of Health Stroke Scale},
journal = {Journal of the Neurological Sciences},
volume = {357},
number = {1},
pages = {152-156},
year = {2015},
issn = {0022-510X},
doi = {https://doi.org/10.1016/j.jns.2015.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S0022510X15004438},
author = {Haitham M. Hussein and Amr {Abdel Moneim} and Tamer Emara and Yousry A. Abd-elhamid and Haitham H. Salem and Foad Abd-Allah and Mohammad A. Farrag and M. Amir Tork and Ali S. Shalash and Khaled H. {Ezz el dein} and Gamaleldin Osman and Shady S. Georgy and Peter G. Ghali and Patrick D. Lyden and Ramez R. Moustafa},
keywords = {NIHSS, Cross-cultural, Translation, Arabic, Ischemic stroke, Neurological examination, Stroke scale, Stroke severity},
abstract = {Introduction
The National Institutes of Health Stroke Scale (NIHSS), the most commonly used tool to quantify neurological deficit in acute stroke, was initially developed in English. We present our experience in developing and validating an Arabic version of the NIHSS (arNIHSS).
Methods
A)Scale development phase: 6 bilingual neurologists translated the scale to Arabic. Items 9 and 10 were modified to suit the Arabic language and culture. A panel of 11 Arab neurologists reviewed the final product and an Arabic language expert did final editing.B)Scale validation phase: 10 examiners (four neurology residents and six nurses), who had no experience with the NIHSS, were trained to use the arNIHSS. Patients with acute stroke were recruited at two academic institutions in Egypt. Each patient was examined on admission by 3 examiners using the arNIHSS and at 24hours by one of the three examiners. The agreement between the first three examinations was used to calculate the interrater agreement. The agreement between the admission and the 24-hour arNIHSS performed by the same examiner was used to calculate the intrarater agreement. Construct validity was evaluated by correlating the arNIHSS on admission with the infarct volume on initial the diffusion weighted imaging (DWI) using the Alberta Stroke Program Early CT score (DWI-ASPECTS) and the functional outcome at 3months assessed by the modified Rankin Scale (mRS).
Results
In 6months, 137 patients were recruited (mean age±standard deviation 62±12years; 48 women). For interrater agreement, weighted kappa value ranged from 0.36 to 0.66 and intraclass correlation coefficient (ICC) for the whole scale was excellent at 0.95 (95% confidence interval [CI] 0.94–0.97). For intrarater agreement, weighted kappa ranged from 0.52 to 1.0 and the ICC was 0.94 (95% CI 0.87–0.98). The construct validity of the arNIHSS is demonstrated by its correlation with the DWI-ASPECT and the 3months mRS score (Spearman correlation −0.46 and 0.58 respectively; P<0.001 for both).
Conclusion
We developed and validated a culturally adapted Arabic version of the NIHSS. Further validation in other Arab countries is recommended.}
}

@article{DOMINGUEZ2006476,
title = {Spanish Cross-Cultural Adaptation and Validation of the National Institutes of Health Stroke Scale},
journal = {Mayo Clinic Proceedings},
volume = {81},
number = {4},
pages = {476-480},
year = {2006},
issn = {0025-6196},
doi = {https://doi.org/10.4065/81.4.476},
url = {https://www.sciencedirect.com/science/article/pii/S0025619611618958},
author = {Raúl Domínguez and José F. Vila and Federico Augustovski and Vilma Irazola and Pablo R. Castillo and Roberto Rotta Escalante and Thomas G. Brott and James F. Meschia},
abstract = {OBJECTIVES
To adapt and validate a Spanish-language version (SV) of the National Institutes of Health Stroke Scale (NIHSS) to facilitate its use in Spanish-speaking contexts.
PATIENTS AND METHODS
The methods recommended by the International Quality of Life Assessment Project were followed. Two forward translations and 1 back translation of the NIHSS were developed to ensure lingual and cultural equivalence. A final revised SV-NIHSS was administered by 8 physicians to patients with stroke in 3 clinics in Buenos Aires, Argentina, from September 2003 to December 2003.
RESULTS
The study included 102 patients (mean ± SD age, 73.3±6.5 years; 56% women) with stroke (86% ischemic). The SV-NIHSS mean baseline score was 9.78±7.04. Interrater reliability was independently evaluated for 98 patients, showing a high agreement: κ, 0.77 to 0.99 for the 15 items; interrater correlation coefficient, 0.991 (95% confidence interval, 0.987-0.994). Intrarater reliability was excellent: κ, 0.86 to 1.00 for the 15 items; mean intrarater correlation coefficient, 0.994 (95% confidence interval, 0.991-0.996). Construct validity was also adequate; the SV-NIHSS had a negative correlation with baseline Glasgow Coma Scale (Spearman coefficient = -0.574, P<.001) and with Barthel index at 3 months (Spearman coefficient = -0.658, P<.001). Patients with different Rankin scores at 3 months also had significantly different baseline SV-NIHSS scores, from a mean of 4.29±2.21 for Rankin score of 0 to a mean of 29.40±3.97 for Rankin score of 6 (P<.001).
CONCLUSION
This study shows that a Spanish-language version of the NIHSS developed with internationally recommended methods is reliable and valid when applied in a Spanish-speaking setting.}
}

@article{Oh2012ValidityAR,
  title={Validity and Reliability of a Korean Version of the National Institutes of Health Stroke Scale},
  author={Mi Sun Oh and Kyung Ho Yu and Ju-Hun Lee and San Jung and I S Ko and Joon Hyun Shin and Soo-Jin Cho and Hui-Chul Choi and Hyang Hee Kim and Byung‐Chul Lee},
  journal={Journal of Clinical Neurology (Seoul, Korea)},
  year={2012},
  volume={8},
  pages={177 - 183},
  url={https://api.semanticscholar.org/CorpusID:12210538}
}
@article{Prasad2012ValidationOT,
  title={Validation of the Hindi version of National Institute of Health Stroke Scale.},
  author={Kameshwar Prasad and Deepa Dash and Amit Kumar},
  journal={Neurology India},
  year={2012},
  volume={60 1},
  pages={
          40-4
        },
  url={https://api.semanticscholar.org/CorpusID:22448831}
}

@article{Rombach2021HighResolutionIS,
  title={High-Resolution Image Synthesis with Latent Diffusion Models},
  author={Robin Rombach and A. Blattmann and Dominik Lorenz and Patrick Esser and Bj{\"o}rn Ommer},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={10674-10685},
  url={https://api.semanticscholar.org/CorpusID:245335280}
}


@InProceedings{pmlr-v139-ramesh21a,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@article{Ramesh2022HierarchicalTI,
  title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.06125},
  url={https://api.semanticscholar.org/CorpusID:248097655}
}

@inproceedings{Kim2021ViLTVT,
  title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author={Wonjae Kim and Bokyung Son and Ildoo Kim},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231839613}
}

@inbook{torch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
title = {PyTorch: an imperative style, high-performance deep learning library},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",
}