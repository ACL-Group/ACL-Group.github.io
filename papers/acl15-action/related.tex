\section{Related Work}
\label{sec:related}
We first review previous work on selectional preference and 
frame semantics lexicons, which can be seen as alternate ways of
producing abstract concepts for verb arguments, then discuss
some known work on semantic representation of verbs. 

\subsection{Selectional Preference}
The most related work to our problem (AC) is
selectional preferences (SP), which
aims at computing preferences over arguments indicated by a verb,
given the fact that some arguments are more
suitable for certain verbs than others. For example,
``drink water'' is more plausible than ``drink desk''.
While our problem defines a controllable level of abstraction
for verb arguments, selectional preference
often outputs all possible classes for the arguments.
Moreover, SP doesn't consider the overlap between concepts
which results in highly similar classes/concepts.
%We have compared our approach with SP
%in \secref{sec:eval} and concluded that by adding the overlap constraint,
%we can generate lexicons that are more suitable for semantic analysis.

There are several approaches to computing SP.
%The most relevant ones to this paper are {\em class-based} SP approaches.
The original {\em class-based} approaches 
generalize arguments extracted from corpus to human readable
concepts using a taxonomy such as WordNet.
%They first extract class-representation of the verb's seen arguments
%and then generalize to unseen arguments according
%to extracted classes.
%These classes can be seen as the conceptualization of verb arguments.
The most representative of such approach was 
by Resnik~\shortcite{resnik1996selectional},
which is used as a baseline in this paper.
%They used WordNet\cite{wordnet} as candidate set to find the classes for
%a verb's arguments.
%They propose an preference score based on the entropy
%called selectional association to measure the preferences
%of a verb on classes. To measure the preference on words,
%they use the maximum selectional association among
%all classes that the word belongs to.
%He calculated preference score for a predicate as
%$$
%A(p,c)=\frac{p(c|p)log\frac{p(c|p)}{p(c)}}{\sum_{c'\in C}{p(c'|p)log\frac{p(c'|p)}{p(c')}}},
%$$
%where $p$ is predicate, $c$ is a WordNet synset and $C$ is the collection of WordNet synsets,
%thus obtained the associational strength of synset $c$
%on the predicate by measuring the difference between the prior for $c$
%and the probability of $c$ given predicate.
%To rank words, which belong to several synsets, for predicate,
%Resnik went through all the possible $c$ for word $w$,
%and assign the maximum preference score to $w$. We implemented
%this method and compared its accuracy with our algorithms.
Instead of WordNet,
Pantel et al.\shortcite{pantel2003clustering}
proposed a clustering method (named CBC) to
automatically generate semantic classes, which are nonetheless
not human readable. 
%to overcome the problem of rare senses and the
%lack of domain specific senses in WordNet.
%They aim to find several tight clusters for words
%which are distinct to each other in the feature (TF-IDF)
%space to represent the semantic classes.

{\em Cut-based} approach~\cite{li1998generalizing} 
looks for an appropriate level of generalization of classes
on WordNet hierarchy, to prevent class representation of arguments
from being too specific or too general. However, they assumed each word 
belongs to only one semantic class which is not true for multi-sense words.

%They induced the problem to
%estimating a tree cut model on a thesaurus tree, and the method is
%based on Minimum Description Length (MDL) principle.
%The thesaurus tree was built on top of WordNet in which leaf nodes are
%nouns and internal nodes are noun classes.
%MDL serves as a criterion for the best cut, representing the
%tradeoff between model complexity and goodness.
%They fitted the model
%by minimizing the sum of model description length and
%data description length.
%The resulting best tree cut forms several sets of semantic classes
%in the appropriate generalization level. Then, the same technique as
%class-based SP was applied to compute the preference score.

{\em Similarity-based} SP\cite{clark2001class,erk2007simple},
and {\em generative model-based} SP\cite{Ritter:2010} are less relevant to
our problem. Similarity-based SP
methods focus on the relation between verb and argument without considering 
classes, and cannot generalize the arguments. Probabilistic model-based SP 
such as LDA-SP use latent classes which are not human readable.
Each class is a probability distribution over arguments. 
Recently, a random walk approach is proposed \cite{Tian13}, 
but it is still not capable of generalizing
the arguments to classes/concepts.

%The next category of methods are {\em similarity-based}.
%Clark and Weir\cite{clark2001class} proposed another method
%of determining the appropriate level of generalization.
%They use Chi-square test to determine whether a class should
%be selected or expended to some of its children in the taxonomy. Specifically,
%Chi-test measures how similar the probabilities between the parent class
%and children classes are, given the same argument and verb.
%A significant difference indicates that the parent class is an appropriate
%generalization. Otherwise, the algorithm
%continues to test the child classes. Erk\cite{erk2007simple} proposed
%to make use of the similarity between arguments to compute a
%heuristic preferences score for ranking arguments.
%These two distribution similarity based methods have no
%ability to generalize, which is important for us.
%
%Recently, a {\em generative model} approach called LDA-SP
%was proposed by Ritter et al.\cite{Ritter:2010}. They apply a Link-LDA model
%to extract latent argument classes for each verb. However,
%each latent class learned by this method is a word distribution,
%which is not human readable.

%In CBC, the centroid of a cluster is constructed by
%averaging the feature vectors of a subset of the cluster members.
%And the subset is the committee that determines what
%other elements belong to the cluster. In the CBS algorithm,
%committee is constructed as follows: first compute each element's
%top-$k$ similar elements ($k$ is small), then construct
%a collection of tight clusters using the elements above,
%and now those elements serve as committee in each cluster.
%If a newly formed committee is similar to existed committee,
%then it is discarded. In the end, each element $e$ is
%assigned to its most similar clusters.

%Other approaches to solve SP include
%similarity-based methods \cite{dagan1999similarity, erk2007simple},
%discriminative approach \cite{bergsma2008discriminative},
%and generative probabilistic models \cite{rooth1999inducing,
%ritter2010latent, seaghdha2010latent}.
%These methods do not use classes as intermediate representation,
%thus less related to our work.

\subsection{Frame Semantics Lexicons}
%Another related problem is Semantic Role Labeling (SRL), which aims at
%assigning semantic roles to a verb's related arguments.
%The set of roles are universal and predefined,
%and consequently SRL provides coarse-grained output compared
%to our action concepts. The task is commonly solved
%by supervised learning algorithms with manually labeled data.
%%which are two significant deviations from our problem.
%
%Some proposals based on grammatical rules were first introduced to solve
%the SRL problem, including Link parser\cite{sleator1995parsing} and
%MiniPar\cite{lin1994principar}. Rule-based approaches require
%a large mount of human efforts for building rules and are limited to specific domains.
%
FrameNet is a manually annotated semantic role lexicon,
which defines around 1200 semantic frames based on
the theory of Frame Semantics. Each semantic frame specifies an event
and all roles related to the event. Each role in the event
is also called a frame element.
%For example, in the sentence ``Will you help the Government
%find your brother?'',
%``help'' evokes a frame {\em Assistance}.
%{\em Assistance} has several frame elements,
%including {\em Helper}: ``you'', {\em Benefited party}:
%``the Government'', and {\em Goal}: ``find your brother''.
%Different terms can evoke the same frame, e.g., ``assist''
%and ``aid'' may also evoke frame {\em Assistance}.
%In our work, action frames which are similar can be generated
%automatically and the results was shown in \secref{sec:eval}.
PropBank labels verbs and their arguments
in the sentence without generalization.
VerbNet\cite{KipperDP00} focuses on verbs and provides mapping
between lexical resources such as FrameNet, PropBank and WordNet.
Because all these lexicons are manually curated, their scales are
limited: FrameNet 1.5 (the latest publicly available  version) 
has the annotations for just 2385 annotated verbs, 
whereas our automatically constructed lexicon only only 
covers 90\% of the verbs in FrameNet,
but also many more frequently used verbs that are not found in
FrameNet.  

%Several machine learning algorithms\cite{gildea2002automatic,pradhan2004shallow,pradhan2005semantic,marquez2008semantic}
%using large scale annotated
%corpora, such as FrameNet\cite{baker1998berkeley}
%and PropBank\cite{kingsbury2002treebank} as training data
%are used to learn domain-specific, automatic semantic role labeler.
%
%In order to do SRL automatically, Jurafsky and Gildea\cite{gildea2002automatic} proposed one of the fundamental approaches to the problem by using WordNet and FrameNet. A supervised classifier is trained based on both syntactic and semantic features extracted from corpus, including
%\begin{description}\setlength{\itemsep}{-\itemsep}
%\item[phrase type]Directly from constituent parse
%\item [governing category] Indication if a Noun Phrase(NP) is subject or object of the verb
%\item [parse tree path] Path from target word
%\item [position] Where is the constituent regarding to the predicate
%\item [voice]Active/passive concluding from passive identifying patterns
%\item [head word]Head word of constituent
%\end{description}
%
%More works are done following the idea (\cite{pradhan2004shallow,
%pradhan2005semantic}), they cooperating more features in
%svm training including name entities in constituents, part of
%speech tag of the headword, etc, and explore more on automatic SRL
%by extending basic features and changing machine learning algorithms,
%and state-of-art performance is proposed by Marquez et al
%\cite{marquez2008semantic}.
%
%
%One of our approaches applying The Strength Pareto Evolutionary Algorithm 2 (SPEA2) to our problem, in which we induce our problem to a multi-objective issue and aiming at finding the approximation of the Pareto set. In our problem, we consider several different objectives to achieve both statistical representative and human plausible results. Considering objectives as vector, defined by Pareto dominance, objective vertor $y^1$ dominate objective vector $y^2$ indicate that all components of $y^1$ is larger or equal to $y^2$ and at least one component of $y^1$ is larger than that of $y^2$. Thus optimal solutions are solutions not dominated by any other solutions, i.e. Pareto set, and such solutions may not be unique. To deal with the large search space of optimal solutions, evolutionary process is introduced to approximate Preto Set. The basic idea is keeping good results while randomly generating new "population" expecting some of them are results worth remaining, during the procedure two selections have to be done:mating selection, which decides which solutions will be used in generating population; and environmental selection, which decides which solutions are survived.
%}

\subsection{Semantic Representation of Verbs}
From past literature, the semantics of a word (including verbs) can be
represented by the {\em context} it appears~\cite{}. 
There are a number of ways to define the context. The simpliest is by the 
words from a surrounding window. 
A slightly different type of context takes advantage of structural information
in the corpus, e.g., Wikipedia. The third type of context comes from 
a knowledge base or lexicon,
such as WordNet. For a verb, the gloss, its hypernyms, huponyms, antonyms and
synonyms can be used as its context~\cite{}. 
Finally, most recently, the dependency structure surround the verb in a 
sentence has been used as its context~\cite{}. This is also the approach
adopted in this paper. 

With different types of context, a common way to represent a verb is by
a vector of distributional properties, extracted from the contexts within
a large corpus. For example, LSA \cite{DeerwesterDLFH90} 
uses the window of words context, 
while ESA \cite{GabrilovichM07:ESA} uses the TF-IDF score of the word w.r.t.
the article it appears to form a vector of Wikipedia concepts.
Another popular approach is to map the word distribution in the context
into another high-dimensional space, which is known as word embedding~\cite{}.
Our approach can be thought of mapping the words in the context, in this
case, the subject-verb-object triplets into a concept space.
 

%\subsection{Automatic Lexicon Construction}
%Several efforts have been devoted to automatically
%generating lexicons for semantic analysis.
%PATTY\cite{nakashole2012patty} is a lexicon of binary relations
%which are not limited to predicates.
%%Similar to conceptualizing verb arguments in this paper,
%Binary relation patterns in PATTY are represented by
%%a linguistic pattern and ontological types, which is
%semantic classes from YAGO2\cite{SuchanekKW07}.
%PATTY can also generate subject-predicate-object triples from
%the binary relations\cite{nakashole2013discovering}. However, PATTY
%focuses on mining relations, with no constraints on the
%numbers of possible semantic classes or the overlap between
%them.
%%This goes against our main objective of minimizing
%%the number of concepts for each arguments.
%%Moreover, there is also no constraint on overlap between two ontological
%%types.

%ReVerb is an open information extraction project, aiming to
%extract relations between two terms. However, all the relations in
%ReVerb are not generalized such that it cannot recognize any
%relation that it has never witnessed before. Thus ReVerb
%is often insufficient for large scale semantic analysis.
%%During the construction of relational extractions knowledge base,
%There exist several pieces of work to generalize ReVerb.
%Velardi et al.\cite{velardi2013open} induce an ontology
%using instances of relations in ReVerb.
%%, thus extracting concept
%%level relation.
%Min et al.\cite{min2012ensemble} use ReVerb data
%as a corpus and extract relations in an unsupervised way.
%ReVerb instances are then clustered to get semantic classes,
%and the final output would be $\langle$semantic class,
%relation, semantic class$\rangle$ triples.
%The generalization processes in these proposals are only based
%on the relation instances in ReVerb, without using external
%knowledge, i.e., taxonomies. Therefore, these proposals still suffer
%from a limited scale.
%
%%\KZ{Is there any work that makes use of ReVerb or Google n-gram data
%%to do verb semantic analysis in general?}
%
%Google syntactic n-gram data, extracted from English books,
%was also used for understanding the semantics of plain texts.
%Polajnar et al.\cite{polajnar2013learning}, use the
%verb's subjects and objects in the data
%to construct verb tensors to represent the semantics
%of transitive verbs; Welke et al.\cite{welkegrounded} focused
%on using the prepositional relations between objects and locations
%to make the robots understand the position of objects; others
%\cite{borin2013mining,riedlscaling,kaiserextracting}
%used the data as a general corpus.
%This corpus is the primary source of the action instances
%that we collect for building ActionNet. We choose this 
%corpus rather than extracting action instances from web data 
%because it's less noisy than web data and has very good 
%coverage of common English verbs. One potential problem 
%with this corpus is that it is not as big as the web data 
%and may not have recently created new verbs.
%However, the technique we develop in this paper can be 
%applied to other data sources to generate corresponding 
%lexicons.
%%We have also reported the action concepts
%%extracted from the Google n-gram data. The lexicon learned from
%%Google n-gram can achieve high accuracy and low overlap.
%%However, this
%%data set is small compared to the web scale: we only extracted
%%2323 verbs, hence there are not as many verb-subject and verb-object pairs
%%as the web data. Moreover, books are written in formal
%%language and newly created terms are often not covered in this data.
%%
%%As for search algorithm, we apply Simulated annealing(SA) to solve our problem. SA is a probabilistic metaheuristic obtaining approximation of the global optimum in a large search space. SA gets satisfying approximation for our NLP problem within reasonable time comparing to other search algorithm such as gradient descent.
