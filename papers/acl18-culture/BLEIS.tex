\section{Task 2: Finding most similar words for slang across languages}
\label{sec:bleis}
%In this section, we evaluate our model on the second task, which   
%In this section, we first introduce the ground truth and baseline methods for comparison. Then, we analyze the experimental results quantitatively and qualitatively.
\textbf{Task Description:} This task is to find the most similar English words of a given Chinese slang term in terms of its slang meanings and sentiment, and vice versa. 
The input is a list of English/Chinese slang terms of interest and two monolingual social media corpora; the output is a list of Chinese/English word sets corresponding to each input slang term.
Simply put, for each given slang term, we want to find a set of the words in a different language that are most similar to itself and thus can help people understand it across languages.
We propose Average Cosine Similarity (\secref{sec:exp}) to evaluate a method's performance with the ground truth (presented below).

\subsection{Ground Truth}
\label{sec:ground}
\textbf{Slang Terms.}
We collect the Chinese slang terms from an online Chinese slang glossary\footnote{\scriptsize {\url{https://www.chinasmack.com/glossary}}} consisting of 200 popular slang terms with English explanations. 
For English, we resort to a slang word list from OnlineSlangDictionary\footnote{\scriptsize {\url{http://onlineslangdictionary.com/word-list/}}} with explanations and downsample the list to 200 terms.
%To evaluate the performance of our model, 
%we propose to build the ground truth based on above-mentioned explanations from the glossary and slang dictionary. 
%Since the subtle and latent semantics of slang are too difficult to translate without losing any information, exact translation are always missing.

\noindent
\textbf{Truth Sets.}
For each Chinese slang term, its truth set is a set of words extracted from its English explanation. 
For example, we construct the truth set of
the Chinese slang term ``二百五'' by manually extracting significant words about its slang meanings (bold) in the glossary:
%\vspace{-10pt}
\begin{description}
	\item[二百五:] A \textbf{\textit{foolish}} person who is lacking in sense but still \textbf{\textit{stubborn}}, \textbf{\textit{rude}}, and \textbf{\textit{impetuous}}.
\end{description}



\noindent
Similarly, for each English slang term, its  Chinese word sets are the translation of the words hand picked
from its English explanation.
%Different methods should produce a list of translation terms 
%as similar as possible to the ground truth target terms.

%and then computing the average similarity between the source term and the target terms is a better approach to evaluate a bilingual slang lexicon induction system.



%what are the good word translation that best preserve and convey the meaning, sentiment tendency and usage context ot the original slang. 
%However most of the slangs possess most subtle senses that are related to native culture background, general characteristic and language style of netizens in different language worlds. 
%Thus it is really difficult, if not impossible, to find a exact word in another language that carries the exact same meaning with the original slang. 
%Because of this, our ground truth does not pursue exact Internet slang translation from one language to another's corresponding slang, since most likely such slang does not exist yet. 
%Our aim for the task is using IV (in-vocabulary) normal related words in target language to describe and translate the OOV (out-of-vocabulary) slang words in another language, which is more viable and feasible, and easier for people in different culture/language to understand not just literal meaning but the deeper buried and more subtle sense and context it conveys.
%
%Following this goal, we could build our ground truth based on filtered glossary. For Chinese slangs, we tokenize and lemmatize the definition sentences in English and manually remove the stop words that does not contribute to the meaning of a specific slang, left with a list of English words that have either same meaning or high relatedness to the Chinese slang.
%The same process is applied to English slang glossary as well, with the only difference is that due to the lack of direct Chinese definitions, we have to use Google Translate to translate the definition sentences to Chinese and human annotators tokenize, filter and paraphrase the definitions into Chinese word lists, resulting in the ground truth of slangs in the same format as the Chinese ones.
%Note that we do not manually add word translations by ourselves, we only delete irrelevant words for later evaluation, thus lowering the human error, cross-cultural and bilingual requirement to the minimum.

\begin{table}[t] 
	\small
	\centering
	\begin{subtable}[h]{\columnwidth}
		\centering
		\begin{tabular}{|ccccc|}
			\hline
			Gg&  Bi& Bd & CC & LT   \\ 
			18.24 &  16.38&  17.11 & 17.38 & 9.14 \\ \hline   
			TransBL& MCCA & MCluster & Duong   & SV \\ 
			18.13 &  17.29 & 17.47&  20.92& \textbf{23.01}\\ \hline  
		\end{tabular}
		\subcaption{Chinese Slang to English\vspace{5pt}}
	\end{subtable}
%	\vfill \hfill 
	
	\begin{subtable}[h]{\columnwidth}
		\centering
		\begin{tabular}{|ccccc|}
			\hline
			Gg&  Bi& Bd &  LT   & TransBL\\ 
			6.40  &   15.96 &  15.44  & 7.32 & 11.43\\ \hline   
			MCCA & MCluster & Duong   & SV & \\ 
			15.29 & 14.97&  15.13& \textbf{17.31} & \\ \hline  
		\end{tabular}
		\subcaption{English Slang to Chinese}
	\end{subtable} 
	\caption{ACS Sum Results of Slang Translation}
	\label{tab:bleis_acs}
\end{table}

\begin{table*}[th!]
	\small
	\centering
	\caption{Bidirectional Slang Translation Examples Produced by \socvec}
	\begin{tabular}{L{1.2cm}|L{4.7cm}|L{1.7cm}|L{1.7cm}|L{1.7cm}|L{2.8cm}}
		\textbf{Slang} & \textbf{Explanation} & \textbf{Google}& \textbf{Bing}& \textbf{Baidu} & \textbf{Ours} \\ \hline 
		浮云 &something as ephemeral and unimportant as ``passing clouds''& clouds& nothing& floating clouds & nothingness, illusion \\ \hline
		水军 &``water army'', people paid to slander competitors on the Internet and to help shape public opinion& Water army& Navy& Navy & propaganda, complicit, fraudulent\\ \hline
		%		城管 & ``City administrators'', who enforce city regulations, with poor reputation as being corrupt and violent, best known for physically bullying illegal street peddlers & urban management& urban management& urban management & terrorist, rioting, threaten\\ \hline \hline
		floozy & a woman with a reputation for promiscuity & N/A&劣根性 (depravity)&荡妇(slut)&骚货(slut),妖精(promiscuous)\\ \hline
		fruitcake& a crazy person, someone who is completely insane & 水果蛋糕 \quad(fruit cake)&水果蛋糕 \qquad(fruit cake)&水果蛋糕 \quad(fruit cake)& 怪诞(bizarre),厌烦(annoying)\\  
		%		nonce &  A person convicted (or simply guilty) of sexual crimes, especially pedophilia. Or a common British insult regardless of the tendencies of the person &随机数 (random numbers)&杜撰 (fabricate)&杜撰 (fabricate) & 伤风败俗(immoral),十恶不赦(extremely evil),畜类(beast),令人发指(heinous)\\ \hline
	\end{tabular}
	\label{tab:bleis_3}
\end{table*}
\subsection{Baseline and Our Methods} 
We propose two types of baseline methods for this task. 
The first is based on well-known {\em online translators}, 
namely Google (Gg), 
Bing (Bi) and Baidu (Bd). Note that experiments using them are done in August, 2017.
%With our test set's slang as input, we retrieve the output of translation. 
Another baseline method for Chinese is  CC-CEDICT\footnote{\scriptsize {\url{https://cc-cedict.org/wiki/}}} (CC), an online public Chinese-English dictionary, which is constantly updated for popular slang terms. 

Considering situations where many slang terms have literal meanings, it may be unfair to retrieve target terms from such machine translators by solely inputing slang terms without specific contexts. 
Thus, we utilize example sentences of their slang meanings from some websites (mainly from Urban 
Dictionary\footnote{\scriptsize {\url{http://www.urbandictionary.com/}}}). 
%as input to them, so that the translators have a greater chance of knowing this is a slang use, rather than an ordinary term. \footnote{Nevertheless, we noticed
%that on-line translators often cannot capture such slang contexts and still produce literal translations.}
The following example shows how we obtain the target translation terms for the slang word ``fruitcake'' (an insane person):

Input sentence: {\textit{Oh man, you don't want to date that girl. She's always drunk and yelling. She is a total \underline{\textbf{fruitcake}}.}}\footnote{\scriptsize {\url{http://www.englishbaby.com/lessons/4349/slang/fruitcake}}} 


{Google Translation:}
{\small 哦, 男人, 你不想约会那个女孩。她总是喝醉了, 大喊大叫。她是一个\underline{\textbf{水果蛋糕}}。}

%Since all possible target terms come from the bilingual lexicon, 
Another lines of baseline methods is scoring-based.
The basic idea is to score all words in our bilingual lexicon and consider the top K words as the target terms. 
Given a source term to be translated,  the Linear Transform (LT), MCCA, MCluster and Duong methods score the candidate target terms by 
computing cosine similarities in their constructed bilingual vector space (with the tuned best settings in previous evaluation). 
A more sophisticated baseline (TransBL) leverages the bilingual lexicon: 
for each candidate target term $w$ in the target language, we first obtain its translations $T_w$ back into the source language and then calculate the average word similarities between the source term and the translations $T_w$ as $w$'s score. 
%Now over 20,000 words in the other language of the bilingual lexicon have their corresponding similarity score to the given slang. 
%A word may have multiple possible translation words in the other language. In this case, we choose to take average over all of them in terms of similarity score.
%We then rank the words by their scores and take top 5 words to form a word set, while other online translation baselines directly produce a word set for later comparison with the ground truth word set. 

Our {\em SocVec-based method} (\textbf{SV}) is also scoring-based. It simply calculates the cosine similarities between the source term and each candidate target term within \textit{\socvec} space as their scores.

\subsection{Experimental Results}
\label{sec:exp}

To quantitatively evaluate our methods, we need to measure similarities between 
a produced word set and the ground truth set. 
Exact-matching Jaccard similarity is too strict to capture 
valuable relatedness between two word sets.
We argue that average cosine similarity (ACS) between two sets of word 
vectors is a better metric for evaluating the similarity between two word sets.
\begin{equation*}
	\text{ACS} (A,B)=
	{\frac{1}{|A||B|}}{\sum_{i=1}^{|A|}{\sum_{j=1}^{|B|}} \frac{\mathbf{A_i }\cdot \mathbf{B_j}}{\|\mathbf{A_i }\|\|\mathbf{B_j }\|}}
\end{equation*}
The above equation illustrates such computation, where $A$ and $B$ are the two word sets:
{$A$ is the truth set and $B$ is a similar list produced by each method. In the previous case of ``二百五'' (\secref{sec:ground}), $A$ is \{foolish, stubborn, rude, impetuous\} while $B$ can be \{imbecile, brainless, scumbag, imposter\}.}
$\mathbf{A_i}$ and $\mathbf{B_j}$ denote the word vector of the $i^{th}$ word in $A$ and $j^{th}$ word in $B$ respectively. 
The embeddings used in ACS computations are pre-trained \textit{GloVe} 
word vectors\footnote{\scriptsize \url{https://nlp.stanford.edu/projects/glove/}} and thus the computation is fair among different methods.
%\tabref{tab:bleis_acs} shows the sums of ACS over 200 slang translations. 
\begin{table}[t]
	\small
	\centering
	\caption{{Slang-to-Slang Translation Examples}}
	\begin{tabular}{C{1.92cm} C{2.5cm} C{2.0cm}}
		\textbf{Chinese Slang} & \textbf{English Slang} & \textbf{Explanation} \\ \hline
		萌 & adorbz, adorb, adorbs, tweeny, attractiveee & cute, adorable \\ \hline
		二百五 & shithead, stupidit, douchbag & A foolish person\\ \hline
		鸭梨 & antsy, stressy, fidgety, grouchy, badmood & stress, pressure, burden \\ \hline
	\end{tabular}
	\label{tab:bleis_4}
\end{table}

Experimental results of Chinese and English slang translation in terms of the sum of \textit{ACS} over 200 terms are shown in~\tabref{tab:bleis_acs}.
The performance of online translators for slang typically depends on human-set rules and supervised learning on well-annotated parallel corpora, which are rare and costly, especially for social media where slang emerges the most. 
This is probably the reason why they do not perform well. 
The Linear Transformation (LT) model is trained on highly confident translation pairs 
in the bilingual lexicon, which lacks OOV slang terms and social contexts around them. 
%Hence, it performs badly.
The {TransBL} method is competitive because its similarity computations 
are within monolingual semantic spaces and it makes great use of the bilingual lexicon, but it loses the information from the related words 
that are not in the bilingual lexicon.
%Experiment results of Chinese and English slang translation in terms of the sum of \textit{ACS} over the translations of 200 slang terms are shown in~\tabref{tab:bleis_acs}.
%The performance of online translators for slang terms typically depends on human-set rules and supervised learning on well-annotated parallel corpora, which are rare and costly, especially for social media where Internet slang emerges the most. 
%This could be a possible reason why they do not perform well. 
%~Linear transformation model is trained on translation pairs with high confidence in the bilingual lexicon, which contains little information about the OOV slang terms and social context on them, which is the reason why  LT method performs badly.
%\textit{BL} method is competitive for its similarity computations are within monolingual word vector spaces and uses a bilingual lexicon to transform, while it loses the information from the related words which are not in the lexicon translation pairs.
Our method (SV) outperforms baselines by directly using the distances in 
the~\textit{SocVec} space, which proves 
that the~\textit{SocVec} well captures the cross-cultural similarities between terms.
%utilizes comparable English and Chinese social media corpora and 
%encodes the context and usage of a given slang term by computing its similarities with words in the socio-linguistic vocabulary of the source language. Therefore, 
%our model keeps the cross-cultural socio-linguistic features, which is a most important reason why we outperform baselines.
%the best among all the baseline methods. 
%Then, we are able to find the most similar counterparts in the target language by computing the similarity in \textit{SocVec} space through \textit{BSL}. 
%Therefore, our performance is better than the others.    

To qualitatively evaluate our model, in~\tabref{tab:bleis_3}, 
we present several examples of our translations for Chinese and English slang 
terms as well as their explanations from the glossary.
Our results are highly correlated with these explanations and 
capture their significant semantics, whereas most online translators just offer 
literal translations, even within obviously slang contexts.
% They often offer just literal meanings as translation even with the specific slang context using the example sentences from Urban Dictionary.
We take a step further to directly translate Chinese slang terms to 
English slang terms by filtering out 
ordinary (non-slang) words in the original target term lists, with
examples shown in~\tabref{tab:bleis_4}. 
