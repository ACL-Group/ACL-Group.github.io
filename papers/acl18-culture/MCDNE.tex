\section{Task 1: Mining cross-cultural differences of named entities}
\label{sec:mcdne}

%We first explain how we obtain the ground truth from human annotators, then present several baseline methods to this problem, and finally 
%discuss our experiment results in detail.
\textbf{Task definition:}
	This task is to discover and quantify cross-cultural differences of concerns towards named entities.  
	Specifically, the input in this task is a list of 700 named entities of interest and two monolingual social media corpora; the output is the scores for the 700 entities indicating the cross-cultural differences of the concerns towards them between two corpora. 
	The ground truth is from the labels collected from human annotators.
%Thus, 

\subsection{Ground Truth Scores}
\label{sec:mcdne_truth}
\citet{harris1954distributional} states that the meaning of 
words is evidenced by the contexts they occur with. 
Likewise, we assume that the cultural properties of an entity 
can be captured by the terms they always co-occur within a large social media corpus. 
Thus, for each of randomly selected 700 named entities, we present human annotators with two lists of 20 most co-occurred terms within Twitter and Weibo corpus respectively. 
%We for annotators to label.
%,which
%are the most frequently mentioned both in Twitter and Weibo. 

Our annotators are instructed to rate the topic-relatedness between the 
two word lists using one of following labels: ``very different'', 
``different'', ``hard to say'',  ``similar'' and 
``very similar''. We do this for efficiency and avoiding subjectivity. 
As the word lists presented come from social media messages, the social and cultural elements are already embedded in their chances of occurrence.
{All four annotators are native Chinese speakers 
but have excellent command of English and lived in the US extensively, and they are trained with many 
selected examples to form shared understanding of the
labels. The inter-annotator agreement is 0.67 by Cohen's kappa coefficient, suggesting substantial correlation~\cite{Landis1977TheMO}.} 



 
\begin{table*}[th!]
	\small
	\centering
	\caption{{Selected culturally different entities with summarized Twitter and Weibo's trending topics }}
	\begin{tabular}{L{1.5cm} L{5cm} L{8cm}}
		\textbf{Entity} & \textbf{Twitter topics} & \textbf{Weibo topics}
		\\ \hline
		Maldives & coup, president Nasheed quit, political crisis & holiday, travel, honeymoon, paradise, beach \\ \hline
		Nagoya & tour, concert, travel, attractive, Osaka & Mayor Takashi Kawamura, Nanjing Massacre, denial of history\\  \hline
				Quebec & Conservative Party, Liberal Party, politicians, prime minister, power failure & travel, autumn, maples, study abroad, immigration, independence   \\ \hline
				Philippines & gunman attack, police, quake, tsunami & South China Sea, sovereignty dispute, confrontation, protest  \\ \hline
		Yao Ming & NBA, Chinese, good player, Asian  & patriotism, collective values, Jeremy Lin, Liu Xiang, Chinese Law maker, gold medal superstar   \\ \hline USC & college football, baseball, Stanford, Alabama, win, lose & top destination for overseas education, 
Chinese student murdered, scholars, economics, Sino American politics \\ \hline
	\end{tabular}
%\vspace{-10pt}
	\label{tab:mcdne_res_4}
\end{table*}
%\vspace{-10pt}
\subsection{Baseline and Our Methods} 
We propose eight baseline methods for this novel task:
%The first three are \emph{distribution}-based, while the next two 
%are \emph{transformation}-based.
\textbf{distribution-based} methods (BL-JS, E-BL-JS, and WN-WUP) compute cross-lingual relatedness between two lists of the words surrounding the input English and Chinese terms respectively ($\mathcal{L}_E$ and $\mathcal{L}_C$);
\textbf{transformation-based} methods (LTrans and BLex) compute the vector representation 
in English and Chinese corpus respectively, and
then train a transformation;
MCCA, MCluster and Duong are three typical \textbf{bilingual word representation models} for computing general cross-lingual word similarities. 

%\noindent
The $\mathcal{L}_E$  and $\mathcal{L}_C$ in the BL-JS and WN-WUP methods are the same as the lists that annotators judge.
\textbf{BL-JS} (\textit{Bilingual Lexicon Jaccard Similarity}) uses the bilingual lexicon to translate $\mathcal{L}_E$  to a Chinese word list 
$\mathcal{L}_E^*$ as a medium, and then calculates the Jaccard Similarity between 
$\mathcal{L}_E^*$ and $\mathcal{L}_C$ as $J_{EC}$. Similarly, we compute $J_{CE}$. 
Finally, we regard $(J_{EC}+J_{CE})/{2}$ as the score 
of this named entity.
%\noindent
\textbf{E-BL-JS} (\textit {Embedding-based Jaccard Similarity}) differs from BL-JS in that it instead compares the two lists of words gathered from the
rankings of word embedding similarities between the name of entities and all English words 
and Chinese words respectively. 
%\noindent
\textbf{WN-WUP} (\textit{WordNet Wu-Palmer Similarity}) uses Open Multilingual 
Wordnet~\cite{wang2013building} to compute the average 
similarities over all English-Chinese word pairs constructed from the $\mathcal{L}_E$ and $\mathcal{L}_C$.
%\noindent
 
We follow the steps of \citet{Mikolov:2013tp} 
to train a linear transformation (\textbf{LTrans}) matrix between \textit{EnVec} and \textit{CnVec}, 
using 3,000 translation pairs with maximum confidences in the bilingual lexicon. 
Given a named entity, this solution simply calculates the cosine similarity
between the vector of its English name and the \textit{transformed} vector 
of its Chinese name. 
%\noindent
\textbf{BLex} (\textit {Bilingual Lexicon Space}) is  similar to our \textit{SocVec} but it does not 
use any social word vocabularies but uses bilingual lexicon entries as pivots instead.
 
%\noindent
\textbf{MCCA}~\cite{ammar2016massively} takes two trained monolingual word 
embeddings with a bilingual lexicon as input, and develop a bilingual word 
embedding space. It is extended from the work of \citet{DBLP:conf/eacl/FaruquiD14}, which performs slightly worse in the experiments.
%\noindent
\textbf{MCluster} \cite{ammar2016massively} requires re-training the bilingual word embeddings from the two mono-lingual corpora with a bilingual lexicon.
%\noindent
Similarly,~\textbf{Duong}~\cite{duong2016learning} retrains the embeddings from monolingual corpora with an EM-like training algorithm. 
We also use our BSL as the bilingual lexicon in these methods to investigate its effectiveness and generalizability. 
The dimensionality is tuned from $\{50,100,150,200\}$ in all these bilingual word embedding methods.

%\noindent
With our constructed \textbf{\textit{\socvec}} space, given a named entity with its English and Chinese names, we can simply compute the similarity between their \textit{SocVec}s as its cross-cultural difference score. 
%Note that our method is more efficient than Duong and  because it does not require time-consuming re-training on original corpora. Thus, models can be efficiently updated and examined every time when we have a new bilingual (social) lexicon.
Our method is based on monolingual word embeddings and a BSL, and thus does not need the time-consuming re-training on the corpora. 


\subsection{Experimental Results}

For qualitative evaluation, \tabref{tab:mcdne_res_4} shows some of 
the most culturally different entities mined by the \socvec\ method. 
The hot and trendy topics on Twitter and Weibo are 
manually summarized to help explain the cross-cultural differences. 
The perception of these entities diverges widely between English and
Chinese social media, thus suggesting
significant cross-cultural differences.
{Note that some cultural differences are time-specific. We believe such temporal variations of cultural differences can be valuable and beneficial for social studies as well. Investigating temporal factors of cross-cultural differences in social media can be an interesting future research topic in this task.}
%\footnote{}


\begin{table}[t]
	\small
	\centering
	\caption{{Comparison of Different Methods}}
	\begin{tabular}{c|c|c|c}
		
		\textbf{Method} & \textbf{Spearman} & \textbf{Pearson}  & \textbf{MAP} \\ \hline
		BL-JS& 0.276 & 0.265 & 0.644   \\ 
		WN-WUP  & 0.335 & 0.349 & 0.677 \\ 
		E-BL-JS & 0.221 & 0.210  & 0.571\\ 
		LTrans& 0.366 & 0.385  & 0.644  \\
		BLex & 0.596 & 0.595  & 0.765 \\ \hline 
		
MCCA-BL(100d)&0.325&0.343&0.651\\  
MCCA-BSL(150d)&0.357&0.376&0.671\\ 
MCluster-BL(100d)&0.365&0.388&0.693\\ 
MCluster-BSL(100d)&0.391&0.425&0.713\\ 
Duong-BL(100d)&0.618&0.627&0.785\\ 
Duong-BSL(100d)&0.625&0.631&0.791 \\ \hline 
		SocVec:opn& 0.668 & 0.662   & \textbf{0.834} \\ 
		SocVec:all& \textbf{0.676} & \textbf{0.671}  & \textbf{0.834}\\ 
		SocVec:noun & 0.564 & 0.562 & 0.756 \\ 
		SocVec:verb & 0.615 & 0.618 & 0.779 \\ 
		SocVec:adj. & 0.636 & 0.639 & 0.800 \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_1}
\end{table}


In~\tabref{tab:mcdne_res_1}, we evaluate the benchmark methods and our approach with three metrics: Spearman and 
Pearson, where correlation is computed between truth averaged scores (quantifying the labels from 1.0 to 5.0) and computed cultural difference scores from different methods; Mean Average Precision (MAP), which converts averaged scores as binary labels, by setting 3.0 as the threshold. 
The \textbf{\textit{SocVec:opn}} considers only OpinionFinder as the ESV, while \textbf{\textit{SocVec:all}} uses the union of Empath and OpinionFinder vocabularies\footnote{The following tuned parameters are used in \textit{SocVec} methods: 5-word context window, 150 dimensions monolingual word vectors, cosine similarity as the \textit{sim} function,  and ``\textit{Top}'' as the pseudo-word generator.
}. 

\textbf{Lexicon Ablation Test.} To show the effectiveness of social words versus other type
of words as the bridge between the two cultures, we also compare the
results using sets of nouns (\textbf{\textit{SocVec:noun}}), verbs (\textbf{\textit{SocVec:verb}}) and adjectives (\textbf{\textit{SocVec:adj.}}).
All vocabularies under comparison are of similar sizes 
(around 5,000), indicating that the improvement of our method 
is significant.
Results show that our \textit{SocVec} models, and in particular, the \textit{SocVec} model using the social words as cross-lingual media, performs the best. 
\begin{table}[t]
	\centering
	\small
	\caption{{Different Similarity Functions}}
	\label{tab:mcdne_res_2}
	\begin{tabular}{l|c|c|c}
		\textbf{Similarity} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline
		PCorr. & 0.631 & 0.625 & 0.806\\ 
		L1 + M & 0.666 & 0.656 & 0.824 \\  
		Cos & \textbf{0.676} & 0.669 & \textbf{0.834} \\ 
		L2 + E & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
\end{table}

\begin{table}[t]
	\centering
	\small
	\caption{{Different Pseudo-word Generators}}
	\begin{tabular}{c|c|c|c}
		\textbf{Generator} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\  \hline
		Max. & 0.413 & 0.401 & 0.726\\ 
		Avg. & 0.667 & 0.625 & 0.831\\ 
		W.Avg. & 0.671 & 0.660 & 0.832 \\  
		Top & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_3}
\end{table}
\textbf{Similarity Options.} We also evaluate the effectiveness of four different similarity options in 
\textit{\socvec}, namely, Pearson Correlation Coefficient 
(\textit{PCorr}.), L1-normalized Manhattan distance (\textit{L1+M}), 
Cosine Similarity (\textit{Cos}) and  L2-normalized Euclidean distance (\textit{L2+E}).
%It is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking.
From~\tabref{tab:mcdne_res_2}, we conclude that among these four options, \textit{Cos} and \textit{L2+E} perform the best. 
%Although it is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking, we can find that 

\textbf{Pseudo-word Generators.} 
\tabref{tab:mcdne_res_3} shows effect of using four pseudo-word generator functions, from which we can infer that ``\textit{Top}'' generator function performs best for 
it reduces some noisy translation pairs. 
