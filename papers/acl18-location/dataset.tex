\section{Datasets}
\label{sec:data}
Our proposed vocabulary of single-word physical objects is constructed by the intersection of all ConceptNet concepts and all entities that belong to ``physical object'' class in \textit{Wikidata}~\cite{VK:wikidata14}. 
We manually filter out some 
words that have the meaning of an abstract concept, which 
results in 1,169 physical objects in total.

Afterwards, we utilize a cleaned subset of the Project Gutenberg corpus~\cite{lahiri:2014:SRW}, which contains 3,036 English books written by 142 authors.
% 
An assumption here is that sentences in fictions are more likely to describe real life scenes. 
We sample and investigate the density of \lnear~ relations in Gutenberg with other 
widely used corpora, namely \textit{Wikipedia}, 
used by~\citeauthor{mintz2009distant}~(2009) and \textit{New York Times} corpus~\cite{riedel2010modeling}. 
In the English \textit{Wikipedia} dump, out of all sentences which mentions at least two
physical objects, 32.4\% turn out to be positive. 
In the \textit{New York Times} corpus,
the percentage of positive sentences is only 25.1\%. 
In contrast, that percentage in the Gutenberg corpus is 55.1\%, much higher 
than the other two corpora, making it a good choice for \lnear~ 
relation extraction.
%\subsection{\lnear~ Sentences Dataset}
%\label{lsd}

From this corpus, we identify 15,193 pairs that co-occur in more than 10 sentences.
Among these pairs, we randomly select 500 object pairs and 
10 sentences with respect to each pair for annotators to label their commonsense~\lnear. 
%We do not distinguish between \lnear\ and \textsc{atLocation} relations,
%where the latter typically refers to objects which are adjacent to or
%contained by each other, e.g., {\em room} and {\em door}.
Each instance is labeled by at least three annotators who are college students
and proficient with English. 
The final truth labels are decided by majority voting. 
The Cohen's Kappa among the three annotators is 0.711 which suggests substantial agreement~\cite{Landis1977TheMO}. 
%This dataset will be used to train and test models for relation
%classification.
%We compare the statistics of our \lnear\ sentence
%dataset with a few datasets on other well known relations in 
%\tabref{tab:datasets}.
This dataset has almost double the size of those most
popular relations in the SemEval task~\cite{sem}, and the sentences in our
data set tend to be longer.
{We randomly choose 4,000 instances as the training set and 1,000 as the test set for evaluating the sentence-level relation classification task.}
%\begin{table*}[th]
%\centering
%\small
%\begin{tabular}{|l|c|c|c|c|} \hline
%Data set & Frequency & Percentage & Words per entry & Chars per word  \\ \hline \hline
%\lnear & 2,754 & 55.1 & 18.6 & 4.51  \\ \hline
%Not \lnear& 2,246 & 44.9 & 19.1 & 4.32  \\ \hline\hline
%Cause-Effect & 1,331  & 12.4 & 17.3 & 4.71\\ \hline
%Component-Whole &1,253 & 11.7 & 17.9 & 4.12 \\ \hline
%Others &1,864 & 17.4 & 17.8 & 4.34 \\ \hline
%\end{tabular}
%\caption{Comparison between our \lnear~ dataset and 
%the most popular relations from SemEval 2010 Task 8 dataset for 
%relation classification}
%\label{tab:datasets}
%\end{table*}
%\subsection{Commonsense \lnear~ object pairs}
%We randomly sampled 500 pairs of objects by the number of sentences they
%appear in. 
%This tends to give us pairs which are more popular.
For the second task, we further ask the annotators to label whether each pair of objects are likely to locate near each other in the real world. 
Majority votes determine the final truth labels.
The inter-annotator agreement here is {0.703} (substantial agreement).  
%Both datasets will be made publicly available.