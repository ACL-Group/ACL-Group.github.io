\section {Evaluation}
\label{sec:eval}
\begin{table*}[th]
	\centering
	\small
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			& Random       & Majority    & SVM  & SVM(-BW)    & SVM(-BPW) & SVM(-BAP)   & SVM(-GF)    \\ \hline
			Acc.  & 0.500        & 0.551       & 0.584                                & 0.577          & 0.556        & 0.563          & \textbf{0.605} \\ \hline
			P & 0.551        & 0.551       & 0.606                                & 0.579          & 0.567        & 0.573          & \textbf{0.616} \\ \hline
			R    & 0.500        & 1.000       & 0.702                                & 0.675          & 0.681        & \textbf{0.811} & 0.751          \\ \hline
			F1        & 0.524        & 0.710       & 0.650                                & 0.623          & 0.619        & 0.672          & \textbf{0.677} \\ \hline \hline
			& SVM(-SDP) & SVM(-SS) & DRNN & LSTM+Word    & LSTM+POS   & LSTM+Norm    &                \\ \hline
			Acc.  & 0.579        & 0.584       & 0.635                                & 0.637          & 0.641        & \textbf{0.653} &                \\ \hline
			P & 0.597        & 0.605       & \textbf{0.658}                       & 0.635          & 0.650        & 0.654          &                \\ \hline
			R    & 0.728        & 0.708       & 0.702                                & \textbf{0.800} & 0.751        & 0.784          &                \\ \hline
			F1        & 0.656        & 0.652       & 0.679                                & 0.708          & 0.697        & \textbf{0.713} &                \\ \hline
		\end{tabular}
	\caption{Performance of baselines on co-location classification task with ablation. (Acc.=Accuracy, P=Precision, R=Recall, ``-'' means without certain feature)}
	\label{tab:aprf}
\end{table*}
In this section, we first present our evaluation of our proposed methods and the state-of-the-art general relation classification model on the 
first task.  
Then, we evaluate the quality of the new \lnear~triples we extracted.
%
%\subsection{Experimental Setup}
%\label{sec:experiment}
%
%which leverages
%a four-channel input, stacked deep recurrent neural network model to represent the original sentence with original words, POS-tags, WordNet Supersenses and grammatical relations/dependency roles separately. 
%A major differences between it and our proposed LSTM-based methods is that 
%i) it only uses the information of the shortest dependency path between the two objects; 
%ii) 
%it does not take out the two objects out of the original sentence;  \BL{frank work on this plz}
%For our proposed feature-based baseline methods, 
%we trained our word embeddings on our Gutenberg corpus with window size of 5 and dimensionality of 100. 
%The pre-trained GloVe word embeddings is also 100-dimensional.
%
%We use grid searching method to tuning the hyperparameters of the SVM classifier. The candidate parameters are shown in~\tabref{tab:grid}, we do 5-fold cross-validation under each candidate situation and choose the one with the highest precision.
%
%\begin{table}[th!]
%\small
%	\centering 
%	\begin{tabular}{|c|c|c|}
%		\hline
%		\textbf{kernel} & \textbf{C} & \textbf{gamma} \\ \hline \hline
%		linear & 1,10,{100},1000 & N/A  \\\hline
%		\textbf{rbf} & 1,10,\textbf{100},1000 & $10^{-4}$,{$\mathbf{10^{-3}}$},$10^{-2}$ \\\hline
%	\end{tabular}
%\caption{Candidate Hyperparameters of the SVM Model for Grid Searching; The bold settings are the best.}
%\label{tab:grid}
%\end{table}

%As for our LSTM-based methods, we initialize the weight of embedding layer for input tokens and positions with uniform distribution, except for the lemma words, which we initialize the weight from the pre-trained embeddings. 
%The dimensionality of input token embedding layer and position embedding layer are 100 and 5 respectively. 
%For the original two physical object words, we also use the widely-used 100-dimension pre-trained GloVe word embeddings, the same as aforementioned SVM setting. 
%We set both the input dropout rate as well as the recurrent state update dropout probability as 0.5 and the number of training epochs as 40, use the validation accuracy as monitor metric and early stop with the patience of 5 epochs.
%
%We used the released code of the DRNN baseline, but with their original hyperparameter settings, it was unable to properly train their model on our dataset. The reason for this is mainly due to the large gap between tasks and corpus. Thus we manually tuned the learning rate to 0.2 down from 0.3 for best possible results.
%
%\subsection{Results and Discussion}

\subsection{Sentence-level \lnear\ Relation Classification}
We evaluate the proposed methods against the state-of-the-art general domain relation 
classification model (DRNN)~\cite{Xu2016ImprovedRC}. 
The results are shown in~\tabref{tab:aprf}.
For feature-based SVM, we do feature ablation on each of the 6 feature types. For LSTM-based model, we experiment on variants of input sequence of original sentence:
``LSTM+Word'' uses the original words as the input tokens;
``LSTM+POS'' uses only POS tags as the input tokens; 
``LSTM+Norm'' uses the tokens of sequence after sentence normalization. 
Besides, we add two naive baselines: ``Random'' baseline method
classifies the instances into two classes with equal probability. 
``Majority'' baseline method considers all the instances to be positive.

From the results, we find that the SVM model without the Global Features performs best, which indicates that bag-of-word features benefit more in shortest dependency paths than on the whole sentence.
Also, we notice that DRNN performs best (0.658) on precision but not significantly higher than LSTM+Norm (0.654). 
The experiment shows that LSTM+Word enjoys the highest recall score, while
LSTM+Norm is the best one in terms of the overall performance. 
One reason is that the normalization representation reduces the vocabulary
of input sequences, while also preserving important syntactical 
and semantic information.  
Another reason is that the~\lnear\ relation are described in sentences decorated
with prepositions/adverbs.
These words are usually descendants of the object word in the dependency tree, 
outside of the shortest dependency paths. 
Thus, DRNN cannot capture the information from the words belonging to 
the descendants of the two object words in the tree, 
but this information is well captured by LSTM+Norm. 
%For the rest of the experiments, we use LSTM+Norm.

\subsection{\lnear\ Relation Extraction}
Once we have obtained the probability score for each instance using LSTM+Norm, we can extract \lnear\
relation using the scoring function $f$. 
We compare the performance of 5 different heuristic choices of $f$, by quantitative results. 
We rank 500 commonsense \lnear\ object pairs described in \secref{sec:mine}. \tabref{tab:3m} shows the ranking results using
\textit{Mean Average Precision} (MAP) and \textit{Precision} at $K$ as the metrics. 
Accumulative scores ($f_1$ and $f_3$) generally do better. 
Thus, we choose $f = f_3$ with a MAP score of 0.59 as the scoring function.
\begin{table}[t]
	\centering
	\small
	\begin{tabular}{|cccccc|}
		\hline
		${f}$	& {MAP} & {P@50} & {P@100}  &  {P@200}& {P@300}\\ \hline \hline
		$f_0$ & 0.42 & 0.40 & 0.44 & 0.42 & 0.38 \\ \hline
		$f_1$	& 0.58  & {\bf 0.70} & 0.60& 0.53 & {\bf 0.44}\\\hline
		$f_2$	& 0.48 & 0.56 & 0.52  & 0.49 & 0.42\\\hline
		$f_3$	& {\bf 0.59} & 0.68& {\bf 0.63} & {\bf 0.55} & {\bf 0.44}\\\hline
		$f_4$	& 0.56 & 0.40 & 0.48 & 0.50 & 0.42\\\hline
	\end{tabular}
	\caption{Ranking results of scoring functions.}
	\label{tab:3m}
\end{table} 


\begin{table}[th]
	\centering
	\small
	\begin{tabular}{|ccc|}
		\hline
		(door, room)  & (boy, girl)     & (cup, tea)      \\
		(ship, sea)   & (house, garden) & (arm, leg)      \\
		(fire, wood)  & (house, fire)   & (horse, saddle) \\
		(fire, smoke) & (door, hall)    & (door, street)  \\
		(book, table) & (fruit, tree)   & (table, chair)  \\ \hline
	\end{tabular}
	\caption{Top object pairs returned by best performing scoring function $f_3$}
	\label{tbl:toppairs}
\end{table} 

Qualitatively, we show 15 object pairs with some of the highest $f_3$ scores
in \tabref{tbl:toppairs}.
Setting a threshold of 40.0 for $f_3$, which is the minimum non-zero
$f_3$ score for all true object pairs in the \lnear\ object pairs 
data set (500 pairs), we obtain a total of 2,067 \lnear\ relations, with
a precision of 68\% by human inspection.

