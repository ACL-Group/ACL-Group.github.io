\section{Related Work}
\label{sec:related}

%Summarization is the task of condensing a piece of document into a shorter paragraph or a single sentence, while retaining correctness, fluency, consistency as well as the core idea of the original document. 

%Generally, this task is tackled in two different ways: extractive and abstractive. Extractive approach \cite{Dorr2003HedgeTA,DBLP:journals/corr/NallapatiZZ16} takes sentences from the source text to compose a summary, while abstractive one \cite{RushCW15,SeeLM17,PaulusXS17} \textit{generates} sentences word by word after comprehending the source text as a whole. For abstractive method, the algorithm has a vocabulary from which it can freely choose words and phrases to compose sentences, while the extractive one is more similar to catching the key sentences of the source text. Extractive summarization can more easily produce acceptable summaries, as copying sentences from the source text guarantees correctness and consistency. However, extractive methods lacks creativity. More often than not, humans summarize documents in an abstractive manner, where they comprehend the whole document before selecting elements from their vocabulary to compose a short text that encapsulates the main idea and even the underlying intent from the source sentences. This is where a sense of expressiveness and elegance can be found.

%Many \cite{RushCW15,SeeLM17} choose to build an abstractive model using sequence-to-sequence model using recurrent neural networks and attention mechanism. In pursuit of speed and  parallelism, \cite{gehring2017convs2s} proposed a convolutional seqence-to-sequence model with Gated Linear Units \cite{DauphinFAG17}, attention mechanism to tackle a series of generation tasks. It achieves state-of-the-art accuracy in abstractive single-sentence summarization and is much faster than recurrent approaches.

%Gehring\shortcite{gehring2017convs2s} shows that
%CNN enables much faster training and more
%stable gradients than RNN without losing accuracy. 
%Bai\shortcite{bai2018empirical} shows that 
%CNN is more powerful than RNN for sequence modeling. 
%Therefore we select
%CNN seq2seq model as our basic model and compare our 
%model with those models which can be converted to CNN seq2seq models.
%Models are found to in some cases generate similar or the same words 
%and phrases repeatedly, which causes grammatical incorrectness 
%and redundancy. 
%This reflects a insufficiency in the decoder's awareness of previous generations. To address this issue,  \cite{SeeLM17} use coverage to keep track of what has been summarized, which discourages repetition in an indirect manner, \cite{PaulusXS17} propose intra-decoder attention to avoid attending to the same parts in the source text by dynamically revising attention scores while decoding. \cite{PaulusXS17} also avoid repetition in test time by directly banning the generation of repeated trigrams in beam search. However, the weakness of this method is that the action of eliminating highly probable sentences with repeated trigrams disturbs the process of beam search, giving rise to grammatically incorrect sentences, whose probabilities are actually low in the model. 
Repetition is a persistent problem in the task of 
neural-based summarization. 
Repetition in summarization is tackled broadly from two directions in recent years. 

One involves information selection or sentence
selection before generating summaries.
%The existing %hierarchical 
%RNN-based models %to deal with summarization
%can not be converted to CNN seq2seq models. 
Chen~\shortcite{P18-1063} uses an extractor agent 
to select salient sentences or highlights and then employs 
an abstractor network to rewrite these sentences.
It can not solve repetition in seq2seq model.
Li~\shortcite{D18-1205,D18-1441} represents sentence vector by words 
and predicts words from sentence vector in sequential order 
whereas CNN-based models are naturally parallelized. 
While transferring RNN-based model to CNN model, the kernel size and the number of 
convolutional layer can not be easily determined at conversion between sentence and word vector 
Therefore, we do not compare our models to those models. 
%\KZ{What about Chen? Also not parallel?
%Have you really considered carefully whethere they can be adapted to our framework?}

The other direction is to improve the memory of previously generated words.
Suzuki~\shortcite{SuzukiN17} and Lin~\shortcite{LinSMS18} 
deal with word repetition in single-sentence summaries, 
while we primarily deal with multi-sentence summaries with 
sentence-level repetition. 
There is almost no word repetition in CNN-based model.
Jiang~\shortcite{JiangB18} adds new decoder without attention mechanism.
In CNN-based model, attention mechanism is necessary to connect encoder 
and decoder.
%\KZ{The following ``thus'' is very strange. Why we compare with the following and not
%the above two?? You need to make the case stronger!}
%Thus, we convert the following models to construct our baselines.
Thus, our model also is not compared with the above models. 
The following models can be transferred to CNN seq2seq model and
are used our baselines.
%See\shortcite{SeeLM17}, Paulus\shortcite{PaulusXS17} 
%and Fan\shortcite{FanGA18} choose to adopt sequence-to-sequence approach, 
%where source document and summary are treated as two long sequences, 
%without the idea of separate sentences. 
See~\shortcite{SeeLM17} integrate coverage, 
which keeps track of what have been summarized, as a feature that helps 
redistribute the attention scores in an indirect manner,
in order to discourage repetition. 
Paulus~\shortcite{PaulusXS17} proposes intra-temporal attention and 
intra-decoder attention which dynamically revises the attention distribution while decoding. 
It also avoids repetition in test time by directly banning the generation of 
repeated trigrams in beam search. 
%These two models are RNN-based. 
Fan~\shortcite{FanGA18} borrows the idea from Paulus~\shortcite{PaulusXS17} and builds a CNN-based model. 

Our model deals with the attention in both encoders and decoders. 
Different from the previous methods, 
our \textit{attention filter mechanism} does not 
treat with the attention history as a whole data structure,  
but rather divide it into sections (\figref{fig:model_main}). 
%\KZ{I actually think it might be better to draw a diagram
%to show this in approach.} 
Previously, the distribution curve of accumulated attention scores 
for each token in the source document tends to be flat, which means critical information is washed out during decoding.
Our method amplifies most preceding attended sections so that 
important information is retained.
Given our observation that repetitive sentences in the source is 
another cause for repetition in summary, 
which cannot be directly resolved by manipulating attention values, 
we introduce \textit{sentence-level backtracking decoder}. 
Unlike \cite{PaulusXS17}, 
we do not ban repetitive \textit{trigrams} in test time. 
Our decoder regenerates a sentence that is similar to previously generated ones.
With the two modules, our model is capable of generating summaries with
natural level of repetition while retaining fluency and consistency.
