Review Report

Title:	Open-domain Concept Distractor Generation for Multiple Choice Questions
Authors:	Xinzhu Cai and Kenny Zhu
Status:	Reject
MetaReview
Comments: All reviewers agree that this paper should not be accepted in its present form.

Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper describes a method for generating incorrect but plausible distractor answers for MC questions, using a two-stage approach that involves creating a large candidate pool (candidate set generation) and filtering those down (distractor selection). Authors create a dataset of MCQs covering a variety of types of knowledge, and show that their candidate generation method mimics human-level generation of distractors best according to a set of metrics.
Strengths:

The creation of a dataset that covers various types of knowledge.

I liked that the authors implemented Probase to generate candidates, and that they showed how much better it was than other knowledge bases.

Weaknesses:

I wish authors made more clear what part of the candidate set generation/distraction selector method is a novel contribution; over-generating MC candidates and filtering those down has been done before (e.g., by Zellers et al. 2018 recently) -- this section was very hard for me to understand in general.

Using web search is a good idea, however I wish authors discussed previous work doing so; additionally, I'd love to see a discussion of reporting bias (Gordon & Van Durme 2013) -- this section was also very hard to understand in general.

Human evaluation: I would like to see more details on the human evaluation (e.g., what is "reliability" in this context).

Automated metrics: While the proposed CSG+DS method outperforms even the human-generated MCQs, I wish authors had assessed the human performance on (a subset of) the generated questions; how well do humans do at finding the right answer? how well did they do on the original QA datasets?

Reasons to accept
As is, it's not clear what the reasons are to accept this, given the weaknesses.
Reasons to reject
The contributions aren't clear, plus the weaknesses listed above.
Overall Recommendation:	2
Questions for the Author(s)
Thoughts on varying difficulty by answers? Human generated answers often have at least one easy to rule out answer.
Missing References
SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference, Zellers et al. 2018

Reporting bias and knowledge acquisition, Gordon & Van Durme 2013

Typos, Grammar, Style, and Presentation Improvements
Unfortunately, much of the paper was written in a very confusing and circuitous way, which hinders the understanding of the work done. There are also various typos throughout the paper.

Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper focuses on generating distractors for multiple choice questions. The authors proposed approaches for the two main steps: candidate generation and candidate ranking. For candidate generation, a concept-dependent approach using information in the Probase is proposed. Since Probase is large and general, it is suitable for open-domain problems. For candidate ranking step, the authors propose a feature-based ranker, which includes features from a language model and web search. The goal is to have a better trade-off between plausibility and reliability. The authors also construct a large dataset by combining several multiple choice question datasets, which could be very useful for the research community.
Strength: The performance of the proposed system is much better than all the baselines. There are several interesting ideas in both candidate generation and ranking.

Weakness: My main concern is that the presentation is not very clear in several places. I think the paper can be re-organized a bit and more explanation should be added to the proposed approach. I don't fully understand some details in the key steps.

Overall Recommendation:	2.5
Questions for the Author(s)
I don't really get the candidate generation process. Do you consider every concept in Probase in calculating equation (1)? Wouldn't that be very slow given the number of concept in Probase? What's z_w in line 269? I understand z is generated from q and a, but I'm not sure what does z_w mean. Also, what's C_{ck} and C_{wk} in equation 2?
In the paragraph before section 3, it seems reliability is captured by the LM features and web-search features. I don't understand the intuition behind how would they help the trade-off between plausibility and reliability. Perhaps adding some examples would help.

Line 458, what's the nature of the problem?

For the WP baseline, how do you generate distractor candidate?

Typos, Grammar, Style, and Presentation Improvements
The beginning of section 3.2 could be moved to the candidate generation section, since the readers didn't know that you run a topic model. This may clear out some notation problem there.
Table 2 is pretty interesting. It would be better if you can add another candidate generation baseline in this table, so that the readers can understand the effectiveness of your proposed method.

In the caption of Table 3, change "3.2" to "(Section 3.2)"


Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper presents a method for generating distractors for multiple choice questions.
Its main strengh is the method for generating distractors in open domain with Probase.

Its main weakness is that some details are missing, which make the paper hard to understand sometimes.

Reasons to accept
The use of Probase for distractor generation is interesting.
Reasons to reject
The method is unclear at some points, so the comparison of further work to these results could be difficult.
Overall Recommendation:	2
Questions for the Author(s)
p.2 the link between the trade-off between plausibility and reliability, and the need to consider the distractors together could be explained

The authors write that they devise evaluation metrics for distractor generation, but which metrics had been used before?

p.3 the probability pi is used but explained further on in the article

What do the authors mean by "concept-dependent" candidate set generation method?

Is Context-dependent conceptualization close to WSD?

In equation (1), what is z ?

The stems considered seem to be Fill in the blank questions, but it is not clearly stated in the beginning of the paper

Why is there a need for a POS similarity measure? Can hyponyms of the same concept have different POSs? And why not use the identity? Where do these POS come from?

On which corpus is the language model for the LM score learned?

What is the "correctness" score?

The dataset used compiles web sources; what are the rights on these MCQs?

Are all MCQs in English? (it can be guessed but it is never written)

Why not give the key to the human annotators?

For the thesaurus-based baseline, the hypothesis is not given: supposes that if synonyms do not appear in the results, they do not refer to the right sense?

Typos, Grammar, Style, and Presentation Improvements
p.2 We -> we; consider than them -> consider them

p.5: totally -> -
