\section{Experimental Results}
% \KZ{Put a preamble here... In general, don't use the word ``prove'' unless you
% have mathematical proof. Experiments can only ``show'' or 
% ``demonstrate'' things, not prove things.}

In this section, we first introduce the experimental setup, including dataset, baselines, evaluation metrics, and implementation details. Then, we show the results and compared with five unsupervised baselines and six supervised+domain-adapted baselines in \secref{sec:result}. Finally, we analyze the result from four aspects: the influence of dataset, ablation study, case study, and human evaluation.
\subsection{Datasets}
We evaluate our framework on four different datasets, namely Quora, WikiAnswers, MSCOCO, and Twitter. Following \citet{liu2019unsupervised}, we randomly choose 20K parallel paraphrase pairs as the test set and 3K parallel paraphrase pairs as the validation set for Quora, WikiAnswers, and MSCOCO. 

We randomly sample the remaining parallel paraphrases pairs and pick one sentence from each pair to construct the non-parallel training data.
The number of selected sentences is the same as the work by \citet{liu2019unsupervised}, which is 400K for Quora, 500K for WikiAnswers, 320K for MSCOCO and 110K for Twitter.

\paragraph{Quora. } Quora\footnote{\url{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}} dataset is released by Quora in
January 2017. It contains 400K pairs of questions with manual annotation about whether questions in each pair are duplicates of each other. Through these annotations, there are 140K pairs marked as paraphrases and 320K pairs masked as non-paraphrases.

\paragraph{WikiAnswers. } WikiAnswers \citep{fader2013paraphrase} dataset contains 2.3M pairs of question paraphrases extracted from the WikiAnswers website. The dataset is collected automatically without manual annotation.

\paragraph{MSCOCO. } MSCOCO \citep{lin2014microsoft} contains human-annotated captions for 120K images. Each image contains five captions considered as paraphrases of each other, we take four pairs from each image and get 500K parallel pairs.

\paragraph{Twitter. } Twitter \citep{lan2017continuously} is a paraphrase detection dataset, containing 110K pairs of potential paraphrases and 60K manually annotated paraphrases. There are only 600 sentences marked as paraphrases in the test set, and we take them all for testing.

\paragraph{Training on Cross-Domain Data} \label{sec:indomain}
When there is no sufficient target-domain non-parallel data, or when we cannot use any data from the target-domain to train the set2seq model, it is hard to train unsupervised models or fine-tune supervised models in the target-domain. Our solution is to train the set2seq model with a big cross-domain dataset and apply it to the target-domain. We name the model ``set2seq-common''. We test the performance of our framework with set2seq-common on four datasets to show the generality of our framework. Further, we apply set2seq-common in \secref{sec:app} for data augmentation since we cannot train the set2seq model with the translation data to be augmented.

\subsection{Baselines and Evaluation Metrics}
We compare our framework with five unsupervised methods and six supervised methods with domain adaptation.

\paragraph{Unsupervised methods. } The current state-of-the-art unsupervised method is Unsupervised Paraphrasing by Simulated Annealing (UPSA), proposed by \citet{liu2019unsupervised}, which is also our main target of comparison. Other unsupervised methods include CGMH from \citet{miao2019cgmh}, ParaNMT from \citet{wieting2017paranmt}, ParaBank(-$3^{rd}$ IDF) from \citet{hu2019parabank}, and VAE from \citet{kingma2013auto}. Note that ParaNMT used back-translation to generate paraphrases, so it can be viewed as ``back-translation only''.

\paragraph{Supervised methods with domain adaptation. } Decomposable Neural Paraphrase Generation (DNPG) \citep{li2019decomposable} is the current state-of-the-art method for supervised paraphrase generation. \citet{li2019decomposable} raised the issue of domain adaptation in his paper and demonstrated that DNPG also performed best with domain adaptation, so we mainly compare our framework with DNPG. Other baselines are shallow fusion from \citet{gulcehre2015using}, Multi-Task Learning (MTL) from \citet{domhan2017using}, Pointer-generator from \citet{see2017get}, Transformer \citep{vaswani2017attention} with copy mechanism, and MTL with copy mechanism. 

\paragraph{Evaluation metrics. } For the fairness of comparison, 
we take the same evaluation metrics as in UPSA and 
DNPG~\footnote{The evaluation script can be found at \url{https://github.com/anonymity-person/UPSA}}, which are iBLEU \citep{sun2012joint}, BLEU \citep{papineni2002bleu} and ROUGE \citep{lin2004rouge} scores. 
BLEU and ROUGE scores are common evaluation matrics for NLP tasks while 
iBLEU is especially designed for paraphrase generation tasks. 
It penalizes similarity between paraphrase and the original sentence. 
Suppose the input sentence is $src$, the output paraphrase is $out$, 
and the ground truth paraphrase is $trg$, we calculate iBLEU as follows:
\begin{multline}
\text{iBLEU} = \alpha \cdot \text{BLEU}(out, trg) - \\
(1-\alpha) \cdot \text{BLEU}(out, src)
\end{multline}
BLEU and ROUGE only consider the accuracy but ignore the 
diversity of generated paraphrases, while iBLEU considers both. 
So we use iBLEU as our main evaluation metric.

\begin{table*}[ht]
\small
\centering
\begin{tabular}{p{2cm}p{3.4cm}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}}
\hline
\\ [-1.7ex]
& & \multicolumn{4}{c}{\textbf{Quora}} & \multicolumn{4}{c}{\textbf{WikiAnswers}} \\
\\ [-1.7ex]
\cline{3-6} \cline{7-10} 
\\ [-1.8ex]
 &Model&iBLEU&BLEU&R-1&R-2&iBLEU&BLEU&R-1&R-2\\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
Supervised & DNPG (SOTA) & 
18.01 & 25.03 & 63.73 & 37.75 & 34.15 & 41.64 & 57.32 & 25.88 \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\multirow{6}{3cm}{Supervised + \\Domain-Adapted}
& Pointer-generator & 
5.04 & 6.96 & 41.89 & 12.77 & 21.87 & 27.94 & 53.99 & 20.85 \\
& Transformer+Copy &
6.17 & 8.15 & 44.89 & 14.79 & 23.25 & 29.22 & 53.33 & 21.02 \\
& Shallow fusion &
6.04 & 7.95 & 44.87 & 14.79 & 22.57 & 29.76 & 53.54 & 20.68 \\
& MTL &
4.90 & 6.37 & 37.64 & 11.83 & 18.34 & 23.65 & 48.19 & 17.53 \\
& MTL+Copy &
7.22 & 9.83 & 47.08 & 19.03 & 21.87 & 30.78 & 54.10 & 21.08 \\
& DNPG &
10.39& 16.98& 56.01 & 28.61 & \underline{25.60} & \underline{35.12} & \underline{56.17} & \underline{23.65} \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\multirow{8}{3cm}{Unsupervised}
& VAE & 
8.16 & 13.96 & 44.55 & 22.64 & 17.92 & 24.13 & 31.87 & 12.08 \\
& ParaNMT\scriptsize{(back-translation)} & 
10.76& 15.84 & 52.34 & 25.18 & 14.71 & 19.73 & 30.34 & 9.91 \\
& ParaBank & 
9.86 & 14.61 & 49.90 & 23.71 & 13.08 & 17.47 & 28.89 & 9.32 \\
& CGMH & 
9.94 & 15.73 & 48.73 & 26.12 & 20.05 & 26.45 & 43.31 & 16.53 \\
& UPSA & 
\underline{12.02}& \underline{18.18} & \underline{56.51} & \underline{30.69} & 24.84 & 32.39 & 54.12 & 21.45 \\
\\ [-1.8ex]
\cline{2-10}
\\ [-1.8ex]
& set2seq \scriptsize{(ours)} & 
13.67 & 20.57 & 58.33 & 32.54 & 26.26 & 33.74 & 56.16 & 23.12 \\
& set2seq-common+BT \scriptsize{(ours)} & 
12.52 & 18.74 & 57.09 & 31.13 & 24.98 & 33.36 & 55.75 & 23.03 \\
& set2seq+BT \scriptsize{(ours)} & 
\textbf{14.65} & \textbf{22.43} & \textbf{59.94} & \textbf{34.02} & \textbf{28.31} & \textbf{37.47} & \textbf{56.82} & \textbf{24.91} \\
\\ [-1.8ex]
\hline
\\ [-1.5ex]
& & \multicolumn{4}{c}{\textbf{MSCOCO}} & \multicolumn{4}{c}{\textbf{Twitter}} \\
\\ [-1.7ex]
\cline{3-6} \cline{7-10} 
\\ [-1.8ex]
 &Model&iBLEU&BLEU&Rouge1&Rouge2&iBLEU&BLEU&Rouge1&Rouge2\\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\multirow{8}{3cm}{Unsupervised}
& VAE & 
 7.48 & 11.09 & 31.78 &  8.66 &  2.92 &  3.46 & 15.13 &  3.40 \\
& ParaNMT\scriptsize{(back-translation)} & 
 7.48 & 10.83 & 30.89 &  8.70 &  \underline{7.60} & \underline{10.83} & \underline{35.45} & \underline{14.78} \\
& ParaBank & 
 6.43 &  9.44 & 29.12 &  8.25 &  6.55 &  9.79 & 34.51 & 13.94 \\
& CGMH & 
 7.84 & 11.45 & 32.19 &  8.67 &  4.18 &  5.32 & 19.96 &  5.44 \\
& UPSA & 
 \underline{9.26} & \underline{14.16} & \underline{37.18} & \underline{11.21} &  4.93 &  6.87 & 28.34 &  8.53 \\
\\ [-1.8ex]
\cline{2-10}
\\ [-1.8ex]
& set2seq \scriptsize{(ours)} & 
\textbf{11.51} & 17.52 & 39.75 & 13.66 & 5.77 & 7.56 & 31.63 & 10.97 \\
& set2seq-common+BT \scriptsize{(ours)} & 
 8.99 & 13.31 & 35.82 & 11.03 &  9.71 & \textbf{14.25} & \textbf{39.14} & \textbf{18.77} \\
& set2seq+BT \scriptsize{(ours)} & 
11.37 & \textbf{17.91} & \textbf{40.27} & \textbf{14.12} & \textbf{9.86} & 13.88 & 39.09 & 18.15 \\
\\ [-1.8ex]
\hline
\end{tabular}
\caption{\label{tab:result}
Evaluation results on Quora, WikiAnswers, MSCOCO and Twitter. The comparison 
with supervised + domain adapted methods is only on Quora and WikiAnswers 
because results of current state-of-the-art 
method~\citep{li2019decomposable} are only available on these two datasets.
}
\end{table*}

\subsection{Implementation and Training Details} \label{sec:exset}
To be consistent with the pre-processing of UPSA and DNPG, we convert the input words into lower-case and truncate all sentences to up to 20 words. 
For the convenience of hybrid decoding, 
we learn a shared byte-pair encoding (BPE, \citet{sennrich2016edinburgh}) 
with size 50k from the training data for translation models, 
and use a 50K vocabulary for all models. For baselines using back-translation (ParaNMT and ParaBank), we use the same vocabulary. For other baselines, we include all words that appear in the training set into the vocabulary for a fair comparison. For the hyper-parameter $\lambda$ mentioned in 
\secref{sec:joint}, we set it to 0.5 for all datasets.

For the translation models in back-translation, we train them with the WMT17\footnote{\url{http://statmt.org/wmt17/translation-task.html}} zh-en dataset \citep{ziemski2016united}. We train them with a standard transformer for 3 days on two Tesla V100 GPUs. For the set2seq-common model mentioned in \secref{sec:indomain}, we use the news-crawl-2016 English monolingual data from WMT17 and train 1.5 days with the same transformer as in the translation models. For the domain-specific set2seq models, we use a 2-layer transformer with 300 embedding size, 4 heads, 1024 feed-forward dimensions, AdamOptimizer, and 0.1 dropout for all layers to train them. The training lasts 3 hours on a single Tesla V100 GPU for each dataset.

To calculate iBLEU and BLEU, four references are used for MSCOCO, five for WikiAnswers, and one for other datasets. For some test cases, WikiAnswers does not have 5 references, so we evaluate them on reduced references. 
For ROUGE scores, we take the average of all references if there exists 
more than one reference.

\subsection{Results} \label{sec:result}

Table~\ref{tab:result} presents our experimental results. For all evaluation metrics, a higher score represents better performance. We mark the previous highest scores by underlining them and mark the present highest scores with the bold font.
The supervised method (DNPG (SOTA)) here is only for reference since it is an unfair comparison between supervised and unsupervised methods.

We compare three different models with the previous methods, 
namely set2seq, set2seq-common+BT, and set2seq+BT, where BT stands for 
back-translation. We show the set2seq alone here to demonstrate that
useful information comes not only from translation, as the
set2seq model alone can already outperform almost all competitors. 

Our framework outperforms all existing unsupervised methods and 
supervised methods with domain adaptation. The results from our framework 
are even close to the state-of-the-art supervised model DNPG. 

\subsection{Analysis} \label{sec:analysis}
\paragraph{Datasets. } Due to the domain-specific differences between four datasets, it is understandable that scores on all metrics vary a lot across different datasets. Sentences in Quora and WikiAnswers are of the best quality.
% , they are in appropriate lengths and have high correlations between paraphrases in the same pair. 
Experiments on these two datasets are the most persuasive and representative.

Paraphrases from MSCOCO are descriptions of images, the set2seq model fits this dataset quite well since the process of generating paraphrases are similar: 
one extends information from a static picture; 
the other extends from a word set. 
The set2seq-common model cannot learn the in-domain properties of MSCOCO, 
so it does relatively poorly here as opposed to its performance 
in other datasets. 

Lack of training data for Twitter leads to insufficient training of most models. Models containing back-translation perform extraordinary well since they have adequate information. Besides, set2seq-common+BT achieves an excellent result, which shows the advantages of the set2seq-common model compared with the set2seq model trained with insufficient in-domain data.

\paragraph{Ablation Study. }

Table~\ref{tab:ablation} shows the result of ablation study on Quora dataset, where $\text{BLEU}_{ref}$ is the BLEU between reference and output, the higher the better and $\text{BLEU}_{src}$ is the BLEU between source sentence and output, the lower the better. 

We demonstrate that removing stopwords outperforms retaining high-IDF words. For high-IDF words, we keep top $k\%$ high-IDF words in the original sentence. For the value of $k$, we set $k=50$, which is the best among $[30, 40, 50, 60, 70]$. 

Removing random replacement and adding position encoding can both lead to 
a high BLEU between source sentences and output paraphrases, 
which substantially reduces the diversity of generated sentences. 
% \KZ{Consider dropping this: 
% We try to augment the NMT training data with these two methods in 
% section~\ref{sec:app}, but fail to get any improvement. 
% Since we do not mainly conduct NMT experiments, we only propose an observed phenomenon here, more detailed experiments can be done in future works.}

\begin{table}
\small
\centering
\begin{tabular}{lp{0.8cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}}
\hline 
\\ [-1.8ex]
\textbf{Model Variants} & iBLEU & $\text{BLEU}_{ref}$ & $\text{BLEU}_{src}$ \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
set2seq+BT & \textbf{14.65} & 22.43 & \textbf{55.40} \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\multicolumn{1}{m{3cm}}{$\ominus$ excluding stopwords \par $\oplus$ retaining high-IDF} & 13.63 & 22.39 & 65.21 \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\multicolumn{1}{m{3cm}} {$\ominus$ random replacement} & 13.83 & \textbf{23.82} & 76.08 \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\multicolumn{1}{m{3cm}}{$\oplus$ position encoding} & 14.21 & 23.43 & 68.77 \\
\\ [-1.8ex]
\hline
\end{tabular}
\caption{\label{tab:ablation} Ablation Study on Quora.}
\end{table}

\paragraph{Case Study. } Table~\ref{tab:case} shows the examples of generated paraphrases through different strategies.

Two kinds of information are easily lost in set2seq: 
one is the information in stopwords; the other is the information in the 
sequential expression. 
In the first example, set2seq model loses the word ``When'' 
when generating paraphrase from the word set. 
In the second example, set2seq model mistakes the relationship between the 
universe and the black hole since it cannot obtain any sequential information. 

For back-translation, the correct paraphrase sometimes cannot be 
generated due to the limited capacity of the translation models, 
``seed funding'' should be a fixed phrase in Example 3, 
but back-translation cannot recognize it.

For seq2seq+BT, the generated sentences are too close to the 
original sentences by the order of the words. 
Our goal is to generate sentence level paraphrases, 
but seq2seq model limits the sequential expression.

\begin{table}
\small
\centering
\begin{tabular}{lp{4.6cm}}
\hline 
\\ [-1.8ex]
\textbf{Example 1} & \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
Input & when will be end of world ? \\
Word Set & (stop, earth, ?) \\
BT & when is the end of the world ? \\
set2seq & will the world end ? \\
seq2seq+BT & what is the end of the world ? \\
set2seq+BT & when will the world end ? \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\textbf{Example 2} & \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
Input & could this universe be inside a black hole ? \\
Word Set & (universe, in, dark, cave, ?) \\
BT & can universe be a black hole ? \\
set2seq & is there a black hole in the universe ? \\
seq2seq+BT & could the universe be in a black hole ? \\
set2seq+BT & is the universe in a black hole ? \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
\textbf{Example 3} & \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
Input & do product ideas get seed fundings ? \\
Word Set & (produce, mind, incur, germ, financing, ?) \\
BT & does the product concept receive seed money ? \\
set2seq & where can i get funding for my product idea ? \\
seq2seq+BT & do product ideas get seed funding ? \\
set2seq+BT & how do i receive seed funding for my product idea ? \\
\\ [-1.8ex]
\hline
\end{tabular}
\caption{\label{tab:case} Case Study }
\end{table}
% \KZ{The fonts in Table 4 too small.}
\begin{table}[th]
\small
\centering
\begin{tabular}{lc}
\hline 
\\ [-1.8ex]
\textbf{Method} & score \\
\\ [-1.8ex]
\hline
\\ [-1.8ex]
set2seq & 2.79 \\
back-translation & 3.02 \\
seq2seq+BT & 3.34 \\
set2seq+BT & 3.57 \\
\\ [-1.8ex]
\hline
\end{tabular}
\caption{\label{tab:human} Results for Human Evaluation}
\end{table}

\paragraph{Human Evaluation.}
We choose 100 sentences from Quora and ask 3 human annotators to score 
the result from 1 to 5, the higher score indicates the better 
quality of the generated paraphrases. All annotators are asked to 
consider the result from both accuracy and diversity. We give a reference for scoring:
\begin{itemize}
\item[\textbf{1.}] The meaning is totally different.
\item[\textbf{2.}] Exactly the same sentence.
\item[\textbf{3.}] The meaning is slightly different.
\item[\textbf{4.}] Express the same meaning in a slightly different expression.
\item[\textbf{5.}] Express the same meaning in a totally different expression.
\end{itemize}
Table~\ref{tab:human} shows the average rating of 
all annotators on all sentences. 
Our framework performs the best among different baselines. 
