\section{Conclusion}
\label{sec:conclude}

In this paper, we extend the concept of style to general writing styles with limited size of data. To tackle this new problem, we propose a multi-task style transfer (ST$^2$) framework, which is the first of its kind to apply meta-learning to small-data text style transfer. We use the literature translation dataset and the augmented standard dataset to evaluate the state-of-the-art models and our proposed model. 

Both quantitative and qualitative results show that ST$^2$ outperforms the state-of-the-art baselines. Compared with state-of-the-art models, our model does not rely on a large dataset for each style pair, but is able to effectively use off-domain information to improve both language fluency and style transfer accuracy.

Since baseline models might not be able to learn an effective language models from small datasets, which is a possible reason for their bad performances, we further eliminate this bias by pretraining the base models using data from all tasks. From the results, we ascertain that the enhancement of meta-learning framework is substantial.
