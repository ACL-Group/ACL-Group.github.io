\section{Introduction}
\label{sec:intro}

Text style transfer aims at rephrasing a given sentence in a desired style. It can be used to rewrite stylized literature works, generate different styles of journals or news (e.g., formal/informal), and to transfer educational texts with specialized knowledge for education with different levels.

Due to lack of parallel data for this task, previous works mainly focused on unsupervised learning of styles, usually assuming that there is a substantial amount of nonparallel corpora for each style, and that the contents of the two corpora do not differ significantly~\cite{shen2017style,john2018disentangled,fu2018style}. Existing state-of-the-art models either attempt to disentangle style and content in the latent space~\cite{shen2017style,john2018disentangled,fu2018style}, directly modifies the input sentence to remove stylized words~\cite{li2018delete}, or use reinforcement learning to control the generation of transferred sentences in terms of style and content~\cite{wu2019hierarchical,luo2019dual}. However, most of the approaches fail on low-resource datasets based on our experiments. This calls for new few-shot style transfer techniques. 

%\KZ{Existing unsupervised style transfer methods make two assumptions: i) the content from the two corpora is similar; ii) the size of the corpora is large enough. Our approach in this paper addresses ii), but what about i)?} 

The general notion of style is not restricted to the heavily studied sentiment styles, but also writing styles of a person. However, even the most productive writer can't produce a fraction of the text corpora commonly used for unsupervised training of style transfer today.
% \KZ{I think you need to make it clear what you mean by style here. If it's personal writing style, it maybe hard to obtain in large volume because a person can only write so much. But if it's some more general style, e.g., senior people vs. children, it may not be so hard to obtain. You have to be careful with every word you say here.}
Meanwhile, in real world, there exists as many writing styles as you can imagine. Viewing the transfer between each pair of styles as a separate domain-specific task, we can thus formulate a multi-task learning problem, each task corresponding to a pair of styles. To this end, we apply a meta-learning scheme to take advantage of data from other domains, i.e., other styles to enhance the performance of few-shot style transfer~\cite{finn2017model}.

Moreover, existing works mainly focus on a very limited range of styles. In this work, we take both personal writing styles and previously studied general styles, such as sentiment style, into account. We test our model and other state-of-the-art style transfer models on two datasets, each with several style transfer tasks with small training data, and verify that information from different style domains used by our model enhances the abilities in content preservation, style transfer accuracy, and language fluency.

Our contributions are listed as follows:
\begin{itemize}
	\item We show that existing state-of-the-art style transfer models fail on small training data which naturally shares less content (see \secref{sec:st} and \secref{sec:pretrain}).
	\item We propose Multi-task Small-data Text Style Transfer (ST$^2$) algorithm, which adapts meta-learning framework to existing state-of-art models, and this is the first work that applies meta-learning on text style transfer to the best of our knowledge (see \secref{sec:approach}).
	\item The proposed algorithm substantially outperforms the state-of-the-art models in the few-shot text style transfer in terms of content preservation, transfer accuracy and language fluency (see \secref{sec:eval}).
%	\item By testing the effect of pretraining of encoders/decoders in state-of-art methods, we verify that models with meta-learning framework is not only superior in language fluency because by setting they have more data to learn from, but also in style transfer accuracy.
	\item We create and release a literature writing style transfer dataset, which the first of its kind (see \secref{sec:lt}).%\KZ{Dataset...}
\end{itemize}



