1. introduction
	- motivation of task
		a). style transfer between different authors
		b). journal writer (news writer)
		c). intimate a character's talking style
		d). transfer domain-specific knowledge for education, e.g., from complex language to easy-to-read version
	- motivation of method
		a). dataset is small in each domain
		b). multiple similar tasks are available
		c). meta-learning can enhance transfer learning abilities

2. related work
	- baselines
		a). cross-align
		b). template
		c). DRG
		d). dualRL
		e). vae
	- cons
		a). rely on large corpora, not suitable for few-shot learning
	
	=> model-agnostic meta-learning comes to the stage

3. method
	- cross-align
	- vae
	- maml

4. experiments
	- datasets
		a). translations (7 pairs)
		b). multi-task
			t1 = yelp (health), [0-neg, 1-pos]
			t2 = amazon (musical instrument), [0-neg, 1-pos]
			t3 = GYAFC (relationship), [0-informal, 1-formal]
			t4 = wikipedia [0-esy, 1-std]
			t5 = bible [0-esy, 1-std]
			t6 = britannica [0-esy, 1-std]
			t7 = shakespeare [0-modn, 1-orig]
	- experimental setup
		a). baseline => s1 (t1-t7), s2 (t1-t7)
		b). maml => maml-t1-t7 + t1/.../t7-transfer
		c). metrics
			+ bleu (0-0, 0-1, 1-0, 1-1)
			+ transfer accuracy (pretrained classifier)
			+ perplexity (pretrained bigram model)
	- results

5. conclusion and future work
	- define a new problem: multi-task few-shot text style transfer
		a). large copora not available for general text style transfer => few-shot style transfer algorithms worth studies
		b). in practice, datasets in multiple domains with small size in each domain are available
	- verified that maml can be applied to solve the problem
	- discussion: causes of bad performances in small datasets:
		a). not sufficient to learn a style
		b). not sufficient for generating sensible sentences
	- extend maml algorithm for domain-specific generation tasks
