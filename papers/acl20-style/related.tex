\section{Related Work}
\label{sec:related}
%\KZ{Should be related work...}
%disentangling style and content
\citet{fu2018style} devised a multi-encoder and multi-embedding scheme to learn a style vector via adversarial training. Adapting a similar idea, \citet{zhang2018shaped} built a shared private encoder-decoder model to control the latent content and style space. Also based on a seq2seq model, \citet{shen2017style} proposed a cross-align algorithm to align the hidden states with a latent style vector from target domain using teacher-forcing. More recently, \citet{john2018disentangled} used well-defined style-oriented and content-oriented losses based on a variational autoencoder to separate style and content in latent space.

%direct modification
\citet{li2018delete} directly removed style attribute words based on TF-IDF weights and trained a generative model that takes the remaining content words to construct the transferred sentence. Inspired by the recent achievements of masked language models, \citet{wu2019mask} used an attribute marker identifier to mask out the style words in source domain, and trained a ``infill'' model to generate sentences in target domain.

%reinforcement learning
Based on reinforcement learning, \citet{xu2018unpaired} proposed a cycled-RL scheme with two modules, one for removing emotional words (neutralization), and the other for adding sentiment words (emotionalization). \citet{wu2019hierarchical} devised a hierarchical reinforced sequence operation method using a \emph{point-then-operate} framework, with a high-level agent proposing the position in a sentence to operate on, and a low-level agent altering the proposed positions. \citet{luo2019dual} proposed a dual reinforcement learning model to jointly train the transfer functions using explicit evaluations for style and content. Although their methods work well in large datasets such as Yelp~\cite{asghar2016yelp} and GYAFC~\cite{rao2018dear}, it fails in our few-shot scheme.

%MT based
\citet{prabhumoye2018style} adapted a back-translation scheme in an attempt to remove stylistic characteristics in some intermediate language domain, such as French.

%meta-learning
There are also meta-learning applications on text generation tasks. \citet{qian2019domain} used the model agnostic meta-learning algorithm for domain adaptive dialogue generation, which requires paired data for training and is different from our task. In order to enhance the content-preservation abilities, \citet{li2019domain} proposed to first train an autoencoder on both source and target domain. But in addition to utilizing off-domain data, we are applying meta-learning method to enhance models' performance both in terms of language model and transfer abilities.

