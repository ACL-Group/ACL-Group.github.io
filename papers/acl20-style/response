General

Q: Content preservation
A: We follow most recent works (Li et al. 2018, John et al. 2018, Luo et al. 2019) to use BLEU to automatically evaluate content preservation. Luo et al. (2019) suggested that "BLEU score significantly correlates with content preservation". We agree with you that BLEU alone might be insufficient, hence we include the human evaluation results (same metric as Luo et al. 2019, on a scale of 5) below and this can go into the revision with minimum effort. This result indeed correlates with Table 3 and 5.

    Template   CA   DeleteRetrieve DualRL VAE ST2-CA ST2-VAE
LT     3.9    1.1        1.0        1.2   1.7  2.3     1.9
GSD    4.2    1.0        1.3        1.5   2.1  3.8     2.7


Review #1

Q: Table 4
A: Although the randomly selected examples are not always satisfying, our methods are generating (reasonably) fluent sentences, and do a better job in transfering styles. For instance, in "the staff is ...", ST2-CrossAlign transferred part of the style to negative, while ST2-VAE is even more successful. 

Sec. 1:
	Q: Point to ...
	A: Table 3-5, Sec. 3.3
	
	Q: Throughout ...
	A: In this paper, we use the CNN classifier (Kim 2014) to distinguish styles, which is quite accurate: 80% for GSD and 77% even for LT, the fine-grained writing styles. We show some statistical characteristics for a pair of styles of for each dataset below: (LT: Michael R. Katz/Richard Pevear; GSD: simple/standard Wikipedia). More will go into appendix.

         vocab      avg. length   # of adj.    Flesch readability (0-100, higher = easier)
LT    12589/13643    19.8/21.3   15338/13623       65.1/66.2
GSD   18124/20112    11.1/11.8   10692/12070       58.5/52.3

Sec. 3:
	Q: Data
	A: http://gen.lib.rus.ec/.

	Q: Add a ...
	A: Given two comparable documents, we first discover some similar pairs of sentences using simple measurements such as BLEU, and use these pairs as anchors. Between every two anchors, we further align similar sentences using more delicate similarity measurements which is documented in (Chen 2019). 

	Q: How do ...
	A: Both are randomly sampled.

	Q: Need more ...
	A: We use the original large dataset to train the LM for GSD, because they have been reduced in our style transfer task.

	Q: Why are ...
	A: By "full", we mean we use the training data before it is reduced.

	Q: Human eval: What ...
	A: We show the best and worst sentences to the annotators first so they know the upper and lower bound and thus score more linearly.
	
	Q: Not very ...
	A: In our experiments, all baseline models learn to transfer one specific pair of styles, e.g., simple vs. standard Wikipedia in GSD, or one pair of authors in LT.

	Q: Statistical significance
	A: BlEU, PPL and ACC are computed based on a test size of 1k, and these results are significant. Human scores are computed over a small sample size. The p-values for the hypothesis "ST2 is better than baselines" are listed as below.
		LT:
		          Template    CA    DeleteRetrieve   DualRL     VAE
		ST2-CA	    1.0     <1e-6      <1e-6         <1e-6      0.992
		ST2-VAE     0.9     <1e-6      <1e-6         <1e-6      0.027

		GSD:
		          Template    CA    DeleteRetrieve   DualRL     VAE
		ST2-CA	    0.99    <1e-6      <1e-6          0.7       0.99
		ST2-VAE     0.04    <1e-6      <1e-6         <1e-6      0.54

	Q: Add details ... & What ...
	A: The generator of the model learns to reconstruct sentences first, and learns how to transfer styles second. In meta-learning, the model learns both tasks simultaneously. 


Review #2
Q: The incremental ...
A: We define a fine-grained text style transfer problem which inheretly demands small training data. We are the first to develop a meta-learning framework to tackle this problem better than previously known approaches. Evaluations show the effectiveness of this framework. Our generated sentences are not only more fluent, but also more accurately transferred.

Q: The description ...
A: We have described the two most important parts of our method in detail, including steps in Algorithm 1.

Review #3
Q: ... are lower than ...
A: In Table 3, Template has large BLEU scores and low perplexity because it almost outputs identical sentences (Table 4). Our method is more balanced between fluency and transfer accuracy. As is also mentioned in reviewer’s comments, we have demonstrated the improvements of meta-learning framework by adding a pretraining phase to the base models.

Review #4
Q: BLEU ...
A: Please refer to general responses and Q2 of Review #1. 

Q: Examples ...
A: Here are some more examples. More can be included in appendix.

original -> modern Shakespeare
original: hear me a word , for i shall never speak to thee again .
-------
Template: hear me a word , for i shall never speak to thee again .
CA: described described that
CA (pretrained): hear me a little , i shall never give thee to answer .
DeleteRetrieve: i ’ s not a s .
DualRL: look , i a have been a earth , a tree hath find have .
VAE: do i have to speak with thee as i can be a man
VAE (pretrained): i ’ ll speak to this
ST2-CA: hear me , i ’ ll speak to hear thee for her .
ST2-VAE: you know that i think i would have to speak with you

informal -> formal
original: do n't even do that to yourself ! ! !
-------
Template: do n't even do that to yourself allow
CA: whatever you doing
CA (pretrained): do n't even do that to do ! ! !
DeleteRetrieve: i do not be a guy .
DualRL: do n't even do that to yourself success ! !
VAE: do n't do anything without do things do n't do
VAE (pretrained): do n't even that to do anything
ST2-CA: do n't tell you want to do that .
ST2-VAE: you 're not really good to ask yourself to do that

Yelp positive -> negative
original: the staff is welcoming and professional .
-------
Template: the staff is welcoming and professional .
CA: glad glad glad
CA (pretrained): the staff is welcoming and professional .
DeleteRetrieve: the staff is a time .
DualRL: less expensive have working .
VAE: the staff is rude and rude
VAE (pretrained): the staff is extremely welcoming and professional .
ST2-CA: the staff is friendly and unprofessional
ST2-VAE: the staff are rude and unprofessional .

Yelp negative -> positive
original: these people do not care about patients at all !
-------
Template: these people wonderful about patients at all !
CA: glad glad glad
CA (pretrained): these people do not care about patients at all !
DeleteRetrieve: i was n't be a a appointment and i have .
DualRL: and just like that it was over and i was .
VAE: these people do not care about patients or doctors
VAE (pretrained): these guys do n't care about the patients at time
ST2-CA: these people do not satisfied at all !
ST2-VAE: i was so happy and i did n't consent

