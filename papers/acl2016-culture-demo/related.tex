\section{Related Work}
\label{sec:related}
Cognitive linguistic studies \cite{kovecses2006language} have shown that equivalent terms in different languages may have very different meanings. This phenomenon proves to hold between English and Chinese \cite{chen2007chinese,tavassoli1999temporal,krifka199511}.  In computational linguistics, discovery of relationships across languages is an emerging topic. There are generally two research directions: graph-based knowledge network or distribution-based vector representation. 

BabelNet\cite{Navigli:2012dn} and Yago3\cite{mahdisoltani2014yago3} are representatives of graph-based knowledge network, with an ambition 
to construct a unified multilingual knowledge base (just like WordNet). They 
integrate resources such as WordNet and Wikipedia to achieve 
this goal. The knowledge base thus built can be used to 
calculate relatedness across languages. However, both of them 
rely on existing structured resources to create the networks, 
which limit their scale and extendability.

For distributional models, the predominant approach to 
represent the semantics of words is word embedding. 
The embeddings are usually trained 
using co-occurrence matrix, 
matrix factorization\cite{lebret2013word,levy2014neural,li2015word} or 
neural network\cite{Mikolov2013distributed}. 
Traditionally, these vectors are trained on monolingual data and 
the vector spaces of different languages are not directly comparable 
with each other. To solve this, some researchers try to train 
unified representations from multilingual 
corpus~\cite{Klementiev:2012uk,hermann2014multilingual,Vulic:2015to} 
or construct a mapping between the vector spaces of 
different languages~\cite{Mikolov:2013tp}. 
These vectors are then evaluated 
in tasks such as bilingual lexicon induction or cross-lingual 
word sense disambiguation, and have shown to achieve 
state-of-art performance.

Our task is similar to bilingual lexicon induction, 
though we want to detect semantic difference instead of 
finding similar words. Tomas Mikolov\cite{Mikolov:2013tp} 
shows the potential to detect errors 
in bilingual dictionary with Word2Vec and linear transformation among 
different vector spaces. In this paper, we implement their idea 
(linear-trans) and compare with several ideas of ours.
