# Thanks for the valuable comments. Below we seek to
# clarify some questions and misunderstandings.

R1: 
1. (knowledge base completion ...) Our first experiment is indeed KBC.
Suppose we extend a KB by a new predicate "nephew_of" along with some of its facts. 
The new KB is still a KB. Our goal is to predict whether a new entity pair belongs 
to "nephew_of", by examining existing facts in this extended KB, which is the 
same task as Gardner's.

2. (experimental setting from ...) The KBC evaluation in Sec 7.2 and 7.3 are fair, because: 
	1) all test relations are hidden during training step; 
	2) both our model and SFE received observable entity pairs of the test relation 
	   and tried to predict if a new pair is true;
	3) in order to avoid information leak, we never computed silver score for 
	   candidate schemas extracted from observable pairs. Instead, we performed 
	   feature encoding and predicted the final distribution.

3. (word2vec representation ...) Word vectors are trained on part of Google News 
dataset (~100M words), and was downloaded from w2v's Google code. 
These vectors can be used for general domains.

4. (representations of relations ...) A relation is represented as a 
probability distribution of schemas (a vector) (line 671), where each schema has 
a "skeleton+constraints" structure (line 219-227). Real examples are shown in Figure 1.
The size of distribution for each relation is the same, with dimensions
of vectors perfectly aligned. For example, if relation r_1 has candidate schemas (a, b, c), 
while r_2 has candidates (c, d), then schema distribution for each relation is: 
(a, b, c, d), the union of all the relations.
Thus elements of probability vectors are comparable. 
# Our intuition is, if two relations are similar, their schema probability distributions 
# should be close.

5. Reddy et al. build ground graphs of questions through CCG parsing. 
However, in our work, input relations are short phrases, 
and CCG parsing results tend to be much simpler, which are hard
to map to suitable ground graphs. Though our schemas are similar to their 
grounded graphs, our method is data-driven, and could explore complex 
representations no matter how simple the phrase is. 
Due to unavailability of code, we didn't compare with Reddy in the paper.
Since Reddy outperforms Berant et al. (2014) by 10% on QA task, 
while our method outperforms Berant's by 16% on average (Table 5). 
It's conceivable that our method is competitive against Reddy's.

R2: 
1. (silver label function ...) 
# The confidence score function (Equation 3) 
# aims to judge whether a schema makes a positive/negative contribution to 
# some subject entity. The first case (positive confidence) is based on 
# set inclusion score between obj_s and obj_r, while we used ln function for smoothing.
# The second and third cases (non-positive confidence) are the same, 
# when no correct entity is returned by the schema. The intuition is
# that the more incorrect entities a schema includes, 
# the more negative score it received. 
*** KZ: this is not enough explanation for why the silver label function works.

The confidence score function (Equation 3) roughly estimates the quality of
schemas by measuring how the schema fits input pairs.
Intuitions are listed below:
	1) The schema receives a positive confidence if at least one input pair
	   can be returned by querying the schema. Confidence is based on the 
	   ratio of pairs covered by the schema in input data.
	2) The schema receives a non-positive confidence if none of input pairs
	   are covered. The negative confidence increases if more (wrong) pairs
	   are covered by the schema, otherwise, it decreases because we are not 
	   sure whether this schema has a poor quality (due to potential KB incompleteness).


R3:
1. For Reddy's work, please check (5) for R1.

2. Yih's work is indeed very related, and will be cited. 
However, there exist fundamental differences between our work and Yih's.
First, Yih focused on mining equivalence between a (syntactically complex) question
and a query graph, while constraints can be added only if the question has enough components. 
Our work aims at finding paraphrases between a (semantically complex) relation 
phrase and a schema in KB, using data-driven methods to explore a complex semantics
even when the input phrase is simple.
# Second, Yih proposed several heuristic rules for
# augmenting constraints, which are restricted to limited domains, such as time and gender.
# In contrast, the constraints we produce are more general and for open domain.
Finally, our ultimate goal is to construct a lexicon mapping NL phrases into KB
schemas, which can be used in many scenarios beyond QA. 
It's not our goal to compete directly with state-of-the-art QA systems.
Since we never used Webq training questions as most other systems do,
direct comparison is not exactly fair.
However, our results show that schema representations is competitive
and can be useful in QA.

3. For line 323-325, while a CVT node can be associated with multiple edges, 
in our definition, two of those edges are already part of skeleton 
(not considered constraints).
# If two constraints are added to a CVT node, 
# it is connected to four entities, which is very rare for a relation 
# expressed by a short phrase.
# Therefore we made this assumption.
Therefore we made the assumption under this definition.

4. (Silver label function): Please see R2.

# 5. We loaded Freebase triples into memory for speeding up our program, 
# due to the hardware constraint of our machine (48GB), 
# we couldn't load the entire Freebase, but instead a subset of
# 10M entities and 50M triples, which is still 
# challenging for all evaluation tasks. The full dataset contains ~200M triples, 
# which takes ~200GB memory, which is still possible on a more powerful machine.

6. ("120 questions too small"):
In QA experiment, we focus on questions containing complex relations (line 146-147),
which are known to be long-tailed. For this specific purpose, we selected almost all
questions with complex relations from Webq testing set, which came out to be 120.
The question list is available at "http://202.120.38.145/pp/120.txt".

7. We think F1@K measurement is reasonable, because the outputs of QA (in most
previous work) are entities with scores (derived from multiple parsing trees), 
and these entities are usually ranked. F1@K is a common evaluation metric
for ranked retrieval tasks. 
