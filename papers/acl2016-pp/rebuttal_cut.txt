R1: 
1. (KB completion...) 
Our first experiment is indeed KBC.
Suppose we extend a KB by a new predicate "nephew_of" along with some of its facts. 
The new KB is still a KB. Our goal is to predict whether a new entity pair belongs 
to "nephew_of", by examining existing facts in this extended KB, which is the 
same task as Gardner's.

2. (experimental setting from...) 
The KBC evaluation in Sec 7.2 and 7.3 are fair, because: 
	-all test relations are hidden during training step; 
	-both our model and SFE received observable entity pairs of the test relation 
	   and tried to predict if a new pair is true;
	-in order to avoid information leak, we never computed silver scores during testing. 
	   Instead, we performed feature encoding and predicted the final distribution.

3. (word2vec representation...) 
Word vectors, directly downloaded from w2v's Google project,
are trained on part of Google News dataset, and can be used for general purposes.

4. (representations of relations...) 
A relation is represented as a 
probability distribution (vector) of schemas (line 671), where each schema has 
"skeleton+constraints" structure (line 219-227). 
The size of distribution for each relation is identical, with all dimensions
perfectly aligned. For example, if relation r_1 has candidate schemas (a, b), 
while r_2 has candidates (b, c), then schema distribution for each relation is: 
(a, b, c), the union of all candidate schemas. Thus these vectors are comparable. 

5. (Reddy...) 
Please see R3 (1).

R2: 
1. (silver label function...) 
The confidence score function (Equation 3) roughly estimates the quality of
schemas by measuring how the schema fits input pairs. The schema receives
	1) positive confidence if it covers some input pairs; 
	2) negative confidence if it covers some (presumably) incorrect pairs not in the input;
	3) zero confidence if it covers no pairs at all, which might be the result
	  of KB incompleteness and nothing wrong with the schema itself. 

R3:
1. Work by Reddy and Yih is indeed very related. We have cited Reddy and will cite
Yih. However, despite structural similarity between schemas, grounded graphs and query graphs,
there exist fundamental differences between our work and theirs.
First, because they both use parsing, their assumption is input questions
are (syntactically complex) sentences. In Reddy's, only complex CCG parse trees can find
suitable matching grounded graphs, while in Yih's, constraints can be added only if 
the question has enough syntactic components (according to rules in Appendix). 
In contrast, we discover paraphrases between (syntactically simple but 
semantically complex) NL relation phrases and schemas in KB, 
using data-driven methods.
Second, our ultimate goal is to construct a lexicon mapping NL phrases into KB
schemas, which can be used in many scenarios beyond QA. 
It's not our goal to compete directly with state-of-the-art QA systems.
Since we never used Webq training questions as most other systems do, 
direct comparison is not exactly fair. Further, it doesn't help when 
neither Reddy nor Yih released implementation source. 
Nevertheless, since Reddy outperforms Berant (2014) by 10% on QA task, 
while our method outperforms Berant's by 16% on average (Table 5), 
it's conceivable that our method is competitive against Reddy's.

2. (Silver label function) 
Please see R2.

3. ("120 questions too small") 
In QA experiment, we focus on questions containing complex relations (line 146-147),
which are known to be long-tailed. For this specific purpose, we selected almost all
questions with complex relations from Webq testing set, which came out to be 120.

4. We think F1@K measurement is reasonable, because the outputs of most QA systems
are entities ranked by scores. F1@K is a common evaluation metric
for ranked retrieval tasks. 
