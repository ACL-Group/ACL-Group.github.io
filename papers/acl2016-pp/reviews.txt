Title:	Paraphrasing Natural Language Relations to Knowledge Base Schemas
Authors:	Kangqi Luo, Xusheng Luo and Kenny Zhu
Instructions

The author response period has begun. The reviews for your submission are displayed on this page. If you want to respond to the points raised in the reviews, you may do so in the box provided below.

Please note: you are not obligated to respond to the reviews.

Review #1

APPROPRIATENESS:	5
CLARITY:	2
ORIGINALITY:	3
EMPIRICAL SOUNDNESS / CORRECTNESS:	3
THEORETICAL SOUNDNESS / CORRECTNESS:	3
MEANINGFUL COMPARISON:	3
SUBSTANCE:	4
IMPACT OF IDEAS OR RESULTS:	3
IMPACT OF ACCOMPANYING SOFTWARE:	3
IMPACT OF ACCOMPANYING DATASET:	3
RECOMMENDATION:	2
MENTORING:	YES
AUTHOR RESPONSE:	N/A
Comments

SUMMARY.

The paper presents a model for paraphrasing (e.g. transforming, matching) a relation triple extracted with an open information extraction system, such <as e_1, grandfather_of, e_2>, where e_1 and e_2 are two named entities, in a path between e_1 and e_2 in a knowledge graph, for example Freebase. The system take as input a relation r (such as grandfather_of) and a set of entity pairs that instantiate this relation. The first step is generating a set of candidate paths. A candidate path (or schema) is a template that matches instances triples from the input. In order to find candidates the authors perform a breadth-first search of the knowledge graph starting from one of the entity in the input instances, until the other entity is reached. After this step other edges are added to this paths in order to add detailed constraint to the general path with the aim of making the model more precise. For each relation r, the candidate paths are then scored according to the number of input relation instances are covered with a candidate path (support), and the number of entity pairs covered in the knowledge base (coverage). This score is used as silver label for learning a classifier that given the relation r and features extracted from all the instances of the relation r, predicts the most appropriate path among the path candidates.

The system is tested on two tasks, (what the authors call) knowledge base completion and question answering. The authors further show the ability of the learned relation representation to encode similarity informations.

----------

OVERALL JUDGMENT

The model presented in the paper is interesting. I liked the intuitions for building the set of path candidates and for calculating the silver label scores.

The paper though has several flaws. First of all I found it very hard to read, the authors often call concepts in slightly different ways throughout the paper. I would suggest more uniformity in the naming of important and recurring concepts. The definitions in section 2, especially definition 2, are not very clear. I found the structure of Section 3 slightly confusing, the approach overview gives some technical details that would be more understandable at the end of the section instead of at the beginning.

The main issues come from the experimental part. First of all the definition of knowledge base completion (lines 499 -- 509) is not clear, usually knowledge base completion is defined as the task of "filling in missing facts by examining the facts already in the KB, or by looking in a corpus" from Gardner and Mitchell (2015). The authors should make clear if the task they seek to solve is the same. Secondly, I could not understand the experimental setting from line 532 to line 547, so I could not really understand if the experimental evaluation is fair or not. The same applies for the experimental setting in section 7.3.

Regarding the relation similarity task I have some doubts. First the authors do not state on what dataset word2vec representations are learned. Second I do not understand how the representations of relations learned with the proposed model are shaped. Is the size of distribution over candidates the same for each relation? And more importantly, are the single elements of probability vectors comparable in different relations? I doubt it.

Finally, this model has a lot of commonalities with the recent paper of Reddy et al. (2014), and I would have liked a thoroughly comparison with this model.

----------

PRESENTATION QUALITY

I found the presentation a bit too messy, the English is fairly good, although it should be improved. I suggest the authors to use more examples to explain concepts, instead of lengthy descriptions.

----------

DETAILED COMMENTS

lines 483-484, typos.

lines 485-489 I would suggest to rephrase this sentence to make it clearer.

line 616 rasied -> raised, I would suggest the authors to check spelling on the entire paper.

Check the format of bibliography

lines 866 - 867 the actual citation should be "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space".

Review #2

APPROPRIATENESS:	5
CLARITY:	5
ORIGINALITY:	3
EMPIRICAL SOUNDNESS / CORRECTNESS:	4
THEORETICAL SOUNDNESS / CORRECTNESS:	4
MEANINGFUL COMPARISON:	4
SUBSTANCE:	4
IMPACT OF IDEAS OR RESULTS:	4
IMPACT OF ACCOMPANYING SOFTWARE:	1
IMPACT OF ACCOMPANYING DATASET:	1
RECOMMENDATION:	4
MENTORING:	NO
AUTHOR RESPONSE:	N/A
Comments

- Strengths:

The work presents substantial progress on using Knowledge Bases and generalisation to unknown relations not presented directly in the original Knowledge Base. It extends the path representation of a schema to a tree. This allows representing more complex natural language relations. The evaluation is of the presented technique is solid.

- Weaknesses:

The essential parts of the work have not been discussed in detail enough. First, I would like to see a formal algorithm for the candidate schema generation as the textual description is not the best do describe an algorithm. Second I would like to a discussion why their choice of silver label function is good and what it means. The work presents a clear description of the function but no analysis nor explanation.

- General Discussion:

The work presented the usefulness of the approach on three reasonable tasks and showed that it brings improvements on the state-of-the-art or it has competitive performance.

Review #3

APPROPRIATENESS:	5
CLARITY:	4
ORIGINALITY:	3
EMPIRICAL SOUNDNESS / CORRECTNESS:	3
THEORETICAL SOUNDNESS / CORRECTNESS:	3
MEANINGFUL COMPARISON:	3
SUBSTANCE:	3
IMPACT OF IDEAS OR RESULTS:	3
IMPACT OF ACCOMPANYING SOFTWARE:	1
IMPACT OF ACCOMPANYING DATASET:	1
RECOMMENDATION:	2
MENTORING:	NO
AUTHOR RESPONSE:	N/A
Comments

This paper presents a framework to match the natural language relational patterns to schema graphs defined in a curated knowledge base. The proposed approach is quite reasonable and the problem is indeed important for both question answering and relation extraction. My main concern is that the proposed schema graph design is awfully similar to some work in semantic parsing for question answering using Freebase (e.g., Reddy et al. and Yih et al.), published in 2015, but such connection is not discussed at all. In addition, the experiments are done in relatively small scales. The practical value of this work remains to be proven.

I actually like the proposed research problem, and think it's a very important topic that hasn't been addressed fully. While the schema graph design is quite reasonable, very similar ideas have nevertheless already been proposed, such as the semantic parse in a graphical representation in (Reddy et al., 2015). Also, if we view x_subj as the "topic entity" and x_obj as the "answer entity" in (Yih et al., 2015), then the skeleton is essentially what's been referred to as "inferential chain" there. The design of "constraints" also has appeared in the previous work. Since the design of schema graphs is one of the key contributions in this work, it is important to articulate how it defers from existing work.

Another potential issue is the scale of the experiments. I appreciate that the authors have conducted experiments on several tasks and datasets. However, the sizes of the testing cases seem quite small and sometimes the baseline methods are not really the state of the art. As a result, the usefulness of the proposed approach remains questionable.

More detailed comments:

1. Lines 323-335: "In practice, multiple constraints on the same variable seldom make sense." Is this really true? I can see that several constraints associated with a CVT node make sense, as they are effective other arguments of the same multi-argument relation.

2. Sec. 5: The scoring functions are somewhat ad-hoc. It would be better if the chosen functions can be better justified.

3. Sec. 7.2: Why do you just use a subset of Freebase? Is this related to the scalability of the proposed method? If so, it's better to discuss the computational complexity (either in theory or in practice) as well.

4. Lines 618-619: 120 questions seem too small, given that the whole set contains more than 2000 questions in the test set. Also, the output of several new state-of-the-art systems in 2015 is available, and should be compared to instead.

5. Lines 631-632: Controlling the number of returned answers is a bit weird because if the parse is correct, then all the answers should be correct.

6. Sec. 7.5: Given that the size of the dataset is quite small and the results of different methods/baselines are pretty close, it's hard to draw very meaningful conclusion.
