\section{Experiments}
\label{sec:eval}
In this section, we first present the setup of the experiments, then show
three main experiments. The first experiment compares several different
decoder models in the basic RE tagging task. The second experiment compares 
the three triple construction algorithms and the last one shows the end-to-end
RE results from the baselines and our methods. We conclude the
section with a case study to explain why our method outperforms the rest.
\KZ{Add an experiment to evaluation the RE tagging task itself.}

\subsection{Setup}
There are three public datasets for the relation extraction task,
BioInfer~\cite{}, Wiki-KBP~\cite{} and NYT~\cite{}. BioInfer is a dataset in the medical area, and over
50\% of sentences contain overlapping relations, i.e., an entity may take part
in multiple relations in a sentence. This is beyond the scope of this paper.
The test set of Wiki-KBP contain many relation types not found in the 
training set. This make supervised learning approach impractical. Therefore
in this paper, we choose NYT as our dataset.
NYT contains 236k training pairs, and 395 test pairs with 24 relation types. 
%For each pair, there is a sentence, a entity list and
%a relation triple list. NYT dataset contain all the entity information and
%relation information in whole sentence. So NYT dataset can be used to do both
%NER task and relation extraction task. There are 24 types of relation. The
%entity type is 3 typical entities just like the specific dataset for NER task,
%``Person'', ``Location'' and ``Organization''.
%To conform the quality of test set, all the relation triples and entities in the test set have been checked by human effort.
%
%\subsection{Evaluation Metrics}
In each run, we random sample 10\% pairs from the
original test set as out validation data, and the remaining 90\%  for
test. 
%For each time of running, this random split operation will repeat. To make
%the result stable and convincing, 
Every result we show in this section is the average of 10 runs.

We use standard Precision, Recall and F1 to measure the 
performance of our model. 
%For NER task, we treat a generated entity as correct if the entity
%type and all the tokens are correct. 
For the end-to-end RE task, a relation
triple is correct when the two entities and the relation type are all correct.
%Note that the relation has direction. So the entities in a relation triple must
%have the right role. 
%For RTD task, we do not take the repeated relation type
%into consideration. This means if a sentence mentions 2 relation triple, and
%these 2 relation triple share the same relation type, we just use 1 to
%represent the existance of this relation type.
%
%\subsection{Hyperparameters}
We pretrain the word embedding using Word2Vec on the training data.
\tabref{tab:hyper} shows all the hyper-parameters used in this section. 
\KZ{Check the hyperparams again. I think some of the names are not
right.}

\begin{table}[th!]
\small
  \begin{center}
  \caption{Hyper Parameters} \label{tab:hyper}
    \begin{tabular}{l|c}
      \hline \bf Hyperparameters & \bf Value \\ \hline
      embedding size & 300 \\
%      NER decoder LSTM size & 300 \\
      kernel size & 6 \\
      channel out & 300 \\
      RE decoder LSTM size & 600 \\
      RTD balance weight & 0.1 \\
      RE balance weight & 0.1 \\  
      batch size & 64 \\
      dropout & 0.5 \\
      lr & 0.0001 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

%\subsection{Baselines}

Our baseline methods are divided into three categories, pipeline
methods (FCM \cite{Gormley2015}, DS+logistic
\cite{Mintz2009} and LINE \cite{Tang2015}), joint-learning methods
(MultiR \cite{Hoffmann2011}, DS-Joint \cite{Li2014}
and CoType \cite{Ren2017}), and Zheng's basic RE tagging methods (with
three types of decoders)~\cite{Zheng2017}.
%
%For pipeline methods, we choose  for compairson. For
%Multi task methods, we choose  for comparison. As far
%as we know, Zheng's work is the first one to do end2end relation extraction.
%They proposed 3 variant models. LSTM-LSTM-Bias  is the state of the art model.
%Zheng's work is very similar to the single RE model here. We use the sample
%tagging schema. However Zheng use a different decoder LSTM archetecture and the
%relation constructor is different from us. The constructor that zheng used is
%order first algorithm. Also we will do some experiments to compare Zheng's constructor and our.
%
\input{latex/results}
