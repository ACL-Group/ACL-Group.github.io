\subsection{Different Decoder Models}
Zheng et al. used relative complex variant of LSTM as the simple RE tagging
decoder. Evaluate the effectiveness of different alternatives, 
we compare Zheng's LSTM cell with two popular
RNN cells, namely normal LSTM and GRU on the basic RE tagging task.
To enable comparison with Zheng's results, we use order-first algorithm to
construct triples. \tabref{tab:decode} shows the results.

\begin{table}[th!]
  \begin{center}
  \small
  \caption{Results from Different Decoders}
  \label{tab:decode}
    \begin{tabular}{c|ccc}
      \hline
      \bf Models & \bf Prec. & \bf Rec. & \bf F1 \\
      \hline
      Zheng-LSTM  & \textbf{.615} & .414 & .495 \\
      LSTM   & .592 & .444 & \textbf{.507} \\
      GRU    & .579 & .438 & .499 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

Vanilla LSTM beats the rest by narrow margin in F1. 
We can conclude that Zheng's complex LSTM is not any better than the
basic LSTM, therefore in all other experiments, we adopt the vanilla
LSTM as our decoder.
%no much difference in F1 value between 4 LSTM cells. This conclusion is
%consistent with many work by others. However, the ability to balance precision
%and recall is really diffenrent. The difference between precision and recall
%values is beyond $0.20$, while this difference of any one of other 3 LSTM cell
%is limited in $0.15$. The precision value of Zheng is the highest and the recall
%is the lowest. Therefore,  we choose Pure-LSTM as our decoder in later experiments.


\subsection{Different Construction Algorithm}
In this experiment, we compare three different triple construction algorithms
in \tabref{tab:cons1}. $e_1$-first algorithm has a clear advantage
than $e_2$-first, which is in turn better than the default order-first.
This can be explained if we look at the accuracy of $e_1$ and $e_2$ in
the RE-tagging results in \tabref{tab:cons2}.
\begin{table}[th!]
  \small
  \begin{center}
  \caption{Different Construction Algorithms on Triple Accuracy}
  \label{tab:cons1}
    \begin{tabular}{c|ccc}
      \hline
      \bf Models & \bf Prec. & \bf Rec. & \bf F1 \\
      \hline
      $e_1$-first   &  \textbf{.631} & \textbf{.480} & \textbf{.545}  \\
      $e_2$-first   &  .605 & .450 & .516  \\
      Order-first  &  .592 & .444 & .507  \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\KZ{Add another algorithm that gives the nearest pairs.}
From \tabref{tab:cons2}, the accuracy for tagging $e_1$ is higher than
$e_2$ consistently. \KZ{Why is that?} As a result, the $e_1$-centric 
triple construction algorithm gives better overall accuracy than if we
use $e_2$ as the dominating entity. Order-first algorithm is worst because
it imposes a strict order of selecting entity pairs, which is more brittle
than other two algorithms. In the rest of this section, we will
use $e_1$-first as our triple construction algorithm.

\begin{table}[th!]
  \small
  \begin{center}
  \caption{RE Tagging Accuracy on the Entities}
  \label{tab:cons2}
    \begin{tabular}{ccc|ccc}
      \hline
       \multicolumn{3}{c|}{$e_1$} & \multicolumn{3}{c}{$e_2$} \\
      \hline
      	 Prec. & Rec. & F1 & Prec. & Rec. & F1 \\
      \hline
%        .650 & .606 & .627 & .616 & .603 & .609 \\
        .660 & .603 & .630 & .622 & .600 & .610 \\
%        .653 & .594 & .621 & .620 & .601 & {\bf .610} \\
      \hline
    \end{tabular}
  \end{center}
\end{table}



%As we can see from table 3, Order-First is worst, and Ele2-First is a little better
%than Order-First. Ele1-First is the best algorithm which achieve near $4$
%percent points than Order-First. The reason can be found in tabel 4 which show
%the preformance on signle element and element pair.
%
%The metrics on $e_1$ and elment2 is very near between these 3 experiments.
%Because the only different between these 3 experiments is the construction algorithm
%which do not take part in the training process. The ability to predict correct tag
%sequence should be near each other. The prediction of signle element has nothing
%to do with construction algorithm. By comparing the performance of $e_1$ and
%$e_2$, we can find the ability to predict correct $e_1$ is pretty high
%than $e_2$. The recall value of $e_1$ and $e_2$ is all most the same.
%But the precision values have a big difference. This means the model has predict
%more wrong $e_2$ than $e_1$. In other word, there is more correct $e_1$
%than $e_2$. Therefore, it is better to choose $e_1$ as the dominating
%entity rather than $e_2$. This is the reason why Ele1-Frist outperforms
%Ele2-First a lot. Order-First treats $e_1$ and $e_2$ in equal position
%which means this algorithm depends on the accuracy of both $e_1$ and
%$e_2$. In other words, the performance of Order-First is limited by the worse
%one of the two elements. This is the reason why Order-First is a little worse
%than Ele2-Frist.

\subsection{End-to-end RE Results}
%We have proved Pure-LSTM is more suitable for the decoder of EndRE module in 4
%kinds of LSTM cells. And Ele1-First algorithm is the best one to construct
%triple from relation tag sequence. We test the performance of our full
%multi-task model based on Pure-LSTM and Ele1-Frist algorithm. 
\tabref{tab:e2e} shows the accuracy of end-to-end relation triple extraction from
all baseline methods as well as our methods. Because our method enhances
Zheng's LSTM-LSTM-Bias (LLB) model, we name our method LLB+, which means
LLB plus RTD.

\begin{table}[th!]
  \small
  \begin{center}
    \begin{tabular}{cccc}
      \hline
      \bf Models & \bf Prec. & \bf Rec. & \bf F1 \\
      \hline
      FCM & .553 & .154 & .240 \\
      DS+logistic & .258 & .393 & .311 \\
      LINE & .335 & .329 & .332 \\
      \hline
      MultiR & .338 & .327 & .333 \\
      DS-Joint & .574 & .256 & .354 \\
      CoType & .423 & .511 & .463 \\
      \hline
      LSTM-CRF & \textbf{.693} & .310 & .428 \\
      LSTM-LSTM & .682 & .320 & .436 \\
      LSTM-LSTM-Bias & .615 & .414 & .495 \\
      \hline
      LLB+ & .623 & \textbf{.517} & \textbf{.565} \\
%      LLB+ (-NER) & .618 & .513 & .561 \\
      LLB+ (-RTD) & .618 & .498 & .552 \\
%      LLB+ (-NER-RTD) & .631 & .480 & .545 \\
      \hline
    \end{tabular}
  \end{center}
  \caption{End-to-end RE Results}
  \label{tab:e2e}
\end{table}

\begin{table*}[th!]
  \small
  \begin{center}
  \caption{Accuracy of Entity Tagging}
  \label{tab:entity}
    \begin{tabular}{c|ccc|ccc}
      \hline
      Entity & \multicolumn{3}{|c|}{E1} & \multicolumn{3}{|c}{E2} \\
      \hline
      	& Prec. & Rec. & F1 & Prec. & Rec. & F1 \\
      \hline
      LSTM-CRF & .596 & .325 & .420 & .605 & .325 & .423 \\
      LSTM-LSTM & .593 & .342 & .434 & .619 & .334 & .434 \\
      LSTM-LSTM-Bias & .590 & .479 & .529 & .597 & .451 & .514 \\
      \hline
      LLB+  & .628 & .657 & .642 & .594 & .647 & .619 \\
%      LLB+ (-NER) & .620 & .649 & .634 & .589 & .645 & .615 \\
      LLB+ (-RTD) & .623 & .661 & .641 & .592 & .647 & .618 \\
%      LLB+ (-NER-RTD) & .650 & .606 & .627 & .616 & .603 & .609 \\
      \hline
    \end{tabular}
  \end{center}
\end{table*}

Compared to the state-of-art model LSTM-LSTM-Bias, our method LLB+
logged 7\% substantial improvement on F1. This gain is primarily due to 
the advantage in recall, which is more than 10\%. 
%The precision values between LSTM-LSTM-Bias and MT-EndRE is not different
%too much, but MT-EndRE improve the recall value very much which achieves more
%than $10$ percent points gain. 
By looking at the accuracying of tagging $e_1$ and $e_2$ in \tabref{tab:entity}, 
we see that the recall of both $e_1$ and $e_2$ improve dramatically over 
LSTM-LSTM-Bias, while the precision remains relatively unchanged. 
This means our multi-task model predicts more entity pairs correctly
without lowering the precision.
%Therefore, our model can achieve remarkable gain in recall of relation triple.
%Finally, MT-EndRE can beat the state-of-the-art model a lot. Also our multi-task
%model is much better than all the pipeline and joint approches.

Further, to show the effect of the two auxiliary tasks in our model, we conduct
the ablation tests (shown in lowest section of \tabref{tab:e2e}). 
We find that the auxiliary task RTD is useful in boosting the basic RE model. 
%while RTD task provides more boost than NER.  
%LLB+(-NER-RTD) is the model without NER and RTD module 
%which is just the RE tagging module by itself. By comparing with full multi-task model
%MT-EndRE, without NER module the F1 value of triple will drop a little while
%without RTD module the F1 value will drop more. If getting rid of both NER and
%RTD, the performance will drop a lot. This means RTD module can provide more
%help to the EndRE by comparision with NER. There may be two reasons which cause
%RTD is more helpful than NER. 
This is because 
%i) NER is a relatively easy task, and previous research has shown
%accuracy above 0.92, so the basic RE tagging module itself can handle it
%rather well; ii) 
RC is a harder task but its relative, RTD, is able to  
provide global information of whole sentence which
complement the information obtained by the RE tagging alone. 
\KZ{Say a little more here?}

\subsection{Case Study}
To illustrate the effectiveness of our proposal, we present two cases
from the test set. The first sentence is used to show the effectiveness of our
joint model LLB+ by comparing with LLB. \{India, Contains, Bihar\} is the gold
triple. Both LLB+ and LLB mistake the number ``351'' as $e_2$ because
of the misleading context ``in''. Based on the tag of ``351'', LLB treat
``Bihar'' as the $e_2$ which is wrong. However, the LLB+ model 
predicts ``Bihar'' as $e_2$ which seems to conflict with the existing $e_2$ ``351''.
``India'' is predicted as $e_1$ correctly for two models. Eventually, 
LLB+ succeeds in constructing the correct triple. 
This is because LLB heavily relies on the local information (nearby entities) 
while LLB+ has a more global view.

The second sentence illustrates the difference between the triple construction
algorithms. In this case, all of the construction algorithms operates on the
same RE tag sequence. $e_x$-first re-constructs the correct triple \{Iowa,
Contains, Waverly\} while order-first does not. Because this tag sequence contain
several redundant $e_2$, the correct $e_2$ is the nearest to $e_1$ ``Iowa''.
However, order-first algorithm re-constructs the wrong triples \{Iowa,
Contains, Wartburg College\}. As for $e_2$-First, it only
predicts the two correct entities, it can only re-construct one triple which is
the correct one. \KZ{Find another example with the same tag sequence.}

\begin{table*}[th!]
  \small
  \begin{center}
  \caption{Two Examples in Case Study}
  \label{tab:case}
    \begin{tabular}{lp{12cm}}
      \hline
      Gold Standard 1& the death toll stood at 351 in Bihar[Contains-2-S] state , which is home to this village and many others sitting on some of the most vulnerable floodplains in India[Contains-1-S] . \\
      LLB+ & the death toll stood at 351[Contains-2-S] in Bihar[Contains-2-S] state , ... the most vulnerable floodplains in India[Contains-1-S] . \\
      LLB & the death toll stood at 351[Contains-2-S] in Bihar[Country-1-S] state , ...  the most vulnerable floodplains in India[Contains-1-S] . \\
      \hline
      Gold Standard 2 & Jean-Luc Portal , 51 , a wine consultant in Somers[Contains-2-S] who grew up in the tiny village of Alzonne[Contains-2-S] in southern France[Contains-1-S] , said he became obsessed with the game when he was 6 . \\
      LLB ($e_1$-First) & \{ France, Contains, Alzonne \} \\
      LLB ($e_2$-First) & \{ France, Contains, Somers \} \\
      LLB (Order-First) & \{ France, Contains, Somers \} \\
      \hline
    \end{tabular}
  \end{center}
\end{table*}




