\section{Model Training}
\label{sec:train}

There are two tasks in our joint model. We need one loss function for each 
task to train the joint model. We use a weighted sum of these losses as
the joint loss.

%\subsection{NER Tagging Loss}
%As for NER module, we maximize the log-probability of correct tags. Given a
%sentence, most of tokens do not take part in any entities, they share the same
%tag ``O'' while only a few tokens take part in some entities. Obviously, this
%is a unbalance classification problem. Therefore, we weight the loss for
%different tags.
%
%
%\begin{equation}
%  \begin{aligned}
%    L^E = \frac{1}{M} \max{
%      \sum_m^M{
%        \sum_i^N{
%          \log(p_{i, k_{\hat{e}}}^E) \cdot I^E(\hat{e}),
%        }
%      }
%    } \nonumber
%  \end{aligned}
%\end{equation}
%where $M$ is the size of the corpus. $N$ is the length of sentence.
%$\hat{e}$ is the correct tag of token $w_i$, and $k_{\hat{e}}$ is the index of
%tag $\hat{e}$ is the tag set. $p_{i, k_{\hat{e}}}$ is the probability value of the
%correct tag $\hat{e}$. $I^e(e)$ is the loss weight function which is defined as
%follows:
%
%\begin{equation}
%  \begin{aligned}
%    I^E(e) =
%    \begin{cases}
%      1,        & if \  e = \text{'O'} \\
%      \alpha_e, & if \  e \neq \text{'O'}, \\
%    \end{cases} \nonumber
%  \end{aligned}
%\end{equation}
%where $\alpha_e$ is the loss weight of entity tag. The loss weight of tag ``O'' is constant
%$1$.
%

\subsection{RTD Loss}
In the RTD module, we max the log probability of correct relation label. Because
most of sentence does not mention even one relation type, this is a unbalance
multi-label classification problem. We also weight the loss function.

\begin{align*}
  L^R = \frac{1}{M} \max &\sum_m^M\sum_i^N
  \biggl[ \hat{r}_i \log(p_i^R) \cdot I^R(\hat{r}_i) + \\
  &{} (1 - \hat{r}_i) \log(1 - p_i^R) \cdot I^R(\hat{r}_i)
    \biggr],
\end{align*}
where $\hat{y}_i$ is the correct relation type vector. $p_i^R$ is the predicted
probability vector. $I^R(r)$ is the loss weight function which is defined as
follows:

\begin{equation}
  \begin{aligned}
    I^R(r) =
    \begin{cases}
      1,        & if \  r = 0 \\
      \alpha_r, & if \  r = 1 \\
    \end{cases} \nonumber
  \end{aligned}
\end{equation}
where $\alpha_r$ is the weight of mentioned relation type. For all unmentioned type,
the weight is 1.

\subsection{Basic RE Tagging Loss}
For end-to-end RE module, we max the probability of correct relation tag in the
same way as NER module.

\begin{equation}
  \begin{aligned}
    L^T = \max{
      \sum_m^M{
        \sum_i^N{
            \log(p_{i, k_{\hat{t}}}^T) \cdot I^T(\hat{t}),
        }
      }
    } \nonumber
  \end{aligned}
\end{equation}
where $\hat{t}$ is the correct tag of token $w_i$. $k_{\hat{t}}$ is the index of
$\hat{t}$ in the tag set. Due to $p_i$ is the probability vector of token $w_i$
over tag set, $p_{i, k_{\hat{t}}}^T$ is the probability of the correct tag of
$w_i$. $I^T(t)$ is loss weight function which is defined as follows:

\begin{equation}
  \begin{aligned}
    I^T(t) =
    \begin{cases}
      1,        & if \  t = \text{'O'} \\
      \alpha_t, & if \  t \neq \text{'O'}, \\
    \end{cases} \nonumber
  \end{aligned}
\end{equation}
where $\alpha_t$ is the loss weight of relation tag. The loss weight of tag ``O'' is
constant $1$.

%\subsection{Joint Loss}
%We use the weighted sum of these 3 loss as joint loss.
%\begin{equation}
%  \begin{aligned}
%    L = \lambda_eL^E + \lambda_rL^R + \lambda_tL^T, \nonumber
%  \end{aligned}
%\end{equation}
%where $\lambda_e$, $\lambda_r$ and $\lambda_t$ are weight values of 
%3 tasks respectively. The larger weight value is, 
%the bigger impact of the task on the shared parameters.
%


