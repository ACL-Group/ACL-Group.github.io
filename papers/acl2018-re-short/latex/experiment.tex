\section{Experiments}
\label{sec:eval}
In this section, we introduce the dataset and baseline models.
We first evaluate the effectiveness of vanilla LSTM decoder,
and compare our triple results with other relation extraction models.
In addition, we give a case study to explain the differences between different
triple construction algorithms.

%In this section, we first present the experimental settings, then show two
%experiments. The first experiment examines several choices of
%LSTM decoders and we conclude that vanilla LSTM is a best choice.
%The second experiment compares 
%the four triple construction algorithms with a number of baselines. 
%We conclude the section with a case study to explain the differences between 4
%construction algorithms.

\subsection{Experimental Setup}

\noindent
\textbf{Relation Extraction Datasets:}
there are 3 public datasets \footnote{ All these 3 datasets can be downloaded at
https://github.com/shanzhenren/CoType} for the relation extraction task,
BioInfer \cite{pyysalo2007bioinfer}, Wiki-KBP \cite{ling2012fine} and NYT \cite{Ren2017}.
This NYT dataset containing 236k training sentences, and 395 testing sentences
with 24 relation types.
All training pairs are automatically generated via distant supervision,
%between NYT corpus and triple facts from FB,
and testing pairs are manually labeled.
BioInfer has over 50\% of sentences which contain overlapping relations, which is
beyond the scope of this paper.
Wiki-KBP is built from 1.5 million Wikipedia articles with 12 relation types,
while 6 out of 12 relation types are totally unobserved in its training set,
%The test set of Wiki-KBP contain many relation types not found in the training set,
which is not suitable for supervised learning approaches.
Therefore, we choose NYT as the our dataset.
%Therefore in this paper, we choose NYT as our dataset. 

\noindent
\textbf{Baseline Models:}
our baseline methods are divided into three categories, pipeline RE
methods FCM \cite{Gormley2015}, DS+logistic
\cite{Mintz2009} and LINE \cite{Tang2015}), joint RE methods
MultiR \cite{Hoffmann2011}, DS-Joint \cite{Li2014}
and CoType \cite{Ren2017}), and RE tagging method proposed by Zheng et
al.~\shortcite{Zheng2017}.
In addition, 3 different LSTM-based tagging networks are evaluated by Zheng et al.,
we follow the proposed LSTM-LSTM-Bias structure, as reaching the best performance.
We use LLB (*) to indicate our sequential tagging model with variational triple
construction algorithms, for example, LLB ($e_1$-First).

%Among them, LSTM-LSTM-Bias proposed
%by \cite{Zheng2017} is state-of-the-art. Because our model is build on
%LSTM-LSTM-Bias by using different LSTM cell and different relation construction
%algorithm, we use LLB(*) to indicate our models where * is one of the
%construction algorithm like LLB($e_1$-First).


\noindent
\textbf{Evaluation Metrics:}
we use standard precision, recall and F1 score to measure relation triple
performance of our model.
A relation triple is correct if $e_1$, $e_2$ and $r$ are all correct.
Besides, to evaluate the accuracy of the RE tagging model,
we calculate the precision, recall and F1 score over all labeled entities $e_1$ and $e_2$.
The effectiveness of the tagging step is not impacted by construction algorithms.
%We also calculate these metrics on triple's entities, namely $e_1$ and $e_2$.
%The performance of these entities only depends on the predicted tag sequence,
%and are not impacted by construction algorithms.
Following the setting of Zheng et al.~\shortcite{Zheng2017} and Ren et al.~\shortcite{Ren2017},
for each run, we randomly sample 10\% sentences from the original test set as validation data,
and the remaining 90\% for test. All results we report are averaged over 10
times run.

\noindent
\textbf{Hyper parameters:}
We set hidden sizes of BiLSTM and decoder LSTM to be 300 and 600. 
The word embeddings are pre-trained using Word2Vec \cite{mikolov2013distributed}
on the training data, and its dimension is 300.
We apply dropout to the embedding layer with keep rate 0.5.
We adopt RMSProp~\cite{tieleman2012lecture} as the optimizer with learning rate 0.0001,
and the batch size is 64.
All hyper parameters above are tuned on validation data.



% \begin{table}[th!]
%   \small
%   \caption{Hyper Parameters}
%   \label{tab:hyper}
%   \begin{center}
%     \begin{tabular}{l|c}
%       \hline \bf Hyperparameters & \bf Value \\ \hline
%       embedding size & 300 \\
%       BiLSTM cell size & 300 \\
%       decoder LSTM size & 600 \\
%       dropout rate & 0.5 \\      
%       batch size & 64 \\
%       lr & 0.0001 \\
%       \hline
%     \end{tabular}
%   \end{center}
% \end{table}

%\subsection{Baselines}


\input{latex/results}
