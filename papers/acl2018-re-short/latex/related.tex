\section{Related Work}
RE \cite{Bach} is a fundamental task in NLP. Most of previous work consider it 
as comprising two sequential sub-tasks, NER \cite{Nadeau2007} and 
RC \cite{Zeng2014,Wang2016,Wen2017,Cai2016,Zhou2016,Xu2015}. 
%NER is responsible for the entity part in triples while RC is 
%to identify relation type. Both NER and RC has been well-studied.
In early stage of NER, many classical machine learning methods have been applied
successfully such as Hidden Markov Models (HMM) \cite{Passos2014} 
and Conditional Random Fields (CRF) \cite{Luo2015}. With the rapid 
developement of deep learning, many DNN models have been
proved effective. LSTM-CRF \cite{Lample2016} model is a successful DNN model for NER. 
In this model, a bidirectional LSTM is used to extract features from sentence for each
token. CRF is the tag decoder which predict entity tag for each token. Then
entities can be constructed from tag sequence. 
LSTM-CNNs-CRF \cite{Ma2016} is a variant of
LSTM-CRF, which uses CNNs to extract character-level features.

%Sequential RE approaches extract entities using a NER model and identify relation
%type using a totally separated RC model. 
Both CNN and LSTM have been shown to be effective for RC task. 
Zeng et al. proposed a CNN architecture \cite{Zeng2014} to extract
lexical and sentence level features to predict relation type for an entity pair
which beats the state-of-the-art statistical machine learning models. Moreover,
position embedding was used in their model to encode entity location features.
Wang et al. added multi-level attention \cite{Wang2016} to CNN in order to 
better extract features in heterogeneous contexts.

Joint learning RE methods also sequentially attack NER and then RC. 
Different from pipelined approaches, NER module and RC module share some training
parameters with each other which can make use of features from each other to
some extent. Many feature-base approaches \cite{Ren2017,Li2014,Miwa2014,Kate2010} 
have been applied. Recently, several netural networks
\cite{Zheng2017,Katiyar2017,Miwa2016,Zheng2017a,Li2017a,Miwa2014} have shown to
be effective, too. CoType \cite{Ren2017} is a domain-independent
framework which uses a text segmentation algorithm to extract entities and embeds
relation mentions into low-dimensional space. The embedding can be used to
predict relation types.

Both pipelined and joint approaches extract triples based on the entity results
from NER module which must cause error propagation. To tackle this issue, 
Zheng et al. redefined the RE as a tagging problem through a novel 
tagging scheme \cite{Zheng2017}. In this approach, instead of the usual
NER followed by RC sequence, a setence is first tagged and then triples are
reconstructed from the tag sequence.
This approach appears to be more robust than previous pipelined or joint approaches,
at least on the NYT data set. Our work extends Zheng's approach by using a
multi-task learning model boosted by the relation type detection module. 
We also propose new triple reconstruction algorithms that outperforms 
the previous approach.

