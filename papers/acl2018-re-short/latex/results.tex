\subsection{Vanilla LSTM Decoder Results}
Zheng et al. \shortcite{Zheng2017} used a relative complex variant of LSTM as the RE tag
decoder. To evaluate the effectiveness of normal LSTM cell, we do an experiment
using vanilla LSTM as decoder. To enable comparison with Zheng's results, here
we also use order-first algorithm to construct triples.

\begin{table}[th!]
  \small
  \caption{Triple results of two LSTM decoders}
  \label{tab:decode}
  \begin{center}
    \begin{tabular}{c|ccc}
      \hline
      \bf Decoder & \bf Prec. & \bf Rec. & \bf F1 \\
      \hline
      Zheng-LSTM  & \textbf{.615} & .414 & .495 \\
      LSTM   & .554 & \textbf{.456} & \textbf{.500} \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

From the results in \tabref{tab:decode},
%Vanilla LSTM beats two kinds of RNN cells by narrow margin in F1.
we find that vanilla LSTM outperforms Zheng-LSTM by narrow margin in F1 score.
Although F1 results are similar,
the precision and recall show that the LSTM variations work in a rather different way.
%the values of precision and recall are really different.
Zheng-LSTM prefer to predict fewer entities in the tagging step,
leading to high precision but low recall.
As the comparison, the vanilla LSTM is more balanced,
and we adopt the vanilla LSTM as our decoder in the remaining experiments.
%For example, Zheng-LSTM is much higher in precision while vanilla
%LSTM is much higher in recall. According to the F1 value, in later experiments,
%we adapt the vanilla LSTM as our decoder.


%Vanilla LSTM beats Zheng-LSTM by narrow margin in F1. Although
%there are little differences in F1, the values of precision and recall are really
%different. Zheng-LSTM is much higher in precision while vanilla
%LSTM is much higher in recall. According to the F1 value, in later experiments,
%we adapt the vanilla LSTM as our decoder.


%is much better than
%$e_2$-First, and it is lower than Distance-First by nearly $1$ percent point.
%These results can be explained if we look at the metrics of $e_1$ and $e_2$ in
%\tabref{tab:entity}.

%\subsection{Compare Construction Algorithms}
\subsection{Relation Triple Results}
In this part, we evaluate the performance of different triple construction algorithms,
and compare with all the baseline methods.
We report the relation triple results in \tabref{tab:e2e}.
Among all the triple construction algorithms, our Distance-First strategy is the best one,
which outperforms the F1 score of the state-of-the-art by an absolute gain of 0.07.
%which ourperforms the worest algorithm, Order-First of \cite{Zheng2017} by $6.5$ percent points in F1.
The second best algorithm, $e_1$-First, shows a slight F1 decrease (less than 0.01),
but is still much higher than the opposite algorithm $e_2$-First.
The most interesting point is that, the Order-First algorithm works even worse than
the baseline of random combination.
%is much better than
%$e_2$-First, and it is lower than Distance-First by nearly $1$ percent point.
%These results can be explained if we look at the metrics of $e_1$ and $e_2$ in
%\tabref{tab:entity}.


\begin{table}[ht]
  \small
  \caption{Triple results of different models.}
  \label{tab:e2e}
  \begin{center}
    \begin{tabular}{cccc}
      \hline
      \bf Models & \bf Prec. & \bf Rec. & \bf F1 \\
      \hline
      FCM & .553 & .154 & .240 \\
      DS+logistic & .258 & .393 & .311 \\
      LINE & .335 & .329 & .332 \\
      \hline
      MultiR & .338 & .327 & .333 \\
      DS-Joint & .574 & .256 & .354 \\
      CoType & .423 & .511 & .463 \\
      \hline
      LSTM-CRF & \textbf{.693} & .310 & .428 \\
      LSTM-LSTM & .682 & .320 & .436 \\
      LSTM-LSTM-Bias & .615 & .414 & .495 \\
      \hline
      LLB (Random) & .571 & .470 & .515 \\
      LLB ($e_1$-First) & .617 & .508 & .557 \\
      LLB ($e_2$-First) & .579 & .476 & .523 \\
      LLB (Distance-First) & .626 & \textbf{.515} &\textbf{.565} \\
      LLB (Order-First) & .554 & .456 & .500 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}



\begin{table}[h!]
  \small
  \caption{Results of triple's entity.}
  \label{tab:entity}
  \begin{center}
    \begin{tabular}{ccc|ccc}
      \hline
      \multicolumn{3}{c}{\bf $e_1$} & \multicolumn{3}{|c}{\bf $e_2$} \\
      \hline
       Prec. & Rec. & F1 & Prec. & Rec. & F1 \\
      \hline
      .622 & .663 & .641 & .585 & .658 & .619 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

In order to discuss the results in detail, we evaluate the RE tagging module
and evaluate the P/R/F1 metric of predicted correct $e_1$ and $e_2$ entity.
%Because we test different construction algorithms based on same RE tagging module.
%Entity results for different algorithms should be the same.
%There just one line in \tabref{tab:entity}.
Experimental results in \tabref{tab:entity} show that,
%By comparing the performance of $e_1$ and $e_2$, we can find
the RE tagging module is better at predicting correct $e_1$ than $e_2$.
Specifically, with the same level of recall (0.663 versus 0.658),
the model outputs much more incorrect $e_2$ than $e_1$.
%This means much more wrong $e_2$s has been predicted than $e_1$s. In other word, there are more correct $e_1$s than $e_2$.
Therefore, it is better to choose $e_1$ as the dominating entity rather than $e_2$,
which explains why $e_1$-First outperforms $e_2$-First by a large margin.

In terms of the Distance-First approach, though no explicit dominating entity is used,
the distance information is an important feature for generated relation triples,
because human prefers to express the entity arguments of one triple in a near manner.
Finally, the Order-First approach looks like intuitive,
but the alignment process is rather brittle: refer to \algoref{algo:construct},
once an incorrect $e_1$ is accidentally added into the entity set $E_1$,
the triple combination of all the other entities after it will be affected.
The detail analysis is given in the section of case study.


%Because Order-First treats $e_1$ and $e_2$ in equal position
%which means this algorithm depends on the accuracy of both $e_1$ and
%$e_2$. Therefore, the performance of Order-First is limited by the worse
%one of $e_1$ and $e_2$. This is the reason why Order-First is worse
%than both $e_1$-First and $e_2$-Frist. Distance-First algorithm is the
%best algorithm which even beat $e1$-First. This shows that human prefers to
%express two entities with some relationship in a near manner.


%Finally, the our LLB(Distance-First) algorithm achieve $7$ percent points
%gain over the state-of-the-art result. Also we beat all the pipeline and joint
%model in the baselines.

%\KZ{Why is LLB(Order-first) diff from LSTM-LSTM-Bias in terms of P and R?
%They might argue that this is not a truthful implementation of Zheng..}


\subsection{Case Study}
We choose a representative example to illustrates the difference between
different triple construction algorithms.

\begin{table}[ht]
  \small
  \caption{A sentence with gold tags and our generated tags}
  \label{tab:case}
  \begin{center}
    \begin{tabular}{p{7cm}}
      \hline
      Mr. Carter explained that his firm, Davis Carter Scott Architects, which has offices in McLean and \textbf{Middleburg[contains-2-S]} in \textbf{Virginia[contains-1-S]}, planned '' a development that would fit into the community rather than look like it was inserted into it.  \\
      \hline
      Mr. Carter explained that his firm, Davis Carter Scott Architects, which has offices in \textbf{McLean[contains-2-S]} and \textbf{Middleburg[contains-2-S]} in \textbf{Virginia[contains-1-S]}, planned '' a development that would fit into the community rather than look like it was inserted into it. \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

The upper part in \tabref{tab:case} is the example sentence with gold tags,
and the lower part is the generated tag sequence.
For the relation type \textit{contains}, the RE tagger recognizes the only $e_1$ ``Virginia''
and two $e_2$ candidates, ``McLean'' and ``Middleburg''.
The gold triple within the sentences is (``Virginia'', \textit{contains}, ``Middleburg''),
while the incorrect $e_2$ ``McLean'' may disturb construction algorithms.
%In this sentence, (``Virginia'', ``contains'', ``Middleburg'') is the correct relation triple.
%As we can see from generated tag sequence, it generates an incorrect $e_2$,
%``McLean'', which may distrub construction algorithms.
In this case, both $e_1$-First and Distance-First construct the gold triple,
while $e_2$-First and Order-First produce the incorrect triple,
and Random algorithm generates the correct one by 50\% chance.

This case is very typical in the testing set, where the RE tagger generates
the correct entity pair along with another incorrect entity. 
If we focus on one specific relation type, then the tagging sequence has the format
[$\dots$, $e_1$, $\dots$, $e_1$, \dots, $e_2$, \dots]
(or [$\dots$, $e_2$, $\dots$, $e_2$, \dots, $e_1$, \dots]).
In most of the cases, the gold triple pairs the second $e_1$ (or $e_2$) with the only $e_2$ (or $e_1$).
Distance-First always pick the nearer entity pairs, leading to a better accuracy;
Order-First always pick the entity pair far away from each other, which is a mistake for the majority of such cases;
while $e_1/e_2$-First can pick the correct pairs for half of the cases.
This is the reason why Order-First performs worse among all proposed algorithms.


%At this time,
%the random algorithm choose the correct $e_2$ ``Middleburg'' and construct
%correct triple. 
%For most of this case, Order-First
%often choose the entity pair far away from each other, while random algorithm
%have one half chance to construct the correct triple. This is the reason why
%random algorithm is better than Order-First.






% Sentence 1 shows the difference between Distance-First and $e_1$-First. Two
% $e_1$s are predicted. One is vey near with the single $e_2$ which tha other is
% very far.  Distance-First can reconstruct correct triple by pair the nearest
% $e_1$ with $e_2$.

% Sentence 2 shows the difference between $e_1$-First and $e_2$-First. One
% incorrect $e_2$ is predicted which distrub $e_2$-First. However $e_1$-First can
% find the correct triple depend on the single correct $e_1$, ``Staten Island''.

% Sentence 3 shows the difference between $e_2$-First and Order-First. As we have
% mentioned, Order-First is depend on the accuracy of both $e_1$ and $e_2$. There
% is a incorrect $e_1$ which distrubs Order-First. However $e_2$-First find the
% correct triple based on the single correct $e_2$, ``Manaroneck''.


% \begin{table*}[ht]
%   \begin{center}
%     \small
%     \caption{An Example in Case Study}
%     \label{tab:case}
%     \begin{tabular}{l|p{12cm}}
%       \hline
%       \multirow{2}*{S1} & xxx \\
%       \cline{1-1}
%       ~ & yyy \\
%       S1 & Indeed , last year , the conference was perhaps better known for sightings of Ms. Jolie and her boyfriend , Brad[nationality-1-B] Pitt[nationality-1-E] , than for the comments of Shimon[nationality-1-B]  Peres[nationality-1-E]  of Israel[nationality-2-S]  on the day that Hamas was elected . \\
%       \hline
%       S2 & You can line up along the route to cheer for the 32,000 riders , whose 42-mile trip will start in Battery[contains-2-B] Park[contains-2-E] and end with a festival at Fort[contains-2-B] Wadsworth[contains-2-E] on Staten[contains-1-B] Island[contains-1-E] . \\
%       \hline
%       S3 & In New[Contains-1-B] York[Contains-1-E] , Mr. Paulison began his tour of the flood-ravaged areas on Monday in Mamaroneck[Contains-2-S] , in Westchester[Contains-1-B] County[Contains-1-E] , where he met with Senator Hillary Rodham Clinton , homeowners and the minister of the First Baptist Church , which was heavily damaged by floodwaters . \\
%       \hline
%     \end{tabular}
%   \end{center}
% \end{table*}





% \begin{table*}[ht]
%   \begin{center}
%     \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%     \small
%     \caption{An Example in Case Study}
%     \label{tab:case}
%     \begin{tabular}{l|p{8cm}|l}
%       \hline
%       S1 & Indeed , last year , the conference was perhaps better known for sightings of Ms. Jolie and her boyfriend , Brad[nationality-1-B] Pitt[nationality-1-E] , than for the comments of Shimon[nationality-1-B]  Peres[nationality-1-E]  of Israel[nationality-2-S]  on the day that Hamas was elected . &  \tabincell{c}{Gold: \{Peres, nationality, Israel\} \\ Distance-First: \{Peres, nationality, Israel\} \\ $e_1$-First: \{Brad Pitt, nationality, Israel\}} \\
%       \hline
%       S2 & You can line up along the route to cheer for the 32,000 riders , whose 42-mile trip will start in Battery[contains-2-B] Park[contains-2-E] and end with a festival at Fort[contains-2-B] Wadsworth[contains-2-E] on Staten[contains-1-B] Island[contains-1-E] . & \tabincell{c}{Gold: \{Staten Island, contains, Fort Wadsworth\} \\ $e_1$-First: \{ Staten Island, contains, Fort Wadsworth \} \\ $e_2$-First: \{ Staten Island, contains, Battery Park \}} \\
%       \hline
%       S3 & In New[Contains-1-B] York[Contains-1-E] , Mr. Paulison began his tour of the flood-ravaged areas on Monday in Mamaroneck[Contains-2-S] , in Westchester[Contains-1-B] County[Contains-1-E] , where he met with Senator Hillary Rodham Clinton , homeowners and the minister of the First Baptist Church , which was heavily damaged by floodwaters . & \tabincell{c}{Gold: \{ Westchester Country, contains, Manaroneck \} \\ $e_2$-First: \{ Westchester Country, contains, Manaroneck  \} \\ Order-First: \{   New York, contains, Manaroneck \}} \\
%       \hline
%     \end{tabular}
%   \end{center}
% \end{table*}



