\section{Conclusion}
\label{sec:conclude}
We presented a simple approach to modify existing CNN seq2seq model with
a summary length input and was able to train a model that produces summaries of
desired length that is fluent and coherent. This is a better solution 
than the current practice of summary truncation. Compared with the existing
summarization methods, we show that our model has the ability to control the
output length on its own using its internal state without losing semantic 
information and sacrificing ROUGE score. 
%We find that the basic CNN seq2seq model 
%still has some problems, such as generating repeated word sequence. 
%We also argue that ROUGE is not a perfect evaluation metric for the abstractive 
%summarization. Our future work will focus on these two aspects.
