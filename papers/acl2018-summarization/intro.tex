\section{Introduction}
\label{sec:intro}

%Summarization is the task of creating a short, accurate,
%informative and fluent summary for a longer text document.
%There are two fundamental approaches to text summarization:
%extractive and abstractive.
%The former generates summaries by using the words or sentences
%that appeared in the source document.
%The latter attempts to reproduce the semantics and topics of original text
%by paraphrasing. Thus, abstractive summarization
%can introduce fresh words and phrases into the summaries and is considered
%more favorable.

The recent work \cite{RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17}
has made great advances on abstractive summarization.
Many use sequence to sequence model based on RNN
and attention mechanism \cite{RushCW15}, which was originally used
for machine translation \cite{SutskeverVL14,BahdanauCB14}.
Recently, Gehring \shortcite{gehring2017convs2s} proposes an architecture
of an entirely convolutional sequence to sequence model that is equipped with
Gated Linear Units \cite{DauphinFAG17} and residual connections \cite{HeZRS16}.
The attention mechanism is also used in every decoder layer.
Such convolutional model achieves comparable results as the
best abstractive summarization model on single sentence summarization.
Particularly, it achieves better ROUGE-2 score on DUC-2004 and Gigaword
datasets. Moreover, the convolutional model is much faster
than previous recurrent models because it can be easily parallelized.
And, unlike recurrent models, the convoluational model has more stable
gradients because of its backpropagation path. 
Bai \shortcite{bai2018empirical} proved that the convolutional
network is a more powerful toolkit for sequence modeling.


Constraining summary length, while largely neglected in the past,
is actually an important aspect of abstractive summarization.
For example, given the same input document,
if the summary is to be displayed on mobile devices,
we may want to produce a much shorter summary.
Unfortunately, most existing abstractive summarization models are not
trained to react to summary length constraints.
%When such constraint, e.g., no more than $N$ words,
When the constraint is given at test time, the current
practice is \textbf{i)} to truncate the generated summary after $N$ tokens are generated when you want 
the summaries of length no more than $N$,
and \textbf{ii)} ignore EOS (end of summary) token until the first $M$ tokens are generated when you want
the summaries of length more than $M$.
Such a crude way of controlling
summary length makes the output summary incomplete or incoherent.
%such as getting information in a limited time or save the information
%in a limited device.

Previous research on controlling length of abstractive summary
has been scarce.
Fan \shortcite{abs-1711-05217}, who applies convolutional sequence to sequence model on multi-sentence summarization,
converts length range as some special markers which are predefined and fixed.
These markers are included in the training vocabulary.
At training time, the model prepends the input of the summarizer with the marker.
At test time, it controls the length of the generated summary also by
prepending a corresponding length marker.
%It is the state of art model for the abstractive summarization with
%length controlling in convolutional sequence-to-sequence model.
Unfortunately, this approach can not generate the summaries of
arbitrary lengths. Instead, it only generate summary in predefined
ranges of length, thus only meets the length constraints approximately.
This is shown in \tabref{tab:existing-example}.
~\footnote{The red parts exceed the desired length.}
%As the design of this model,
%the first bucket denote the range from 0 to 33 tokens. We use special marker
%of first bucket when generating the summary and set length as 30. The red part
%is the summary generated under 100 tokens.

\begin{table}[th!]
\begin{center}
\caption{\label{tab:existing-example} Example summaries generated by
different models with a desired length of 10.}
\small
\begin{tabular}{lclclclc}%{|p{7cm}|rl|}
\hline \bf Reference summary (42 tokens) \\
\hline floyd mayweather holds an open media workout from 12 am uk ( 7 pm unk ) .\\
       the a merican takes on manny pacquiao in las vegas  on may 2 . \\
       mayweather 's training is being streamed live across the world . \\
\hline \bf Basic CNN summary (25 tokens)\\
\hline the session will be streamed live across the world and \color{red}{you can watch it .}\\
       \color{red}{the session will be streamed live across the world .} \\
\hline \bf \cite{abs-1711-05217} summary (19 tokens) \\
\hline the session will be streamed live across the world . \\
       \color{red}{it will be streamed live across the world .} \\
\hline \bf  Our Length Control summary (LC) (10 tokens)\\
\hline the session will be streamed live across the world . \\
%\hline \bf LRC summary \\
%\hline $<$unk$>$ rashid , 19 , is charged with terror offenses after he arrived on a flight from istanbul .
%he is due to appear in westminster magistrates ' court on \\
%\hline \bf LRCP summary \\
%\hline $<$unk$>$ rashid , 19 , is charged with terror offenses after he arrived on a flight from istanbul .
%he is due to appear in westminster magistrates ' court on \\
\hline
\end{tabular}
\end{center}
\end{table}

In our work, we extend the convolutional sequence to sequence
model \cite{gehring2017convs2s} by
controlling the length of summarization. Our approach seeks to generate summaries
of any desired number of tokens (also shown in \tabref{tab:existing-example}).
%We propose three methods to implement length control
%for abstractive summarization: length control (LC), length range control without
%penalty (LRC) and length range control with penalty (LRCP).
%Length control method aims to generate summaries of exact length.
To do this, a length constraint is added to each convolutional block
of the initial layer of the model. This information is
propagated layer by layer during training. Next, we present the basic
convolutional sequence to sequence  model and our extension, followed by the evaluation of
our approach.
%This approach focuses on the information passing between layers.
%LRC and LRCP aim to generate summaries in desired range of lengths.
%In LRC, we embed the length range into
%a vector, then add the vector to each convolutional block in the
%initial layer of the model. LRWP has similar operation
%except it has a different loss function that penalizes gold summaries whose
%length doesn't fall into the desired length range.
%All of our models have the ability to manage the summary length by itself.
%Our methods can also generate summaries in any length or
%length range as we want.

%Next, we describe the convolutional model we use in \secref{sec:model}
%and present the details of
%the length controlling methods in \secref{sec:approach}. Then, we compare our model with other model \cite{abs-1711-05217}
%on the standard CNN/DailyMail benchmark and release comprehensive experimental
%results in \secref{sec:eval}. We discusse some related work in \secref{sec:related} and give a
%conclusion in \secref{sec:conclude}.
