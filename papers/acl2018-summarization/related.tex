\section{Related Work}
\label{sec:related}

In this section, we discuss some previous work about length control in
abstractive summarization.

\subsection{Length Control for Abstractive Summarization}
When summarizing a document, it is desirable to be able to control
the length of summary so as to cater to different users and scenarios. 
Most abstractive summarization systems are based on encoder-decoder models
that are data-driven, therefore generate
summaries whose length depends on the training summaries.
%How to control the summaries length is essential for summarization methods 
%based on neural encoder-decoder models.
Due to the variability of the sequence generation models, such 
as the different structures and different functions, it is hard to design
a length constraint method on all summarization models.

Previous methods of controling the summary length is by generating the 
end-of-sentence(EOS) tag. Rush \shortcite{RushCW15} use an ad-hoc method, 
in which the system is inhibited from generating the EOS tag by assigning a score of -$\infty$ to the tag and generating a fixed number of words. In addition, 
Yuta \shortcite{KikuchiNSTO16} proposes two
different methods for RNN seq2seq model which can control the summaries length by taking length
embedding as an additional input for the LSTM and adding desired length into initial memory cell for the LSTM. 
In this model, they use the Gigawords as dataset and focus on the abstractive summarization in sentence level 
which just generates one sentence as the summary.
The RNN seq2seq model needs more time to training
than convolutional model. For CNN seq2seq model, Fan \shortcite{abs-1711-05217}
puts some special markers into the input vocabulary which denotes different length range.
It prepends the input of the summarizer with the marker during training and test. These special marks are predefined
and fixed. 

%In our work, we use multi-layers CNN seq2seq model on both encoder and decoder. We set the
%length constraint at the first layer of decoder to implement the length control of
%the summarization. 

\subsection{Encoder-Decoder for Abstractive Summarization}

The goal of automatic document summarization is to generate short summaries for originally
long documents. A summary should cover the information which original document or documents focus on.
Besides, a good summary should be coherent, non-redundant and grammatically readable \cite{YaoWX17}.
The research in abstractive summarization with sequence to sequence neural networks \cite{SutskeverVL14,RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17,abs-1711-05217} have got a great succes .

Most of them using recurrent networks with different attention mechanisms \cite{NallapatiZSGX16,SeeLM17,PaulusXS17}. 
Rush \shortcite{RushCW15} use recurrent network with
soft-attention, which denotes that many words in the target summary are retained from the source document.
Paulus \shortcite{PaulusXS17} use the recurrent network with intra-attention,
which is an attention mechanism relating different positions of a single sequence. Recently, some research start to
use convolutional networks instead of the recurrent networks. The convolutional networks enable faster training time.
In addition, Bai \shortcite{bai2018empirical} proved that CNN is more powerful than RNN for sequence modeling.
So we do not compare our model with RNN seq2seq model. 

In this paper, we use multi-layers CNN seq2seq model on both encoder and decoder. We set the
length constraint at the first layer of decoder to implement the length control of
the summarization. 
%In this paper, we use the convolutional network to implement 
%our model as our basic summarization model and add length
%constraint upon convolutional network.


