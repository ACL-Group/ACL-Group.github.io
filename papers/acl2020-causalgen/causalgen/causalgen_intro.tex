\section{概述}
\label{sec:causalgen-intro}
常识性因果推理的本质在于
发现事件或行为之间蕴含的因果关系。
例如，
\emph{Amanda feels hot}（“阿曼达感觉很热”）
是一个合理的、可以导致\emph{Amanda turns on the fan}（“阿曼达打开了风扇”）这一结果的原因。
这二者之间具有因果关系。
我们期望设计模型来捕获这种因果关系，
并用于因果推理等相关任务。
在\chapref{chap:causalnet-main}中我们
曾提到过Gordon~\cite{roemmele2011choice}将常识性因果推理问题
形式化为一种选择问题。
给定一个原因（或结果），
它要求模型从几个人为预设的候选中
选出最合理的结果（或原因）。
之前的工作~\cite{roemmele2011choice,LuoSZHW16}
提出了各种因果关系强度度量
标准为候选进行排序，然后从中选出得分最高的最优
选项作为合理的因果事件答案。
这种基于排序模型框架来解决常识性因果推理问题
常常受限于为前提预先设定的候选事件集。
为了满足自然语言处理中关于生成场景的应用需求
（例如，基于因果的自动问答系统），
我们在本章主要研究因果生成问题。
即根据给定的原因（或结果）事件作为前提（premise），
我们期望模型能够为前提
自动生成相应的结果（或原因），称为目标（target）。
我们将给定原因推出结果的推理过程称为正向推理
（ forward reasoning），
将已知结果推出原因的过成称为反向推理
（ backward reasoning）。
例如，正向推理
给定原因\emph{Amanda feels hot}（“阿曼达感觉很热”），
可能的结果是\emph{Amanda turns on the fan}
（阿曼达打开了风扇）
或者是\emph{Amanda takes off
her coat}（阿曼达脱掉了外套）；
相反地，反向推理中将给定结果，反推出原因，
若给定\emph{Amanda feels hot}（“阿曼达感觉很热”）
作为结果，反推出的一个合理的原因是
\emph{The air-conditioner stops working}（“空调坏了”）。

之前关于因果推理的研究
工作大多是基于选择的方法
~\cite{roemmele2011choice,goodwin2012utdhlt,jabeen2014using, LuoSZHW16}，
它们可以适配到因果生成的任务中来。
我们首先为给定的前提自动地生成一个候选集，
然后用基于选择的方法从中挑选出最合适的推理结果。
但这种适配方法有两种局限性：
1）需要一个额外的步骤来为每一个前提建立候选集，
十分昂贵耗时，且推理结果受限于候选集质量；
2）模型只能从候选集中挑选推理答案，生成的目标
比较固定单一，不够灵活。
本章中，我们基于卷积神经网络序列到序列学习框架，
提出了一种新颖的模型，从CausalNet中引入因果知识，
设计因果注意力融合机制，增强模型的生成效力，
以更好的进行因果生成。
在基于编码器-解码器的序列到序列学习框架下，
注意力机制用来学习当前解码状态与
各编码状态之间的语义依存关系。
这种语义依存关系在不同的场景下有不同的解释意义，
它在机器翻译和文本摘要中意味着语义对齐，
而在因果推理中则解释为因果依存关系。

我们用句法因果模式从文本语料中自动抽取因果关系。
与\chapref{chap:causalnet-main}
中关注词项不同，
本章中我们以句子作为事件单元。
这是因为因果关系是一种复杂的多对多关系，
即一个事件作为原因可以导致多种结果事件，
一个事件作为结果也可以被多种原因导致；
同时，因果关系是否成立很大程度上取决于
上下文语境信息，变化性与歧义性较大。
若仅利用词项作为事件进行因果生成，
即为每个词学习一个在因果空间中的分布表示，
则生成模型退化为因果词向量的表示学习。
Sharp等人~\cite{sharp2016creating,zhao2017constructing}研究了相关的工作，但由于前面讨论的
因果关系的特殊性，已有的实验结果表明
基于因果词向量的因果生成效果不佳。
对于因果生成来说，我们需要更多的语境信息
来为输入事件生成相应的原因或结果事件，
所以，本章主要关注基于句子的事件单元，
以丰富形成因果的语境信息。

我们提出句法因果模式以便从文本语料中自动获取因果关系。
然而，嵌入在文本中的因果关系是稀疏且含糊的。
高精度训练数据的匮乏，对生成模型
能否学到足够强大的因果依存关系模型用于因果生成
提出了极大挑战。
为了改善这一局限性，
我们旨在引入外部因果知识来帮模型进行因果生成。
我们首先利用我们在\chapref{chap:causalnet-main}中
构建的因果知识库CausalNet~\cite{luo2016commonsense}
提出了一种因果注意力机制（causal attention mechanism）。
进一步地，我们为序列到序列模型
提出了一种注意力融合机制，
将该因果注意力与原模型中的软注意力
（soft attention）相结合，
完善其对因果语义依存关系的建模。
因果注意力代表了来自于外部知识库所蕴含的
全局因果依存信息，而软注意力则代表了
当前训练数据所蕴含的局部因果依存信息。
我们将二者有机结合在一起，以期望
模型更好的学习这种因果依存关系。
CausalNet的具体介绍可参见本文\chapref{chap:causalnet-main}中的\secref{sec:causalnet-approach}和\secref{sec:causalnet-dataset-network}。

本章的贡献可以总结为以下三个部分：

\begin{itemize}
	\item 我们定义了一个新问题，因果生成问题，这使得许多
	自然语言处理应用受益，例如问答系统，故事补全等等。
	\item 为训练因果生成模型，我们提出了一种方法从文本语料中自动抽取因果事件对构建数据集。
	\item 我们提出了一种新颖的因果注意力融合机制，将
	外部因果知识融入到序列到序列学习框架中，帮助因果生成。实验结果表明我们的方法明显优于对照的基线模型。
\end{itemize}

