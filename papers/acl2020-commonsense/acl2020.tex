%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{caption}
\usepackage{soul}
\usepackage{color}
\usepackage{verbatim}
\usepackage{multirow}

\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{diagbox}
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\RP}[1]{\textcolor{red}{Rephrase: \ul{#1}}}

\newcommand{\eve}[1]{\textcolor{red}{Eve: #1}}

\title{Is There Any Commonsense in Large Text?}

\author{First Author \\
	Affiliation / Address line 1 \\
	Affiliation / Address line 2 \\
	Affiliation / Address line 3 \\
	\texttt{email@domain} \\\And
	Second Author \\
	Affiliation / Address line 1 \\
	Affiliation / Address line 2 \\
	Affiliation / Address line 3 \\
	\texttt{email@domain} \\}
\date{}

\begin{document}
	\maketitle
	\begin{abstract}
Commonsense knowledge is important for machine intelligence.
We give a preliminary study on why typical relation extraction models 
that work reasonably well on factual knowledge extraction 
fail on commonsense relations. We suggest several possible reasons
and give some empirical evidences. 
\end{abstract}
	

\section{Introduction}
\label{sec:intro}

In the past few decades, a variety of knowledge bases such as Freebase~\cite{bollacker2008freebase}, 
YAGO~\cite{suchanek2007yago}, Cyc~\cite{lenat1995cyc}, ConceptNet~\cite{speer2017conceptnet}, have been constructed to facilitate human-like
reasoning and inference. Such knowledge is usually presented in
triple format ($h$, $r$, $t$), where $r$ is the relation symbol, 
$h$ and $t$ are the two arguments. One example is 
\textit{(google, founders, larry page)}. These KBs can be 
generally classified into two categories: {\em factual} (or encyclopedic) and 
{\em commonsense}. 

%These two kinds of knowledge are applied to different types of downstream natural language process (NLP) tasks: factual knowledge can assist question answering while commonsense knowledge have been utilized for textual entailment and reasoning.

A fact (or factual relation) is absolute truth where at least one of the
arguments is a named entity. But for simplicity, we call both arguments
of a fact \textit{entity}. The negation of a fact is \textbf{always} false. 
For example, given a fact \textit{(italy, administrative\_divisions, sicily)}, 
the negation ``Sicily is NOT a administrative division of Italy'' is considered
wrong.  
A commonsense, on the other hand, describes a common belief or 
experience involving two generic objects (denoted as \textit{concept}). 
These objects can be general events, actions, concepts or even situations. 
It is important to note that concepts are usually {\em generatlized} 
from specific instances.
Concepts can refer to both noun and verbal phrase (or event) while 
entities are typically noun. For example, instead of mentioning a specific place, it would prefer a triple like \textit{(country, contains, city)}. The negation of a commonsense triple is \textbf{not always} false. 
For instance, the negation of \textit{(bread, AtLocation, refrigerator)}, 
``bread is not in refrigerator'', is not always incorrect. 
For human beings, factual knowledge is often domain-specific and acquired
through learning, but commonsense knowledge is more universal and can be
gathered through experiences. 
%Artificial intelligence systems equipped with commonsense knowledge are able to think and infer like human. Thus, collecting such knowledge is an essential task. 

%\KZ{You need to spell out the def of factual knowledge and commonsense knowledge
%and their diff up front. You can show snippets of Freebase and ConceptNet
%to help with the explanation.}

There are mainly two ways to construct knowledge bases: 
via manual curation or automatic extraction. Much previous work (Freebase, Cyc, 
ConceptNet and ATOMIC~\cite{sap2019atomic}) relied on crowd-sourcing human annotation. 
Such curated KBs have high precision but low or skewed coverage. 
For example, in ConceptNet, \textit{CreatedBy} has only 263 triples 
while \textit{Causes} contains 16801.  
%Research on automatic extraction has been focused on factual knowledge in the past. 

Efforts on automatic extraction from text has been limited ~\cite{yatskar2016stating,tandon2016commonsense,tandon2017webchild,xu2017automatic}, which solely cover a small subset of commonsense relations.

%\KZ{For example, .... \cite{}}
%\KZ{Efforts on extracting commonsense knowledge from text has been limited ( WebChild~\cite{tandon2017webchild}, LocatedNear~cite{xu2017automatic}). Summarize the bad results on these attempts: webchild, locatednear.} 


%Moreover, the quality and quantity of extracted knowledge is not comparable with curated ones. WebChild2.0 is composed of over 18 million triples but .... (?)
%However, the curated KBs also have shortcomings. The consumption of a lot of resources and time make it far from complete coverage. The number of triples in a certain relation could be particularly low: 

%While efforts on commonsense relation extraction made little progress, 
%relation extraction of factual knowledge thrives (). \KZ{You sound like previous attempts such as WebChild didn't use these RC
%methods? Then what did they use? --- We attempt to borrow the efforts of 
%this well-studied area and apply the model on commonsense relation.} 

%We attempt to borrow the efforts of 
%this well-studied area and apply the model on commonsense relation. (LocatedNear) has also modeled commonsense relation extraction task as sentence-level relation classification problem, similar with factual relation extraction task. It determines whether a sentence describes two objects are physically located near each other. The author consider that if more sentences express \textit{LocatedNear} relation between two objects, it is more likely that such commonsense relation truly hold. 
%However, according to (reporting bias), the frequency of text involving concept pair does not reflect the degree of commonsense.
%That is why we will regard it as a corpus-level relation classification problem.

In this paper, we want to understand why existing relation extraction
techniques are inadequate for extracting commonsense knowledge from large 
text data. In particular, we investigate characteristic differences between
these two types of knowledge and their potential presence in natural language
text. We use OpenNRE~\cite{lin2016neural}, a popular relation extraction model 
that gives satisfactory 
%near state-of-the-art 
results on factual relations, to attempt the 
extraction of both facts and commonsesense from New York Times, Wikipedia and
Gutenberg text. We make the following discoveries:
\begin{itemize}
	\item The arguments of commonsense relations takes on a 
larger variety of forms (concept polymorphism) than factual relations; 
	\item The arguments of commonsense relations takes on more
meanings (concept polysemy) than factual relations;
	\item The relations in commonsense are more ambiguous (relation
polysemy).
\end{itemize}

\section{Factual Relation Extraction}
\label{sec:factual}
Modern relation extraction is formulated as a sentence-level relation classification (RC) problem.  Formally, the problem is defined as: 
given an argument pair \textit{(h,t)} and a sentence \textit{s} 
in which they co-occur, predict the probability that $h$ and $t$ are related by
$r \in R\cup\{NA\}$, where $R$ is a set of all possible relations and $NA$ is a special
not-related relation. 

A variant of the problem is the corpus-level relation extraction problem, in which
the input is an argument pair \textit{(h,t)}, a set of sentences \textit{S} 
mentioning the pair, and $r \in R \cup \{NA\}$. The output is the same prediction probability.

%Factual relation extraction aims to classify an entity pair to a set of pre-defined relations by consuming documents mentioning the entity pair. Sentence-level extraction focus on single sentence while corpus-level extraction retrieve all sentences the given concept pair co-occur and regard them as a whole. Sentence-level method predicts relation restricted on one sentence but corpus-level method gives more general output.

%One of the standard corpus for factual relation extraction is the New York Times (NYT) dataset(). The text is from New York Times and is aligned with Freebase knowledge base as resource KB. The approach is based on the distant supervision assumption():
Because labeling the ground truth relations in sentence is hard,
previous work often makes the assumption that, if ($h$, $r$, $t$) is true,
and $h$ and $t$ co-occur in sentence $s$, then $h$ is related to $t$ by $r$ in
$s$. This is the basis of distance supervision used in many works~\cite{riedel2010modeling, mintz2009distant}.

%\KZ{Don't mention specific datasets. These are implementation details.Just mention the common models used in HRERE and OpenNRE. Revise below:}

Mainstream corpus-level models evaluated on NYT dataset aim to learn a representation over the set of sentences $S$. Some utilize side information such as aliases of relations~\cite{vashishth2018reside} and knowledge base embeddings~\cite{xu2019connecting}. 

\section{Extraction Results}
%\KZ{This is about how we implement the SOTA RC methods to extract 
%both factual and commonsense knowledge from a number of datasets. Ideally
%we should extract both kinds of knowledge from every available dataset.
%We should have two test sets (targeted triples to be extracted), 
%one for facts and one for CS.}
%\KZ{You never mention
%what is a resource knowledge base previously. You should define it when
%you talked about Freebase earlier.}

%(HRERE) is SOTA on NYT dataset. It utilizes knowledge base embeddings trained on subset of Freebase without any entity pairs present in NYT dataset to improve performance. But the limited scale of ConceptNet (34 million triples) compared with Freebase (1.9 billion triples) makes it hard to divide it into two big enough subsets. 
For commonsense knowledge, there are few available KBs to obtain side information. 
Hence, we choose OpenNRE~\cite{lin2016neural} because it also performs well without any side information and thus straightforward to be directly applied on commonsense relation extraction task.

We apply OpenNRE to extract both factual and commonsense knowledge
on three datasets: New York Times, Wikipedia and Gutenberg. Because 
Gutenberg mainly consists of fictions in which real-world entities are
scarce, we only try to extract commonsense from it. 
For fact extraction, we use a subset of relations (54) from Freebase as
our resource KB (called FB$^*$); while for commonsense, we use a 
subset of relations (27) from ConceptNet (called CN$^*$).

%To apply the factual model on commonsense, we simulate the process in the same way. 
%ConceptNet serves as resource knowledge base to obtain triples. 
%Considering there are some non-commonsense relations in ConceptNet, we manually exclude some relations, such as ExternalURL and EtymologicallyRelatedTo, leaving 27 relations in total (denoted as ConceptNet$^*$).
%
%\KZ{The following is how you create your training and test set. The
%same procedure should be applied to both factual and commonsense extraction.}	
%Based on the triples from these relations, we find all sentences that contain these two concepts. We choose NYT as resource corpus to make fair comparison with factual relation. Gutenberg and Wikipedia are also included due to its different genre from news report and potential in containing more commonsense knowledge. 
%

First we use all the triples in CN$^*$ to retrieve input sentences for the
RC model. To make triples the same across all three corpus, 
we randomly select some of the triples that have over 50 matching sentences each. 
These triples are further divided into training and test set. 
To create negative data, we randomly select head and tail concepts per relation and 
regard them as triples of \textit{NA} relation. Besides, we found that 
predictions will be restricted to some relations that have much more training sentences 
than others. Thus we set the upper bound of training triples as 100 per relation 
to make data uniformly distributed over relations. 

As for factual relation, NYT (FB$^*$) is actually NYT dataset. To construct Wikipedia (FB$^*$), we randomly select at most 10 triples per relation from training triples of NYT (FB$^*$) and extract sentences from Wikipedia. The test triples are all non-NA test triples of NYT (FB$^*$) that can find mentions in Wikipedia. Traditional relation extraction evaluation will remove the probability of NA relation for each pair. Thus we don't keep NA triples in Wikipedia (FB$^*$) test set.
The specific statistic is listed in Table~\ref{table:sen distribution}.


\begin{table}[h]
	\small
	\centering
	\begin{tabular}{|l|c|c|c|c|}
		\hline
		& \multicolumn{2}{c|}{\textbf{Train}} & \multicolumn{2}{c|}{\textbf{Test}} \\\hline 
		Corpus                & \multicolumn{1}{c|}{non-NA} & \multicolumn{1}{c|}{NA} & \multicolumn{1}{c|}{non-NA} & \multicolumn{1}{c|}{NA} \\ \hline
		NYT (FB$^*$)  & 156,664                               & 413,424                           & 6,444‬                                & 166,004                           \\ \hline
		NYT (CN$^*$)& 10,937                                & 190,129‬                          & 120,340                                & 22328                             \\ \hline
		Gutenberg             & 11,353                                & 203,295                           & 124,968                               & 24,368                            \\ \hline
		Wiki(CN$^*$)             & 11,375                                 & 215,805                           & 130,212                                & 26,434                             \\ \hline
		Wiki(FB$^*$) & 5,690& 46,594 & 19,137 & 0 \\ \hline
	\end{tabular}
	\caption{\# of Sentences in Different Corpora}
	\label{table:sen distribution}
\end{table}



	
~\citet{lin2016neural} use word2vec tool to train the word embeddings on NYT corpus. The resulting vector dimension is 50. In our experiment, we use 100-dimensional pretrained GloVe embedding~\cite{pennington2014glove} instead. We choose PCNN+ATT architecture which performs best 
on factual relation extraction task.



\begin{table}[h]
	\small
	\centering
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Corpus}            & \textbf{MRR}   \\ \hline
		NYT (FB$^*$)     & 0.93 \\ \hline
		NYT (CN$^*$) & 0.31 \\ \hline
		Wiki (FB$^*$)& 0.37 \\ \hline
		Wiki (CN$^*$)& 0.26 \\ \hline
		Gutenberg         & 0.26 \\ \hline
	\end{tabular}
	\caption{MRR of Factual and Commonsense RC}
	\label{table:results}
\end{table}


The test result is listed in Table~\ref{table:results}. 
%For factual relations, we directly re-run the source code and reproduce the result.
Because the number of relations is different between factual (54 relations) 
and commonsense (27 relations), it is not suitable to apply the commonly-used metrics: 
P@N or AUC. Instead, we use Mean Reciprocal Rank (MRR) here. 
For pairs with multiple relations, we consider the relation of highest rank 
as the only ground truth relation.
One can see that when applying the same model on the same dataset, extracting
commonsense is much harder than facts. In the following, we will explore
why.


%\begin{comment}
%\begin{table*}
%	\small
%	\centering
%	\begin{tabular}{|l|c|c|c|c|l|l|}
%		\hline
%		corpus                & \multicolumn{1}{l|}{P@100(\%)} & \multicolumn{1}{l|}{P@200(\%)} & \multicolumn{1}{l|}{P@300(\%)} & \multicolumn{1}{l|}{Mean} & AUC        & max F1     \\ \hline
%		NYT dataset (factual)  & 77.228                        & 72.139                        & 68.439                        & 72.602                   & 0.341   & 0.406    \\ \hline
%		nytimes (commonsense) & 10.891                        & 11.940                        & 11.628                        & 11.486                   & 0.080   & 0.185    \\ \hline
%		Gutenberg             & 9.901                        & 14.925                      & 12.625                      & 12.484                 & 0.067 & 0.125 \\ \hline
%		Wikipedia             & 13.861                      & 21.393                      & 22.259                      & 19.171                 & 0.074 & 0.130 \\ \hline
%	\end{tabular}
%	\caption{Evaluation Metrics for Factual and Commonsense Relation Extraction}
%	\label{table:test result}
%\end{table*}
%\end{comment}
%\KZ{Trim all numbers in results to two decimal places.}


	
\section{Analysis}
\label{sec:analysis}
%Though the size of training data for commonsense relation is smaller than factual relation, \KZ{Why not make them the same? Let's pretend there's only one
%NYT corpus, from which we extract both factual and cs.} the gap between test results in all dimensions is disproportionately wide. In order to find out why, we propose some hypotheses and verify them in this section.

%We extract triples from NYT dataset as a small factual knowledge base, denoted as NYT KB. NYT corpus and Wikipedia are selected as resource corpus. Gutenberg is excluded because it contains little entities of NYT KB.
	
To answer why factual RC model fails on 
commonsense relations, we try to analyze it from two perspectives: 
the arguments and relations. We hypothesize that 
the argument concepts of commonsense relations take more forms and contain
more meanings than their factual counterparts; and that the commonsense
relations have much broader and more ambiguous semantics than usual factual
relations. We will analyze from these three perspectives below. 
	
\subsection{Polymorphism of Commonsense Argument Concepts}
\label{sec:polymorphism}
A concept has various expressions and they may differ greatly from each other while an entity usually has fewer and similar variants. 
%\KZ{This example doesn't back up the claim above. Can we have some stats to show that concepts vary more than entities?}
For example, 
%concept \textit{[low temperature]} variant: \textit{cold}, 
concept \textit{exercise} variant: \textit{[play sport, work out, go to the gym, build up body]}, entity \textit{Italy} variant: \textit{[Italia, Italian Republic]}. Moreover, the variants of a concept could have different POS tag but still express the same meaning, such as \textit{anger} and \textit{angry}. 

%The larger vocabulary of commonesence concepts means if some variants are missed, we could not find all matching sentences which express the same meaning of the concept triple. 
%\KZ{This doesn't make much sense.}
	
For both factual and commonsense relation, we randomly sample 3 
triples per relation from FB$*$ and CN$*$. We use Wikidata API\footnote{See \url{ https://www.wikidata.org/w/api.php?action=help&modules=wbsearchentities}} to retrieve \textit{aliases} of 
an entity and manually write variants of head and tail concepts. 
For example, triple \textit{(exercise, Entails, move)}, variants of head and 
tail concepts are \textit{[sport, work out, go to the gym, build up body]} 
and \textit{[movement, motion]}, which results in 15 pair variants. 

%\KZ{This subsection is about vocab of concepts, why are you talking about the ambiguity of relations here?}

	Next we use all the variants to match sentences in corpus, 
calculate entropy of each triple and take the average.

\[ entropy(triple) = -\sum_{i=1}^{|V|}P_{i}log_{2}P_{i} \]
\[ P_{i} = \frac{count(variant_{i})} {\sum_{j=1}^{|V|} count(variant_{j}) } \]
\noindent
where $V$ denotes the variant set of a triple and $count(variant_{i})$ means the number of matching sentences of $variant_{i}$.

%\KZ{Give the formula how you compute the entropy. Earlier we talked about various ways of computing the sentence complexity. Here you didn't mention which way.} 

Triples with no matching sentences are 
not included for calculation. The result is in Table~\ref{table:entropy}. 

\begin{table}[th]
	\small
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\diagbox [width=7em,trim=l] {\textbf{Type}}{\textbf{Corpus}}      & NYT         & Wiki \\ \hline
		Factual     & 0.26&   0.50
		\\ \hline
		Commonsense & 1.28&    1.27
		\\ \hline
	\end{tabular}
	
	\caption{Entropy of Factual and Commonsense Relation}
	\label{table:entropy}
\end{table}


We can see that entropy of commonsense relation is higher than factual relation, indicating that given a triple, the number of matching sentences across the variants distributes analogously. In other words, if one particular variant of a concept is missed, we will lose a relatively large number of matching sentences.
	


	

\subsection{Polysemy of Commonsense Argument Concepts}
A concept may have multiple senses while an entity is more likely to be linked with an unique named entity. Plus, we can't exclude some senses of 
a concept according to the POS tags of the relation arguments. 
For example, in CreatedBy relation, there are triples 
\textit{(seed, CreatedBy, plant)} and \textit{(cake, CreatedBy, baking)}. 
The concepts of former triple are both nouns while the tail concept 
of the latter one is a verb in form of present participle .
	
A concept may contain single or multiple words. When the number of words of 
a concept increases, the number of possible senses decreases. 
We show the percentage of \textit{single-word concepts} and 
\textit{single-word triples} (contains exclusively single-word head/tail
arguments) in CN$^*$ in ~\tabref{tab:single-word-all}. 
Though the overall percentage is low, 
there are some relations with high proportion of single-word triples 
(see~\tabref{tab:single-word-some}). 

\begin{table}[!h]
	\small
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Type}        & \textbf{Entity/Concept (\%)} & \textbf{Triple (\%)} \\ \hline
		FB$^*$& 29.92              & 23.11       \\ \hline
		CN$^*$ & 21.22              & 12.76      \\ \hline
	\end{tabular}
	\caption{\% of Single-word Entity/Concept and Triple proportion in FB$*$ and CN$^*$}
	\label{tab:single-word-all}
\end{table}

\begin{table}[th]
	\centering
	\small
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{KB} & \textbf{Relation} & \textbf{\% of SWTs}\\ \hline
		\multirow{4}{*}{CN$^*$} &  CreatedBy                                   & 56.65                              \\ \cline{2-3} 
		& Entails                                     & 84.69                              \\ \cline{2-3} 
		& LocatedNear                                 & 63.27                              \\ \cline{2-3} 
		& MadeOf                                      & 58.35                              \\ \hline
		\multirow{4}{*}{FB$^*$} 
		
		& country/languages\_spoken         & 66.67 
		\\	\cline{2-3}		& country/administrative\_divisions & 67.83 
		\\	\cline{2-3}		& province/capital                  & 100          
		\\ 	\cline{2-3} & country/capital                   & 73.53\\
		\hline
	\end{tabular}
	\caption{Examples of CN$^*$ and FB$^*$ Relations with high percentage of
		single-word triples (SWTs). All FB$^*$ relations are from the /location.}
	\label{tab:single-word-some}
\end{table}




%\KZ{Why not show these numbers for factual relations?}
	
%	\begin{comment}
%	\begin{table}[th]
%		\small
%		\centering
%		\begin{tabular}{|c|c|}
%			\hline
%			Single-word Concepts(\%) & 21.219  \\ \hline
%			Single-word triples(\%) & 12.756   \\ \hline
%		\end{tabular}
%		\caption{\% of Single-word Concepts and Triples proportion in ConceptNet$^*$}
%		\label{tab:single-word-all}
%	\end{table}
%	\end{comment}
	

	
%\begin{comment}
%	\begin{table}[th]
%		\centering
%		\small
%		\begin{tabular}{|l|c|c|}
%			\hline
%			Relation    & Single-word triples/All triples(\%) \\ \hline
%			AtLocation  & 38.557                              \\ \hline
%			CreatedBy   & 56.654                              \\ \hline
%			Entails     & 84.691                              \\ \hline
%			LocatedNear & 63.265                              \\ \hline
%			MadeOf      & 58.349                              \\ \hline
%			PartOf      & 49.786                              \\ \hline
%		\end{tabular}
%		\caption{Examples of ConceptNet$^*$ Relations with High proportion of Single-word Triples}
%		\label{tab:single-word-some}
%	\end{table}
%	\end{comment}
	
	% Please add the following required packages to your document preamble:
	% \usepackage{multirow}

%In the following experiments, we only focus on triples with single-word arguments.
%, \RP{because given a sentence containing both head and tail, a longer head/tail concept will naturally narrow down the range of senses of another concept.} 

We randomly select 10 single-word triples from FB$*$ and 
CN$*$ separately and at most 10 matching sentences per triple 
to manually label if the matched arguments in sentence has 
the same meaning as the original triple. For example, given triple \textit{(ship, AtLocation, port)} and a matching sentence: \textit{U.A.E. ports service U.S. military ships , and U.A.E. firms have made major investments in Chrysler and Time Warner , somehow without turning them into fundamentalist bastions .}, the sentence will be labeled as \textbf{false} because the ``port'' 
here does not refer to the harbour area but part of the compound ``ports service''. 
Around 29\% of commonsense sentences are labeled as \textbf{false} while 
0\% sentences are annotated as \textbf{false} in the factual samples. 

%\KZ{Can you provide actual number?}



\subsection{Polysemy of Commonsense Relations}
%\KZ{Consider rephrase ``broad coverage of relation''. Not clear what it means.}
The factual relation types are fine-grained while commonsense relations may 
have wide coverage, which means that it can be further decomposed into 
several sub-relations. For example, triples \textit{(acting in play, Causes, 
applause)} and \textit{(acting in play, Causes, getting famous)}, the subjects of head concepts both refers to \textbf{actor} but the subjects of tail concepts are \textbf{audience} and \textbf{actor} accordingly. We randomly select 50 triples from PartOf relation and label which sub-relation they belong to (see Table~\ref{table:sub-rel}).

\begin{table}[th]
	\small
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Sub-relation} & \textbf{\# of Triples} & \textbf{Example}       \\ \hline
		conceptually part of & 10                & music, song   \\ \hline
		physically part of   & 33                & wheel, bike   \\ \hline
		member of            & 6                 & cyclist, team \\ \hline
		substance of         & 1                 & rubber, wheel \\ \hline
	\end{tabular}
	\caption{Distribution of Sub-relations for PartOf}
	\label{table:sub-rel}
\end{table}

We speculate that the broad coverage of commonsense relation 
increases the complexity of matching sentences. For a certain commonsense relation, there could be more patterns due to the diversity between triples. Thus, for commonsense relation extraction, it requires much more training data compared with factual relation extraction. We verify this speculation using
the following experiment.

	
For factual and commonsense relations, we randomly select 10 
single-word triples per relation and 10 matching sentences at most per triple. 
%We only select single-word triples because it is more straightforward to 
%analyze the dependency path between head and tail concept. 


\begin{table}[th]
	\small
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\textbf{Corpus}& \textbf{RT} & \textbf{SL} & \textbf{Dist} & \textbf{DPL} & \textbf{DT} \\ \hline
		\multirow{2}{*}{NYT}       & F& 34.76                       & 8.81                  & 3.41                     & 91                        \\ \cline{2-6} 
		& C& 34.52                       & 9.20                  & 3.09                     & 143                       \\ \hline
		\multirow{2}{*}{Wiki} & F& 36.75                       & 10.34                 & 3.83                     & 93                        \\ \cline{2-6} 
		& C& 35.53                       & 9.53                  & 2.97                     & 158                       \\ \hline
	\end{tabular}
	\caption{Metrics of Complexity of Sentences. Legend: RT: Relation Type, F: Factual, C: Commonsense, 
		SL: Sentence Length, Dist: Number of words between two arguments in sentence, DPL: Dependency Path Length between two arguments, DT: Number of distinct dependency types between two arguements.} 
	\label{table:complexity metric}
\end{table}
	
	% Please add the following required packages to your document preamble:
	% \usepackage{multirow}

%Values of the first three columns between factual and commonsense relation 
%differ little. However, the number of dependency label types of commonsense 
%relations is consistently much larger than factual relations. 


~\tabref{table:complexity metric} demonstrates some metrics to model the complexity of matching 
sentences.


We can see that though the textual distance and dependency path between
the two arguments in commonsense relations are slightly shorter than that
of factual relations, there is significantly larger variety of
dependency relations for the commonsense relations. 
This shows that commonsense relations inhabit in much more diverse linguistic
context. 	

%\KZ{rephrase semantic features to semantic characteristics vs. syntatic/lexical characteristics.}
The semantic characteristics of concept prevent us from finding matching 
sentences of both large quantity and high quality while the 
broad coverage nature of commonsense relation make it require 
much more training data.
	
\section{Conclusion}
%Reporting bias is considered to exist in commonsense knowledge.
%However, it can be attributed to lack of variants of a concept after our analysis. There could be text about commonsense undiscovered. 
We discovered several characteristics of commonsense relations
and their context in natural language text. These characteristics
make existing relation extraction techniques uneffective.
%Solving these issues may lead us to extract commonsense knowledge from text corpus. 
Promising directions of commonsense relation extraction include 
generating paraphrases of argument concept to obtain more variants,
and tools for detecting commonsense concepts in sentences.

%We could generate paraphrases of a concept to obtain variants.
%Above strategies bring in adequate training data.
%During test, different from that NER is well-studied to extract entities for factual relation, there is no tool to detect concept in text. One possible solution is to operate chunking and treat noun phrases and verb phrases as concepts. These phrases can be further generalized into more abstract concept.
%
%Above strategies bring in adequate training data.
%During test, different from that NER is well-studied to extract entities for factual relation, there is no tool to detect concept in text. One possible solution is to operate chunking and treat noun phrases and verb phrases as concepts. These phrases can be further generalized into more abstract concept.
%
%polymorphism of arguments
%polysemy of arguments
%polysemy of relations
%
%>>>>>>> .r34901
\begin{comment}
In this paper, we first experiments to extract commonsense relation from text using a typical factual relation extraction model. Compared with good test results on factual relation, evaluations metrics including P@N, AUC and F1 decline sharply. We give several explanations for why factual relation extraction method can't be directly applied on commonsense relation. 
The semantic features of concept block us from collecting enough size of training data and introduce noise in already-found data. On the other side, the wide coverage of commonsense relation results in greater variety of patterns between head and tail concept, leading to a consequent need of larger quantity of training data.
	The combination of intrinsic features of commonsense concept and relation deteriorates extraction performance. Future works on automatically mining commonsense relation from text could focus on solving these issues. 
	
\end{comment}

\bibliography{acl2020}
\bibliographystyle{acl_natbib}
\end{document}
