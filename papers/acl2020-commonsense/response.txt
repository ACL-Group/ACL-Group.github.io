Review #1

Thanks for your review!

1. Q: What is 'side information'?
A: As in L124-125, it means 'aliases of relations and knowledge base embeddings'.

2. Q: L142: Which subset? What is selection rationale?
A: ['AtLocation', 'CapableOf', 'Causes', 'CausesDesire', 'CreatedBy', 'Desires', 'DistinctFrom', 'Entails', 'HasA', 'HasFirstSubevent', 'HasLastSubevent', 'HasPrerequisite', 'HasProperty', 'HasSubevent', 'InstanceOf', 'LocatedNear', 'MadeOf', 'MannerOf', 'MotivatedByGoal', 'NotCapableOf', 'NotDesires', 'NotHasProperty', 'PartOf', 'ReceivesAction', 'SimilarTo', 'SymbolOf', 'UsedFor']. We exclude lexical relations (such as FormOf and Synonym) and 
non-commonsense relations (such as ExternalURL). 

3. Q: L322: Argument unclear.
A: The complete version of these two triples should be: (actor acting
in play, Causes, audience applause) and (actor acting in play,
Causes, actor getting famous). Subjects of head are the same but subjects of tail are different.


Review #2

Thanks for your review!

1. Q: In section 3, explanation of OpenNRE is in order
A: A brief task definition is in section 2.

2. Q: Are relation types the same across the different corpora?
A: Yes, relations types are fixed.

3. Q: An explanation of MRR is missing.
A: For a pair (head, tail), triple (head, tail, rel) is sorted by rel probability output by model. MRR is the average of reciprocal ranks of correct relations.

4. Q: Wiki (FB) performance is far from being good.
A: Training data of Wiki(FB) is much smaller.

5. Q: "removal of NA relation" in lines 163 ff. is unclear.
A: For a pair (head, tail), triple (head, tail, NA) is deleted.

Review #3

Thanks for your review!

1. Q: 'Obama is Canadian.' ...
A: I don't think it's a fact. Not all statements with one argument being a NE are fact. 

2. Q: How to retrieve input sentences?
A: By exact match. 

3. Q: Define 'head' and 'tail'.
A: 'head' and 'tail' are commonly used terms in KB. We will expand the abbreviation of Line28 to '(head, relation, tail)'.

4. Q: Define 'negative example'.
A: Following the convention, 'negative example' means random combination of entities in the KB.

5. Q: Haven't explained the model part. How to use GloVe? Why word2vec first then GloVe?
A: We briefly explain the model in Line 121-123 and it's cited in Line 176. PCNN+ATT collect sentences containing the same entity pair as a set and use CNN to represent each sentence as embedding. It uses attention to calculate the weighted sum of these sentences as the set vector. After a representation matrix of relations, it outputs the probability distrition over all relations. We use GloVe as word embeddings to feed in CNN. We mentioned word2vec first just to show the difference between the original setting and ours.

6. Q: Define entropy.
A: Definition is in Line 232-243.

7. Q: 'when the number...decreases'
A: We didn't consider this statement as a discovery but an explanation of why we only focus on single-word triples.

8. Q: Define DPL and DT. 
A: Definition is in Table 7 caption (Line 356-358). 

9. Q: Say the tool for parsing and POS tagging.
A: The tool is Stanford CoreNLP. We will add the citation.

10. Q: Explain MRR.
A: Please see Review#2 Point 3.

11. Q: 'baking' has ambiguous meaning.
A: Such ambiguity exactly shows that we can't exclude some senses of a concept. 

Review #4

Thanks for your review!

1. Q: Sample size is small.
A: We plann to increase the sample size in future.


General Response to Reviewers

1. Q: Selected corpus are too general.
A: We focus on open-domain not domain-specific corpus. What we concern is: is it possible to extract commonsense from general-purpose large corpus. In addition, conventional factual relation extraction focus on general corpus. Thus we think it's natural to use the same corpus.
