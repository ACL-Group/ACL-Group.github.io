\section{Introduction}
% What is RC. Given sufficient data good, otherwise bad.
Relation classification(RC) is an indispensable problem in natural language processing(NLP). Given a sentence (e.g., \emph{Washington is the capital of the United States}) containing two entities we focus on (e.g., \emph{Washington} and \emph{the United States}), an RC model aims to distinguish the semantic relation between the entities conveyed by the sentence (e.g., \emph{capital of}).

Conventional relation classification has been an extensively investigated task. Recent approaches substantially base on neural networks and deep learning \citep{RecursiveNNRC, zeng-etal-2014-relation, RNNRC, vu-etal-2016-combining}, where the models are trained with sufficient human-annotated data and achieve satisfactory results.
%\KZ{What does this mean? OK}
However, human-annotation is expensive. It is burdensome to attain considerable amount of labeled data, under which circumstance a sharp decrease occurs in the performance of conventional RC models.
% Distant supervised ways
Distant supervised methods have been adopted to enlarge annotation quantity by utilizing existing knowledge bases to perform auto-labeling on massive raw corpus. The NYT-10 dataset \citep{NYTdataset} is a typical large dataset constructed under distant supervision. Although distance supervised approaches greatly augment labeled data, significant shortcomings expose: %\KZ{Elaborate a bit what you mean bylong-tail problem. OK}
(1) Long-tail problems \citep{xiong-etal-2018-one, han-etal-2018-fewrel, ye-ling-2019-multi} exist in knowledge bases. While some particular relation classes contain a great proportion of instances, most classes consist of only tens of instances.  (2) Noise issues occur during auto-labeling, demanding for manual screening.

% Few-shot RC
Few-shot relation classification is a particular RC task under minimum annotated data. In few-shot relation classification tasks, a model is required to distinguish the category of a new incoming query instance given only few support instances (e.g., 5 or 10). An example is given in Table~\ref{FewShotRCExample}.

\begin{table}[t]
\centering
\small
\begin{tabular}{p{7.3cm}}
\hline
\textbf{Support Set} \\ \hline
\textbf{Class1} \emph{~mother}: \\
\qquad \textbf{Instance1}~\lbrack Emmy Acht\'e\rbrack$_{entity1}$ was the mother of the internationally famouse opera singers \lbrack Aino Ackt\'e\rbrack$_{entity2}$ and Irma Tervani. \\
\qquad \textbf{Instance2}~He was deposed in 922, and \lbrack Eadgifu\rbrack$_{entity1}$ sent their son, \lbrack Louis\rbrack$_{entity2}$ to safety in England. \\
\qquad \textbf{Instance3}~Jinnah and his wife \lbrack Rattanbai Petit\rbrack$_{entity1}$ had separated soon after their daughter, \lbrack Dina Wadia\rbrack$_{entity2}$ was born. \\
\qquad \textbf{Instance4}~Ariston had three other children by \lbrack Perictione\rbrack$_{entity1}$: Glaucon, \lbrack Adeimantus\rbrack$_{entity2}$, and Potone.\\
\qquad \textbf{Instance5}~She married (and murdered) \lbrack Polyctor\rbrack$_{entity2}$, son of Aegyptus and \lbrack Caliadne\rbrack$_{entity1}$. Apollodorus.\\
\textbf{Class2} \emph{~spouse}: ... \\
\textbf{Class3} \emph{~child}: ... \\
\textbf{Class4} \emph{~follows}: ... \\
\textbf{Class5} \emph{~crosses}: ... \\ \hline
\textbf{Query Instance} \\ \hline
Dylan and \lbrack Caitlin\rbrack$_{entity1}$ brought up their three children, \lbrack Aeronwy\rbrack$_{entity2}$, Llewellyn and Colm. \\
\hline
\end{tabular}
\caption{\label{FewShotRCExample}
An example of 5 way 5 shot relation classification scenario from the FewRel \citep{han-etal-2018-fewrel} validation set. The query instance is of Class1:\emph{mother}. The support instances of Class2-5 are omitted.
}
\end{table}

% Meta learning
Meta-learning is a major method under few-shot learning circumstances and is broadly studied in computer vision (CV) \citep{LakeHuman, Santoro2016, proto}. Instead of training a neural network to learn a specific task, in meta-learning, the model is trained with a variety of similar but different tasks to gain the ability of quick adaptation to new tasks without meeting a ton of data.
One framework of meta-learning is to train an additional meta-learner %
%\KZ{Rephrase this: on the upper to guide the upgrading steps of the conventional learner on the lower OK}
which sets and adjusts the update rules for the conventional learner \citep{Andry2016, Finn2017, HN}. Another bases on metric learning and is aimed at the acquisition of distance distribution among the relations \citep{Koch2015, Vinyals2016, proto}.
%Prototypical networks \citep{proto} is a typical and widely used metric learning based meta-learning framework.

% Recent works on meta-RC
In recent years, meta-learning has been adopted in NLP as solutions to multiple few-shot learning tasks \citep{gu-etal-2018-meta, han-etal-2018-fewrel, huang-etal-2018-natural, obamuyide-vlachos-2019-meta, ye-ling-2019-multi}. In this paper, we focus on meta-learning methods on few-shot relation classification task.
 \citet{han-etal-2018-fewrel} constructed the FewRel dataset and first applied distinct meta-learning frameworks intended for CV tasks on the FewRel dataset. Prototypical networks \citep{proto} with CNN encoder turned out to have the best performance. \citet{ye-ling-2019-multi} improved the framework of prototypical networks by interactively encoding the support and query instances at both local and instance level, and adding weights while calculating prototypes.

% What's different in our work
Admitting that meta-learning frameworks outperform conventional methods in few-shot relation classification, we see room for further improvements.
Firstly, the previous models concern much about
%\KZ{the outputs of the query instances OK}
computations on query instances but lose sight of the significance of information within the support instances. Secondly, the demand for much human annotation is not \emph{really} settled in previous works. Even though just few support instances are needed during testing, the training set is still sufficient and large (e.g., the FewRel dataset \citep{han-etal-2018-fewrel} contains 700 instances per relation). And performance drops when the training data size is restricted (e.g., tens of instances per relation).

Our proposed
%\KZ{Find another name?}
\emph{Metric-learning based Meta-learner Enhanced} (MME) meta-learning framework improves these weaknesses. Firstly, in MME, to exploit the underlying knowledge within support instances, we add a supplementary classifier over the support instances which contributes to the update of the model by a unique fast-slow learner strategy. Secondly, we show that meta-learning is capable of learning cross-domain underlying knowledge and we improve the model performance by augmenting training data with open-source relation classification instances. This is extremely helpful under circumstances where few training data of the target domain is available.

Additionally, we propose our own dataset, \emph{Few-shot Relation-classification Medical} (FRM) dataset, a Chinese few-shot relation classification dataset in medical domain. The FRM dataset contains 27 relations with 50 instances per relation. Experiments are conducted on both our proposed FRM dataset and the FewRel dataset \citep{han-etal-2018-fewrel}. Experimental results show that (1)
%\KZ{How do you show it's a hard task? OK}
The FRM dataset provides a hard task. Strong baselines behave poorly on the FRM dataset. (2) Our proposed MME framework works out to have the best performance on FRM dataset and achieves competitive results on the FewRel dataset.

% Our contributions
In summary, our contributions include: (1) We propose a novel \emph{Metric-learning based Meta-learner Enhanced} (MME) meta-learning framework which fully utilize the information brought in support instances with the help of a supplementary classifier and a meta-learner. (2) We illustrate that meta-learning process learns cross domain underlying knowledge and propose an approach to add supplementary data during training process to achieve performance gains. (3) We propose \emph{Few-shot Relation-classification Medical} (FRM) dataset, a Chinese few-shot relation classification dataset in medical domain. (4) Our approach achieves state-of-the-art performance on FRM dataset and competitive results on FewRel \citep{han-etal-2018-fewrel} dataset.
