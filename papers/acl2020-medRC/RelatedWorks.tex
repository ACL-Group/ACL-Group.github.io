\section{Related Work}
%\subsection{Relation Classification}
Relation classification task aims to categorize the semantic relation between two entities conveyed by a given sentence into a relation class. In recent years, deep learning has become a major method for relation classification. \citet{RecursiveNNRC} fed the recurrent neural network (RNN) with a syntactic tree to introduce attention mechanism in relation classification tasks. \citet{zeng-etal-2014-relation} utilized a convolutional deep neural network(DNN) during relation classification to extract lexical and sentence-level features. \citet{vu-etal-2016-combining} employed a voting scheme by aggregating a convolutional neural network with a recurrent neural network.

Traditional relation classification models suffer from lack of data. To eliminate this deficiency, distant-supervised approaches are proposed, which take advantage of knowledge bases to supervise the auto-annotation on massive raw data to form large datasets. \citet{NYTdataset} constructed the NYT-10 dataset with Freebase \citep{Freebase} as the distant supervision knowledge base and sentences in the New York Times (2005-2007) as raw text. Distant-supervised methods suffer from long tail problems and excessive noise. \citet{zeng-etal-2015-distant} proposed piecewise convolutional neural networks (PCNNs) with instance-level attention to eliminate the negative effect caused by the wrongly labeled instances on NYT-10 dataset. \citet{liu-etal-2017-soft} further introduced soft label mechanism to automatically correct not only the wrongly labeled instances but also the original noise from the distant supervision knowledge base.

Few-shot relation classification is a newly-born task. \citet{han-etal-2018-fewrel} proposed the FewRel dataset for few-shot relation classification and applied meta-learning methods in \citep{metanet, snail, proto} with CNN and PCNN core on the FewRel dataset. Prototypical networks \citep{proto} with CNN core turned out to have the best test accuracy among the reported results. ProtoHATT \citep{hatt} reinforced the prototypical networks with hybrid attention mechanism. MLMAN \citep{ye-ling-2019-multi} improved the prototypical networks by adding mutual information between support instances and query instances.
%\subsection{Meta-learning}
% in CV... 组会的papers!
%Meta-learning aims to .... It is first proposed by [??] where the authors ...image classification... and is widely researched in computer vision. One category of meta-learning methods trains a meta-learner above the traditional learner. The meta-learner guides the learning steps of the traditional learner[?? ??]. Another category bases on metric learning, which aims to find the underlying distribution among all the classes[?? ??]. Prototypical networks[??] is a typical framework of metric-learning based meta-learning. It proposes the concept of prototype, which is a centroid among all the support vectors within the same class, and uses the prototypes to represent the vectors of each relation. In [??], prototypical network is adopted to handle NLP tasks such as few-shot relation classification.

The methods of updating models in previous works rely on the output of the query instances to a great extent, which lose sight of the significant and precious information within the support instances. Inspired by \citet{chen-2019-image}, who introduced a softmax classifier over the support images in image deformation task, we add a supplementary classifier over the support instances during the training process. The supplementary classifier helps with the update process of the model with a fast-slow learner schema.


%\subsection{Datasets}
Relation classification datasets have been released in past decades. Conventional ones include SemEval-2010 Task 8 dataset \citep{semeval8}, ACE 2003-2004 dataset \citep{ace}, TACRED dataset \citep{zhang-etal-2017-position} and NYT-10 dataset \citep{NYTdataset}. All these datasets encompass sufficient data to train a strong model. \citet{han-etal-2018-fewrel} first released the FewRel dataset, a dataset for few-shot relation classification, which contains 100 relations with 700 instances per relation. Although the dataset is intended for few-shot relation classification, the training data is still adequate (the model meets 700 instances times 100 relations during training). % And the performance of models decrease if the training data decreases.
We propose \emph{Few-shot Relation-classification Medical} (FRM) dataset, which is a \emph{real} few-shot relation classification dataset with 27 relations and only 50 instances per relation. The FRM provides a much harder task than the previous few-shot relation classification tasks. %In our paper, we also propose a method that is capable of handling few training data.
