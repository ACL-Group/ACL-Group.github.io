\section{Conclusion}
\label{sec:conclude}
In this work, we present a new automatic evaluation framework called ChatMatch. 
We first make the chatbots converse directly with each other. 
Then we use a three-level rule-based scoring framework to rank their performances which mimics the process of a double round-robin sports tournament. 
We have provided an example about how to implement this kind of automatic evaluation framework in \secref{sec:experiment}. 
The results show a good correlation with human judges. 
Another remarkable advantage of our framework is that it's totally automatic and time-saving which costs 1 min 6 secs on average to get the final ranking results among 6 chatbots. 
We believe that this kind of automatic interactive evaluation framework provides a good direction for future research on dialogue system evaluation.
