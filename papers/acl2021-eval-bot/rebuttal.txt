R1:
Thanks for your review.

Your main complaint on our paper is that our approach is too simple. However, our goal is to propose a simple but effective evaluation framework. We have tested our framework with six chatbots which are all considered to be competitive enough (e.g., blender bot). While comparing our evaluation results with other automatic evaluation metrics such as context coherence (CC in our paper) that use much more complex models such as GPT-2, we find that our model shows stronger correlation with human judgements. 

Q2: about using BERTScore as baseline:
We agree that BERTScore is considered as one of the state-of-the-art metric for text generation tasks such as machine translation. However, it doesn’t perform well for open-domain dialogue system evaluation tasks since BERTScore is a supervised approach that requires reference responses as training data. Open-domain dialogue systems may produce more than one plausible response given the context, which leads poor correlation between reference-based automatic metrics and human judgements. 

R2:
Thanks for your review with valuable questions and constructive suggestions. 

Q1: about “reproducibility and inconsistency issues”:
Here is our understanding of your complaint: when we evaluate two sets of
bots (e.g., {A, B, C, D} and {A, B, E, F}) separately using two tournaments in our framework, we may end up with a situation where A is ranked higher than B in the first tournament but A is ranked lower than B in the second tournament. We agree that this may happen. However, we do not consider this difference an issue of irreproducibility. Our evaluation framework attempts to measure the capability of the bots relative to the others in the tournament. The same bot may exhibit different capabilities depending one who it competes with. This is the same in sporting tournaments. The inconsistency is not due to the design of our framework but the number of bot candidates in the tournament. When that number gets large enough, the relative ranking of bots will be more stable and results will be more consistent.

Q2: about “missing references”:
Thanks for your suggestions about adding sources related to error taxonomy in dialogues. We choose to use repetitiveness and consistency as metrics as we find that even recently developed advanced chatbots keep making these kinds of mistakes. Meanwhile, we believe memorization is an important capability for a chatbot. It’s therefore reasonable to evaluate the ability of chatbots according to these three metrics. The goal of our paper is to provide a heuristic and light-weight framework for chatbot evaluation, which is not limited to these three metrics,
but may also include those metrics used in dialogue breakdown detection tasks.

R3:
Thanks for your valuable suggestions and elaborate questions!

Q1: about Kendall’s \tau:
We never used accuracy or precision as measures in evaluating our framework.
Actually, we only use Kendall's \tau here to evaluate correlation between the rankings produced by Chatmatch and the rankings
produced by human judges. Kendall’s tau is a statistic used to measure the ranking correlation between two measured quantities, which determines the strength of association based on the concordance and discordance between the pairs. A \tau close to 1 or -1 corresponds to an extremely strong positive or negative correlation between two measured quantities. Even though there’s no universal standard interpretation between weak and strong correlation and exact boundary of \tau, a \tau which is bigger than 0.35 is usually considered as a high correlation. 

Q2: about more state-of-the-art chatbots:
We strongly agree with you that it will be more valuable to use more state-of-the-art chatbots in our experiments. We will include more such chatbots and also enlarge the size of tournament candidate pool later. However, we believe that the the quality of chatbots is not the most critical part in evaluating our framework design. Our framework can be applied to any open-domain black-box chatbots 
and we have already included some competitive bots.  

Q3: about \tau below 0.35:
We have tried several different greeting examples and they are all very close to 0.35 as slight variation of greetings does not lead to different direction of conversation between bots. 

Q4: about the last 5 exchanges and the long sequence of conversation:
It differs in bots. As for the conversations between worse-behaving bots such as DG and DD, they are always repeating themselves or some utterances from the history. As for conversations between more competitive ones like BB and CS, they are more likely to return some greetings or blessings with different form such as "Hope you enjoy your work and have a great day" and "Good luck for the rest of the day". 

We believe that long sequences of conversations provide the bots with more opportunities to expose their strengths and weaknesses. 

General Response:
We found some common doubts about our paper:
1. As for the design of framework, we choose the three metrics based on our preliminary observations on chat logs between human and bots, which show that they are of fundamental for evaluating chatbot conversational ability. 
2. We try to make our ChatMatch framework simple enough meanwhile highly correlated to human judgements. We hope that our framework can make judgements by some basic heuristics rather than being complicated as a bot that can participate in the match by itself.
3. The focus of our framework is to give an overall ranking of a group of bots through a complete tournament. We believe this leads to more objective and comprehensive results comparing with traditional point-wise or pair-wise evaluation. 

To Chairs:
For Review #1:
We are a little disappointed by the quality of Review #1. The criticism is very subjective and unfounded. There is no explantion or evidence to support their complaints. We would appreciate it if an additional review can be sought.

For Review #2:
The complaint in Review #2 about reproducibility and inconsistency is not clear at all. As a result, we respond to the comment based on our guess, which may negatively affect the quality of our response and the outcome of this paper.

