View Reviews
Paper ID1341
Paper TitleCombating Short Circuit Behavior in Natural Language Reasoning: Crossover and Mutation Operations for Enhanced Robustness
Reviewer #1
Questions

1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The main contributions of the paper are related to the identification of the short circuit in LLMs with two different tests: 1) a white-box test that exploits attentions in LLMs and 2) a black-box test that exploits several text modifications using some existing approaches a new approaches like "crossover". Crossover consists in change the training data, in particular one of the false options with a true option from another example.
Text modification in black-box tests is then used, in particular the crossover operation and the mutation ("swapping of words in the answers"), to generate new training data useful to teach the model to focus on the premise and not use the short circuit.
Experiments show improved performance on all the used datasets.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Fair: The paper contributes some new ideas
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Fair: The paper is likely to have moderate impact within a subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Poor: The shared resources are unlikely to be useful to other AI researchers.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Poor: key details (e.g., proof sketches, experimental setup) are incomplete/unclear, or key resources (e.g., proofs, code, data) are unavailable.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above)
S1) A deep study on the short circuit identification through two tests, and data augmentation techniques to address the short circuit problem
S2) The Experimental evaluation is good even though another stress test can be considered
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
W1) Back translation could be used as a stress test since it seems to be natural.
W2) No resources, no reproducibility.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
Q1) I don't understand why adding or removing negation can't be used to generate more training data. Why do you claim that only crossover and mutation can be applied to generate a consistent amount of training data (Sec 2.2)? Moreover, there are also other approaches like back translation that can be considered. In my opinion, adding or removing negation + back translation can generate a consistent amount of training data for your purpose.
Q2) Why you didn't use the back translation as a stress test?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
Your experiments are about positive and negative examples. For data augmentation, there is a system that generates positive and negative claims wrt tabular data using ambiguities in the language. Maybe this can help you to also catch short circuits (Data Ambiguity Profiling for the Generation of Training Examples, ICDE 2023).
But this also leads to another discussion. Is the short circuit related to the pretraining of LLMs? i.e. is it related to the actual data used in the pretraining? Do you know for sure that the pretraining data do not contain any of your test data? To answer this question, in your experiments, you can also use a new (never seen data), for example, a private dataset manually created, or a randomly generated dataset, or a dataset generated by a dataset generator.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Weak Accept: Technically solid, moderate to high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.
15. (CONFIDENCE) How confident are you in your evaluation?
Very confident. I have checked all points of the paper carefully. I am certain I did not miss any aspects that could otherwise have impacted my evaluation
16. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.
Reviewer #2
Questions

1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper explores the issue of "short-circuiting" in Natural Language Understanding (NLU) tasks, where the models by exploiting statistical biases of training data instead of understanding the context.
The authors propose two methods to detect short-circuiting: a white-box method using attention weight thresholding and a black-box "crossover" test, which switch the true choices of two examples. 
To mitigate short-circuiting, the paper introduces two operations as novel data augmentation techniques: : 1) "crossover" swaps the correct responses between examples, and 2) "mutation" switches words in the responses. These methods have been shown to improve model performance by up to 24% on stress tests and 10% on original test data.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR moderate impact across more than one subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Good: The paper is well organized but the presentation could be improved.
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Fair: The experimental evaluation is weak: important baselines are missing, or the results do not adequately support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Fair: The shared resources are likely to be moderately useful to other AI researchers
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Good: key resources (e.g., proofs, code, data) are available and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to confidently reproduce the main results.
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above)
1) The problem of "short-circuiting" is important.
2) The proposed augmentation method is effective in improving model robustness.
3) The paper is presented in a clear, organized structure, which are easy to follow.
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
1) The experiments for selecting short circuit testing methods (as discussed in section 3.2.1) are not compelling: 
a) The sample size of the test cases is limited to 30, which is relatively small.
b) The authors assume that the ensemble result of multiple methods is the ground truth, which needs explanation. 
c) The paper omits “choice-only tests”, which supports the occurrence of “short-circuit” phenomenon as claimed in section 1.
2) The paper does not consider the operation in stress tests, such as Syn, which potentially could enhance the model's robustness.
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
Q1. The definition of the proposed issue, "short-circuiting," remains unclear. As mentioned in section 3.2.1, “The model is considered not short-circuiting on a case according to a test operator if it still gets the right answer after the operation”, which implies that the "short-circuit" is defined independently of the operator. If so, why are "AW" and "CO" used to signify the "short-circuit" test in Table 5?
Q2. What is the model performance against different operators in Table 1? 
Q3. What are the implementation details of attention visualization in the case study?
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
There should be more quantitative experiments to show the proposed two methods is better for short-circuiting detection.
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Weak Accept: Technically solid, moderate to high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.
15. (CONFIDENCE) How confident are you in your evaluation?
Somewhat confident, but there's a chance I missed some aspects. I did not carefully check some of the details, e.g., novelty, proof of a theorem, experimental design, or statistical validity of conclusions.
16. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.
Reviewer #3
Questions

1. {Summary} Please briefly summarize the main claims/contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
The authors investigate the "short-circuit" phenomenon observed in natural language reasoning tasks, propose tests and introduce data augmentation techniques to alleviate the problem.
2. {Novelty} How novel are the concepts, problems addressed, or methods introduced in the paper?
Good: The paper makes non-trivial advances over the current state-of-the-art.
3. {Soundness} Is the paper technically sound?
Good: The paper appears to be technically sound, but I have not carefully checked the details.
4. {Impact} How do you rate the likely impact of the paper on the AI research community?
Good: The paper is likely to have high impact within a subfield of AI OR moderate impact across more than one subfield of AI.
5. {Clarity} Is the paper well-organized and clearly written?
Excellent: The paper is well-organized and clearly written
6. {Evaluation} If applicable, are the main claims well supported by experiments?
Good: The experimental evaluation is adequate, and the results convincingly support the main claims.
7. {Resources} If applicable, how would you rate the new resources (code, data sets) the paper contributes? (It might help to consult the paper’s reproducibility checklist)
Not applicable: For instance, the primary contributions of the paper are theoretical.
8. {Reproducibility} Are the results (e.g., theorems, experimental results) in the paper easily reproducible? (It may help to consult the paper’s reproducibility checklist.)
Fair: key resources (e.g., proofs, code, data) are unavailable but key details (e.g., proof sketches, experimental setup) are sufficiently well-described for an expert to confidently reproduce the main results
9. {Ethical Considerations} Does the paper adequately address the applicable ethical considerations, e.g., responsible data collection and use (e.g., informed consent, privacy), possible societal harm (e.g., exacerbating injustice or discrimination due to algorithmic bias), etc.?
Not Applicable: The paper does not have any ethical considerations to address.
10. {Reasons to Accept} Please list the key strengths of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above)
relevant and interesting problem
original approach and solution
well-written paper
good evaluation
11. {Reasons to Reject} Please list the key weaknesses of the paper (explain and summarize your rationale for your evaluations with respect to questions 1-9 above).
-
12. {Questions for the Authors} Please provide questions that you would like the authors to answer during the author feedback period. Please number them.
-
13. {Detailed Feedback for the Authors} Please provide other detailed, constructive, feedback to the authors.
well done research, well written paper
14. (OVERALL EVALUATION) Please provide your overall evaluation of the paper, carefully weighing the reasons to accept and the reasons to reject the paper.
Strong Accept: Technically strong paper with, with novel ideas, excellent impact on at least one area of AI or high to excellent impact on multiple areas of AI, with excellent evaluation, resources, and reproducibility, and no unaddressed ethical considerations.
15. (CONFIDENCE) How confident are you in your evaluation?
Quite confident. I tried to check the important points carefully. It is unlikely, though conceivable, that I missed some aspects that could otherwise have impacted my evaluation.
16. (EXPERTISE) How well does this paper align with your expertise?
Knowledgeable: This paper has some overlap with my current work. My recent work was focused on closely related topics and I am knowledgeable about most of the topics covered by the paper.
