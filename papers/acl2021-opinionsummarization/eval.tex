\section{Evaluation}
\label{sec:eval}
In this section, we introduce the dataset and experimental setup.
%We compare our proposed model
We analyze the experimental results
%with existing opinion summarizaiton models 
and demonstrate the advantages of our models
~\footnote{All data and source code
can be downloaded from \url{http://anonymized.for.blind.review}.}.

\subsection{Datasets}
In this experiment, we use two datasets.


\textbf{Yelp} 
%\footnote{https://www.yelp.com/dataset}
contains a large number of reviews on business.
%in the area of business. 
%For summarization purpose, 
\citet{MeanSum19} also released the development and test set with 8 reviews for each human-written summary under the same business.

\textbf{Amazon} 
%\footnote{https://cseweb.ucsd.edu/~jmcauley/datasets.html}
is a similar dataset~\cite{HeM16} made up of product reviews from four different categories. %Similary, a set of summaries are created by Amazon Mechanical Turk (AMT) workers in~\cite{Copycat20} . 
The development and test set~\cite{Copycat20} accompany with 8 reviews and 3 human-written summaries for each product.

For training, we construct the synthetic training set for these two datasets according to Section~\ref{sec:data} respectively. Details are shown in Table~\ref{tab:datasets}.

\begin{table}[th]
	\small
	\centering
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{} & \textbf{Training} &\textbf{Development} & \textbf{Testing}\\
		\hline
		Yelp & 100k & 100 & 100 \\
		Amazon & 90k & $28\times3$ & $32\times3$ \\
		\hline
	\end{tabular}
	\caption{Data statistics. $\times3$ means three human-written  summaries per entity.}
	\label{tab:datasets}
\end{table}

\subsection{Implementation Details}
In this experiment,
we apply aspect-variant guided models on transformer seq2seq model~\cite{Transformer17}
and BART~\cite{BART20}
\footnote{https://github.com/pytorch/fairseq}
which is a pretrained transformer model.

For creating synthetic data, 
we set $N_r$ (\secref{sec:data}) as $8$ 
since every summary in the human-annotated development and test set has $8$ reviews.
For transformer seq2seq model, we use SGD as optimizer and set the initial learning rate as $0.1$.
A momentum $\beta=0.1$ and a decay of  $\gamma=0.1$.
For BART,
we follow~\citet{BART20} in fine-tuning BART with
$lr=3e-05$ and warmup $=500$.
At decoding, we used Beam Search with a beam size of 5.
We train our models on one RTX 2080Ti GPU with 11G RAM. 
The average training time of our approachs about $10$ hours.
Previous methods using different versions of ROUGE,
the ROUGE scores shown in papers cannot be compared.
Thus, in our experiment, we use the general ROUGE evaluation tool {\em file2rouge} in summarization tasks
~\cite{BART20,DialogMV2020}.




\subsection{Models under Comparison}
In this experiments, we evaluate different methods on above datasets.
The brief description of these methods are shown in \tabref{tab:baselines}.

%\begin{itemize}
%	\item \textbf{MeanSum}~\cite{MeanSum19}. An unsupervised auto-encoder model decodes the summary based on the mean representation of input reviews.
%	\item \textbf{CopyCat}~\cite{Copycat20}. A hierarchical variational autoencoder model with controlling of novelty between inputs and the generated new review.
%	\item \textbf{OpinionDigest}~\cite{SuharaWAT20}. A framework consists of opinion extraction, opinion selection and summary generation.
%	\item \textbf{Denoise}~\cite{Denoise20}. A denoising summarization model with linguistically motivated noising datasets.
%	\item \textbf{FewSum}~\cite{Fewshot20}. A conditional transformer model with specially design on consideration of review properties, including content coverage, writing style and length deviations.
%	\item \textbf{PlanSum}~\cite{abs-2012-07808}. A method explicitly take the form of aspect and sentiment distributions for synthetic dataset construction and summary generation.		
%\end{itemize}

\begin{table}[th]
   \small
	\centering
	\begin{tabular}{|m{1.2cm}<{\raggedleft}|p{5.5cm}|}
		\hline
		\textbf{Abbrev.} & \textbf{Description} \\ 
	    \hline
         MeanSum &Mean representations~\cite{MeanSum19}  \\
        \hline
		Copycat & Copycat-Review~\cite{Copycat20}\\
		\hline
		OpiDig & OpinionDigest~\cite{OpiDig20}\\
		\hline
		Denoise & \tabincell{l}{Nosing \& denoising \\ ~\cite{Denoise20}} \\
		\hline
		FewSum & Few-shot learning~\cite{Fewshot20}\\
		\hline
		PlanSum & Content Planning~\cite{Plansum20}\\
		\hline
		MB & Training MAI based on pretrained BAG \\
		\hline
		X-T & \tabincell{l}{Model X with transformer encoder-decoder \\ ~\cite{Transformer17}} \\
		\hline
		X-B & Model X with BART~\cite{BART20} \\
		\hline
	\end{tabular}
	\caption{The abbreviation and description of methods.}
	\label{tab:baselines}
\end{table}


\subsection{Evaluation Metrics}
We evaluate the performance of our methods by {\em automatic metrics}
and {\em human evaluation}.

%\textbf{Automatic Metrics.}

%\begin{itemize} 
%\item 
\textbf{ROUGE} scores (F1) include
%is the standard evaluation metric in summarization task,
ROUGE-1 (R-1), ROUGE-2 (R-2) and
ROUGE-L(R-L)~\cite{rouge}.

%\item 
\textbf{Aspect Coverage} (AC).
We extract aspects from summaries 
by a rule-based method~\cite{aspect14},
which is different from MIN-MINER in our approach.
\footnote{This is to eliminate the bias caused by using the same aspect extraction tool in the approach and evaluation.}
We take aspects of gold summary as reference,
and compute R-1 recall between reference  
and aspects of generated summaries.
AC is the average of these R-1 recall scores.

%\item 
\textbf{Diversity} (Div).
We use self-BLEU~\cite{SelfBleu18} to evaluate the diversity of a summary. 
It measures BLEU score of each generated sentence by considering others as reference. The self-BLEU score is the average of these BLEU scores.
The lower values means more diversity.
%\end{itemize}

\textbf{Human Evaluation.}
We randomly select 32 samples (the same as Amazon) from Yelp and all samples from Amazon.
We ask three human annotators
%\footnote{The Cohen's Kappa coefficient among annotators are $0.6$, indicating substantial agreement.}
who are 
native or proficient English speakers to evaluate generated summaries.
They score gold summary and summaries 
generated by our best model and 
PlanSum according to the consistency with multi-review and informativeness.
i.e., best (1.0), average (2.0) and worst (3.0).
%For a generated summary, we average the scores by
For a generated summary, we average the scores by
three annotators.
The human evaluation score of a model is the average scores of its all generated summaries.


\subsection{Analysis}
\label{sec:results}
In this section, we analyze our proposed synthetic training data  and 
aspect-variant guided models.


\textbf{Synthetic training data.}
%Existing synthetic training datasets
%have different training model,
%because the model is designed for making full use of data.
The synthetic data of the state-of-the-art model, PlanSum,
consists of multi-review and summary pairs.
To evaluate the usefulness of training on semi-structure data
versus text data,
we compare the synthetic data of PlanSum trained on its own model and
semi-structured PlanSum data with model MB-B.
We apply different models because
the synthetic dataset is compatible with its model.
%that synthetic datasets are specific and 
%the models designed for datasets can make full use of data.
The semi-structured PlanSum data
takes opinion-aspect pairs (OAs) and implicit sentences (ISs)
of multi-reviews as input, which should use MB-B for training.
As shown in \tabref{tab:traindata}, 
the semi-structured data of PlanSum training on MB-B
has higher ROUGE scores and AC scores.
which shows that semi-structured data
is helpful in highlighting the aspects and opinions.

The synthetic data of OpiDig takes
a review as output and the OAs of this review as input.
During generation, OpiDig clusters the OAs from multi-review and
takes centroids of clusters as input.
We construct semi-structured OpiDig synthetic data
by taking reviews in OpiDig as summary and 
generating the noisy OAs and noisy ISs for summary as input.
As shown in \tabref{tab:traindata},
the ROUGE scores and Div scores of MB-B training on semi-structured
OpiDig dataset is better than OpiDig without semi-structured data.  
This denotes that the ISs in semi-structured data help model
capture implicit information.
The ROUGE scores of
 BAG model in \tabref{tab:abla} (b) training on BART with only OAs as input is better than OpiDig-B
 shows that our approach avoids the noise caused by the clustering in OpiDig
 and make model have the ability of denoising.

\begin{table*}[th]
	\begin{center}
		\small
		\begin{tabular}{|r|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\multicolumn{2}{|c|}{\bf Dataset} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\
			\hline
			\textbf{Synthetic data} & \textbf{Approach} & R-1 & R-2 & R-L & AC & Div & R-1 & R-2 & R-L & AC & Div \\
			\hline
			%\multirow{2}{*}{OpiDigest} & OpiDigest-B & 28.49 & 5.64 & 26.48 & 0.39 & 0.25 & 29.71 & 5.58 & 26.14 & 0.25 & 0.28\\ \cline{2-12}
			OpiDig & OpiDig-B & 28.49 & 5.64 & 26.48 & 0.39 & 0.25 & 29.71 & 5.58 & 26.14 & 0.25 & 0.28\\ 
			%\cline{2-12}	 
			semi-OpiDig& MB-B & 30.94 & 6.41 & 28.11 & 0.41 & 0.21 &31.42 & 6.16& 27.95 & 0.28 & 0.27\\
			\hline
			PlanSum & PlanSum & 31.19&6.83 &28.02 &0.38 & 0.27 & 31.89 &6.13 & 28.53 & 0.23 & 0.32\\  %\cline{2-12}
			semi-PlanSum& MB-B & 32.03& 6.92 & 28.98 & 0.41 & 0.23 & 31.29 & 6.23 &  27.95& 0.28 & 0.28 \\
			\hline
			OURS & MB-B & \bf 32.20 & \bf 7.06 & \bf 29.15 & \bf 0.44 & \bf 0.20 & \bf 32.65 & \bf 6.78 & \bf 29.14 & \bf 0.34 & \bf 0.25 \\ 
			\hline
		\end{tabular}
	\end{center}
	\caption{Automatic evaluation on Yelp and Amazon. The different synthetic datasets in semi-structured version
		are trained on our best model MB-B because of the characteristics of semi-structured data.}
	\label{tab:traindata}  
\end{table*}

Our synthetic dataset with semi-structured data 
achieves the best scores, which shows  
usefulness of representing review by semi-structured OAs and ISs. 
And the way of getting noisy OAs and ISs according to sampled summary 
is optimal.

\textbf{Ablations.}
\tabref{tab:abla} and \tabref{tab:acdv} show that the performance of
BAG is worst because
of only using OAs as input.
The generated summary of BAG in \tabref{tab:exp} 
shows that BAG with only OAs as input always generate 
sentences in general and cannot capture
the implicit information in multi-review.
BAI uses the same model structure as BAG
but takes the concatenation of OAs and ISs as input.
BAI introduce implicit information 
to model by ISs and performs better than BAG on ROUGE and Div score.
However, BAI equally deals with OAs and ISs,
which impact the attention of model on aspects.
In \tabref{tab:acdv} and \tabref{tab:exp},
BAI gets lower AC and fails to filter some noisy aspects, such as atmosphere and experience.
For our semi-structured data,
the model should learn to expand important OAs as sentences and summarize ISs in a better way.
MAI consists of an OA encoder and IS an encoder,
which deals with OAs and ISs through different ways in parallel.
The better ROUGE scores and lower Div scores
of MAI means that the implicit information can be captured.
To make full use of semi-structured data,
we finetune MAI model with pretrained BAG,
which first gets general information about OAs and
then utilize the ISs to get specific information.
The AC scores of models in \tabref{tab:abla} are similar
because all of them directly use OAs as input.

\begin{table}[th]
	\centering
	\small
	\subtable[Transformer]{
		\begin{tabular}{|m{0.7cm}<{\raggedright}|m{0.6cm}<{\centering}|c|c|c|c|m{0.6cm}<{\centering}|}
			\hline
			\multirow{2}{*}{\bf Model} & \multicolumn{3}{c|}{\bf Yelp} &  \multicolumn{3}{c|}{\bf Amazon} \\ \cline{2-7}
			& R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
			\hline
			BAG & 28.04 & 5.37 & 25.60 & 29.20 & 5.42 & 26.47 \\
			BAI & 28.99 & 5.55 & 26.40 & 30.27 & 5.73& 27.39 \\
			MAI & 29.49 & 5.71 & 26.86 & 29.94 & 5.88& 26.69\\
			MB & \bf 32.43 & \bf 6.37  & \bf 29.39 & \bf 31.79 &\bf 6.08 & \bf 28.52 \\
			\hline
		\end{tabular}
	}
	\qquad
	\subtable[BART]{
		\begin{tabular}{|m{0.7cm}<{\raggedright}|m{0.6cm}<{\centering}|c|c|c|c|m{0.6cm}<{\centering}|}
			\hline
			\multirow{2}{*}{\bf Model} & \multicolumn{3}{c|}{\bf Yelp} &  \multicolumn{3}{c|}{\bf Amazon} \\ \cline{2-7}
			& R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
			\hline
			BAG & 29.79 & 5.83 & 27.07 & 30.01 & 5.77 & 26.54  \\
			BAI & 30.27 & 5.94  & 26.90 & 30.18 & 5.90 &27.14 \\
			MAI & 30.93 & 6.40  & 28.11 & 31.47& 6.27& 28.03\\
			MB & \bf 32.20 & \bf 7.06 & \bf 29.15 & \bf 32.65 & \bf 6.78 & \bf 29.14  \\
			\hline
		\end{tabular}
	}
	\caption{The ROUGE scores of summaries generated by models training on our created synthetic data.
	}\label{tab:abla}  
\end{table}

\begin{table}[th]
	\begin{center}
		\small
		\begin{tabular}{|l|m{6.2cm}|}	
			%\hline \bf{BAG} \\
			\hline
			BAG & the \textbf{servers} are always \textbf{friendly} and the \textbf{food} is \textbf{great} . it is a \textbf{good mexican restaurant .} \\
			%\hline \bf{BAI} \\
			\hline
			BAI & \textbf{great food} , \textbf{great service} , \textbf{great atmosphere} , and \textbf{great prices} . \color{gray}{i have been there a few times and have \textbf{never} had a \textbf{bad experience} . }\\
			%\hline \bf{MAI} \\
			\hline
			MAI &i love this place. the \textbf{food} is \textbf{good} and the \textbf{service} is \textbf{great} . the \textbf{atmosphere} is \textbf{great} too. \color{gray}{the only thing is that it 's a little pricey for what you get .}\\
			% i will definitely be coming back.}  \\
			%\hline \bf{MB} \\
			\hline
			MB & it 's one of the \textbf{authentic mexican restaurant} in the area. the \textbf{food} is \textbf{great} . the \textbf{servers} are \textbf{very friendly} and \textbf{knowledgeable} . \textit{they took the order patiently . } the \textbf{chips} and \textbf{salsa} are \textbf{good} too . \textbf{it is huge and has patio seating .}  
			%\color{gray}{you can get a lot of food for the price you pay . }
			\\
			\hline
		\end{tabular}
	\end{center}
	\caption{The summaris of the example in \tabref{tab:example} generated by models based on BART.
	}\label{tab:exp}  
\end{table}


\begin{table}[th]
	\centering
	\small
		\begin{tabular}{|l|l|c|c|c|c|}
			\hline
			\multicolumn{2}{|c|}{\multirow{2}{*}{\bf Model}} & \multicolumn{2}{c|}{\bf Yelp} &  \multicolumn{2}{c|}{\bf Amazon} \\ \cline{3-6}
			\multicolumn{2}{|c|}{} & AC & Div & AC & Div \\
			\hline
			\multirow{4}{*}{Trans} & BAG & 0.35 & 0.26  & 0.26& 0.28 \\
			& BAI & 0.33 & 0.24 &0.24 & 0.27 \\
			& MAI & 0.39&  0.23 & 0.30 & 0.26 \\
			& MB & \bf 0.41 & \bf 0.22 & \bf 0.33 & \bf 0.26\\
			\hline
			\multirow{4}{*}{BART} & BAG &0.40 & 0.25 &0.28 & 0.27 \\
			& BAI & 0.38 & 0.22 & 0.28 & 0.26 \\
			& MAI & 0.43 & 0.22 & 0.32 & 0.26 \\
			& MB & \bf 0.44 & \bf 0.20 &\bf 0.34 & \bf 0.25 \\
			\hline
		\end{tabular}
	\caption{The aspect coverage and diversity scores of summaries generated by transformer (Trans) and BART training on our created synthetic training data.
	}\label{tab:acdv}  
\end{table}


   
    
\textbf{Generation.}
As shown in \tabref{tab:all},
compared with PlanSum, the ROUGE scores of MB-T is lower
because PlanSum use a pretrained model, BERT~\cite{BERT19}.
Similar to PlanSum, we convert MB-T models to MB-B on BART.
MB-B achieves the best scores in terms of ROUGE, AC and Div.
This shows that our MB-B model 
fully utilize our created synthetic training data.
MB trained on synthetic training data with semi-structured input
learns how to expand important OAs from input to generate some general
sentences and abstract OSs to describe implicit information 
in specific sentences.
The higher AC score of MB-B means
that the model benefits from directly taking OAs as input.
The lower Div scores denote the model capture the information of
ISs.
The improvement of our best model on Yelp is less than Amazon.
This is because that each sample in Amazon has 3 reference summaries and each sample in Yelp only has one reference summary.
For one reference summary,
the evaluation on ROUGE is strict.
This causes that even if the generated summary covers
more important information of multi-review, 
the ROUGE scores may not be changed much.
The multiple reference summaries are friendly to various abstractive summaries.
The improved generated summaries are more likely to match tokens
in multiple reference summaries. 

\begin{table*}[th]
	\centering
	\small
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{\bf Dataset} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\ \cline{2-11}
		& R-1 & R-2 & R-L& AC &Div & R-1 & R-2 & R-L& AC & Div\\
		\hline
		Meansum & 26.84 & 3.57 & 23.62 & 0.34 & 0.38 &  28.20 &4.67 & 24.15& 0.17& 0.40\\
		Copycat & 28.05& 4.90& 25.32& 0.38 & 0.34 & 29.71 & 5.78 & 26.42  & 0.18 & 0.43 \\
		OpiDig & 28.68 &5.00 & 25.33& 0.39 & 0.33 & 29.52 & 5.26 & 26.65 & 0.23 & 0.27 \\
		Denoise & 29.75 & 5.00 & 26.88 & 0.39 & 0.27 & - & - & - & - & - \\
		FewSum \tablefootnote{In our experiments, the summary is generated by FewSum without finetuned on human-annotated data, to be fair.} 
		& 29.64 & 5.80 & 27.01 & 0.38 & 0.28 & 31.02 & 6.06 & 27.94 & 0.20 & 0.30 \\
		MB-T & 30.43 & 6.37  & 28.09 &0.41 & 0.22 &31.79 &6.08 & 28.52 & 0.28 & 0.26 \\
		\hline
		PlanSum & 31.19&6.83 &29.02 & 0.38 & 0.27 & 31.89 &6.13 & 28.53 & 0.23 & 0.32\\
		MB-B & \underline{\bf 32.20} & \underline{\bf 7.06} & \underline{\bf 29.15} & \bf 0.44 & \bf 0.20 & \underline{\bf 32.65} & \underline{\bf 6.78} & \underline{\bf 29.14} & \bf 0.34 & \bf 0.25 \\
		\hline
	\end{tabular}
	\caption{Automatic evaluation. The scores underlined are statistically significantly better than PlanSum  with p$<$0.05 according to t-test.
	}\label{tab:all}  
\end{table*}


As the abstractive summary of multi-reviews is various,
we use human evaluation.
to help automatic evaluation.
The lower score on human evaluation means that generated summaries are better.
As shown in  \tabref{tab:human}, 
the human evaluation score of Gold is the smallest
since the Gold summaries are written by human.
The score of our model less than PlanSum
denotes that 
%human-annotators thinks that
the summaries generated by our model 
are more consistent with multi-reviews.


\begin{table}[th]
	\centering
	\small
	\begin{tabular}{|l|c|c|}
		\hline 
		& Yelp & Amazon\\
		\hline
	   Gold & \bf{1.3} & \bf{1.8} \\
	   PlanSum & 2.4 & 2.4 \\
	   MB-B & 2.2 & 2.0 \\
	   \hline
	\end{tabular}
	\caption{Human evaluation on Yelp and Amazon. 
	The Cohen's Kappa coefficient between annotators are $0.6$, indicating substantial agreement.
	}\label{tab:human}  
\end{table}







