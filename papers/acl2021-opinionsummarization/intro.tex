\section{Introduction}
\label{sec:intro}
Abstractive opinion summarization is the process of 
automatically producing a summary of a set of 
subjective user reviews about a given entity 
(maybe a place, a product or a service). 
Such a set of reviews is called a {\em multi-review} in this paper. 
The deep learning (DL) techniques has made a great success 
in abstractive text summarization
~\cite{NallapatiZSGX16,SeeLM17,LiuLZ18,CelikyilmazBHC18,BART20}, which need training on large datasets.
Unfortunately, the opinion summarization task 
generally lacks the multi-review and summary pairs for training,
as it is difficult for annotators to write summaries for 
multiple reviews on a large scale.

\begin{table}[th]
      \centering
		\small
		\begin{tabular}{|m{0.4cm}<{\centering}|m{6.4cm}|}
			\hline
		    \multicolumn{2}{|c|}{\rule{0pt}{10pt} \bf Multi-review}  \\	
			\hline
			R1 & \textcolor{gray}{\textbf{great place}}  \textcolor{gray}{...it can be pretty noisy }. \textbf{thin chips} and \textbf{good} , \textbf{spicy salsas}..
			\textcolor{gray}{\textbf{pretty decent prices} .} \\
			\hline
		    R2 &\textbf{authentic mexican food} at a \textcolor{gray}{\textbf{reasonable price}} . \textbf{free chips} and \textbf{salsa} ...  \textbf{delicious margaritas} . \textit{very patiently}\\
		     \hline
			 R3 &\textcolor{gray}{we love this place ! our date night always consist of margaritas ...} \\
			\hline
			R4 &\textbf{amazing experience} ... \textit{server mauricio came and took our order .} \textit{he was polite and knowledgeable} \\
			\hline
	    	R5 & \textit{having patio seating} ... it ' s a \textbf{good place} ... we had a \textcolor{gray}{\textbf{great experience}} at this place . \\
			\hline
		   R6 &the \textbf{best service} in the area jose . \color{gray}{consistently great . } \textbf{good management} .\\
			\hline
			R7 &one of the \textbf{best mexican restaurants} ... \textbf{service} is \textbf{consistently good} ... the \textcolor{gray}{\textbf{carne asada}} is \textcolor{gray}{\textbf{excellent}} \\
			\hline
			R8 &the \textbf{service} is \textbf{pretty good} but \textbf{not extraordinary} . the \textbf{food} is \textbf{satisfying} but \textbf{not great} ...\\
			\hline 
		    \end{tabular}
	        
			\begin{tabular}{|p{7.2cm}|}
			\hline \rule{0pt}{10pt}
			 \makecell[c]{\bf Gold Summary} \\
			\hline
			the \textbf{servers} are \textbf{kind} and \textbf{knowledgeable} . \textit{they will} \\
			\textit{patiently answer your questions .} \textit{they offer patio  seating} \\
			%\textit{seating if you ' d prefer to sit outside .} 
			the \textbf{free chips} and \textbf{salsa} are always a plus , and the \textbf{margaritas} are \textbf{amazing} too . the menu is \textbf{full tasty authentic mexican food .} \\
%			\hline \bf{Generated summary of basic model} \\
%			i love this place ! the \textbf{servers} are always \textbf{friendly} and the \textbf{food} is \textbf{great} . it is a \textbf{good mexican restaurant .} \\
%			\hline \bf{Generated summary of modified model} \\
%			\hline
%			it 's one of the \textbf{authentic mexican restaurant} in the area. the \textbf{food} is \textbf{great} . the \textbf{servers} are \textbf{very friendly} and \textbf{knowledgeable} . \textit{they took the order patiently . } the \textbf{chips} and \textbf{salsa} are \textbf{good} too. \textbf{it is huge and has patio seating .}  you can get a lot of food for the price you pay. 
%			\\
			\hline
		\end{tabular}
	\caption{A human-written summary of a Mexican
restaurant in Yelp. The words in bold denote opinion-aspect pairs, 
such as kind servers. The sentences in italics are implicit sentences having no 
opinion-aspect pairs. The words in gray denotes the noise of opinion-aspect pairs or implicit sentences of summary.
	}\label{tab:example}  
\end{table}

Consequently, recent work~\cite{Copycat20, Denoise20, 
Fewshot20, Plansum20} 
on abstractive opinion summarization focuses on 
creating synthetic training data, which consists of 
multi-reviews and summaries.
Typical approaches sample some reviews from a set of reviews
about an entity,  and treat them are summaries.
Such methods have showed to outperform
earlier auto-encoder models~\cite{MeanSum19} which requires no
synthetic data. 
Some of these methods~\cite{Copycat20,Fewshot20} randomly sample
a review as the output summary and take all other reviews of this 
entity as input.
Their downside is that the content of randomly sampled summary 
and its input may be completely irrelevant.  
Others create multi-review in terms of sampled summary. 
\citet{Denoise20} generates syntactically noisy versions or 
extract lexically similar reviews as input for sampled summaries. 
%They suppose that all reviews with overlapping vocabulary are similar to summary, 
The reviews that share the most words with the summary are 
more likely to be selected as input, which is quite different 
from the reality, 
since the overlapping of unimportant words between review 
and summary cannot express their semantic similarity.
%Thus, the existing synthetic datasets 
%are very different from the actual multi-review and summary pairs.
All of the above methods overlooks the fact
that the match between the multi-review and summary on 
\textit{salient information} is the key to creating 
a good synthetic pair.

By observing real-world reviews and summaries such as those in
\tabref{tab:example}, we realize the association between aspects and 
users opinions within a review are the salient information for 
opinion summarization. 
For example, restaurant reviews often contain important aspects
such as ``servers'' and ``food''. To that end, 
\citet{Plansum20} utilizes the rating of reviews 
to train a model to obtain aspect and sentiment probability distribution
for each review. 
They select reviews with the most similar aspect and 
sentiment distribution as output summaries for training.
The aspects and sentiments of selected reviews might be similar.
But, as shown in \tabref{tab:example},
not all reviews of actual multi-review have similar aspects and opinions.
As this method considers the aspects indirectly,
some aspects in the summary may not be found in the corresponding 
multi-review.
OpinionDigest~\cite{OpiDig20} directly 
extracts opinion-aspect pairs from each review
and use them as input and review as output
%the opinion-aspect pairs as input and review as output
in a self-training sequence-to-sequence model.
At testing, they cluster the extracted opinion-aspect pairs 
of multi-reviews and select the center pairs as input to 
generate a summary. This model suffers from the noise 
inherent to clustering and may ignore sentences that don't
come with opinion-aspect pairs in the first place.

%\KZ{Combined the following three paras into one
%and shorten it significantly. Focus on the motivation
%of your approach by observing the example, not talking
%about the details of the approach. Leave the details
%to the approach section. Only briefly mentioning the solution
%using a sentence or two.}
In this paper, we tackle the pitfalls of the previous
work by using semi-structured data instead of free text 
or only opinion-aspect pairs as training data. 
We create synthetic training data by 
sampling some reviews as summaries first.
The sampled summaries resemble 
the human-written summaries of multi-reviews in writing style.
As opinion and aspect are the most important for opinion summarization,
we directly extract opinion-aspect pairs (OAs) from summary
to generate input.
%and get input by them.
%extracted OAs.
As shown in \tabref{tab:example},
%In \tabref{tab:example},
%the aspects of summary can be found in multi-review.
the aspects in multi-review are the noisy version of the aspects in summary.
%We generate noisy version of OAs in summary by 
Therefore, we generate noisy OAs for summary by
extracting appropriate OAs from reviews under the 
same entity with summary,
%The extraction make sure that 
%each aspect of summary has OAs and 
which simulates the OAs in actual multi-review.


%We introduce a basic aspect guided model with noisy OAs as input and
%summary as output, which learns to select salient OAs from input and
%make sentence with selected OAs.

However, 
%this model always generates simple and general sentences, 
%such as ``it is a good mexican restaurant .''.
%The reason is that 
only using OAs as input can't make the model capture the specific information,
the sentences in italics in \tabref{tab:example}.
%During actual summarizaiton of multiple reviews,
During actual multi-review summarization,
the specific information in summary
%the sentences without OAs,
is summarized from some sentences without OAs in multi-review.
Thus, we collect the sentences that cannot extract OAs in summary as implicit sentences (ISs),
and generate the noisy version by sampling sentences from 
multi-review. 
%The synthetic dataset with OAs and ISs as input 
%points out the OAs which is the most salience information in opinion summarization.
%Meanwhile, ISs can help reducing the loss of information.




In order to capture generalized (opinion-aspect) information and specific (implicit sentences) information at same time,
we propose an aspect guided model with an OA encoder and
an IS encoder, which deals with OAs and ISs in parallel.
%Better results can be obtained by 
%training on basic model first and then fine-tuning on modified model.
%As shown in \tabref{tab:example}, 
%the summary generated by modified model is more informative.
%Because the pretrained basic model can provides
%the main aspects and its opinion for summary and
%the modified model help supplement specific information.
Different from taking filtered OAs as input, 
taking noisy OAs as input can
avoid inaccurate clustering
and enable model to learn denoising.
As we select noisy OAs and ISs from all reviews under an entity.
Compared with textual input,
noisy OAs and ISs help model attend to salient aspects and opinions,
and weaken the information inconsistency between synthetic multi-review and summary.
We do not simply concatenate noisy OAs and ISs as input since we need expand some important pairs in noisy OAs into sentences and
abstract noisy ISs.
%the noisy OAs need 
%but noisy ISs need abstraction. 
More details are shown in \secref{sec:results}.

In summary, our contributions are as follows:
\begin{enumerate}
\item We create a novel synthetic training dataset with 
semi-structured data, which combines noisy opinion-aspect pairs 
and noisy implicit sentences as input and
summary as output.
%\KZ{We need a better word for ``other sentence''
%so people can understand immediately.}
We show that this is more effective than relying completely on
text or opinion-aspect pairs in previous work. (\secref{sec:data})
%\item We propose aspect-variant guided models to deal with
%opinion-aspect pairs and other sentences. (\secref{sec:model}) 
\item We design an aspect guided model for semi-structured data,
which substantially outperforms the 
state-of-the-art (SOTA) methods on the Yelp and Amazon datasets. (\secref{sec:results}).
\end{enumerate}
