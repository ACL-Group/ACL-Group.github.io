\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[review]{emnlp2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}


\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.

\usepackage{natbib}
\usepackage{caption}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{multirow}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{url}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage[linesnumbered, boxed, ruled]{algorithm2e}
%\hypersetup{
%	colorlinks=true,
%	linkcolor=blue
%}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\triple}[1]{$\langle #1 \rangle$}
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{138} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Exploring and Exploiting Latent Commonsense Knowledge in Pretrained Masked Language Models}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Pretrained masked language models~(PLMs) 
were shown to be inheriting a considerable amount of relational knowledge 
from the source corpora. In this paper, we present an in-depth analysis concerning eliciting relational commonsense knowledge already present in PLMs from the perspective of network pruning. We show that it is possible to find subnetworks capable of representing grounded commonsense relations at non-trivial sparsity meanwhile being generalizable to downstream commonsense knowledge base completion and commonsense reasoning tasks. 
\end{abstract}

\input{intro}
\input{method}
\input{experiments}
\input{related}
\input{conclude}

\bibliography{emnlp2021}
\bibliographystyle{acl_natbib}

\end{document}
