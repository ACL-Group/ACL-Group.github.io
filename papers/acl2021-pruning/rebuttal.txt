Response to Review #1:
Thanks for your valuable review. Regarding your questions:
Q1: "...What does pruning really show..."
A1: The pruning technique learns parts of a PLM that are specicialized for a certain type of 
commonsense relation by keeping the relevant parameters and  reseting the rest of the paramenters. 
The resulting subnetwork can accurately model P(object|subject, relation) for a particular relation. 
This is shown in Section 3.1 line 420-425, 444-447, as well as in Figure 2. 
The main motivation of the paper is to show that PLM can be partitioned into these
relation-specific subnetworks, and not to improve the performance of any particular down-stream
tasks, although we have successfully shown that some of the down-stream tasks can indeed benefit from
the PLM pruning, as a by-product. In contrast, Chen et al.[1] prunes BERT to obtain a subnetwork that targets a specific application and not a relation like in our work, which is more general. 

References:
[1]. The Lottery Ticket Hypothesis for Pretrained BERT Networks. NIPS 2020.


Q2: "knowledge probing with cloze prompts" is not convincing...
A2: The cloze promts are written by human annotators in OMCS. Each prompt describes a 
commonsense relation that can be formulated as a subject-preditcate-object triple. The blank
is carefully chosen by human that is either the subject or the object of the triple.
We use cloze prompts here because they well unifies the human language model and 
structured knowledge.

Q3: "The paper is fraught with...":
A3:
1. "embody" means some MLM pretraining instances require commonsense knowledge of certain relations to correctly predict masked word.
2. "discrepancy" means the difference between MLM pretraining formulation P(word_masked|word_unmasked) and relational knowledge evaluation formulation P(object|subject, relation).
3. "special" means the uniqueness of a particular subnetwork for representing a specific commonsense relation.
We will correct some of the language uses according to your suggestion.

Q4: "no motivation of visualization of attetion weights...":
A4: Before pruning, the attention between words in the example cloze prompt shows no particular 
interesting pattern; after pruning, there is clearly strong attention between the subject and
the object words for the same example. This is a good evidence that our technique strengthens the 
PLM representation of the relation depicted in the prompt. 

Response to Review #2:
Thanks for your valuable review. Regarding your questions:
Q1: "the precision is zero for models finetuned on ..."
A1: First of all, all the P@1 scores of Table 1 are results of LAMA, which tests the model's understanding of structured knowledge, and it's not a downstream task. Second, fine-tuning BERT on downstream tasks is essentially adapting the pretrained representations to capture task-specific features, usually by heavily modifying the outer-most layers of BERT. It has been shown that large language models, when fine-tuned, are prone to exploiting superficial statistical cues (Naik et al.[1], Sanchez et al.[2], McCoy et al.[3]) in the dataset, e.g., trigger word or n-gram indicative for certain label, to achieve high performance without truly utilizing "knowledge". That is why BERT fine-tuned by CONLL and SQuAD suffers the most on the LAMA test, because it "forgets" some of its previously acquired knowledge. The same goes for fine-tuned RoBERTa. The precisions for RoBERTa fine-tuned on SST-2 and SQuADv1.0 are both near zero.
When we apply pruning of fine-tuned BERT, Table 1 shows that we can recover the representation of
some knowledge and the P@1 is improved to 27.1 and 22.5. But some of the knowledge maybe permanently
lost, so the precision is still much lower than BERT without finetuning at 57.6.

References:
[1]. Stress Test Evaluation for Natural Language Inference. COLING 2019.
[2]. Behavior Analysis of NLI models: Uncovering the Influence of Three Factors on Robustness. NAACL 2018.
[3]. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. ACL 2019.

Q2: "huge performance difference between finetuning original BERT vs finetuning pruned BERT"
A2: Please refer to the previous answer. Notice in Table 1, "BERT-BASE-FINETUNED-CONLL03 w/ deterministic pruning" means finetune BERT first and then do deterministic pruning, not the other way around.
This also applies to other fine-tune + pruning experiments. 

Response to Review #3:
Thanks for your valuable review. Regarding your questions:
Q1: "...the SQuAD dataset is also a subset of LAMA?"
A1: In this paper we only used the subset of LAMA exclusively targeting relational commonsense knowledge (line 331-335). It has nothing to do with SQuAD.

Q2: "...results of the pruning will hold even after comparing with fine-tuned models?"
A2: We think you are referring to Table 1, which only includes comparisons with BERT fine-tuned on CONLL03 and SQuAD. We agree that it is conceivable that BERT fine-tuned on commonsense related tasks
in Table 3 might strengthen the representation of commonsense knowledge and improve the
its performance on LAMA. But in reality, that's the case. Our experiments on BERT fine-tuned on
SWAG and aNLI shows that the P@1 on LAMA is only slightly better than zero, significantly lower
than 12.9. The reason is given in A1 to Review #2. With your permission we can add this discussion
into the revised version.

Q3: "What is the value of t in the deterministic pruning?"
A3: The pruning threshold value t is 0.5 for all models.

Q4: "unclear how a specific combination of these relations is useful for a task?"
A4: This framework provides a mask for each relation type. One can try to union the masks of a set of relation types to prune the model. Table 2 of Appendix shows the best combination of relation masks
for different PLMs and different tasks.
When all relations are used, the results on the downstream commonsense tasks deteriorate. This is
because most tasks use only a few relations. A model masked by all relations behaves toward the original network without pruning.
