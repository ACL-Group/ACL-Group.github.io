MetaReview
Comments: This paper presents a set of experiments in which pruning methods are used to identify (relatively sparse) subnetworks within pretrained MLMs that encode "commonsense" knowledge. While the practical use of this approach is not immediately clear, reviewers agreed that this is nonetheless an interesting direction and analysis, and the authors clarified key remaining questions about the work in their response. These clarifications should be integrated into future versions of the manuscript.

Review #1
The core review
This paper presents an in-depth analysis of the nature of relational knowledge elicited by pretrained language models. Along these lines, the authors posit that it is possible to find subnetworks at non-trivial sparsity that can generalize to downstream commonsense reasoning tasks.
Strengths:

Understanding the nature of relational knowledge present in pretrained language models is an important challenge and this paper tackles that by identifying a smaller sparse subnetwork

They validate this by deploying the pruned model to downstream tasks

Weaknesses:

It is hard to understand the motivation of the paper. What does pruning really show ? Presence of relation-only subnetwork ? If yes, the authors have not made an attempt to show that such networks do not capture any additional information. From an end-task perspective, are those the only "subnetworks" that can be used to enhance performance ? If yes, is the relational knowledge distributed differently than the authors claim ?

It is unclear how their network specifically targets "relational commonsense" knowledge. In particular, the current "knowledge probing with cloze prompts" is not sufficiently convincing that it is meant to capture relational knowledge in the first place

The paper is fraught with terms that are either vague or not defined

-- Line 97 - what does "embodies" mean ?

-- Line 89 - what does "discrepancy" mean ?

-- Line 415 - what does "special' mean ?

The authors don't adequately motivate why the visualization of attention weights is necessary to establish any analysis.
Reasons to Accept
The paper is not ready to be accepted for scholarly publication
Reasons to Reject
The paper is not motivated well, and currently lacks some foundational basis albeit having a good idea to understand the relationship between pruning and knowledge.
Overall Recommendation:	1.5

Review #2
The core review
This paper addresses the question of disentangling relational knowledge from pretrained language embeddings. They propose an end-to-end weakly supervised weights pruning method to search for subnetworks within pretrained language models in which relational knowledge is elicited. They show that the proposed method identifies sparse subnetworks which performs well on several downstream tasks. For pruning they propose two different methods - (1) stochastic pruning - where the first method a binary masking variable is sampled from a Bernoulli distribution (2) deterministic pruning - where a hard thresholding function is used.
Reasons to Accept
The paper provides new insights into how relational knowledge is captured in pretrained language models. Experiments are thorough. The authors show a huge performance difference between finetuning the original language model vs finetuning the pruned model.
Reasons to Reject
There is very little discussion about the practical implications of identifying these subnetworks. Apart from the understanding that they exist and can be finetuned for downstream tasks, there is no discussion on why there is such a huge performance difference between finetuning original BERT vs finetuning pruned BERT.
Overall Recommendation:	3.5
Questions for the Author(s)
In Table 1, the precision is zero for models finetuned on the original bert. Any reason why pure Bert finetuning would do so poorly? Also, how does finetuning RoBERTA (without pruning) perform for this task?

Review #3
The core review
In this paper, the authors probe into the common sense knowledge already present in the pre-trained masked language models by using weakly supervised weight pruning techniques. The idea is to find subnetworks within PLMs in which relational knowledge is better represented. The authors show that grounding on external relation schema successfully identifies sparse subnetworks. They specifically investigate two questions (a) if they can disentangle the pre-trained general-purpose knowledge representation into a relation-specific knowledge representation, (b) if the relation-specific knowledge can be used for downstream knowledge-intensive tasks.
Strengths:

The paper proposes novel pruning strategies to find subnetworks that take into account relational knowledge.

Empirical evaluation is conducted on multiple PLMs and tested on 7 common sense reasoning tasks.

Weakness:

As Petroni et. al 2019 and authors mention PLMs are parameterized to have a knowledge representation that is entangled in the shared parameter space. However, when fine-tuning the general knowledge representation is tuned towards a specific task. I wonder if the results presented by the pruning methods proposed would be significant after fine-tuning to the task.

Some of the experiments and results are unclear. Why BERT-BASE-FINETUNED-SQuAD does not perform on the LAMA task (0.0 for P@1) as the SQuAD dataset is also a subset of LAMA?

Reasons to Reject
It is understandable that subnetworks of PLMs are useful for representing knowledge. However, it is unclear their usefulness after fine-tuning PLMs for a task. A fine-tuned model would ideally capture the required common sense relational representation for the specific task.
Overall Recommendation:	3
Questions for the Author(s)
The pruned models definitely give a good starting point for a task. But do you think the results of the pruning will hold even after comparing with fine-tuned models? Arenâ€™t the PLMs supposed to capture general relation knowledge and the fine-tuning on specific tasks learns for the necessary relational representation for the model?
What is the value of t in the deterministic pruning?
It is mentioned that multiple types of knowledge are typically required to effectively reason over concepts for a particular task. It is unclear how a specific combination of these relations is useful for a task? What happens when all the relations are used?
