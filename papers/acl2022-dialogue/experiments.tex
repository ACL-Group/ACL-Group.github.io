\section{Experimental Setup}

\subsection{Datasets}
We implement our experiments on two dialogue summarization datasets.
\textbf{SAMSum}~\cite{gliwa2019samsum} is a human-written online dialogue dataset. Each dialogue has one summary\footnote{We remove a bad sample without the dialogue in the original dataset following previous works.}.
\textbf{DialSumm}~\cite{chen-etal-2021-dialogsum} contains spoken daily dialogues, having more formal style and diverse topics than SAMSum. Each dialogue is accompanied by three human-annotated summaries. More statistics of each dataset is listed in Table \ref{tab:sumdataset}. Compression ratio equals SW divided by DW, showing that summaries in DialSumm are more compressed than in SAMSum.

\begin{table}
	\small
	\centering
	\begin{tabular}{lrrrr}
		\toprule[1pt]
		\textbf{Datasets} & \textbf{Train/Val/Test} & \textbf{DW} & \textbf{SW} & \textbf{CR} \\
		\midrule[1pt]
		{SAMSum} & 14,731/818/819 & 124.10 & 23.44 & 0.25\\
		{DialSumm} & 12,460/500/500 & 187.52 & 31.02 & 0.18 \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Statistics of dialogue summarization datasets. DW, SW and CR represent the number of dialogue words, the number of summary words and compression ratio respectively.}
	\label{tab:sumdataset}
\end{table}

\subsection{Implementation Details}
We use BART\footnote{\url{https://huggingface.co/facebook/bart-large}} as our basic language model. For both post-training and fine-tuning, the speakers and utterances of each dialogue are concatenated into a single sequence and truncated to the first $1024$ tokens.
The learning rate is set to $3e-5$ with weight decay equaling $0.01$. The number of warmup steps is $500$ and dropout is $0.1$. The model is tested on the corresponding validation set after each training epoch and % It stops training when the Rouge-2 F1 score doesn't improve on the validation set or it reach the maximum training epoch. 
the early-stop is activated if there is no improvement on the Rouge-2 F1 score. 
The early-stop and maximum training epochs are set to $3$ and $10$.
During inference, i.e., validation and testing, 
the beam size is set to $4$ with length penalty equaling $1.0$ 
and no repeat n-gram size equaling $3$. 
The minimum and maximum lengths are set to the corresponding lengths 
of the reference summaries for each dataset, allowing for free-length text generation. 
All of our experiments are done on an RTX 2080Ti with 11G GPU 
memory. We run experiments three times and following~\cite{feng-etal-2021-language} show the best results. We will open-source all of the used datasets and codes.

%We also reimplement the baseline BART fine-tuning 
%directly with the above hyper-parameters on different datasets for 
%a fair comparison. 

%There is no difference on results between setting $b$ as a constant or the same as $a$. The number $4$ considers the special token in BART implementation and covers most length of names.

\subsection{Baselines}
\textbf{Lead-3} and \textbf{Longest-3} are simple rule-based baselines that extract the first or the longest $3$ utterances in a dialogue as the summary respectively. 
\textbf{PGN}~\cite{see2017get}, \textbf{Fast-Abs}~\cite{chen2018fast}, and \textbf{PEGASUS}~\cite{zhang2020pegasus} are well-known models for text summarization. \textbf{BART}~\cite{lewis2020bart} is a general PLM and performs well after fine-tuning.
\textbf{CODS}~\cite{wu-etal-2021-controllable}, \textbf{Multi-view}~\cite{chen2020multi} and \textbf{DialoBART}~\cite{feng-etal-2021-language} are representative state-of-the-art models specially designed for dialogue summarization.

\subsection{Evaluation Metrics}
Automatic evaluation metrics and human evaluation tasks are presented
as follows:
%\KZ{A bit too verbose below.}

\textbf{Automatic Evaluation:} We use Rouge-1,2,L~\cite{lin2004rouge} F1-scores as automatic evaluation metrics. Following \citet{feng-etal-2021-language}, we adopt the same Rouge evaluation tool\footnote{\url{https://pypi.org/project/py-rouge/}} and compute between reference summaries and generated summaries. For DialSumm, we use use maximum rouge scores among references for each sample.

\textbf{Human Evaluation:} 
We randomly selected 100 samples from SAMSum test set and hired three proficient English speakers to evaluate the summaries. 
Each original dialogue and its reference summary are shown with generated summaries in a random order simultaneously. Showing summaries from different approaches together helps humans do comparisons between them.
Following \citet{chen2020multi} and \citet{liu2021coreference}, each summary is scored on the scale of $[2, 0, -2]$. $2$ refers to a concise and informative summary which is applicable. $0$ refers to an acceptable summary with minor mistakes or missing important contents. $-2$ means the summary is poor with irrelevant information or doesn't make sense at all. The final scores are averaged among annotators.

We also evaluate generated summaries on 4 frequent error types. 
\textbf{Mis}sing means missing important contents compared with the reference.
\textbf{Red}undancy means repeated or unimportant contents. %with the reference.
\textbf{Cor}eference represents mismatches between person and actions. %caused by wrong coreference resolutions.
\textbf{Rea}soning means incorrect reasonings among utterances.
%Rea and Cor concentrate on comparisons to the dialogue, 
%and the rest two focus on comparisons to the reference. 
We determine the error for each case by majority voting, and count the total error number of each model.

