\section{Conclusion}
\label{sec:conclude}
In this work, we present a new automatic evaluation framework called ChatMatch. 
We first make the chatbots converse directly with each other. 
Then we use a three-level rule-based scoring framework to rank their performances which mimics the process of a double round-robin
tournament. 
%We have provided an example about how to implement this kind of automatic evaluation framework in \secref{sec:experiment}.
Our framework shows a good correlation with human judges, 
better than state-of-the-art automatic and semi-automatic chatbot evaluation frameworks.
Another remarkable advantage of our framework is that it's totally automatic and time-saving which costs 2 min 57 secs on average to get the final ranking results among 7 chatbots, much faster than manual evaluation. %, saving human labor.}
 We believe that this kind of automatic interactive evaluation framework opens up new opportunities for future research on 
dialogue system evaluation.
