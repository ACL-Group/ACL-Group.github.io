\section{Related Work}

Two major approaches are used for evaluating chatbots or dialogue systems. Some evaluate a single turn at a time by comparing the response with a ground truth utterance from a static script of real human dialogue or get a score by combining the response with the context. Others evaluate the interaction between human and bot or between bot and bot by some scoring metrics. We will present some typical systems from each of these approaches below and discuss their pros and cons. 
%systems for evaluating the quality of generated conversations are always based on static scripts. These evaluation systems score the chatbots according to the generated responses under fixed context. Another approach for evaluation dialogue systems, which is considered to be more reliable, is interactive dialogue evluation. Humans and chatbots can chat with each other direcly and then their dialogue logs are used for evaluation. Among these interactive evaluation systems, most of them rely on manual scoring, which is inefficient. Although the correlation between existing automatic evaluation systems and human judgement is considered to be weak~\citep{liu-etal-2016-evaluate}, it is still believed that an effective automatic metric is our first choice while evaluating the chatbots. 

%\KZ{In each of these sections, you need to say what are the pros and cons,
%in comparisons with our work!}

%\subsection{Evaluation Based on Static Scripts}

Most existing evaluation systems are based on static scripts. 
Traditional metrics such as BLEU~\citep{papineni-etal-2002-bleu}, ROUGE~\citep{lin-2004-rouge} 
and METEOR~\citep{banerjee-lavie-2005-meteor} are widely used 
for evaluating text generation systems.
%to measure the 
%ability of a chatbot to generate responses. 
%The advantage of them is they are automatic.
%In recent years,
More recently, 
automatic evaluation methods based on static scripts are gradually 
moving toward using pre-trained language models. 
\citet{pang-etal-2020-towards} uses GPT-2~\citep{gpt2} and
\citet{mehri-eskenazi-2020-usr} uses RoBERTa~\citep{roberta} to 
automatically evaluate the generation in an unsupervised way with
a higher correlation to human evaluation
than traditional ones. 
However, these static evaluation metrics which need fixed contexts are not 
flexible enough to assess chatbot's ability, in need of adapting to dynamic and changing
contexts. We argue that interactive systems like ChatMatch
are more promising as they test chatbots in a real conversation mode. 

%More recently, \citet{sato-etal-2020-evaluating} 
%constructs high-quality negative samples manually in the test set and 
%trains the model to select the appropriate response. 
%\citet{yuma-etal-2020-ubleu} labels one to many responses, 
%and considers multiple responses in the evaluation. 
%Their model collects different reference answers from a large dialogue corpus 
%and then annotates quality of the answers using a neural network 
%trained by the automatically collected training data. 
%\citet{mesgar-etal-2020-dialogue} proposes an evaluation method 
%called dialogue coherence, which takes the prediction task of dialogue as a subtask and uses multi-task learning to model dialogue coherence. 
%\citet{huang-etal-2020-grade} uses dialogue graphs to automatically 
%evaluate the coherence of the dialogue. 

%\subsection{Interactive Evaluation}
Interactive evaluation systems attract increasing attention lately. 
%However, most of them require human engagement. 
%\citet{sedoc-ungar-2020-item} introduces item response theory (IRT) 
%into chatbot evaluation. Annotators judge which system makes better
%responses by making some comparison between two bots directly. 
\citet{DBLP:journals/corr/abs-1906-09308} and 
\citet{deriu-cieliebak-2019-towards} use dialogues between a bot and itself, 
which is called self-talk, to evaluate the bot in a more automatic manner. 
But it often leads to a lot of repeated chat context.
\citet{deriu-etal-2020-spot} designed \textit{Spot The Bot}, 
a framework that enables a group of bots to chat with each other and 
then asks humans to annotate if the bots talk more like a human or more like a
bot. Prior to our work, there was still no interactive and automatic 
evaluation framework that works without any participation of 
human annotators. 
Our work fills this gap and moreover, it's very flexible since 
more complex metrics or algorithms can be plugged in as 
scoring functions.  Our results also show that there is a strong 
correlation between automatic evaluation results and human judgments.
