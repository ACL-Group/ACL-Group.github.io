ChatMatch: Evaluating Chatbots by Autonomous Chat Tournaments 
Anonymous
17 Nov 2021 (modified: 03 Dec 2021)ACL ARR 2021 November Blind SubmissionReaders: November, Senior Area Chairs, Area Chairs, Reviewers, Paper2376 Authors
Abstract: Existing automatic evaluation systems of chatbots mostly rely on static chat scripts as ground truth, which is hard to obtain, and require access to the models of the bots as a form of “white-box testing”. Interactive evaluation mitigates this problem but requires human involvement. In our work, we propose an interactive chatbot evaluation framework in which chatbots compete with each other like in a sport tournament, using ﬂexible scoring metrics. This framework can efﬁciently rank chatbots independently from their model architectures and the domains in which they are trained for.
Software:   zip
Data:   zip
Revealed to Ruolan Yang, Zitong Li, Haifeng Tang, Kenny Q. Zhu
16 Nov 2021 (modified: 16 Nov 2021)ACL ARR 2021 November Submission
Authors:Ruolan Yang, Zitong Li, Haifeng Tang, Kenny Q. Zhu
TL;DR: An interactive chatbot evaluation framework in which chatbots compete with each other like in a tournament.
Preprint: no
Preferred Venue: ACL 2022 
Consent: yes
Consent To Review: yes
Add
Reply Type:all
Author:everybody
Visible To:all readers
Hidden From:nobody
3 Replies
[–]
Official Review of Paper2376 by Reviewer VLY4 
ACL ARR 2021 November Paper2376 Reviewer VLY4
28 Dec 2021 (modified: 28 Dec 2021)ACL ARR 2021 November Paper2376 Official ReviewReaders: Program Chairs, Paper2376 Senior Area Chairs, Paper2376 Area Chairs, Reviewers, Paper2376 Authors
Paper Summary:
The authors propose a novel evaluation method for chatbot systems. They create a tournament system where pairs of chabots converse with each other. Each chatbot is graded on several automatically computed metrics, and the chatbot that gets the higher score wins the round and moves on to the next round of competition. In this way, a ranking can be formed of all of the chatbot "participants" in the competition.

Summary Of Strengths:
The proposed approach can be used to build an overall ranking of many participating chatbot systems, which is something that is difficult to do with other evaluation techniques. The tournament idea is simple enough that it could be implemented without too much hassle. While the authors propose a set of seven dimensions to score chatbots on, this list could easily be extended or modified in the future as better or more task-specific metrics are developed.

Summary Of Weaknesses:
If changes are made to the scoring method, inference would need to be re-run for many of the matches (since different bots might win in the initial matches). Similarly, if one wanted to rank an additional newly developed system, the tournament would need to be re-run from scratch.

On a stylistic note, I found the paper's presentation of its results to be hard to follow. In particular, the over-use of acronyms made it difficult to parse many sections, and I think the tables and figures need improvement. I've included more details below.

Finally, the paper does not mention any plans to open-source any of its implementations. The proposed evaluation scheme would have much greater impact if there were plans for a code framework allowing people to easily rank their chatbot system against standard systems. At minimum, runnable implementations of Algorithms 1-3 would be nice to have.

Comments, Suggestions And Typos:
Questions and Comments

Section 2.2.1: What set of text did you compute Distinct-k over? Did you compute it over all of the words generated by a bot over the course of its conversation in the match? Or did you compute it at the utterance evel?

Section 2.2.3: As someone not familiar with tournament ranking schemes, I would like to see this section expanded a bit to explain in more detail how TrueSkill works.

In Section 3.3, I am confused about how human evaluation was conducted. Were humans annotators actively chatting with the bot, or were they observing and rating conversations between two bots? How is Kendall ranking correlation being used to measure inter-annotator agreement? I think this section needs to be expanded with more details.

Section 4.4.1 notes that evaluating relevance using human-bot dialogs is difficult because humans often deliberately switch topics. I wonder if this issue can be resolved by giving more specific instructions to the human participants.

Table 5: I find this table to be fairly meaningless. The evaluation time for bot inference is hugely dependent on the size of the neural network, the number/type of accelerators available, and other properties of the machine not mentioned in the paper. Also, for the static scripts, are you including the time it took to generate the conversations or are you only including the time it took to evaluate extant conversations?

In Figure 1, it looks like the human has shorter messages than any of the bots. The caption for this figure could direct users to Section 4.5 which discusses this difference in more detail. I also think the discussion in Section 4.5 could be expanded. You say that sometimes conversations between bots carry even more variety than between human and bot. However, the numbers in Table 11 don't really back this. Maybe you could back this statement by counting number of proper nouns used, or some other measure of "variety."

A general comment is that your Figures and Tables don't tend to be informative standalone. The captions should be improved so that readers can grasp the main message of the figure/table without needing to peruse the text. For example, Table 4's caption should reiterate what  is; Table 6 should explain what these results tell the reader about the generalizability of the method; and Figure 3 should explain what the reader is supposed to interpret from these rankings.

Style and Typos

Line 58: "Above all, we want to emphasize the significance of the observation on direct interactions between bots in the evaluation." I don't understand what this sentence is trying to say.

Line 458: "setting" should be plural.

The Results section uses entirely too many short acronyms, and some of them seem unnecessary. I think one of the main difficulties was that both the evaluation methods (CM, TA, BS, etc.) and the systems (BB, PL, CS, etc.) used very similar-looking acronyms so it was hard to remember which acronyms corresponded to which. I think you could improve clarity by using more descriptive names, such as "BertScore" instead of BS, "TokenAcc" instead of TA, etc.

For Tables 4 and 8, it would be easier to read if you instead displayed the transpose of the current tables, and then wrote out in full words the names of the dimensions (i.e., "Fluency" instead of "Flu")

Figure 3 should be a bar graph not a line graph. There is no natural ranking between the methods; i.e., there is no meaning to the point halfway between BB and PL.

Something weird is going on with the Figure 4 x-axis. There is the same amount of distance between 5 and 10 as there is between 100 and 150.

Nitpick: For sentences that end with quotations, such as "white-box testing" in the abstract, grammar rules dictate that the period be placed inside the final quote, not outside of it.

Many tables and figures are not located on the page on which they are introduced (Table 1, Table 7, etc.). This necessitated a lot of page flipping to understand what's going on.

Overall Assessment: 2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper2376 by Reviewer HS9b 
ACL ARR 2021 November Paper2376 Reviewer HS9b
27 Dec 2021 (modified: 27 Dec 2021)ACL ARR 2021 November Paper2376 Official ReviewReaders: Program Chairs, Paper2376 Senior Area Chairs, Paper2376 Area Chairs, Reviewers, Paper2376 Authors
Paper Summary:
This paper proposes to evaluate chatbots by using a tournament in which chatbots talk to each other and are scored from several aspects for ranking. Experimental results show that the proposed method (ChatMatch, CM) can evaluate the ranking of systems more appropriately and cost-efficiently when compared to previous methods, such as by using static metrics and the “spot the bot” method.

Summary Of Strengths:
The idea of using a tournament for chatbot ranking is novel.
The paper is well-written and is an easy read.
Experiments are sufficiently done although the number of judges (four) for creating the ground truth may be small.
The method can greatly reduce the cost of evaluating chatbots and will probably be used by researchers in the field.
Summary Of Weaknesses:
The algorithms for scoring diversity, consistency, and relevance do not seem very novel.
The systems used for ranking may be somewhat old, considering that there are now many more sophisticated deep-learning based chatbots such as Meena and Blender.
The use of only DailyDialog for evaluating systems may not be appropriate. Other dialogue datasets could be used together for a more holistic evaluation.
I wonder if the choice of the seven metrics for creating the ground truth is favoring the proposed method.
The number of judges for creating the ground truth is small.
The method can only be used for ranking, which may limit the application of the method.
Comments, Suggestions And Typos:
Algorithms 1 and 2 may need R(t) ← 0 and C(t) ←0 at the beginning.
Typo: R(t)) in Algorithm 1.
Sec 4.1 Figure 4 depicts ⇒ Figure 3 depicts
I don’t think a line graph is appropriate for Figure 3.
Overall Assessment: 4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.
Confidence: 5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.
Best Paper: Maybe
Best Paper Justification:
The approach seems novel and useful and will be used in the community.

Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 3 = Potentially useful: Someone might find the new datasets useful for their work.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper2376 by Reviewer AaHi 
ACL ARR 2021 November Paper2376 Reviewer AaHi
23 Dec 2021ACL ARR 2021 November Paper2376 Official ReviewReaders: Program Chairs, Paper2376 Senior Area Chairs, Paper2376 Area Chairs, Reviewers, Paper2376 Authors
Paper Summary:
This paper introduces ChatMatch, an evaluation system for chatbots which compete against one another in a tournament. This system allows very fast evaluation of chatbots according to seven dimensions. A tournament was conducted involving 7 bots and the results of the evaluations were compared with human judges. It was found that the scores given by the evaluation were close to human judges and that the time to evaluate was naturally much faster. Analysis of other factors such as the generalizability of the system, the metrics and the starting utterance is also considered.

Summary Of Strengths:
The results seem quite robust. Of particular interest is that the automatic evaluations are fairly similar to real human judges. Although this is just bot-bot chat it seems to hold up quite well.
This paper is well written.
Methodology is robust. I was happy that the authors decided to evaluate the effect of each individual metric.
Analysis is broad. The authors investigated many factors which I think was important.
Good use of example dialogues.
Summary Of Weaknesses:
No real major problems, the ability of the system to evaluate long distance concepts may be an issue.

Comments, Suggestions And Typos:
Do the authors feel it might be possible to game the evaluation system (i.e. find a method which will reliably get good scores even though the conversation responses are not good)?
Overall Assessment: 4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: Maybe
Best Paper Justification:
I feel this paper is quite novel and for automatic evaluation it is useful. Even though the experiment is between bots and not humans the results are quite interesting, particularly the conversations.

Replicability: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
