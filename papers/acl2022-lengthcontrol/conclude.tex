\section{Conclusion}
\label{sec:conclude}
We present a novel approach to produce 
summaries in desired length that are fluent and coherent. 
This approach pretrains a transformer seq2seq model
whose cross attention between
input and output are re-normalized accordingly to the length requirement.
The pretraining is done over synthetic summarization data extracted from
the original training set but with summary lengths evenly distributed.
Our results show that the framework achieves a good balance between
information selection from input documents and length control when
producing summaries.
%Compared with the existing
%length-controllable summarization methods, we show that our model has the ability to generate high-quality summaries with desired length by information selection. Our approach can also generate better short summaries,
%when there are no short reference summaries in the training sets.
