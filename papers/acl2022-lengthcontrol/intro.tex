\section{Introduction}
\label{sec:intro}
Abstractive summarization~\cite{NallapatiZSGX16,SeeLM17,CelikyilmazBHC18,UniLM19,BART19,setlevel21,GSum21} aims at reproducing the semantics and 
topics of the original text in a concise and fluent summary by paraphrasing.
In order to display the summary on different mobile devices 
or websites with space limitations, we have to produce summaries in different lengths.
Length-controllable summarization
is a multi-objective optimization problem, including generating complete summaries within desired lengths and selecting proper information to summarize based on desired lengths.
The existing length-controllable summarization based on encoder-decoder models
can be divided into two categories:
(1) \textit{early-stop during decoding} and (2) \textit{information selection 
before encoding}.

\begin{table}[th!]
	\centering
	\scriptsize
	\begin{tabular}{|c|p{4cm}|}
		\hline 
		\multicolumn{2}{|c|}{\bf Source Document} \\
		\hline
		\multicolumn{2}{|c|}{\tabincell{l}{... iranians erupted in celebration as young people waved flags from their \\ sunroofs , blasted music from stereos and chatted online with the hashtag \\$\#$irantalks . the excitement came after a breakthrough nuclear deal with \\the united states and other world powers ... }} \\
		\hline
		\hline 
	    \bf Length & \bf Reference Summary\\
		\hline 
		10 & \tabincell{l}{iranians celebrate the deal online and in the streets . } \\
		\hline 
		30&
		\tabincell{l}{after a breakthrough nuclear agreement deal with the united \\states and other world powers , celebration broke out in \\ iranians . young people waved flages and chatted online .}\\
		\hline 
	\end{tabular}
	\caption{\label{tab:intro} The reference summaries of one source document with lengths as 10 and 30. }
\end{table}



{\em Early-stop during decoding} methods~\cite{KikuchiNSTO16,LiuLZ18,GOLC19,lenatten21} 
focus on when to output {\em eos} (end of sequence), indicating the end of the summary.
An ad-hoc method~\cite{RushCW15} generates the {\em eos} by assigning a score of 
$-\infty$ to all candidate words at the position of the desired length during test. 
Ad-hoc can be applied to any seq2seq model.
Others learn the relationship between length and the decoder state at training time.
However, these methods simply add length requirements to the decoder
and ignore the fact that encoding the content, or the information selection,
from the source document must also adapt to different length requirements.
\tabref{tab:intro} gives an example.
The content of the reference summary with 10 tokens is the celebration of iranians.
The reference summary with 30 tokens contains the reason for the celebration.
Some generated summaries with short desired lengths are likely to 
be incomplete, 
similar to the truncated version of summaries generated by models without length constraints.
The summaries of ad-hoc and LenAtten in \tabref{tab:genintro} are not complete
and lose the information about ``deal''.

\begin{table}[ht!]
	\centering
	\scriptsize
	\begin{tabular}{|p{7.2cm}|}
		\hline
		\bf Generated Summaries (Desired Length$=$10)\\
		\hline
		\bf BART~\cite{BART19} + Ad-hoc~\cite{RushCW15} (10 tokens) \\
		\hline 
		iranians erupted in celebration as young people waved flags from \\
		\hline 
		\bf LenAtten~\cite{lenatten21} (12 tokens) \\
		\hline  
		the agreement on the final day of persian new year festivities , \\
		\hline 
		\bf  LPAS~\cite{Proto20} (22 tokens) \\
		\hline 
		\tabincell{l}{iranians erupted in celebration . the excitement came after a breakthrough \\ nuclear deal with the united states and other world powers .}
		\\
		\hline
	\end{tabular}
	\caption{\label{tab:genintro} The summaries generated by
		different models. }
\end{table}

Methods based on {\em information selection} 
are {\em two-stage} methods~\cite{SeeLM17,Compress20,Proto20}. 
One prominent example is LPAS~\cite{Proto20}, which in the first stage, 
extracts top $l$ most important tokens from the source document as 
a prototype summary where $l$ is the desired length, and in the second
stage encodes the original source document and prototype summary by 
a dual-encoder. 
On the one hand, such two-stage approaches suffer from noises introduced in the intermediate results.
On the other hand, the second stage of these methods does not have first-hand length information, which weakens the length control.
\tabref{tab:genintro} shows that LPAS contains redundant information about 
``deal'' and its length is much longer than the reference summary.


In this paper,
we propose a {\bf length-aware attention mechanism} (LAAM) 
which extends a transformer seq2seq model with the ability to 
select information in the context according to the length constraint. 
LAAM re-normalizes the attention between encoder and decoder to boost the tokens with higher attention scores based on the desired length, helping with selecting length-aware information from source document.
The number of boosted tokens decreases step by step until $eos$ gets 
the highest attention score, which is helpful in stopping the decoding process 
at desired length.
LAAM can be thought of as a hybrid approach between the two types of previous approaches.

We observe that there is a big difference in the number of summaries 
within different length ranges in the original training set in any
summarization dataset. 
The shorter reference summaries are especially rare.
As shown in \tabref{tab:intro},
given a short desired length, the summaries of
the previous methods and LAAM still select redundant information.
To balance the distribution of summaries in different length ranges, we 
propose a heuristics to create a length-balanced dataset (LBD)
by pre-predefining the length ranges and constructing extractive summaries within different length ranges, which helps model to select different information from source document via desired lengths. 

In our approach, we can create an LBD from original summarization dataset. 
We first train 
LAAM on such LBD to enhance the ability of 
LAAM on information selection with length constraints. 
Then we fine-tune the pretrained LAAM on
original dataset to learn to paraphrase
the selected information as abstractive summaries in different lengths.
The task of generating short summaries by the models fine-tuned on datasets without short reference summaries can be seen as a {\em zero-shot} problem.
Benefiting from the pretraining with LBD, our approach can solve the zero-shot length control problem.

Our contributions are as follows:

\begin{enumerate}
\item We propose a new length-aware attention mechanism (LAAM) to generate high-quality summaries with desired length. %(\cref{sec:model})
LAAM outperforms the state-of-the-art length-controllable methods 
on CNN/Daily Mail and XSUM in terms of ROUGE scores, length variance and human evaluation (\tabref{tab:genall}).
\item We design a heuristics to create a length-balanced dataset (LBD) from original dataset. After pretraining LAAM on LBD, the pretrained LAAM performs better than LAAM and can effectively solve the zero-shot length control problem
(\tabref{tab:zero}).
\end{enumerate}

