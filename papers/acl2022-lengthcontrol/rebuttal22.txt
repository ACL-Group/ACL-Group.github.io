Thank you very much for the reviewers' comments and suggestions. 
And thank you, senior area chair, for reading our responses.

The suggested revisions from the meta-review by Area Chair RKh5 can be easily addressed:
1. `` human evaluation from a detailed perspective'': The human evaluation in Table 6 documents the overall quality scores which are aggregated from informativeness and readability scores (see para 3 of Sec. 3.4). Readability actually covers the fluency and grammaticality. We will include these detailed scores in the final version. 

2.``More explanations and analyses'': 
-Reviewer zrt9
``evaluate model on dataset with longer summaries, such as ArXiv'':
The main objective of controlling summary length is to generate shorter
summaries. Testing our framework on ArXiv which features long summaries
is not consistent with our goal. However, we can add this dataset as a reference
to the final version.

-Reviewer emEy
1) ``Attn_{is} and Attn_{eos} are the same'':
This is not true. They are different and serve different purposes (see Eq (4) and (6)). 
2) ``why it improves the generation quality under soft length control'':
This is because, as shown in Table 8, our method, with the help of Attn_{is}, 
can select more proper content to summarize by given a desired length. 
We showed a case study in the appendix to explain why our method works in detail,
which will be added to Sec 3.5 of the final version.
3) ``no ablation study on LBD'':
This is not true. We did ablation study on LBD in the subsection of Sec. 3.5 (Last para of Page 7).

-Reviewer qBWE
``Add BertScore''
This is a good suggestion and it can be done easily. It will not change our conclusion because our human evaluation in Table 6, which is a stronger metric than BertScore, already shows the advantage of our approach.

3.``Why BART's performance is lower than reported'': 
We explained BART's performance in the footnote in Page 6 of our paper.

