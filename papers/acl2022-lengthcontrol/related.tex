\section{Related Work}
\label{sec:related}

Previously, most length-controllable approaches in abstractive summarization
focused on stoping decoding at a particular time.
Ad-hoc~\cite{RushCW15} generated the $eos$ token by 
assigning a score of -$\infty$ to the tokens in vocabulary and 
generated a fixed number of words. 
LenEmb and LenInit~\cite{KikuchiNSTO16} input length embeddings to decoder respectively.
\citet{RLLC19} took LenEmb and LenInit as an agent and adjusted the reward incorporating with the desired length.
LC~\cite{LiuLZ18} added the desired length into the first layer of CNN encoder.
GOLC~\cite{GOLC19} optimized LenEmb and LC by formalizing loss with an overlength penalty.
\citet{FanGA18} predefined some special markers to denote different length ranges and prepended the input with such markers during training and testing. 
\citet{pos19} extended the sinusoidal positional encoding~\cite{attn17} to take account of stepwise remaining length.
LenAtten~\cite{lenatten21} added a length attention unit to exploit proper length information based on the stepwise remaining length. 

Other length-controllable approaches decided the content to be summarized by length-aware intermediate summaries.
LPAS~\cite{Proto20} extracted a word sequence with the desired length from source document and generated summary by a non-length-controllable model with document and extracted summary as input.
MLS~\cite{Compress20} generated a general summary
and then input it to a length-controllable model.

Compared with previous methods, 
our approach can effectively control the length of generated summaries
by pretraining the length-controllable information selection model on length-balanced dataset. 
Meanwhile, it can generate summaries with length approximate
to the desired length in zero-shot controlling length problem. 

Recently, the approaches fine-tune the pretrained transformer seq2seq models~\cite{BART19,PEGASUS20,GSum21,SimCLS} on summarization datasets. They achieve outstanding performances on summarization tasks. 
Our approach is applied to transformer seq2seq model, which is orthogonal to above pretrained transformer models and can be added to them. 




