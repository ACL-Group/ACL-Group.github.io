Length Control in Abstractive Summarization by Pretraining Information Selection 
Anonymous
17 Nov 2021 (modified: 03 Dec 2021)ACL ARR 2021 November Blind SubmissionReaders: November, Senior Area Chairs, Area Chairs, Reviewers, Paper356 Authors
Abstract: Previous length-controllable summarization models mostly control lengths at the decoding stage, whereas the encoding or the selection of information from the source document is not sensitive to the designed length. They also tend to generate summaries as long as those in the training data. In this paper, we propose a length-aware attention mechanism (LAAM) to adapt the encoding of the source based on desired length. Our approach works by training LAAM on a summary length balanced dataset built from the original training data, and then fine-tuning as usual. Results show that this approach is effective in generating high-quality summaries with desired lengths and even those lengths never seen in the original training set.
Software:   zip
Data:   zip
Revealed to Yizhu Liu, Qi Jia, Kenny Q. Zhu
13 Nov 2021 (modified: 16 Nov 2021)ACL ARR 2021 November Submission
Authors:Yizhu Liu, Qi Jia, Kenny Q. Zhu
Preprint: no
Preferred Venue: ACL 2022 
Consent: yes
Consent To Review: yes
Add
Reply Type:all
Author:everybody
Visible To:all readers
Hidden From:nobody
Metareview:
This paper proposes a method for length-controlled abstractive summarization. Unlike the previous work to control the length of the generated summaries at the decoding stage, this paper proposes a length-aware attention mechanism (LAAM) that modifies the weights in cross-attention. They also propose a way to collect the length balanced dataset and pre-train their LAAM in this collected dataset to further improve the results. This paper is well written and easy to follow. The method proposed is novel and interesting. All the reviewers agree that this paper has the quality to be published in a *ACL venue.

Summary Of Reasons To Publish:
This paper proposes a method for length-controlled abstractive summarization. The proposed method is simple but novel and is quite different from previous methods that control length at the decoding stage, providing a new idea of controlling generated sentence length. The paper is well organized and easy to follow. However, there are still some concerns that need to be addressed in the experimental results.

Summary Of Suggested Revisions:
It would be useful to conduct human evaluation from a detailed perspective, such as whether the summary was fluent, informative, and grammatically correct.
More explanations and analyses can be helpful to explain why this method works and why it can improve the generation quality.
Explain why the BART's performance reported in this paper is lower than those reported in the original paper.
Overall Assessment: 4 = There are minor points that may be revised

3 Replies
[–]
Official Review of Paper356 by Reviewer zrt9 
ACL ARR 2021 November Paper356 Reviewer zrt9
26 Dec 2021 (modified: 27 Dec 2021)ACL ARR 2021 November Paper356 Official ReviewReaders: Program Chairs, Paper356 Senior Area Chairs, Paper356 Area Chairs, Reviewers, Paper356 Authors
Paper Summary:
This paper proposes a length-aware attention mechanism (LAAM) to adapt the encoding of the source based on desired length. It is different from existing methods that the proposed approach achieves high ROUGE scores and lower variance by training LAAM on a summary length balanced dataset built from the original training data.

Summary Of Strengths:
The proposed method is simple and effective.
The paper is well organized and well written.
Summary Of Weaknesses:
The experiments seem a little weak to support the effectiveness of the proposed method.
Comments, Suggestions And Typos:
It would be better to evaluate the model on additional datasets other than CNN/DM and XSUM including longer summaries such as ArXiv. Also, it would be nice to evaluate it for longer summaries in the zero-shot summary experiment.
It would be useful to conduct human evaluation from a detailed perspective, such as whether the summary was fluent, informative, grammatically correct.
Overall Assessment: 3.5 
Confidence: 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.
Best Paper: No
Replicability: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 1 = No usable datasets submitted.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper356 by Reviewer emEy 
ACL ARR 2021 November Paper356 Reviewer emEy
22 Dec 2021 (modified: 22 Dec 2021)ACL ARR 2021 November Paper356 Official ReviewReaders: Program Chairs, Paper356 Senior Area Chairs, Paper356 Area Chairs, Reviewers, Paper356 Authors
Paper Summary:
This paper targets on length-control abstractive summarization. The authors proposed a length-aware attention mechanism to improve the generation quality. Experimental results show that the method can generate quality summaries with desired lengths. This work also demonstrates a method to create length-balanced dataset.

Summary Of Strengths:
Different from LPAS, this method does not need a two-stage generation, without the possible negative impacts from intermediate generation. The proposed LAAM is simple and effective. It uses the control on attention scores to help generate desired-length summaries.
It points out the length distribution and demonstrates some heuristic ways to generate length-balanced dataset, which might be crucial to this task.
Summary Of Weaknesses:
The method seems more like a trick to control the attention weights related to the desired length. It should be admitted that it is lacking in novelty.
More importantly, the authors should clarify why this method works and why it can improve the generation quality. A comprehensive analysis is required.
Comments, Suggestions And Typos:
The key of the method is making the attention weights correlated with the desired length. It is easy to think of Attn_{eos}, which encourages the score of <eos> gradually until the end of decoding. However, how to understand Attn_{is} is still a bit confusing. I believe that it is not necessary to point out the two things, and they are actually the same.
More importantly, it is necessary to point out why it improves the generation quality significantly under the condition of soft length control. Following the idea of soft length control, this method should not be that effective in this scenario. By the way, I also have some questions about the results: why is the BART performance much lower than those reported in the original paper? Are the results reproduced by the authors?
LBD creation seems interesting. However, there should be an ablation study on the pretraining on this dataset. From the paper, I cannot be sure whether pretraining on LBD or continue pretraining itself works.
Overall Assessment: 3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 3 = Potentially useful: Someone might find the new datasets useful for their work.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper356 by Reviewer qBWE 
ACL ARR 2021 November Paper356 Reviewer qBWE
18 Dec 2021ACL ARR 2021 November Paper356 Official ReviewReaders: Program Chairs, Paper356 Senior Area Chairs, Paper356 Area Chairs, Reviewers, Paper356 Authors
Paper Summary:
In this paper, the authors introduce a summarization system capable of producing abstractive summaries of a desired length. Training is done in two stages, first by pretraining on buckets of desired summary length. Buckets are constructed by obtaining extractive oracle summaries of different lengths and adding a source document into the bucket the length of its oracle summary falls into. Second, the model is trained to rephrase the extractive oracle of a document into its reference summary. Extensive experiments and analysis show that the proposed model is better than strong baselines at producing summaries with the length within a desired range.

Summary Of Strengths:
Training explicitly divides information selection from generation which enables the model to learn extract relevant content under a budget constraint.
Generation is trained such that the conditioned context is limited to only the most relevant sentences extracted in the previous step. This could mitigates hallucinations by the decoder but no experiment on that was presented.
Summary Of Weaknesses:
The analysis could benefit from metrics other than Rouge, that relies on lexical overlap. Metrics such as BertScore or Meteor would give a more complete picture of what is going on.
Comments, Suggestions And Typos:
This paper is well-written, the experiments are sound and well executed.

Very minor comments about evaluation. It would be interesting to see how factuality, content selection (as measured by a QA experiment), and grammaticality vary among buckets. I imagine all models would struggle to put right information under shorter budgets and still keep the summary grammatical. Please check [1,2,3] for option on automatic metrics proposed, some of them are restricted to the datasets in your experiments.

Please consider submitting your data buckets to GEM as a challenge set. It would be extremely valuable to have your data for length-control comparisons, and I believe GEM can provide the right exposure.

[1] Intrinsic Evaluation of Summarization Datasets [2] Summeval: Re-evaluating summarization evaluation [3] Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies

Overall Assessment: 5 = Top-Notch: This paper has great merit, and easily warrants acceptance in a *ACL top-tier venue.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 5 = Enabling: The newly released datasets should affect other people's choice of research or development projects to undertake.
Software: 5 = Enabling: The newly released software should affect other people's choice of research or development projects to undertake.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
