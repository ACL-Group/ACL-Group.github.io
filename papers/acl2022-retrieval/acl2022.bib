@article{Transformer,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{uniter,
  author    = {Yen{-}Chun Chen and
               Linjie Li and
               Licheng Yu and
               Ahmed El Kholy and
               Faisal Ahmed and
               Zhe Gan and
               Yu Cheng and
               Jingjing Liu},
  title     = {{UNITER:} Learning UNiversal Image-TExt Representations},
  journal   = {CoRR},
  volume    = {abs/1909.11740},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.11740},
  eprinttype = {arXiv},
  eprint    = {1909.11740},
  timestamp = {Sat, 23 Jan 2021 01:12:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11740.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{oscar,
  author    = {Xiujun Li and
               Xi Yin and
               Chunyuan Li and
               Pengchuan Zhang and
               Xiaowei Hu and
               Lei Zhang and
               Lijuan Wang and
               Houdong Hu and
               Li Dong and
               Furu Wei and
               Yejin Choi and
               Jianfeng Gao},
  title     = {Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks},
  journal   = {CoRR},
  volume    = {abs/2004.06165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.06165},
  eprinttype = {arXiv},
  eprint    = {2004.06165},
  timestamp = {Fri, 21 May 2021 15:45:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-06165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{clip,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               Sandhini Agarwal and
               Girish Sastry and
               Amanda Askell and
               Pamela Mishkin and
               Jack Clark and
               Gretchen Krueger and
               Ilya Sutskever},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  journal   = {CoRR},
  volume    = {abs/2103.00020},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00020},
  eprinttype = {arXiv},
  eprint    = {2103.00020},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vqa,
	author    = {Stanislaw Antol and
	Aishwarya Agrawal and
	Jiasen Lu and
	Margaret Mitchell and
	Dhruv Batra and
	C. Lawrence Zitnick and
	Devi Parikh},
	title     = {{VQA:} Visual Question Answering},
	journal   = {CoRR},
	volume    = {abs/1505.00468},
	year      = {2015},
	url       = {http://arxiv.org/abs/1505.00468},
	eprinttype = {arXiv},
	eprint    = {1505.00468},
	timestamp = {Mon, 13 Aug 2018 16:48:30 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/AntolALMBZP15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nlvr,
	title = "A Corpus for Reasoning about Natural Language Grounded in Photographs",
	author = "Suhr, Alane  and
	Zhou, Stephanie  and
	Zhang, Ally  and
	Zhang, Iris  and
	Bai, Huajun  and
	Artzi, Yoav",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P19-1644",
	doi = "10.18653/v1/P19-1644",
	pages = "6418--6428",
	abstract = "We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",
}

@inproceedings{fasterrcnn,
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
	url = {https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf},
	volume = {28},
	year = {2015}
}

@article{faiss,
	title={Billion-scale similarity search with GPUs},
	author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
	journal={arXiv preprint arXiv:1702.08734},
	year={2017}
}

@article{infonce,
	author    = {A{\"{a}}ron van den Oord and
	Yazhe Li and
	Oriol Vinyals},
	title     = {Representation Learning with Contrastive Predictive Coding},
	journal   = {CoRR},
	volume    = {abs/1807.03748},
	year      = {2018},
	url       = {http://arxiv.org/abs/1807.03748},
	eprinttype = {arXiv},
	eprint    = {1807.03748},
	timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{hinton2015distilling,
	title={Distilling the Knowledge in a Neural Network}, 
	author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
	year={2015},
	eprint={1503.02531},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{simcse,
	author    = {Tianyu Gao and
	Xingcheng Yao and
	Danqi Chen},
	title     = {SimCSE: Simple Contrastive Learning of Sentence Embeddings},
	journal   = {CoRR},
	volume    = {abs/2104.08821},
	year      = {2021},
	url       = {https://arxiv.org/abs/2104.08821},
	eprinttype = {arXiv},
	eprint    = {2104.08821},
	timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08821.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{simclr,
	author    = {Ting Chen and
	Simon Kornblith and
	Mohammad Norouzi and
	Geoffrey E. Hinton},
	title     = {A Simple Framework for Contrastive Learning of Visual Representations},
	journal   = {CoRR},
	volume    = {abs/2002.05709},
	year      = {2020},
	url       = {https://arxiv.org/abs/2002.05709},
	eprinttype = {arXiv},
	eprint    = {2002.05709},
	timestamp = {Fri, 14 Feb 2020 12:07:41 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2002-05709.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{coco,
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in
	object recognition by placing the question of object recognition in the context
	of the broader question of scene understanding. This is achieved by gathering
	images of complex everyday scenes containing common objects in their natural
	context. Objects are labeled using per-instance segmentations to aid in precise
	object localization. Our dataset contains photos of 91 objects types that would
	be easily recognizable by a 4 year old. With a total of 2.5 million labeled
	instances in 328k images, the creation of our dataset drew upon extensive crowd
	worker involvement via novel user interfaces for category detection, instance
	spotting and instance segmentation. We present a detailed statistical analysis
	of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide
	baseline performance analysis for bounding box and segmentation detection
	results using a Deformable Parts Model.},
	added-at = {2020-06-07T20:25:18.000+0200},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll√°r, Piotr},
	biburl = {https://www.bibsonomy.org/bibtex/2f4ab9f41677ee189a8cbc5a92cc0dc74/jan.hofmann1},
	description = {Microsoft COCO: Common Objects in Context},
	interhash = {a3a26c6fe173264a6b812e3b7b4119bd},
	intrahash = {f4ab9f41677ee189a8cbc5a92cc0dc74},
	keywords = {thema:pyramid_scene_parsing},
	note = {cite arxiv:1405.0312Comment: 1) updated annotation pipeline description and figures; 2) added new  section describing datasets splits; 3) updated author list},
	timestamp = {2020-06-07T20:25:18.000+0200},
	title = {Microsoft COCO: Common Objects in Context},
	url = {http://arxiv.org/abs/1405.0312},
	year = 2014
}

@article{flickr,
	author    = {Bryan A. Plummer and
	Liwei Wang and
	Chris M. Cervantes and
	Juan C. Caicedo and
	Julia Hockenmaier and
	Svetlana Lazebnik},
	title     = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for
	Richer Image-to-Sentence Models},
	journal   = {CoRR},
	volume    = {abs/1505.04870},
	year      = {2015},
	url       = {http://arxiv.org/abs/1505.04870},
	eprinttype = {arXiv},
	eprint    = {1505.04870},
	timestamp = {Mon, 13 Aug 2018 16:48:53 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/PlummerWCCHL15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{karparthy,
	author    = {Andrej Karpathy and
	Li Fei{-}Fei},
	title     = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
	journal   = {CoRR},
	volume    = {abs/1412.2306},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.2306},
	eprinttype = {arXiv},
	eprint    = {1412.2306},
	timestamp = {Wed, 15 Sep 2021 14:13:01 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/KarpathyF14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vit,
	author    = {Alexey Dosovitskiy and
	Lucas Beyer and
	Alexander Kolesnikov and
	Dirk Weissenborn and
	Xiaohua Zhai and
	Thomas Unterthiner and
	Mostafa Dehghani and
	Matthias Minderer and
	Georg Heigold and
	Sylvain Gelly and
	Jakob Uszkoreit and
	Neil Houlsby},
	title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
	at Scale},
	journal   = {CoRR},
	volume    = {abs/2010.11929},
	year      = {2020},
	url       = {https://arxiv.org/abs/2010.11929},
	eprinttype = {arXiv},
	eprint    = {2010.11929},
	timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{imagenet21k,
	author    = {Tal Ridnik and
	Emanuel Ben Baruch and
	Asaf Noy and
	Lihi Zelnik{-}Manor},
	title     = {ImageNet-21K Pretraining for the Masses},
	journal   = {CoRR},
	volume    = {abs/2104.10972},
	year      = {2021},
	url       = {https://arxiv.org/abs/2104.10972},
	eprinttype = {arXiv},
	eprint    = {2104.10972},
	timestamp = {Tue, 27 Apr 2021 14:34:45 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2104-10972.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cc,
	title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
	author = "Sharma, Piyush  and
	Ding, Nan  and
	Goodman, Sebastian  and
	Soricut, Radu",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-1238",
	doi = "10.18653/v1/P18-1238",
	pages = "2556--2565",
	abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@article{adamw,
	author    = {Ilya Loshchilov and
	Frank Hutter},
	title     = {Fixing Weight Decay Regularization in Adam},
	journal   = {CoRR},
	volume    = {abs/1711.05101},
	year      = {2017},
	url       = {http://arxiv.org/abs/1711.05101},
	eprinttype = {arXiv},
	eprint    = {1711.05101},
	timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tan-bansal-2019-lxmert,
	title = "{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers",
	author = "Tan, Hao  and
	Bansal, Mohit",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1514",
	doi = "10.18653/v1/D19-1514",
	pages = "5100--5111",
	abstract = "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22{\%} absolute (54{\%} to 76{\%}). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert",
}