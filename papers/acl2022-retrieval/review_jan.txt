Meta Review of Paper1453 by Area Chair xBdg 
ACL ARR 2022 January Paper1453 Area Chair xBdg
27 Feb 2022ACL ARR 2022 January Paper1453 Meta ReviewReaders: Paper1453 Senior Area Chairs, Paper1453 Area Chairs, Paper1453 Authors, Paper1453 Reviewers Submitted, Program Chairs
Metareview:
This short paper is a revised version of a manuscript previously submitted to ACL RR. The work describes a technique for model compression for text-image retrieval. By exploiting knowledge distillation, the work operates in two stages, by first using out-domain data, and then in-domain data. The paper has been improved with respect to the previous version: in particular, the main change is an additional baseline used in the experimentation, which consists in a pre-trained image encoder followed by Tiny-BERT.

Summary Of Reasons To Publish:
Overall, the reviews appreciate the paper, pointing out that, although it does not present a novel methodology, yet it contributes with a practical application of known approaches. The software package that accompanies the submission is indeed seen as one major contribution of the work.

Summary Of Suggested Revisions:
The description of related works should be improved, following the suggestions of the reviewers.

Overall Assessment: 4 = There are minor points that may be revised
Suggested Venues:
The work is certainly suitable for a workshop, but it may also be published as a short paper in a major conference.

Ethical Concerns:
There is no ethical concern with this paper.

[–]
Official Review of Paper1453 by Reviewer 1PMN 
ACL ARR 2022 January Paper1453 Reviewer 1PMN
26 Feb 2022ACL ARR 2022 January Paper1453 Official ReviewReaders: Program Chairs, Paper1453 Senior Area Chairs, Paper1453 Area Chairs, Paper1453 Reviewers Submitted, Paper1453 Authors
Paper Summary:
Because of the large memory footprint and non-trivial inference time, the authors propose to distill the knowledge from a pre-trained CLIP model into a small model which exhibits the same performance (in terms of R1, R5 and R10), while reducing the memory requirements and time complexity of the model. Namely, the authors use the pre-trained CLIP (ViT-B/32) and distill its knowledge into a model that requires significantly less memory (~39% of the original image encoder, and ~60% of the text encoder) and it's significantly faster (~1.5x faster image encoder and ~2.7x faster text encoder). The authors use COCO and Flickr30k to verify their findings, i.e., verify that the resulting model indeed retains its performance while achieving faster inference speed at a lower memory requirement.

To obtain the smaller encoders, the authors design a knowledge distillation method which consists of two stages. In stage one, the individual encoders are distilled with a contrastive knowledge distillation without paired examples, on out of domain data (Conceptual Captions). In stage two, they perform knowledge distillation using in-domain data. In particular, they firstly fine-tune the CLIP image and text encoder. Then, they distill the knowledge from the fine-tuned CLIP to their smaller model, by (1) standard KL divergence, (2) hard-negative mining, (3) the standard fine-tuning language-to-image and image-to-language losses. The authors argue that in order for the distillation to succeed, sequential fine-tuning has to be used, i.e., the encoders are trained one at a time, while gradients are not propagated to the other.

The authors perform ablations on COCO and Flickr30k, where the most important observations are (according to me):

Training from a pre-trained image encoder (ImageNet), and Tiny-Bert (pre-trained) results in significantly lower performance than their proposed method using the same model architecture.
Not using stage-one pre-training, or stage-one pre-training using MSE loss (instead of contrastive knowledge distillation) results in lower performance.
Not using sequential fine-tuning (SF) also significantly decreases the performance.
Lastly, they show that indeed the resulting model is smaller in size and reduces the inference time.

Summary Of Strengths:
I want to clarify that I read the previous version of the paper, the previous reviews, as well as the author response. So I'm writing both the strengths and weakness by taking into account all information.

I fully agree with the previous reviewers that "The engineering part is the real strength of the paper. Though the paper follows standard distillation model but successful implementation of such a model for the problem in hand requires innovation and the paper shows that strength.".
I also appreciate the detailed ablations, and the fact that the authors include a baseline (Tiny-Bert + Vit-32 trained without KD), which in fact, has been brought up by the previous reviews and fixed in the updated version of the paper.
Summary Of Weaknesses:
Compared to some of the previous reviews, I do not agree that there is a methodological novelty, i.e., the loss function. Nevertheless, I consider this to be minor, as the goal of the paper is not to propose a new method, but rather demonstrates an effective way to reuse existing techniques to reach similar performance with a lower memory requirement and increased speed.
Even though the paper reads more as an application/use-case paper while using state-of-the-art methods, I believe that having a discussion of the Related Work is important for this paper to be published at an ACL venue. I see that this has been brought up by the reviewers, but it has not been corrected in this version. After all, giving proper attribution to prior works is important. I understand that it is a 4 page paper, but still, I believe the authors should definitely include such discussion in the appendix.
Comments, Suggestions And Typos:
I want to clarify that I like the paper, and I don't mind the fact that the authors do not propose a novel method. I appreciate that the authors included better ablation studies to strengthen their findings. However, the authors should definitely include a discussion of the related work, in particular methods that perform compression of vision-language models. If that issue is addressed, I would be happy to increase my rating.

Overall Assessment: 3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper1453 by Reviewer 61Rh 
ACL ARR 2022 January Paper1453 Reviewer 61Rh
25 Feb 2022ACL ARR 2022 January Paper1453 Official ReviewReaders: Program Chairs, Paper1453 Senior Area Chairs, Paper1453 Area Chairs, Paper1453 Reviewers Submitted, Paper1453 Authors
Paper Summary:
This paper proposes a method to reduce space and encoding time for a language and vision model architecture for an image search task, including experiments on existing datasets. The model compression method works in two steps. The first one treats language and vision models separately, without regard for the particular task. The second step iteratively fine-tunes the models on each other. The paper provides the link to software that implements the approach for an image search application on a mobile phone.

The work's outcome is interesting for an ACL audience and the paper can be counted as relevant in the ARR track "Efficient Methods for NLP" but might be better placed in a more general ML or CV venue.

Summary Of Strengths:
The paper addresses the important issue of model size and processing time that touches also on the question of accessibility to such models, i.e. whether only users with access to powerful infrastructure make use of these models or whether it is feasible to provide also others easy access.

The paper comes as a presentation of a new method but I consider the accompanying software a strength as well, as it exemplifies the work in a practical application. Since it's not always clear how research outcomes can be implemented in practical applications, the piece of software makes it theoretically easy for anyone to test it and draw further conclusions about applicability in related tasks.

The argumentation and method presentation are clear and comprehensible. The paper is generally well written (see some minor comments below).

Summary Of Weaknesses:
Although the paper is well-written, it suffers from the little space. Some context about related work is included in the introduction but focuses on background about the task while I would be looking for background on model compression.

Similarly, as a researcher working with such models, I would be interested in knowing what the authors' conclusions are about applicability to other similar tasks, also in relation to other existing compression techniques.

However, I think that the short paper format is appropriate for the scope of the research, because it presents a very specific technique in a specific learning task. My suggestion would be to focus more on the compression part than on the tasks in the introduction in order to give the reader the necessary context.

Comments, Suggestions And Typos:
In the abstract and introduction, the paper reports different reduction factors than in the results section (1.6 vs 1.5 and 2.9 vs 2.7)
"alt-text": please give a short explanation of this or simply replace by "text" if it's not relevant for the paper (images are not further specified either)
I haven't completely understood how many times or what the factor is that determines how often stage 2 (the fine-tuning) is repeated.
In several places, nouns are missing determiners, mainly "model", "encoder", "architecture". For example in lines 84 (Should be "The dual-encoder architecture" or "Dual-encoder architectures") or line 128 (should be "and train a over-parametrized large dual-encoder")
in the abstract, it says "performs on par with or better". Please strike the "or better", since this is not further elaborated later. The important point here is that it's "on par with"
l. 171 "alingment" --> "alignment"
l. 263 "affect" --> "effect"
l. 264 "extensive" seems an overstatement here
Figure 1: the text inside the figure is too small
lines 110/111 in LaTeX, you can avoid the separation of the reference number by using "~" (Figure~\ref{xxx})
line 288: what would be an unreal iPhone?
Overall Assessment: 3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.
Confidence: 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper1453 by Reviewer ahQk 
ACL ARR 2022 January Paper1453 Reviewer ahQk
17 Feb 2022ACL ARR 2022 January Paper1453 Official ReviewReaders: Program Chairs, Paper1453 Senior Area Chairs, Paper1453 Area Chairs, Paper1453 Reviewers Submitted, Paper1453 Authors
Paper Summary:
This paper presents a two-stage model compression method for lightweight text-image retrieval, where the teacher models are from the large pre-trained dual-encoder. The resulting model is smaller and faster, meanwhile achieving comparable results compared with the original models.

Summary Of Strengths:
An elegant way to compress large pre-trained dual-encoder.
Publicly-available code/models
Summary Of Weaknesses:
NA

Comments, Suggestions And Typos:
I have reviewed this paper earlier and mentioned a missing baseline, which has been added to this version. In addition, the analysis of the results has also been enriched.

Overall Assessment: 3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
