% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{(Kenny: #1)}}

% ADDED PACKAGES
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Demonstrations are Useful for Numerical Reasoning on Finetuned GPT-2}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
Large pretrained language models~(PLMs) can benefit from in-context learning, 
where the models take a few demonstrations and a test case as input and 
answer the question with fixed parameters. In contrast, the in-context learning ability of small PLMs remains underexplored.
In this work, we try to investigate if combining in-context demonstrations 
and a test case can further improve the performance of a finetuned small PLM. 
We focus on decoder-only models, i.e., GPT-2,  and test our ideas on numerical reasoning tasks. 
Experiment results reveal that, compared to vanilla fine-tuning, models trained via causal language modeling objective benefit the most from demonsrations and that its 
performance is robust to the order or type of demonstrations.
\end{abstract}

\input{intro}

\input{method}
\input{approach}


\input{experiment}

% \input{related}

\input{conclusion}


\section*{Limitations}
Due to the limited computing resource budget, we only conduct experiments on the smallest GPT-2 model. Therefore, the observations drawn from our experiments may not be easily generalized to other model sizes. 
However, extending the evaluation procedure to larger models is straightforward and is considered as future work. Moreover, since we limit our scope to simple arithmetic problems in this paper, the effect of demonstrations on fine-tuned small PLMs on more comprehensive and diverse downstream tasks remains to be explored. We plan to carry out more  systematic study on this in the future.

%\section*{Ethics Statement}
%Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

%\section*{Acknowledgements}
%This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
%EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
%EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
%ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
%ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
%NAACL 2019 by Stephanie Lukin and Alla Roskovskaya, 
%ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, 
%NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
%Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
%ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, 
%ACL 2012 by Maggie Li and Michael White, 
%ACL 2010 by Jing-Shin Chang and Philipp Koehn, 
%ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, 
%ACL 2005 by Hwee Tou Ng and Kemal Oflazer, 
%ACL 2002 by Eugene Charniak and Dekang Lin, 
%and earlier ACL and EACL formats written by several people, including
%John Chen, Henry S. Thompson and Donald Walker.
%Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.
%
% Entries for the entire Anthology, followed by custom entries

\bibliography{custom}
%\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\input{appendix}


\end{document}
