\section{Experimental Setup}
\label{sec:approach}



% 数据集 + 添加Demonstration的方式 + Ensemble的方式
\subsection{Task and Dataset}
We evaluate the effectiveness of demonstrations on fine-tuned GPT-2 on 
simple arithmetic questions, which involves up to 
6-digit addition, subtraction, and multiplication between two integer numbers. It gauges the model's ability to compute the exact value of given math expression. 
We synthesize a dataset consisiting 1.56 million arithmetic questions and split it into training and test set. 
%Another task is math word problem solving, which test both natural language understanding and numerical reasoning capabilities. 
%We employ asdiv-a~\cite{asdiv}, a math word problem 
%dataset that consists of 1218 math problems. 
%In this dataset, we train the model to output the formula to 
%obtain the final answer pertaining to the given problem context and question. 
%Some sample questions of these datasets can be found in 
% \tabref{tbl:sample_questions}.


\subsection{Demonstration Construction}
For each dataset, we manually design a template which can transform the original data into demonstrations. 

After fine-tuning using strategies described in \secref{sec:method}, the model is then evaluated on the task-specific test set given a prompt consisting of 0 to 3 demonstrations preceding each test instance.

To select the demonstrations used at the test time, we first classify demonstrations according to the type of questions they contain. In the synthetic arithmetic dataset, we divide the questions into different categories by the number of digits of the two numbers involved in the question. For instance,  a question of type ``3d-2d'' includes a 3-digit number and a 2-digit number, and we consider the demonstration, 
which includes this question and its corresponding answer, is also of type ``3d-2d''. 

Once the demonstrations are grouped according to their types, we can construct a prompt of $K$ demonstrations through the following steps. First, we can fix the composition of the prompt by choosing $K$ demonstrations types. For example, when $K=2$, a possible composition of the prompt is "3d-2d" and "1d-4d". Then we can get $K$ demonstrations by sampling from the demonstrations of the $K$ chosen types, e.g., ``120+34=154'' and ``8+1200=1208''. In practice, to construct a prompt for a given number of demonstrations, we choose 5 different compositions and construct 5 different prompts for each composition. The models are then prompted to give predictions and we calculate the mean and standard deviation of the 25 results for each number of demonstrations.

%Since the model's prediction may vary given different prompts, we also apply the majority voting ensemble method to aggregate several predictions of the model, i.e. choosing the most frequent prediction in the results.
%The purpose of these demonstrations is twofold, one is to finetune a language model, another is to serve as components in the prompt at test time. More specifically, we finetune a decoder-only language model on these demonstrations using the causal language modeling objective. 
%For instance, in the synthetic dataset, "3-2, 2-3" (two demonstrations of type "3-2" and "2-3" respectively) is a potential composition for a prompt which contains 2 demonstrations. Then we can get 2 demonstrations by sampling based on this composition.

\subsection{Implementation Details}
We use the smallest GPT-2~\footnote{\url{https://huggingface.co/gpt2.}} model with 117M parameters for all of our experiments based on the HuggingFace transformers library~\cite{transformers}. During training, AdamW~\cite{adamw} optimizer with learning rate 5e-5 is utilized for both fine-tuning strategies~(\secref{sec:method}). 
We set the bacch size as 256 to maximally utilize the computing resource and train models up to 6 epochs. The last model checkpoint will then be evaluated on the test set.

