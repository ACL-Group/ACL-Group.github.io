\section{Conclusion}
In this paper, we analyze the effect of in-context demonstrations for small finetuned language models, specificall decoder-only GPT-2 model in this work. Experiments on a synthetic single-operator arithmetic dataset demonstrate that 
once the language model is finetuned via causal language modeling objective on the corpus constructed by packing multiple training samples, at the time of testing the model will benefit from demonstrations obtained by the same template. Deeper analysis further reveals that language models fine-tuned in this way exhibit strong robustness to 
different tpyes/orders of demonstrations. Therefore, we believe that bridging the gap between pre-training and fine-tuning can effectively enhance the in-context learning ability of small PLMs like GPT-2 and future research may continue to explore the potential of them.
