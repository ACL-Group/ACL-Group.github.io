\section{Methodology}
\label{sec:method}
Unlike LLMs which already garnered abundant knowledge from pre-training and can be readily adapted for different tasks via in-context learning, small PLMs 
do not exhibit such ability due to limited model capacity. To adapt small PLMs to downstream tasks, fine-tuning~\cite{elmo,bert} is typically adopted in which model parameters are updated to absorb task-specific knowledge from 
training data. We utilize GPT-2, a representative decoder-only autoregressive language model and investigate the effect of fine-tuning on its in-context learning ability.

Specifically, we explore two types of fine-tuning strategies: vanilla fine-tuning and causal language modeling. The former maximizes the conditional likelihood of the output $y$ given the input $x$ on a per-sample 
basis, while the latter composes multiple samples $\{(x_i, y_i)\}_{i=1}^{n}$into a sequence $S=\mathcal{T}(x_1,y_1),..., \mathcal{T}(x_n, y_n)$ with some template $\mathcal{T}$ and optimizes the causal language modeling objective $\log{p(S)}=\sum_{t=1}^{|S|}\log{p(S_t|S_{<t})}$. For arithmetic problem, the input $x$ is a mathmatical expression and the output $y$ is the evaluated value corresponding to the expression. We illustrate these two fine-tuning strategies in \figref{fig:diff}.

At inference time, the test case $x$ is optionally prepended with $K$ demonstrations $\{(x_i,y_i)\}_{i=1}^K$ and fed to the fine-tuned model to make prediction of $y$.


