Reviewer1.
We thank the reviewer for the valuable comments and suggestions. Here are some responses to the questions:
Question 1A: Yes, we will release the synthetic dataset. 
Question 1B: We intended to also present the results on subtraction and multiplication. Here are the results, which demonstrate the same trend as the addition.

Results for subtraction:

| Types              | Template 1 | Template 2 |
| ------------------ | ---------- | ---------- |
| Vanilla + 0 demo   | 0.023      | 0.004      |
| Vanilla + 1 demo   | 0.000      | 0.000      |
| Vanilla + 2 demo   | 0.000      | 0.000      |
| Vanilla + 3 demo   | 0.000      | 0.000      |
| Causal LM + 0 demo | 0.094      | 0.028      |
| Causal LM + 1 demo | 0.272      | 0.160      |
| Causal LM + 2 demo | 0.273      | 0.158      |
| Causal LM + 3 demo | 0.276      | 0.160      |

Results for multiplication:

| Types              | Template 1 | Template 2 |
| ------------------ | ---------- | ---------- |
| Vanilla + 0 demo   | 0.001      | 0.012      |
| Vanilla + 1 demo   | 0.000      | 0.001      |
| Vanilla + 2 demo   | 0.000      | 0.000      |
| Vanilla + 3 demo   | 0.000      | 0.000      |
| Causal LM + 0 demo | 0.018      | 0.009      |
| Causal LM + 1 demo | 0.067      | 0.065      |
| Causal LM + 2 demo | 0.066      | 0.064      |
| Causal LM + 3 demo | 0.067      | 0.064      |

Question 1C: This remains an open question for us. In future work, we will investigate if finetuning small language models with this technique in other types of tasks will also make demonstrations useful at inference time.

Reviewer2.
We are thankful for the detailed comments and suggestions given by the reviewer and we are sorry for the ambiguities in the paper. Here are some clarifications:

Reject1: Admittedly, there are other possible templates. In the paper, we choose these two templates as they are two common ways of representing the arithmetic problem.

Reject2,Line175: "Both models" refer to models finetuned using template 1 in either of the two proposed fine-tuning methods.

Reject2,Line178: The difference means the models finetuned using template 1 in either of the two proposed fine-tuning methods are better than their counterparts finetuned using template 2. We agree that an error analysis should be added to back up our claim.

Reject2,Line182: Since both models use BPE tokenization, which will tokenize "123456" into "123" and "456".

Reject2,Line187: A number like "123123" will be tokenized into two identical tokens using BPE tokenization, while the number " 123123" consists of two different tokens " 123" and "123", making the model may easily realize the difference between the two tokens, hence make less error like "123123+1=124123". (Align the first "123" with "1" ). 

Reject2,Line119: The motivation is that for GPT-3 when given several 10-digit plus 10-digit examples, GPT-3 will perform better on questions of the same type.

Reject2,Line138: Here we try to describe the experimental setting. To avoid the impact of different demonstrations on the model's performance, we sample 5 types of demonstrations and create 25 different prompts in total.

Reject2,Line219: We meant to test the models on 500 questions of 3 digits plus 3 digits given 1 demonstration from all the 36 possible scenarios of 6-digit addition. We did not finish all the experiments by the deadline, for results on the other two types of questions, please refer to Question1B.

Reject3,Line196: The training setup is different between the two proposed fine-tuning methods. For vanilla fine-tuning, the model is trained to answer 1 question at a time. But for causal language modeling, the model will be trained to answer 1 question conditioned on several question-answer pairs.

Reject3,Line210: The sequences of the arithmetic problem are considered as demonstrations.

Reject3,Line216: We agree that several possible factors may lead to this plateau and we plan to investigate this phenomenon in future work.

Reject3,Table1: The only difference between different models using the same template is the finetuning strategies. As pointed out in Reject3,Line196, we attribute the change of accuracy in response to the number of demonstrations to the "gap" between the training and testing scenarios.

Reject3,Line235: We agree that further experiments should be conducted to verify this hypothesis.

Reject4: We agree that multiple reasons account for this. For instance, in contrast to in-context learning, we will update the model parameters for the given task. 

Reject5: We agree that it will be beneficial to include a comparison with existing work suggested by the reviewer. 

Reviewer3.
We sincerely appreciate the valuable reviews. 

Reject1: Despite all the works using demonstrations, our idea differs from theirs in two ways:

1. Earlier works focus on meta-learning, i.e. adding an intermediate training stage before the downstream task, while our method tries to find a better fine-tuning method that will update the model's parameters for the specific task.
2. Since earlier works consider multiple training tasks, thus they have greater training costs.

Reject2: Due to time constraints, we only test our ideas on the GPT-2 model and a synthetic dataset in this short paper. We will try to incorporate the results of different types of language models into a wider range of tasks. 

Question 3A: We manually tested several 6-digit addition questions on GPT-2, using greedy decoding, and found that the models can not answer any of them even with the help of 1-3 demonstrations. 

Question 3B: The models do not overfit after 6 epochs, since their performance is still increasing. We decide to fix the number of training epochs to 6 for a fair comparison.