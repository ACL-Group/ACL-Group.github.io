Review #1
What is this paper about and what contributions does it make?
In context learning is a well studied area and has shows pretty promising gains. Still most of it is applicable only to large models.
The author here shows that numerical tasks (focused on addition of two numbers), that is very difficult for small models (here GPT-2), can benefit a lot from in-context finetuning on a synthetically generated numeric dataset.
The authors generate a synthetic dataset of mathematical operations between two (up to 6 digit) numbers. Though the dataset is claimed to have addition, substraction and multiplication, all the experimental results are shown on addition.
Reasons to accept
Given that mathematical tasks are pretty difficult for small models, this is a significant leap in their performance and it uses a well studied technique to improve the performance of small models on these tasks.
Reasons to reject
The research contribution is small, and the experiments are pretty limited. It is not clear whether this method scales to other operations (multiplication would be interesting to test), and whether it works for equations with more than one operation.
Questions for the Author(s)
Question A: Will the created dataset be opensourced?
Question B: Do we have any result for operation types other than addition? Section 3.1 mentions dataset of 1.56m examples is created using addition, subtraction, and multiplication of integers, but the experimental results focus only on addition. why?
Question C: The task in question is pretty simple. Demonstrations in this case are very similar to the task in hand. Yes, the simple experiment on addition shows gain, but would it be possible to show gains on more complex equations?
Typos, Grammar, Style, and Presentation Improvements
Section 1, line 26: consumes -> consume Section 3.3, line 152: bacch -> batch Section 5, line 259: specificall -> specifically
Soundness:	3
Excitement (Long paper):	3.5
Reviewer Confidence:	5
Recommendation for Best Paper Award:	No
Reproducibility:	4
Ethical Concerns:	No





Review #2
What is this paper about and what contributions does it make?
This paper investigates the effect of using demonstrations on fine tuned GPT-2 on numerical reasoning tasks. For fine tuning, the authors have used vanilla fine-tuning where each numerical problem has been fed separately to the model whereas the causal language model based fine tuning considers a sequence of problems as a single input. The authors claim that use of demonstrations brought about a huge boost in the model that is fine tuned with the causal language model.
Reasons to accept
Use of demonstrations has significant implications in the domain of few-shot learning. In this context, such experiments are extremely useful to the NLP community. However, I have a number of concerns on the methodology and significance of the conclusions.
Reasons to reject
1) Two templates are used: one with a blank before every number and another with the blanks. The inclusion of template 1 is supported by the fact that GPT-2 used BPE tokenization which treats " NNN” and "NNN” as two different tokens. So, why template 1 is necessary. Or, in other way, why other variations, e.g., "[x1] +[x2]= [y]” or "[x1 + [x2]=y” are not used?
2) There are ambiguities in several places. For example:
Line 175, ‘we can see that for both fine tuning methods, using template 1 (adding a space before each number) is helpful for both models.' Which models are being referred to by ‘both models'?
Line 178: ‘We assume that this difference may be caused by tokenization'. Which ‘difference' is being pointed out here? No experimental support of ‘caused by tokenization' has been provided.
Line 182: It is not clear why a number 123456, a model trained with template 1 will regard it as 123 and 456.
Line 187: If we follow template 1 and 2, then there should not be any ambiguity in detecting the beginning of a number. Hence, I don't see any alignment problem in contrast to what is indicated by the authors.
Line 119: What benefit will the grouping of the demonstrations have? No motivation for such a decision has been provided.
Line 138: The construction of prompt is equally ambiguous. ‘.... to construct a prompt ….. We choose 5 different compositions and 5 different prompts for each composition.' It seems that to create a prompt, 5 different prompts are chosen and it seems unreasonable.
Line 219: The first sentence says the model was tested with 500 questions of 3 digits plus 3 digits. The next sentence mentions about 36 possible scenarios of 6 digit addition. Two sentences do not seem to connect with each other. The second sentence indicates that one number may be of one to six digits. Although the dataset consists of addition, subtraction, multiplication and division problems why only addition?
3) Some claims or results have not been backed up.
Line 196: It has been observed that demonstrations are effective for causal language model tuned GPT-2 and are ‘detrimental' for vanilla fine-tuned model. This has been attributed ‘to the gap between training and testing'. The ‘gap' has not been quantified or discussed. I guess the training and test setup for both the models are the same. So, the same gap should exist for the vanilla finetuned model. The statement ‘For vanilla fine-tuned model, there isn't any demonstrations in the context of training instances….' add more ambiguity.
Line 210: ‘....thus the model is familiar with the task of predicting the answer given a certain number of demonstrations….'. The model is fed with a sequence of arithmetic problems. These cannot be treated as demonstrations. Thus this statement is not clearly supported.
Line 216: ………‘improvements elicited by the demonstrations have an upper limit'. This conclusion has been drawn by observing that the performance of the model (Causal LM) becomes stable after the first demonstration. However, this plateau can be caused by several other factors. The subsequent demonstrations may not be effective or the model is not designed to make use of the other demonstrations.
Table 1: The vanilla finetuned model is dramatically affected by the use of demonstrations. It looks quite surprising as the model is fed with more relevant information in the form of demonstrations. What makes the accuracy to be dropped to 0.0 from 25.8? Also, the Causal LM model seems to be more sophisticated than the vanilla model. Why is performance (0 demonstration) of the causal LM model inferior to the vanilla model.
Line 235: For the demonstration having small digit addition problems, the results are worse. This has been hypothesised to be caused by scarcity of such problems in the dataset. The dataset has been created by the authors and they have full control over the composition of the dataset. This hypothesis could have been easily checked by having a sufficient number of small digit problems. At this problem, this hypothesis has not been validated.
4) The LLMs are highly sensitive to the order of demonstrations. However, the results in this paper suggest that performance of small language models like GPT-3 is stable with respect to the order of demonstration. Is this due to the difference in the size of the language models? If not, what is it? This requires a thorough discussion. However, no explanation or discussion on this has been provided in the paper.
5) The authors should cite and compare with existing works in the use of demonstration in small language models. For example, Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
6) Finally, the paper has several methodological flaws, ambiguous and unsupported claims. No concrete conclusion which has been backed up by solid evidence has been provided.
Missing References
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making Pre-trained Language Models Better Few-shot Learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
Typos, Grammar, Style, and Presentation Improvements
There are several grammar issues. Some examples are given below: Line 152: ‘bacch size' should be replaced with ‘batch size' Line 202: ‘.....there isn't any demonstrations…..' check grammar Line 230: ‘....difference between max accuracy and the min accuracy is 1.6% for template 1 and 3.9%. ‘ The figure 3.9% is meant for template 2? Something is missing here.
Soundness:	1
Excitement (Long paper):	2
Reviewer Confidence:	4
Recommendation for Best Paper Award:	No
Reproducibility:	2
Ethical Concerns:	No





Review #3
What is this paper about and what contributions does it make?
This papers evaluates whether smaller fine-tuned language models can benefit from adding in-context examples during test time for simple arithmetic tasks.
Reasons to accept
1.    Interesting and relevant question: If in-context examples helps fine-tuned models, this is a helpful way to increase performance with minimal overhead. a.     Arithmetic domain: Language models have been shown to be prone to errors even for simple math tasks like arithmetic. b.    Working with smaller language models: Efficiency is important in certain use cases. Exploring how to improve smaller LMs is helpful.
Reasons to reject
1.    Prior work evaluating similar ideas already exist. For example, in-context tuning explores combining fine-tuning and in-context examples at train and test time. Two representative papers include Meta-learning via Language Model In-context Tuning (Chen et al 2022) and MetaICL: Learning to Learn In Context (Min et al 2021). Both these papers provide comprehensive results across a wide variety of tasks.
2.    Generalizability of the idea is inconclusive: Results are only shown on a single synthetic dataset. Results on real math datasets covering different tasks (algebra, geometry, etc) would be helpful to demonstrate generalizability. Evaluating different language model families (flan-T5, OPT, etc) of similar smaller sizes could also test generalizability.
Questions for the Author(s)
Question A: Could you provide zero shot results on the task? What were the results using non finetuned models with 0-3 in-context examples?
Question B: Finetuning of the model was done for 6 epochs. Was there any early stopping using a validation set to prevent overfitting. LMs usually converge in 1-3 epochs for finetuning.
Missing References
Meta-learning via Language Model In-context Tuning (Chen et al 2022) MetaICL: Learning to Learn In Context (Min et al 2021)
Soundness:	1
Excitement (Long paper):	2
Reviewer Confidence:	4
Recommendation for Best Paper Award:	No
Reproducibility:	3
Ethical Concerns:	No