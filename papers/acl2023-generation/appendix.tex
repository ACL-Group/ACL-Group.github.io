\appendix
%\section{Appendix}
%\label{sec:appendix}

\section{Packages used for Baselines}
\label{sec:pkgs}

The packages we adopted to re-implement the baseline are listed as follows:%in Table~\ref{tab:baseline}.

\textbf{Reading Comprehension}
\begin{itemize}
	\item Dataset: \url{https://github.com/nlpdata/dream/tree/master/data}
	\item Baseline Code: \url{https://github.com/huggingface/transformers}
	\item Evaluation Metric: \url{https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py}
\end{itemize}

\textbf{Dialogue Summarization}
\begin{itemize}
	\item Dataset: \url{https://arxiv.org/src/1911.12237v2/anc/corpus.7z}
	\item Baseline Code: \url{https://github.com/huggingface/transformers}
	\item Evaluation Metric: \url{https://github.com/pltrdy/files2rouge}; \url{https://github.com/Yale-LILY/SummEval}
\end{itemize}

\textbf{Style Transfer}
\begin{itemize}
	\item Dataset: \url{https://github.com/martiansideofthemoon/style-transfer-paraphrase}
	\item Baseline Code: \url{https://github.com/martiansideofthemoon/style-transfer-paraphrase}
	\item Evaluation Metric: \url{https://github.com/martiansideofthemoon/style-transfer-paraphrase}
\end{itemize}

\textbf{Question Generation}
\begin{itemize}
	\item Dataset: \url{https://github.com/microsoft/unilm/tree/master/unilm-v1}
	\item Baseline Code: \url{https://github.com/microsoft/unilm/tree/master/unilm-v1}
	\item Evaluation Metric: \url{https://github.com/microsoft/unilm/tree/master/unilm-v1}
\end{itemize}

\textbf{News Summarization}
\begin{itemize}
	\item Dataset: \url{https://drive.google.com/file/d/0BzQ6rtO2VN95a0c3TlZCWkl3aU0/view?resourcekey=0-toctC3TNM1vffPCZ7XT0JA}
	\item Baseline Code: \url{https://github.com/huggingface/transformers}
	\item Evaluation Metric: \url{https://github.com/pltrdy/files2rouge}; \url{https://github.com/Yale-LILY/SummEval}
\end{itemize}

\section{Preliminary Studies on TCL}
\label{sec:preliminary}

Preliminary studies on dialogue summarization for TCL under different settings are shown in Table~\ref{tab:tclpre}.
We can see that the ``soft'' setting does help the TCL with sub-sequence generation curricula, which is consistent with the results in \citet{liang-etal-2021-token-wise}.
Results are opposite for TCL with our proposed sequence completion curricula. The ``soft'' setting considering the loss from prefix tokens actually hurts the intuition that ``the shorter the target is, the easier the tasks is''.  As a result, SC-hard performs better than SC-soft.


\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{cccccc}
		\hline
		{} & {R1} & {R2} & {RL} & {Met} & {BertS} \\
		\hline
		w/o CL& 51.88 & 27.30 & 42.77 & 24.75 & 71.38 \\
		\hline
		SG-hard &50.70 & 27.31 & 43.00 & 23.47 & 70.85\\
		SG-soft & 52.43 & 27.65 & 43.56 & 25.17 & 71.86 \\
		\hline
		SC-hard &  52.69 & 28.28 & 43.89 & 25.08 & 71.95 \\
		SC-soft & 51.39 & 27.53 & 43.06 & 23.84 & 71.35 \\
		\hline
	\end{tabular}
	\caption{Ablations on TCL learning algorithm with different settings.}
	\label{tab:tclpre}
\end{table} 

Experiments on the sensitivity of curriculum step in TCL-SG~\cite{liang-etal-2021-token-wise} are in Table~\ref{tab:tclpre2}. 
It consistently has improvements on dialogue summarization compared with the baseline. However, the performances also vary a lot with different curriculum steps, especially on R1, Meteor and BertScore. The estimation rule proposed in~\citet{liang-etal-2021-token-wise} of computing the number of steps it takes to reach approximately 70\% of final scores doesn't perform well for dialogue summarization. So, we choose to set curriculum steps to 3 epochs for dialogue summarization and news summarization, and 2 epochs for reading comprehension and style transfer, which not only achieve better results, but also are fairer for comparisons. For news summarization, we still adopted their estimation rule and trained with 5200 curriculum steps.

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{cccccc}
		\hline
		{Curriculum Step} & {R1} & {R2} & {RL} & {Met} & {BertS} \\
		\hline
		w/o CL & 51.88 & 27.30 & 42.77 & 24.75 & 71.38 \\
		\hline
		1 epoch & 52.48 & 27.86 & 43.47 & 25.50 &71.83 \\
		1.58 epoch(70\%) & 51.89 & 27.64 & 43.51 & 24.37 &71.55 \\
		2 epoch & 51.93 & 27.75 & 43.37 & 24.73 & 71.57\\
		3 epoch & 52.43 & 27.65 & 43.56 & 25.17 & 71.85\\
		\hline
	\end{tabular}
	\caption{Performances on TCL-SG with different curriculum steps.}
	\label{tab:tclpre2}
\end{table}

%\section{Computational Costs}

%\begin{table} [h!]
%	\scriptsize
%	\centering
%	\begin{tabular}{p{2.5cm}p{1cm}p{1cm}p{1cm}}
%		\hline
%		{Tasks} & {w/o CL} & {TCL-SG} & {ICL-SC} \\
%		\hline
%		Reading Comprehension  &6.67 ep & 13.00 ep & 7.67 ep \\
%		Dialog Summarization &6.00 ep &15.67 ep & 11.67 ep\\
%		Style Transfer &6.50k st & 14.78k st & 9.67k st  \\
%		Question Generation & 17.67k st& 37.73k st  & 21.00k st  \\
%		News Summarization &21.00k st  & 47.20k st &36.00k st\\

%		\hline
%	\end{tabular}
%	\caption{Average number of training steps for different approaches. ``ep'' and ``st'' are short for ``epochs'' and ``steps'' respectively.}
%	\label{tab:humaneval}
%\end{table}

%The computational costs of different approaches are shown in Table~\ref{tab:humaneval}.
%One limitation of our approach is that in-sample curriculum learning methods (both TCL-SG and ICL-SC) always incur extra overhead during training compared with the vanilla model. Nevertheless, the inference time of different approaches is the same as the vanilla model.
%In a word, it's worthwhile because (1) ICL-SC can perform significantly better than both baselines without additional computational requirements during inference in real applications; (2) ICL-SC doesn't rely on task-specific expertise and has strong generalization ability.
