% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
 
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm,algpseudocode}
\usepackage{subcaption}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{makecell}
%\usepackage{subfigure}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\apxref}[1]{Appendix \ref{#1}}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}
\usepackage{color}
\newcommand{\KZ}[1]{\textcolor{red}{Kenny: #1}}
\newcommand{\YZ}[1]{\textcolor{red}{Yizhu: #1}}
\newcommand{\JQ}[1]{\textcolor{green}{JQ: #1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{In-sample Curriculum Learning by Sequence Completion for Natural Language Generation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
\author{Qi Jia$^1$, Yizhu Liu$^2$, Haifeng Tang$^3$, Kenny Q. Zhu$^4$\thanks{\hspace{2mm}The corresponding author.}\\
	$^{1,4}$Shanghai Jiao Tong University, Shanghai, China \\
	$^2$Meituan, Shanghai, China \\
	$^3$China Merchants Bank Credit Card Center, Shanghai, China \\
	\texttt{$^1$Jia\_qi@sjtu.edu.cn},
	\texttt{$^2$liuyizhu@meituan.com} \\ 
	\texttt{$^3$thfeng@cmbchina.com},
	\texttt{$^4$kzhu@cs.sjtu.edu.cn}\\
}
%\author{First Author \\
 % Affiliation / Address line 1 \\
 % Affiliation / Address line 2 \\
 % Affiliation / Address line 3 \\
  %\texttt{email@domain} \\\And
  %Second Author \\
 % Affiliation / Address line 1 \\
 % Affiliation / Address line 2 \\
 % Affiliation / Address line 3 \\
  %\texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
Curriculum learning has shown promising improvements in multiple domains by 
training machine learning models from easy samples to hard ones. 
Previous works which either design rules or train models for scoring the difficulty 
highly rely on task-specific expertise, and cannot generalize. Inspired by the ``easy-to-hard'' 
intuition, we propose to do in-sample curriculum learning for natural language generation tasks. 
Our learning strategy starts training the model to generate the last few words, i.e., 
do sequence completion, and gradually extends to generate the whole output sequence.
Comprehensive experiments show that it generalizes well to different
tasks and achieves significant improvements over strong baselines.
\end{abstract}

\input{intro}
\input{approach}
\input{eval}
\input{results}
\input{relatedwork}
\input{conclusion}

\section*{Acknowledgments}
This work was generously supported by the CMB Credit Card Center \& SJTU
joint research grant, and Meituan-SJTU joint research grant.

% Entries for the entire Anthology, followed by custom entries
\bibliography{acl23}
\bibliographystyle{acl_natbib}

%\newpage

 
\input{appendix}
%\appendix

%\section{Example Appendix}
%\label{sec:appendix}

\end{document}
