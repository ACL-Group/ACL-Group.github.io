We profoundly appreciate the hard work and the precious comments.

Reject 1: Sec 3.4 has a discussion about which tasks benefit the most from our approach. Generally, ICL-SC is more competitive with tasks with shorter outputs.

Question 1A: Data augmentation generally refers to the approaches that create more input-output training pairs by modifying the training data or borrowing from other resources before the training process. Whereas in our approach, the training data is never changed but the optimization goal changes dynamically (from easy to hard) during training (See Fig.1(c)).

Question 1B: The notation of these approaches is formed by concatenating the training algorithm (TCL or ICL) and the curriculum criteria (SG or SC). For example, ICL-SG means we use the ICL training algorithm in Sec. 2.2 with the "sub-sequence generation" curriculum proposed by Liang et al.(2021). This results in 4 combinations (see the end of Sec 3.1). We will clarify them better and add a figure for illustration in the appendix.

---

Thank you very much. We will fix the suggestions in the revised version.

2A: "Soft" and "hard" are two different curriculum strategies under the idea of sub-sequence generation criterion in Liang et al.(2021)'s work. "J metric" is an aggregation metric proposed by Krishna et al.(2020) for the style transfer task. We will add these explanations in the revised version.

2B: Thank you for this comment. We will modify our tone in the revision. We actually value Liang et al.(2021)'s work tremendously which is why we compare with it almost exclusively in the experiments.

2C: Despite that both approaches model sentence completion as part of the optimization, the specific optimization goals are different. In their work, they use a reinforcement learning framework to optimize task-specific rewards in combination with "sentence completion". Whereas our approach optimizes a classic auto-regressive decoding loss, which makes our approach easier to implement and more general.

Specifically, their approach uses the prefix tokens to control the amount of freedom in generating the remaining tokens by inference, while ours uses the prefix tokens to control the difficulty in predicting the target tokens by teacher forcing (the target tokens are the tokens after the prefix in the reference, see Fig. 2). 

---

Thank you for your precious comments and support. We will follow your advice to improve the paper.





%%%
We will rewrite this part in following aspects: 
* Remove the content from line 71 to 79 and replace it by a comparison of differences between our approach and Liang et al.(2021)'s work.
* Add results from Sec. 3.2 to Sec. 3.4 to support the claim that our approach outperforms the baseline on a range of tasks. 

So, they started with a fine-tuned model and then allowed the model to generate freely with a shrinking prefix to adapt the negative log-likelihood loss(NLL) to RL rewards.

Our work is different from this paper in three aspects:

First, 

their motivation is to effectively search the large action space of reinforcement learning(RL). However, our approach aims at finding a more effective learning algorithm. The intuition for our curriculum learning is that when more information is provided, it will be easier for the model to predict the target tokens(the tokens after the prefix in the reference).
Our work can provide a better initialization model for their approach.

Second, their RL approach required elaborately human-designed task-specific rewards. Differently, our work aims at designing a better learning strategy with strong generalization ability among different tasks and without human labor.

Third, specifically, they computed the loss from the prefix tokens, while we computed the loss from the target tokens. 



So, even though the surface of both approaches is "curriculum learning as sentence completion", the motivation and the implementation are quite different. We will add this paper in the related work.

3A: Due to the limited computational resources, we are currently unable to do a more head-to-head comparison with Liang et al.(2021) as explained in Limitations. We will try to do it if possible in the future.
