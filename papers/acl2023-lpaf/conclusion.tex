\section{Conclusion}
We discover that the full-rankness of fine-tuned PLMs is the fundamental bottleneck for the failure of matrix factorization. As a remedy, we employ first-order unstructured pruning to extract the low-rank subnetwork for further factorization. We then propose sparsity-aware SVD and mixed-rank fine-tuning as two optimizations to boost the compression performance. Thorough experiments demonstrate that LPAF can achieve better accuracy-compression trade-offs against existing approaches.


\section*{Limitations}
% \subsection*{Extra Overhead}
LPAF bears certain extra training overhead compared to vanilla fine-tuning. Specifically, during first-order pruning procedure, at certain iterations we need to rank all model parameters according to their importance 
scores. Taking the soring time into account, the pruning process takes about 1.15x times compared to fine-tuning. In the last stage of LPAF, we perform mixed-rank fine-tuning as an effective regularization scheme to 
facilitate the generalization ability of the model being compressed. Because tach mini-batch data samples will be fed to the model twice, LPAF takes approximatedly 1.4x memory and 1.3x time v.s. vanilla fine-tuning. Nonetheless, we believe it is worthwhile since we only need to do it once and the compressed model can be deployed anywhere needed.

