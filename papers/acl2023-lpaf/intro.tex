\section{Introduction}

Transformer-based~\cite{transformer} pre-trained language models~(PLMs)~\cite{bert,roberta} have shown superb performance on a variety of natural language processing tasks. These models are heavily over-parametrized~\cite{overpara} as they usually contain hundreds of millions of parameters, placing a severe burden on local storage, network transferring, runtime memory, and computation cost. Due to this disadvantage, the application of PLMs in low-resource scenarios is limited.

To alleviate this problem, recent studies~\cite{l0,svd} have attempted to compress 
PLMs by exploring and reducing the parameter redundancy in the weight matrices. Matrix factorization~(MF) , originated from linear algebra and matrix theory, is leveraged by modern deep learning towards achieving parameter efficiency. It works by decomposing large matrices into smaller sub-matrices with structural properties. The factorized sub-matrices serve as approximations of the original matrices while having fewer parameters.  \citet{svd} employ singular value decomposition~(SVD) for BERT compression with 2x compression rate and show 5\% drop in average GLUE~\cite{glue} performance compared to full BERT. The degradation is more evident under high compression rates~(\secref{sec:pilot_results}). Through a preliminary study, we identify the reason for the unsatisfactory performance of MF to be the \textit{full-rankness} of a fine-tuned language model. It inevitably causes information loss during the factorization process since the rank of sub-matrices has to be significantly smaller than the fine-tuned model to achieve parameter compression.


In an attempt to address this limitation of matrix factorization, we first explore the effect of network sparsification to produce subnetworks with the majority of weights set to zero. Ideally, we expect the subnetwork to contain low-rank sparse weight matrices and meanwhile preserve useful information for the end task. 
To this end, we conduct a systematic investigation into unstructured pruning~(UP) to study whether the resulting subnetworks exhibit the desirable low-rank property. From our experiments, we make the following important observations: (1) zero-order UP that only considers weight magnitude as pruning criterion produces subnetworks as full-rank as fine-tuned models; (2) first-order UP that incorporates gradient information into pruning decision is able to identify subnetworks that are both accurate and low-rank.

The above  findings motivate us to further explore the possibility of improving MF with UP.  Specifically, we design a sequential framework in which the first-order UP is executed prior to MF. In this way, the accurate low-rank 
subnetworks can be exploited by MF with minimal accuracy degradation while 
enjoying parameter and computation efficiency.

Moreover, we noticed that the vanilla SVD is not designed for 
sparse matrices because it penalizes the reconstruction error of 
each parameter equally~\cite{group}. Also, due to the reduced capacity, 
the joint re-training of low-rank sub-matrices may converge to 
solutions with lower generalization ability. To address the first problem, 
we propose sparsity-aware SVD, a weighted variant of SVD that 
better reconstructs unpruned~(hence more important) parameters. 
To address the second problem, we introduce mixed-rank fine-tuning, 
a regularized training scheme where the low-rank sub-matrices 
are randomly replaced with the sparse matrix from which they are factorized. Our contributions are  as follows:
\begin{itemize}
	\item Through a comprehensive preliminary study, 
	we discover a low-rank phenomenon in models obtained by first-order UP, 
	which highlights the possibility of a more efficient parametrization 
	of low-rank sparse matrices using low-rank factorization.
	\item Based on our findings, we design a sequential framework named  \textbf{L}ow-rank \textbf{P}rung-\textbf{A}nd-\textbf{F}actorize(LPAF) which makes high compression rate using matrix factorization possible.
	As further optimizations, we propose \textit{sparsity-aware SVD} which 
	prioritizes reconstruction of unpruned weights at initialization, 
	and \textit{mixed-rank fine-tuning} to compensate for the reduced capacity 
	during training.
	\item Comprehensive experiments on GLUE and 
	question-answering tasks show that our approach can achieve 
	a 2x-6x reduction in model size and FLOPs while retaining 99.8\%-96.2\% performance of the original BERT.
\end{itemize}
