\section{Proposed Framework}

% \subsection{Overview}
$\forall i \in [1,n]$ domains, given a taxonomy $G_i$ with depth of $d_i$ and $m$ leaf nodes, the path from root to leaf node forms the text which is regarded as hierarchical category label $y_i^{(j)}(j\in [1,m])$. For an input product title $X_i$ along with its meta concept labels $\{\lambda_k\}$, our task is to 
% choose the category with highest matching score. 
output the correct category label it belongs to. Note that only one leaf category will be the correct answer.
Detailed task formulation refers to Appendix~\ref{appdix:task}.

Our $\mathsf{TaLR}$ framework is structured into two stages: \textit{Retrieval} and \textit{Reranking}, as illustrated in \figref{fig:pipeline}.
%  The \textit{Retrieval} stage contains a dense scorer and a mapping scorer to first retrieve possible candidates from one certain hierarchical category set. At \textit{Reranking} stage, we use the pair-wisely matching scorer with contrastive information to score each candidate and predict the final category. 
We will zoom into each component of this framework.

\subsection{Dense Scorer}
% In this part, we introduce the vector-based retrieval unit using one-vs-all vector similarity measurement. Once the similarity scoring model is well trained, we can subsequently retrieve candidate labels $\{y_i\}$ from a certain taxonomy $G_i$. Before this, we prepare to construct the training samples $\mathcal{S}$ from multiple taxonomies. For brevity, we ignore the subscript $i$ when discussing the samples for a particular $G_i$.
We first train a dual-encoder to represent both categories and product titles in the vector space. 
% We then separately encode them and 
% Dense scorer is to compute the similarity of vectors.
\paragraph{Negative sampling}
\label{sec:prepare}
In the original text classification problem, each product title $X_i$ has exactly one positive category label $y_i$.
However in our reformulation, text relevance matching models need negative category labels during training, otherwise they would not succesfully converge.
For each $(X_i, y_i)$ pair, we prepare to construct the training examples $\mathcal{S}$ from multiple taxonomies by sampling $(N-1)$ negative categories. Instead of randomly chosen, ``hard'' negative examples are more informative for better convergence. Inspired by teacher-student paradigm~\cite{hinton2015distilling}, we adopt a teacher classifier-based sampling strategy to sample strong negative categories for dual-encoder learning.
% \KZ{Here we keep saying sample, sample. Are these the same as the same in Fig. 1?}

For each training dataset $S_i$ of taxonomy $G_i$, 
we split it in $k$-fold manner, then take turns to train $k$ BERT classifiers on every $\frac{k-1}{k}$ data 
% as a brand new training set
, with the remain $\frac{1}{k}$ data as the development set. The $m$-class classifiers are optimized with the typical $m$-class cross-entropy loss. 
The $k$ classifiers would inference $(N-1)$ most possible but not correct category labels concurrently in their corresponding development sets, and their results with ground truth positive labels constitutes the point-wise training set for the following dual-encoder training.
% the overall $k$ classifiers are able to construct the full-sized training set for the following encoder model.
\paragraph{Dual-encoder training}
We adopt a siamese network architecture \cite{reimers2019sentence} where the encoder respectively extracts the fixed-sized embeddings of product titles $X_i$ and category names $\hat{y_i}$ which are denoted as $\mathbf{u}_x$ and $\mathbf{v}_y$
% , and this siamese encoder shares weights from both sides
. 
To better align the embedding of $\mathbf{u}_x$ and $\mathbf{v}_y$, we use Circle Loss~\cite{sun2020circle} which allows each similarity score to optimize at its own pace. We simplify it as:
% \footnote{This name is after\href{https://kexue.fm/archives/8847}{https://kexue.fm/archives/8847}}
\begin{equation}
    \label{eq:loss}
    \mathcal{L}=\log \left(1+\sum_{S} e^{\alpha (cos(\mathbf{u}_x^+,\mathbf{v}_y^+)-cos(\mathbf{u}_x^-,\mathbf{v}_y^-))}\right),
\end{equation}
where $\alpha$ is the hyper-parameter, and $+,-$ denotes the positive and negative samples in $\mathcal{S}$ respectively. We also compare this loss function with other alternatives in Appendix~\ref{sec:appendix-dense}.
\paragraph{Candidates retrieval}
% The trained similarity scoring engine is hence capable of encoding both product titles and category labels into embedding vectors during inference period. Moreover, the pre-defined category names 
% that are emerging relatively less rapidly than the product itself
% can be cached in memory in advance, and it is especially favorable when the label space is extremely large so as to limit the run-time overhead.
We can quickly derive relevant category label embeddings given an incoming product title embedding, with one-vs-all similarity measurement like cosine-similarity
% or Manhattan / Euclidean distance. 
implemented by Approximate Nearest Neighbor (ANN) techniques targeting time efficiency.
% and they are powerful with multiple computing units. 
Based on this, we can readily collect top-$k$ candidate list $C_{vec}$. 
% retrieved from the whole label set.
\subsection{Mapping Scorer}
\label{sec:mapping}
Dense scorer usually prioritizes semantic relatedness of literal expressions, neglecting the commonsense co-occurrence probability that lies within cross-domain training data. For example, \textit{``Sunrise Roses 500g''} is often recognized as [$\mathtt{Flower}$] by semantic matching algorithms, however, it is actually a variety of [$\mathtt{Grape}$]. Therefore we introduce a mapping scorer in \textit{Retrieval} stage capturing such commonsense knowledge to complement the above dense-retrieved candidates.

\paragraph{Mapping algorithm}
The shared meta concept set $\mathcal{M}$ is constructed by hybrid NER-related techniques. 
% Details are given in \secref{sec: datasetdetails}.
Details are in Appendix~\ref{sec:datasetdetails}.
% the next Dataset Section.
We can regard ``meta concept'' as a kind of keyword knowledge because they usually contain very concrete and accurate information. In our released datasets, one product title $X$ is tagged with one or more meta concepts $\Lambda=\{\lambda_1, \lambda_2, ... \lambda_k\}$ from $\mathcal{M}$. For example, \textit{``Haagen-Dazs Red Wine Flavor Ice Cream''} is tagged with $\left\langle \mathtt{Red Wine}\right\rangle$, $\left\langle \mathtt{Ice cream}\right\rangle$, $\left\langle \mathtt{Haagen Dazs}\right\rangle$ as meta concepts.

Given product title $X$ and a category label $\hat{y}$, our heuristic strategy establishes $X \rightarrow \hat{y}$ mapping as conditional co-occurrence probability $P(\hat{y} | X)$. 
% The tagging step $X \rightarrow \{\lambda_1, \lambda_2, ... \lambda_k\}$ is accomplished by an industrial Label Tagging System that exploits hybrid approaches including text sequence labeling, classification, literal matching and some expert-defined rules.
% mapping the tagged meta concepts $\Lambda$ to corresponding taxonomy node $\hat{y}$. 
First, we model this conditional probability for each category $\hat{y}$ as:
\begin{equation}
    \begin{aligned}
    P(\hat{y} | X) &= P(\hat{y} \space | \space \lambda_1, \lambda_2, ... \lambda_k) \\ &= \max_{1 \leq i \leq k} P(\hat{y} \space | \space \lambda_i).
    \end{aligned}
\end{equation}
Here we aggregate $P(\hat{y} \space | \space \lambda_1, \lambda_2, ... \lambda_k)$ with the maximum value among multiple $\lambda_i$ referring to the same $\hat{y}$. Each $P(\hat{y} \space | \space \lambda_i)$ is collected from training data distributions:
\begin{equation}
P(\hat{y} \space | \space \lambda_i) = \frac{P(\hat{y} \space , \space \lambda_i)}{P({\lambda_i})} = \frac{\nu(\hat{y} \space , \space \lambda_i)}{\nu({\lambda_i})},
\end{equation}
where $\nu$ denotes the frequency in training data. Then, we collect candidate list ${C_{rule}}$ by empirically setting a threshold of $P(\hat{y} | X) > 0.5$ to ensure both retrieval quantity and quality. 
% We maintains a dictionary of the mapping probability $P(\hat{y} \space | \space \lambda_i)$  for each taxonomy classification task, 
% \subsection{Candidates Fusion Strategy}
% When retrieved candidates from the vector-based and rule-based components are prepared, we propose three fusion strategies to combine two lists of candidates. 
% Assume that we only requires approximate 10 candidate categories for each product title to reduce time consumption in \textit{Reranking} stage. For a given product title $X$ and its corresponding candidate lists ${C_{vec}}$ from the vector-based unit and ${C_{rule}}$ from the rule-based unit sorted in probability descending order:

% \noindent\textbf{De-Dupli} picks at most 6 top candidates from both ${C_{vec}}$ and ${C_{rule}}$, then removes 
% duplicate candidates
% % candidates from these 12 candidates 
% to form $C_{union}$.

% \noindent\textbf{Norm\&Rank} normalizes the probability of candidates in each list respectively and merge them into one list keeping in probability descending order, then picks the top-$10$ candidates as $C_{union}$.

\paragraph{Candidates merging} 
When retrieved candidates from the dense scorer and mapping scorer are prepared, we need to combine the two lists of candidates. Our concept-first strategy prioritizes candidates from ${C_{rule}}$. It puts at most 10 top candidates (usually less than 10) from ${C_{rule}}$ into $C_{union}$, then keeps filling it with top candidates from $C_{vec}$ 
% as long as the size of $C_{union}$ does not reach to 10. 
until its size reaches 10.

% Comparison of these strategies is listed in \tabref{tb:fusion}. \KZ{It's a bit weird to refer to a table that comes so late in the paper.}

\subsection{Matching Scorer}
\label{matching-scorer}
% \paragraph{Training procedure}
To further measure the relatedness of product titles and category names with mutual interactions, we train a matching scorer in \textit{Reranking} stage.
During training, given a product title $X$ and its retrieved candidates $C_{union}=\{c_1, c_2, ... c_l\}$, we concatenate tokenized sequences of $X$ and each of these $c_i \in C_{union}$ with a [$\mathtt{SEP}$] token as the input to BERT-based model. 
% The encoded [$\mathtt{CLS}$] token is followed by a fully-connected layer to output a similarity score $\delta$ and to decide whether these two sentences match or not. 
The ground truth label is 1 if $c_i$ is the correct candidate otherwise 0. Optimization is followed with binary cross-entropy loss.
% \paragraph{Inference procedure}
During inference, the model gives similarity scores for each ($X$, $c_i$) pair, and the candidate with the highest similarity score would be our predicted category.

\subsection{Contrastive Pretraining}
\label{sec:contrastive}
% We take advantage of the contrastive information from both inter-concept and intra-concept of product titles to exploit the mutual interactions between the product title and label text pairs.

For \textbf{multi-domain taxonomies}, category classes vary from one taxonomy to another. 
% Considering this, 
Despite the assorted expressions of category classes among different domain taxonomies, we find their fine-grained concepts of products seldom shift. 
While previous retrieval stage pursues the recall of candidates and focuses less on class discrimination, the cross-encoder in \textit{Reranking} stage possibly suffers from indistinguishable categories.
% Therefore we consider to utilize such a domain independent property.
Inspired by the 
% contrastive learning within groups
supervised derivative of contrastive learning~\cite{wang2021self}, 
% where they generally regard all samples from the same class $y$ as positive samples while samples from other classes are treated as negative, we make a stronger assumption for positive pairs in our problem.
% More specifically, 
we restrict the formation of positive pairs ensuring they not only share the same category class with $X$ but also have at least one meta concept in common with $X$, otherwise they would be considered negative. 
This setting is tailored for the \textbf{multi-domain taxonomies} challenge pursuing cross-domain alignment and uniformity, 
% taxonomies from different domains to make products sharing the same class and concept strongly tied together in their encoded semantic embeddings. 
where inter-concept semantics are tied closer and intra-concept ones are further distinguished.

Given a product title $X$ with label $y$ and tagged meta concept set $\Lambda$, we encode $X$ as vector $\mathbf{u}$ and group encoded product titles as positive vector samples 
$\{\mathbf{v}_1^{y,\Lambda_1}, \mathbf{v}_2^{y,\Lambda_2}, ..., \mathbf{v}_D^{y,\Lambda_D}\}$, which are labeled with the same $y$ and share an overlapped concept set $\Lambda_d$ with $\Lambda$. 
We use BERT as the encoder backbone and tune its parameters with group contrast loss: 
\begin{equation}
    \begin{aligned}
    \mathcal{L}^{GC}&=-\frac{1}{D}\sum_{d=1}^{D}{\log\frac{\exp(\mathbf{u}\cdot\mathbf{v}_d^{y,\Lambda_d}/\tau)}{Pos+Neg}}. \\
    Pos&=\sum_{d=1}^{D}{\exp(\mathbf{u}\cdot\mathbf{v}_d^{y,\Lambda_d}/\tau)}, \\
    Neg&=\sum_{y^{\prime},\Lambda^{\prime}}^D
    {\exp(\mathbf{u}\cdot\mathbf{v}^{y^{\prime},\Lambda^{\prime}}/\tau)},\\
    \end{aligned}
\end{equation}
where $y^{\prime},\Lambda^{\prime}$ denotes samples with either different label $y^{\prime}$ with $y$ or non-overlapping meta concept set $\Lambda^{\prime}$ with $\Lambda$. The BERT model after contrastive pretraining can be used in matching scorer during \textit{Reranking} stage in Section~\ref{matching-scorer}.


