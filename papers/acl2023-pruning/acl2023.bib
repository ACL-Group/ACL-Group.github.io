@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{gpt2,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@article{minilm,
  author    = {Wenhui Wang and
               Furu Wei and
               Li Dong and
               Hangbo Bao and
               Nan Yang and
               Ming Zhou},
  title     = {MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression
               of Pre-Trained Transformers},
  journal   = {CoRR},
  volume    = {abs/2002.10957},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.10957},
  eprinttype = {arXiv},
  eprint    = {2002.10957},
  timestamp = {Tue, 03 Mar 2020 14:32:13 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-10957.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{electra,
  title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  year={2020},
  url={https://arxiv.org/abs/2003.10555},
}

@inproceedings{webnlg,
    title = "{W}eb{NLG} Challenge 2020: Language Agnostic Delexicalisation for Multilingual {RDF}-to-text generation",
    author = "Zhou, Giulio  and
      Lampouras, Gerasimos",
    booktitle = "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)",
    month = "12",
    year = "2020",
    address = "Dublin, Ireland (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.webnlg-1.22",
    pages = "186--191",
    abstract = "This paper presents our submission to the WebNLG Challenge 2020 for the English and Russian RDF-to-text generation tasks. Our first of three submissions is based on Language Agnostic Delexicalisation, a novel delexicalisation method that match values in the input to their occurrences in the corresponding text through comparison of pretrained multilingual embeddings, and employs a character-level post-editing model to inflect words in their correct form during relexicalisation. Our second submission forfeits delexicalisation and uses SentencePiece subwords as basic units. Our third submission combines the previous two by alternating between the output of the delexicalisation-based system when the input contains unseen entities and/or properties and the output of the SentencePiece-based system when the input is seen during training.",
}

@article{squad,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  journal   = {CoRR},
  volume    = {abs/1606.05250},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.05250},
  eprinttype = {arXiv},
  eprint    = {1606.05250},
  timestamp = {Mon, 24 Aug 2020 14:01:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RajpurkarZLL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1804.07461},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.07461},
  eprinttype = {arXiv},
  eprint    = {1804.07461},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{e2e,
  title = {Evaluating the {{State}}-of-the-{{Art}} of {{End}}-to-{{End Natural Language Generation}}: {{The E2E NLG Challenge}}},
  author = {Du{\v{s}}ek, Ond\v{r}ej and Novikova, Jekaterina and Rieser, Verena},
  year = {2020},
  month = jan,
  volume = {59},
  pages = {123--156},
  doi = {10.1016/j.csl.2019.06.009},
  archivePrefix = {arXiv},
  eprint = {1901.11528},
  eprinttype = {arxiv},
  journal = {Computer Speech \& Language}
}

@article{dart,
  title={DART: Open-Domain Structured Data Record to Text Generation},
  author={Dragomir Radev and Rui Zhang and Amrit Rau and Abhinand Sivaprasad and Chiachun Hsieh and Nazneen Fatema Rajani and Xiangru Tang and Aadit Vyas and Neha Verma and Pranav Krishna and Yangxiaokang Liu and Nadia Irwanto and Jessica Pan and Faiaz Rahman and Ahmad Zaidi and Murori Mutuma and Yasin Tarabar and Ankit Gupta and Tao Yu and Yi Chern Tan and Xi Victoria Lin and Caiming Xiong and Richard Socher},
  journal={arXiv preprint arXiv:2007.02871},
  year={2020}
  }

@article{pkd,
  author    = {Siqi Sun and
               Yu Cheng and
               Zhe Gan and
               Jingjing Liu},
  title     = {Patient Knowledge Distillation for {BERT} Model Compression},
  journal   = {CoRR},
  volume    = {abs/1908.09355},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09355},
  eprinttype = {arXiv},
  eprint    = {1908.09355},
  timestamp = {Fri, 04 Sep 2020 16:10:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09355.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{svd,
    title = "Compressing Pre-trained Language Models by Matrix Decomposition",
    author = "Ben Noach, Matan  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.88",
    pages = "884--889",
    abstract = "Large pre-trained language models reach state-of-the-art results on many different NLP tasks when fine-tuned individually; They also come with a significant memory and computational requirements, calling for methods to reduce model sizes (green AI). We propose a two-stage model-compression method to reduce a model{'}s inference time cost. We first decompose the matrices in the model into smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of reducing the number of parameters while preserving much of the information within the model. We experimented on BERT-base model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4x, and increase inference speed by a factor of 1.45x, while maintaining a minimal loss in metric performance.",
}

@article{gupta,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@article{mag2,
  author    = {Alex Renda and
               Jonathan Frankle and
               Michael Carbin},
  title     = {Comparing Rewinding and Fine-tuning in Neural Network Pruning},
  journal   = {CoRR},
  volume    = {abs/2003.02389},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.02389},
  eprinttype = {arXiv},
  eprint    = {2003.02389},
  timestamp = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-02389.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{movement,
 author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20378--20389},
 publisher = {Curran Associates, Inc.},
 title = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
 url = {https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{superticket,
    title = "Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization",
    author = "Liang, Chen  and
      Zuo, Simiao  and
      Chen, Minshuo  and
      Jiang, Haoming  and
      Liu, Xiaodong  and
      He, Pengcheng  and
      Zhao, Tuo  and
      Chen, Weizhu",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.510",
    doi = "10.18653/v1/2021.acl-long.510",
    pages = "6524--6538",
    abstract = "The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of {''}lottery tickets{''}, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as {''}winning tickets{''}, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as {''}super tickets{''}. We further show that the phase transition is task and model dependent {---} as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.",
}

@inproceedings{platon,
  title={Platon: Pruning large transformer models with upper confidence bound of weight importance},
  author={Zhang, Qingru and Zuo, Simiao and Liang, Chen and Bukharin, Alexander and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={26809--26823},
  year={2022},
  organization={PMLR}
}

@article{l0,
  title={Learning sparse neural networks through $ L\_0 $ regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1712.01312},
  year={2018}
}

@InProceedings{ilp1,
author="Marinescu, Radu
and Dechter, Rina",
editor="Van Hentenryck, Pascal
and Wolsey, Laurence",
title="Best-First AND/OR Search for 0/1 Integer Programming",
booktitle="Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="171--185",
abstract="AND/OR search spaces are a unifying paradigm for advanced algorithmic schemes for graphical models. The main virtue of this representation is its sensitivity to the structure of the model, which can translate into exponential time savings for search algorithms. In this paper we introduce an AND/OR search algorithm that explores a context-minimal AND/OR search graph in a best-first manner for solving 0/1 Integer Linear Programs (0/1 ILP). We also extend to the 0/1 ILP domain the depth-first AND/OR Branch-and-Bound search with caching algorithm which was recently proposed by [1] for solving optimization tasks in graphical models. The effectiveness of the best-first AND/OR search approach compared to depth-first AND/OR Branch-and-Bound search is demonstrated on a variety of benchmarks for 0/1 ILPs, including instances from the MIPLIB library, real-world combinatorial auctions, random uncapacitated warehouse location problems and MAX-SAT instances.",
isbn="978-3-540-72397-4"
}


@inproceedings{conll2003,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://www.aclweb.org/anthology/W03-0419",
    pages = "142--147",
}

@InProceedings{deepsparse, 
    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, 
    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, 
    booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
    pages = {5533--5543}, 
    year = {2020}, 
    editor = {Hal Daum√© III and Aarti Singh}, 
    volume = {119}, 
    series = {Proceedings of Machine Learning Research}, 
    address = {Virtual}, 
    month = {13--18 Jul}, 
    publisher = {PMLR}, 
    pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},
    url = {http://proceedings.mlr.press/v119/kurtz20a.html}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{oneshot1,
  title={SNIP: SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@article{oneshot2,
  author    = {Jonathan Frankle and
               Michael Carbin},
  title     = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1803.03635},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.03635},
  eprinttype = {arXiv},
  eprint    = {1803.03635},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-03635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{eigen,
  author    = {Sheng Shen and
               Zhen Dong and
               Jiayu Ye and
               Linjian Ma and
               Zhewei Yao and
               Amir Gholami and
               Michael W. Mahoney and
               Kurt Keutzer},
  title     = {{Q-BERT:} Hessian Based Ultra Low Precision Quantization of {BERT}},
  journal   = {CoRR},
  volume    = {abs/1909.05840},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.05840},
  eprinttype = {arXiv},
  eprint    = {1909.05840},
  timestamp = {Wed, 18 Sep 2019 10:38:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-05840.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{difficulty1,
  title={Unifying distillation and privileged information},
  author={Lopez-Paz, David and Bottou, L{\'e}on and Sch{\"o}lkopf, Bernhard and Vapnik, Vladimir},
  journal={arXiv preprint arXiv:1511.03643},
  year={2015}
}

@article{difficulty2,
  author    = {Seyed{-}Iman Mirzadeh and
               Mehrdad Farajtabar and
               Ang Li and
               Hassan Ghasemzadeh},
  title     = {Improved Knowledge Distillation via Teacher Assistant: Bridging the
               Gap Between Student and Teacher},
  journal   = {CoRR},
  volume    = {abs/1902.03393},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.03393},
  eprinttype = {arXiv},
  eprint    = {1902.03393},
  timestamp = {Mon, 28 Dec 2020 07:43:38 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-03393.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pst,
  title     = {Parameter-Efficient Sparsity for Large Language Models Fine-Tuning},
  author    = {Li, Yuchao and Luo, Fuli and Tan, Chuanqi and Wang, Mengdi and Huang, Songfang and Li, Shen and Bai, Junjie},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {4223--4229},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/586},
  url       = {https://doi.org/10.24963/ijcai.2022/586},
}

@inproceedings{intrinsicdimension,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.568",
    doi = "10.18653/v1/2021.acl-long.568",
    pages = "7319--7328",
    abstract = "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90{\%} of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
}

@inproceedings{compressionbounds,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}


@article{deepsparse2,
  author    = {Eugenia Iofinova and
               Alexandra Peste and
               Mark Kurtz and
               Dan Alistarh},
  title     = {How Well Do Sparse Imagenet Models Transfer?},
  journal   = {CoRR},
  volume    = {abs/2111.13445},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.13445},
  eprinttype = {arXiv},
  eprint    = {2111.13445},
  timestamp = {Wed, 01 Dec 2021 15:16:43 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-13445.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{albert,
  added-at = {2021-05-21T12:19:07.000+0200},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  biburl = {https://www.bibsonomy.org/bibtex/289f1672459805fbdb093117ac4905028/lea-w},
  booktitle = {ICLR},
  ee = {https://openreview.net/forum?id=H1eA7AEtvS},
  interhash = {7bd1a54a73ac5dfa6984702365d9f2a9},
  intrahash = {89f1672459805fbdb093117ac4905028},
  keywords = {},
  publisher = {OpenReview.net},
  timestamp = {2021-05-21T12:19:07.000+0200},
  title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2020.html#LanCGGSS20},
  year = 2020
}

@book{vc,
  author    = {Vladimir Vapnik},
  title     = {Statistical learning theory},
  publisher = {Wiley},
  year      = {1998},
  isbn      = {978-0-471-03003-4},
  timestamp = {Thu, 21 Apr 2011 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0097035.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Fixing Weight Decay Regularization in Adam},
  journal      = {CoRR},
  volume       = {abs/1711.05101},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.05101},
  eprinttype    = {arXiv},
  eprint       = {1711.05101},
  timestamp    = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
