\section{Introduction}



Pre-trained language models~(PLMs)~\cite{bert,gpt2} have significantly advanced the state-of-the-art in various natural language processing tasks~\cite{glue,webnlg,e2e,dart}. However, these models often contain a vast amount of parameters, posing non-trivial requirements for storage and computation. Due to this inefficiency, the applications of PLMs in resource-constrained scenarios are still limited.




To resolve the above challenge, model compression~\cite{pkd,svd,albert} has been actively studied to make PLMs meet the practical requirement. Among them, iterative pruning methods are widely adopted at only a tiny expense of model performance when adapting PLMs to downstream tasks. 
%These methods remove redundant parameters while updating remaining parameters using specific importance criteria to reduce model size effectively. 
During the course of iterative pruning, model parameters can not only be updated but also be pruned based on the rank of their importance scores in order to satisfy the cardinality constraint.
Prevalent importance criteria are based on the parameter's magnitude~\cite{gupta,mag2} or sensitivity~\cite{l0,movement,superticket,platon}. Parameters with low importance scores are pruned and are expected to have little impact on model performance.






Despite the empirical success, existing importance criteria for model pruning still face two major limitations: (1) they are heuristically defined and may not accurately quantify a parameter's contribution to the learning process, e.g., absolute weight value in magnitude-based pruning and gradient-weight product in sensitivity-based pruning; (2) they determine the importance of each parameter individually without considering the effect of coinstantaneous parameter updates on model performance, e.g., sensitivity is estimated by the absolute change in training error if only a single parameter is pruned and others remain unchanged.

%Though effective, such importance criteria are heuristically defined and may not accurately quantify a parameter's contribution to the learning process: (1) magnitude-based criteria measure the importance of parameters by their absolute values. However, even parameters with small magnitude can significantly impact model performance due to the complex compositional structure of neural language models; (2) Sensitivity-based criteria estimate the importance of a parameter by the absolute change in training error if only that parameter is pruned and others remain unchanged. While taking the learning objective into account, such an approach still determines the importance of each parameter individually without considering the effect of coinstantaneous parameter updates on model performance. 







In this paper, we rethink the design of the importance criterion for model pruning from an optimization perspective. We begin by analyzing the temporal variation of any given learning objective based on a single-step gradient descent update under the iterative pruning setting. We show that finding the optimal pruning decision can be framed as solving an equality-constrained 0-1 Integer Linear Programming~(ILP) problem, where the constraint is defined by the specified sparsity. The resulting problem is a particular case of a general 0-1 Knapsack problem in which the weight for each item is the same. The solution to this problem naturally leads to a principled importance criterion which we use to rank all model parameters and derive the optimal stepwise pruning decision.



When a high sparsity~(e.g., 80\%$\sim$90\%) is pursued, the limited capacity often renders the pruned model fails to retain satisfactory performance with conventional fine-tuning. To further improve the model's generalization ability, we propose a self-regularization scheme, where the model prediction is regularized by the latest best-performing model checkpoint during pruning. We show that such a scheme eases model learning with decreasing capacity and effectively yields a tighter upper bound of expected generalization error than learning from training data alone.



To validate the effectiveness of our approach, dubbed PINS~(\underline{P}runing with principled \underline{I}mportance a\underline{N}d \underline{S}elf-regularization), we conducted extensive experiments with various pre-trained language models on a wide variety of tasks, including natural language understanding on GLUE~\cite{glue}), question answering on SQuAD~\cite{squad}, named entity recognition on CoNLL 2003~\cite{conll2003}, and data-to-text generation on WebNLG~\cite{webnlg}, DART~\cite{dart}, and E2E~\cite{e2e}. Experimental results show that PINS provides more accurate models at different sparsity levels. Detailed analysis shed further light on some intriguing properties of models pruned by PINS. By exploiting the resulting high sparsity, we show that the storage/inference can be reduced/accelerated by 8.9x and 2.7x using CSR format and a sparsity-aware inference runtime~\cite{deepsparse} on consumer-level CPUs~\footnote{Code available at \url{https://github.com/DRSY/PINS}}.



In summary, our contributions are:

\begin{itemize}
	
	\item We establish the equivalence between the optimal pruning decision and the solution to an equality-constrained 0-1 Integer Linear Programming problem. The solution to this problem leads to a principled importance criterion that can be used to rank parameters during iterative pruning.

\item We propose a simple yet effective self-regularization scheme to enhance the model's generalization capability, especially under a high-sparsity regime.

\item Comprehensive experiments and analyses confirm the effectiveness of our approach at various sparsity levels. 
	
\end{itemize}
