# Reviewer 1
Response to Question A:
    Please note that during iterative pruning at a specific time step, each parameter can only be in one of two possible situations: (1) kept and updated by the optimizer, or (2) zeroed out. Therefore, a binary variable associated with each parameter is sufficient to express any pruning decision at that time step. If Equation 7 to Equation 8 is convincing to the Reviewer, then Equation 8 to Equation 9 should be straightforward.

Response to Question B:
    We would like to clarify that the learning speed alpha in Theorem 1 is NOT the commonly used "learning rate," but rather a factor negatively correlated to the intrinsic capacity gap [1, 2] between the learning machine (i.e., the model being pruned) and the target being learned. Without self-regularization, the learning target is the fixed empirical data distribution. As pruning progresses, the capacity of the learning machine decreases, which slows down the learning speed and loosens the generalization bound. 
With self-regularization, the learning target is dynamically set to the model being pruned itself with the lowest generalization error at several iterations before, where the capacity gap is much smaller and renders a tighter generalization bound.

Response to Question C:
  The primary objective of model compression is to create a final model that is as efficient as possible while maintaining performance. There are two commonly used methods for comparing two compression techniques: 
(1) which one produces a more efficient model with the same accuracy, or (2) which one produces a more accurate model with the same efficiency. In this paper, we adopt the second method, meaning we compare two techniques under the same sparsity level to determine which produces a more accurate model. We believe that enforcing an equal amount of training cost is unnecessary and not a common practice in prior work.

Response to "A part of the proposed method is not novel": 
    Although both our self-regularization method and the method presented in the CVPR paper are teacher-free, there are significant differences in methodology and motivation. Our self-regularization approach dynamically instantiates the learning target as the previous best-performing model checkpoint with increasing sparsity, aiming at reducing the capacity gap and achieving a tighter generalization error bound (as shown in Theorem 1). Our method is tailored for iterative pruning where the model's capacity is monotonically decreasing. 
In contrast, the method mentioned by the reviewer uses a fixed pre-trained student model as the learning target, which is sub-optimal for iterative pruning. The theoretical and empirical advantages of our self-regularization method are supported by Theorem 1 and Table 6.
Overall, we respectfully disagree with the reviewer's assertion that our proposed method is not novel. We believe that the differences between our self-regularization approach and the method presented in the CVPR paper are significant and provide clear advantages, as demonstrated in our results.

References:
[1]. Unifying Distillation and Privileged Information. ICLR 2016.
[2]. Statistical learning theory. 1998.



# Reviewer 2
Response to "what matrices are being factorized":
The results of w/o self-regularization on GLUE at 80% sparsity are shown below and will be included in the camera-ready version. PINS without self-regularization still consistently outperforms compared baselines(Table 1), which demonstrates the effectiveness of both the derived importance criteria and self-regularization.
|          Task               | RTE  | CoLA | MRPC | STS-B| SST-2| QNLI | MNLI | QQP  | Avg
| PINS w/o self-regularization| 70.9 | 55.4 | 90.6 | 89.1 | 91.7 | 90.7 | 83.6 | 90.8 | 82.9


Response to Question A: 
    We can reframe the parameter updates (delta theta) in Equation 11 as theta at time t+1 minus theta at time t. This results in a form that only requires the gradient at step t and parameters updated by any advanced optimizers, such as Adam, at step t+1. So in practice there is no need to take special consideration for learning rate. 
In other words, PINS is adaptive and agnostic to the specific updating rules of different optimizers.

Response to Question B: 
    The gradient used for Equation 11 is taken w.r.t the learning objective L that we specified for the model. This objective can be the naive empirical risk or other modified learning objectives, such as when the proposed self-regularization is added. Therefore, Equation 11 provides a principled criterion for iterative pruning against ANY learning objective that we want to optimize.



# Reviewer 3
Response to Question A: 
    We use the previous best-performing checkpoint w.r.t the dev set. This is not a problem because, in a standard train-dev-test settign the purpose of the dev set is to help us select the model with an estimated highest generalization performance. We then report the results on the test set to evaluate the model's performance. 
The only exception is GLUE: following prior work, we used a small split (10%) from the original training set as the "dev set" and treated the original dev set as the "test set."

Response to Question B: 
    The sparsity-aware inference runtime we used, DeepSparse, is fully open-sourced and available for anyone to achieve inference speed-up on various consumer-level CPUs. To use DeepSparse, one first convert your sparse model into the ONNX format, which is a standard network file format for storing model weights and other structural information. Once you have the ONNX model file, you can use DeepSparse to achieve inference acceleration by exploiting the model sparsity. We will 
release our implementation regarding the whole pipeline. For more details, please refer to Appendix E.