Review #1
What is this paper about and what contributions does it make?
establishing relation between 0-1 integer linear programming and pruning problem of a neural net and providing importance criterion as a solution based on the equality.
providing self-regularization using previous model checkpoint and its theoretical proof about why it enhances performance based on VC boundary.
empirical validation in various nlp tasks.

Reasons to accept
This paper is well-organized
The proposed method is effective to enhance most NLP task performance.
Some analysis for theoretical understanding of the self-regularization and the importance criterion is introduced.

Reasons to reject
Overall, the novelty of the paper and effectiveness has some flaws.
A part of the proposed method is not novel. e.g. Self-regularization and its positive effect to regularization is not a new.
"L. Yuan, F. E. Tay, G. Li, T. Wang, and J. Feng, "Revisiting knowledge distillation via label smoothing regularization,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 3903–3911.”

It's difficult to get clear evidence that the proposed criterion is effective for performance. (See the following questions.)
Questions for the Author(s)
Question A:
Criterion Deriving Eq.9 from Eq.8 has strong implicit assumption that a single parameter change rarely affects the values of the other parameters. However, pruning (masked as 0) may cause a big change and some subsets (e.g. weight parameters of a layer to generate the same pre-activation for its output node.) determine the pruning decision of another parameter. It seems likely that the relation holds strong dependency with the big change, but regarding the problem as the equal-weight 0-1 knapsack, the proposed solution is k-best selection of the additive terms (S_i^(t)) through greedy selection. Eq.7 to Eq.8 is persuasive to me, but Eq.8 to Eq.9 derivation seems to be risky and not yet analyzed.

Question B:
Self-Regularization In Theorem 1, the learning speed change is introduced as a key condition for the generalization bound reducing. However, in some experiments [refs], the regularization has not controlled this factor as the theorem, but they are strongly regularized. So, it seems that the generalization bound reduction explains only a part of generalization.
Simply, does the regularization effect disappear when the learning speed is a constant?

Question C:
Empirical Demonstration Pruning is a method for practical use in a limited computing environment, but self-regularization uses two models for regularization, which uses 2x larger parameters. For fair comparison, does it need to be compared with other methods using the same number of strongly activated parameter values? (e.g. 90% sparsity results of PINS vs 80% sparsity results with PLATON?).


Typos, Grammar, Style, and Presentation Improvements
.Table 7. needs comparison with other pruning methods. .8 page, Figure 4 -> Figure 2? .no standard deviation (some results seem incremental. To confirm significance, we need this result.) .Table 6. with no explanation about why only the three tasks are selected. (GLUE has more tasks and actually authors conducted all experiments. It needs explanation why the others are omitted. ) .Table 6. ablation study only shows the impact of self-regularization. The impact of the proposed criterion is not shown.
Soundness:	2
Excitement (Long paper):	3
Reviewer Confidence:	4
Recommendation for Best Paper Award:	No
Reproducibility:	3
Ethical Concerns:	No



Review #2
What is this paper about and what contributions does it make?
This paper studies the problem of iterative pruning for compressing large language models, which is an important problem that the community is looking into. Central to their proposed method is that the two steps of gradient update and pruning (Eq 1 and 2) at each iteration can be framed as an optimization problem, which takes both steps into account and actually enjoys a closed-form optimal solution.
Reasons to accept
The method seems simple and elegant, and it will be a nice addition to the current toolbox of iterative pruning for the community.

Comprehensive experiments are presented on different tasks and various sparsity levels to show the effectiveness of the proposed pruning strategy.

Reasons to reject
My main concern is that the ablation is not convincing enough to show that the criterion in Eq11, which is the main contribution of this work, is effective alone. By default, the authors seem to use the importance criteria along with self-regularization for most experiments. Though the ablation from Table 6 shows that self-regularization is clearly useful, it's however still unclear how useful the importance criteria alone would be. Basically, I would like to see evident to rule out the possibility that the performance gain mostly comes from the self-regularization.
Questions for the Author(s)
Question A: Is the criterion in Eq11 affected by the learning rate (because the first term depends on the learning rate)? If true, what would be the practical implication?
Question B: when self-regularization is added, what kind of gradient did you use for Eq11? Is the gradient of L_er or the joint loss of L_er + L_sr? If it is the latter case, I guess it will actually affect the optimization problem a bit, i.e., the result of Eq11 would be different. I wonder whether/how this decision would affect the performance.

Typos, Grammar, Style, and Presentation Improvements
The phrase "latest checkpoint” (e.g., line12 and pseudo-code) is misleading. Maybe "best performing checkpoint” is better.
Soundness:	3
Excitement (Long paper):	3.5
Reviewer Confidence:	4
Recommendation for Best Paper Award:	No
Reproducibility:	4
Ethical Concerns:	No



Review #3
What is this paper about and what contributions does it make?
The authors propose a integer linear programming based method for iterative pruning of large language models. The point of pruning is to compress large models and make them more efficient to run. The authors flag the heuristic definition of pruning criteria and the parameter independence assumption as issues with current approaches to model pruning. The ILP setup is achieved by assigning a binary variable to each parameter and searching for best solution which satisfies the sparsity constraint. After formulating as ILP, it ends up that the solution to the ILP problem can be found by simply sorting the parameters according to a new importance criterion, and keeping the top r parameters while zeroing the rest. Part of the success of the approach is based upon regularization of the loss with respect to earlier checkpoints.
Reasons to accept
The paper is clearly written and presents a nice formulation of the model pruning problem

the ILP approach is well motivated, should be straightforward to implement, and the authors show strong empirical results

Reasons to reject
A large part of the empirical results may be due to the self-regularization, which may allow information from the dev set to leak into the training process. More clarity is needed on exactly how this is implemented
Questions for the Author(s)
Question A: Is self-regularization implemented using the previous best performing checkpoint with respect to the dev set, or are checkpoints simply produced after a fixed number of training steps? If regularization is using the previous best checkpoint wrt a held-out set, it could be argued that the held-out set is being used for training, which could call some of the empirical results into question, please clarify this.
Question B: Please provide some more info on how you used this model with the sparsity-aware runtime -- is this something that others will be able to replicate?

Typos, Grammar, Style, and Presentation Improvements
233: Tylor —> Taylor Table 1 caption: BRET —> BERT

Soundness:	4
Excitement (Long paper):	4
Reviewer Confidence:	4
Recommendation for Best Paper Award:	No
Reproducibility:	3
Ethical Concerns:	No
