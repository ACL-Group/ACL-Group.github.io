\section{Introduction}

In multi-turn dialogues, 
%due to the availability of context, 
speakers naturally tend to 
make heavy use of references or omit complex discourses to save the efforts.
Thus natural language understanding models usually
%must connect 
need the dialogue history to understand the true meaning 
of the current utterance.
The existence of such incomplete utterances increases the difficulty of modeling dialogues.
%\KZ{This needs to be validated or at least put into context, because people will think that the model already
%has the whole context}. 
%\KZ{Redo all the double quotes! SHould be `` and ''!}

\begin{figure}[th]
        \centering
        \includegraphics[width=0.8\columnwidth]{rewrite-example.eps}
        \caption{An example of utterance rewriting. The phrase in the first red box is coreference, and the second is ellipsis.}
        \label{fig:rewrite-example}
\end{figure}

%\KZ{Try not to draw lines across text. And the words are too small.}

%\KZ{The absence of} 
The sources of incompleteness of an utterance can be divided into 
two categories: \textit{coreference} and \textit{ellipsis}. 
The task for solving these two kinds of incompleteness is called
Incomplete Utterance Rewriting (IUR). 
As shown in \figref{fig:rewrite-example}, the third utterance of this multi-turn dialogue is incomplete. If this utterance is taken out alone without 
a context, 
%the third party 
we will not be able to 
understand what ``one'' means and where to buy it. The fourth utterance is a rewriting of the third one. We can see that ``one'' in the third utterance is replaced by ``J.K. Rowling's new book''. In addition, the place adverbial ``from the book store in town'' is inserted after ``for me''. %These two examples show the coreference and ellipsis respectively within an incomplete utterance.
In today's industry strength dialogue systems and applications, 
due to stringent requirements on running time and maintenance cost, 
single-turn models are much more preferred
than multi-turn models.
If an incomplete single-turn utterance can be completed, 
it will be more understandable without the context, 
and the cost of downstream NLP tasks, such as intention extraction and 
response generation, will be reduced.

\figref{fig:rewrite-example} shows that that all the words added in 
the rewritten utterance except ``from'' come from the context. 
Inspired by this, many early rewriting works used 
pointer networks \citep{NIPS2015_29921001} or sequence to sequence models with copy 
mechanism \citep{gu-etal-2016-incorporating, see-etal-2017-get} to 
directly copy parts from the context into the target utterance. 
More recently, pre-trained language models such as T5 \citep{2020t5} 
succeeds %simply 
in many NLP tasks, and it appears that T5 is a plausible choice for
utterance rewriting as well.
%But when it is directly used to do IUR task, 
However, IUR task is different from other generation tasks 
in that new parts typically only need to be 
added in one or two specific locations in the original utterance.
That is, the changes to the utterance are localized.
For example, a typical operation is adding modifiers before or after 
a noun.
On the contrary, end-to-end text generation models such as T5 may 
not preserve the syntactic structure of the input, which
may cause the loss of important information
%may be lost 
and the introduction of wrong information into the output,
which is illustrated as below (Two examples are generated by T5.).

\begin{itemize}
\item Can you buy \textbf{J.K. Rowling's new book}? (Losing original structure)
\item Can you \textbf{publish} new book for me ? (Introducing wrong information)
\end{itemize}
%outside
%the positions to rewrite. 
%should be 
%\textcolor{green}{(Ruolan: Why?)}
%Preserving means that 
%IUR task is different from other generation tasks
%as the newly added parts usually 
%only appear in specific locations in the original utterance. 
%For example, adding modifiers before or after the central word.
%This particular characteristic
%makes rewriting task different from other generation tasks.
%\KZ{You need to set up the problem of large search space first by saying the previous approaches suffers from
%large search space because ... and this leads to bad... I also think that you haven't sufficiently
%demonstrated the badness of prev approaches. You can give some bad examples..}
%Instead, end-to-end pre-trained models would create something like these without original syntactic structure.
%\begin{itemize}
%\item Can you buy \textbf{J.K. Rowling's new book}? (Losing original structure)
%\item Can you \textbf{publish} new book for me ? (Bringing in wrong information)
%\end{itemize}
%\textcolor{green}{(Ruolan: Do we need some bad examples?)}

Another problem of the end-to-end pre-trained models, which generate the rewritten utterances from scratch,
is that they generally incur a large search space and 
are therefore not only imprecise but also inefficient.
%The rewriting task does not need to generate the whole utterance from scratch, but only needs to 
%modify the original utterance partially. Previous works\textcolor{green}{Ruolan:reference?} 
%have too large search space. The syntactic structure has been modified unnecessarily. 
In order to solve the large search space issue, %introduced by common generation model,
%\KZ{What is the large search space issue? you haven't even mentioned it
%before. It's too sudden to talk about like this. I think the way prev method works has two negative
%effects: 1) the results are no good because the syntactic structure may be changed; 2) the search space
%is larger. You need to discuss these two issues better.} 
%recently, some works have designed models with smaller search space.
%than generative methods. 
\citet{hao-etal-2021-rast} treated utterance rewriting as 
%multi-task
a sequence tagging task. For each input word, they predict whether 
it should be deleted and the span that needs to be replaced with. 
\citet{liu-etal-2020-incomplete} formulated IUR as a syntactic segmentation task. They predict segmentation operations required on the utterance to be rewritten. However, they still did not take the important step of predicting the 
site of rewrite, particularly the position within the syntactic structure of
the input utterance. If the model can learn the syntactic structure 
information in the target sentence, it can predict which part of the 
sentence needs to be modified, i.e., which words need to be replaced and 
where new words need to be inserted. 
After that, the model only needs to fill in these predicted positions. 
These two tasks are relatively simple to perform, and they collectively
avoid the above problems.
%this
Our approach is based on the above intuition.

In order to effectively utilize the syntactic structure of the sentence to be rewritten, we divide the IUR task into two phases. The first phase is to predict which positions in the utterance need to be rewritten (including coreference and ellipsis). The second phase is to fill in the predicted positions. In the first phase, we use the sequence annotation method to predict the locations of coreference and ellipsis in the utterance. In the second phase, we take the 
utterances with blanks as input
%fine-tune the pre-trained language model, 
and directly predict the words required for the blank position.
%\KZ{The following seems to repeat what's been said before.
%I suggest you mention the data sets that we experimented with
%because this is actually a highlight since we did a lot of datasets.}
By seperating the
original rewriting
task into two relatively simple phases, 
our results show that 
our model performs the best among 
recent state-of-the-art
rewriting models \footnote{Complete code is available at
\url{https://github.com/AutSky-JadeK/Locate-and-Fill}.}. 
%\KZ{Give an intuition why this might work better.}

Our main contributions are as follows. 
\begin{itemize}
\item A
%\KZ{why do u say that it's extensive? How do you show that?}
two-phase framework for solving incomplete utterance rewriting task is 
proposed. It can complete the Incomplete Utterance Rewriting (IUR) task. (\secref{sec:2-step-supervised-method})
\item  %\KZ{This bullet can go into the above paragraph before
%the contribution. Anyway it's not really a contribution,}
An algorithm 
for aligning the two sentences before and after rewriting
based on the longest common subsequences (LCS)
algorithm. 
%is designed
%, which can 
%preprocess and automatically label the rewriting dataset, and label the rewritten part 
%according to the sentences before and after rewriting to obtain the training data. 
We succinctly and efficiently generated
two kinds of data
which 
can be used for 
predicting the positions to be rewritten (the first phase)
%sequence annotation
and filling the blanks (the second phase)
respectively. (\secref{sec:becky-lcs})
%which is our method to predict the positions to be rewritten.
%\KZ{Too long-winded.}
\item  %\KZ{This bullet should also move up to the previous paragraph.}
%Pre-trained language model is used to fill in the positions to be rewritten in the utterance, 
%while adding prompts and spliting utterance according to the number of blanks are used to improve model's performance. (\secref{blanks-filling})

We have carried out experiments on 5 datasets, and the experimental results show that our two-phase framework achieves state-of-the-art results. (\secref{sec:experiment})

%\item %We demonstrate the improvements in efficiency 
%using direct chat logs between bots.
%\KZ{Maybe this should not be a contribution but part of the conclusion?}
%We show that the chats between bots are impressively informative, 
%even richer than the chats between humans and bots.
%This suggests some possible directions to improve 
%the capabilities of bots in the future.
%(e.g., by having them learn from each other)  (\secref{sec:diversity})
\end{itemize}
%\KZ{You need to re-think what are the technical contributions. All the technical
%contributions need to have backup in the experiments.}
