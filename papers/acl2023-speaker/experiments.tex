

\section{Experimental Setup}
% introduce the datasets, evaluation metrics of different tasks
We define the evaluation metrics for sensitivity, introduce multiple text generation tasks with dialogue data and present implementation details.%at the end




\subsection{Evaluation Metrics for Sensitivity}
\label{sec:quatification}

We uniformly sample names from $P$, which is specified later, to realize $f$ without the loss of generality and re-sample the name if it is not in $p$ but in the conversation. We avoid changing names mentioned during the conversation in case they are grounded entities. Since it's impossible to enumerate all possible $f$, we choose to substitute names of samples in $D_{te}$ for $T=5$ times. It should be noted that varying names in test data is different from the augmentation approach. The additional test data is fixed once constructed for comparing approaches by quantitatively measuring the sensitivity.

%under the uniform distribution
%The divergence of the predicted results among $D^t_{te}|_{t=1}^T$ for each sample reflects the model's sensitivity to speaker names.
%In a word, the speaker name sensitivity of a model can be reflected by the differences in generations or the variation of scores averaged with a number of samples from $D_{te}$(see metrics in Sec.~\ref{sec:quatification}). 


%By replacing names in $D_{te}$ for $T$ times, we can construct multiple test sets. The divergence of the predicted results among $D^t_{te}|_{t=1}^T$ for each sample reflects the model's sensitivity to speaker names. It should be noted that varying names in test data is different from the augmentation approach. The additional test data is fixed once constructed and is used to compare different approaches by quantitatively measuring the sensitivity.
%Dialogue models are expected to perform identically on different replaced samples and get the same scores with task-specific evaluation metrics compared with the reference $o$. 
%i.e., adding $o$ as the input of $Diff(\cdot)$ in Eq.~\ref{eq:ss}. 


%We define specific metrics for evaluating speaker name sensitivity similar to \citet{prabhakaran2019perturbation}' work.
We introduce three kinds of $\delta(\cdot)$ with task-specific evaluation metric $\rm{Score}(\cdot)$ and measure the speaker name sensitivity of a model similar to \citet{prabhakaran2019perturbation}' work.
\textbf{Pairwise Sensitivity(S-*)} is defined as:
\begin{equation}
	E_{i=1}^{N^{te}} E_{t_1=1}^{T}E_{t_2=1, t_1\neq t_2}^T[1-{\rm Score} (\hat{o}^{t_1}_i, \hat{o}^{t_2}_i)]
\end{equation}
$\hat{o}^t_i$ is the generation where replaced names are changed back for evaluation. $N^{te}$ is the number of samples in $D_{te}$. $E(\cdot)$ is the mean operator. 

Dialogue models are also expected to get the same scores with task-specific evaluation metrics compared with the reference $o$. So, we can also add $o$ as the input of $\delta(\cdot)$ in Eq.~\ref{eq:ss} and define the following two metrics:
\textbf{Score Range (R-*)} as
\begin{equation}
	\begin{aligned}
		E_{i=1}^{N^{te}} [& \max(\{{\rm Score}(o_i, \hat{o}^t_i)|_{t=1}^T\}) \\
		&-  \min(\{{\rm Score}(o_i, \hat{o}^t_i)|_{t=1}^T\})]
	\end{aligned}
\end{equation}
and \textbf{Score Deviation (D-*)} as
\begin{equation}
	E_{i=1}^{N^{te}} [ {\rm StdDev} (\{{\rm Score}(o_i, \hat{o}^t_i)|_{t=1}^T\}) ]
\end{equation}
%$o_i$ represents the reference and 
The sensitivity metrics here are the lower the better and are denoted by $\downarrow$ in the following sections. %can be combined with different task-specific metrics, and 

%If test data are constructed by switching all speaker names in each sample, we call it \textbf{change-all-name} test. We also only change the name of a single speaker each time to do \textbf{change-one-name} tests for analyzing fine-grained sensitivity.% and can be evaluated in the same way.

%change-all-name change-one-name

\subsection{Tasks and Datasets}

We implement our experiments on the tasks below. The statistics are in Table~\ref{tab:taskdata} and we calculate the macro-average scores of samples for each metric.

\begin{table}[h]
	\scriptsize
	\centering
	\begin{tabular}{l|lll}
		\toprule[1pt]
		\textbf{Task} & \makecell[c]{\textbf{Dialogue}\\\textbf{Summarization}}& \makecell[c]{\textbf{Question}\\\textbf{Generation}}
		& \makecell[c]{\textbf{Reading}\\\textbf{Comprehension}} \\
		\hline
		Dataset & SAMSum & Molweni & Molweni \\
		\#Train & 14,732 & 20,873 & 20,873 \\
		\#Val & 818& 2,346 & 2,346 \\
		\#Test & 819 & 2,560 & 2,560\\
		Output Length & 23.44$\pm$12.72 & 7.05$\pm$2.02 &4.01$\pm$2.93 \\
		%Outpus Length(std) & 12.72 & 2.02 & 2.93\\
		%\#Speaker & 1,932 & 8,770 & 8,770\\
		\bottomrule[1pt]
	\end{tabular}
	%\begin{tabular}{p{0.8cm}rrrrrr}
	%	\hline
	%	\textbf{Dataset} & \textbf{\#Train} &\textbf{ \#Val} & \textbf{\#Test}  & \textbf{Avg} & \textbf{Std} &\textbf{\#Speaker} \\
	%	\hline
	
	%	SAMSum & 14,732 & 818 & 819 & 23.44 & 12.72 & 1,932\\
	%	Molweni &20.873 & 2,346 & 2,560 & 7.05& 2.02 & 8,770\\
	
	
	%	\hline
	%\end{tabular}
	\caption{A summary of tasks. \#Train, \#Val and \#Test refer to the number of samples in the datasets. Output length are statistics(avg$\pm$std) for the word counts.}% \#Speaker corresponds to the number of samples for change-one-name tests in Sec~\ref{sec:quatification}.}
	\label{tab:taskdata}
\end{table}

\textbf{Dialogue Summarization} outputs fluent and concise summaries covering the salient information in dialogues. We experiment with the SAMSum dataset~\cite{gliwa2019samsum} consisting of around 16k open-domain dialogues among two or more interlocutors. Rouge-2 F1~\cite{lin2004rouge} and BertScore F1~\cite{zhang2019bertscore}\footnote{We adopted microsoft/deberta-xlarge-mnli recommended by {https://github.com/Tiiiger/bert\_score} for BertScore.} are task-specific evaluation metrics. We consider genders to be consistent when switching names following~\citet{khalifa2021bag}. 
%The input to the model is a concatenation of speaker-utterance pairs and the output is a narrative summary. 
%to evaluate the generated summaries by comparing with the reference. 

\textbf{Question Generation} is to generate a question given an input dialogue and its corresponding answer span. We use Molweni dataset~\cite{li2020molweni} made up of around 10k task-oriented dialogues sampled from the Ubuntu Chat Corpus.
Similar to the question generation work based on SQuAD1.1, we extract (dialogue, answer, question) tuples from the original Molweni dataset and ignore unanswerable questions. BLEU~\cite{papineni2002bleu} and Rouge-L F1 are used for evaluations. % An answer is appended at the end of the corresponding dialogue as the model's input.

\textbf{Reading Comprehension} generates an answer by inputting a dialogue with a question. We use the Molweni dataset~\cite{li2020molweni} and ignore unanswerable questions as well. Bleu and Rouge-L F1 are also used for evaluations.



 %, so that it is comparable to sensitivity metrics~\footnote{In the official implementations online, some scores are macro-averaged among samples while others are micro-averaged.}.





\subsection{Implementation Details}

We use BART-large
%~\footnote{\url{https://huggingface.co/facebook/bart-large}} 
as our basic pre-trained model. We truncate inputs to the first 1024 tokens and the learning rate is $3e-5$ with weight decay equaling 0.01. The model is fine-tuned with batch size equaling 32 for 10 epochs. We evaluate the performance on $D_{va}$ after each epoch with Rouge-2 F1 or Bleu. The checkpoint with the highest score on $D_{va}$ is saved for testing. During the inference, we decode with no\_repeat\_ngram\_size=3, length\_penalty=1.0 and num\_beams=4. We search $\alpha$ and $\beta$ in \{1, 10, 20\} empirically and report results with the best validation performance. Specifically, $\alpha$ equals $1$. $\beta$ equals $1$ for reading comprehension and $10$ for the others.
Our experiments are done on a single RTX 2080Ti with 11G GPU memory.
Considering the GPU memory footprint, we set $K=2$, which is the same for Aug and FreAug for fair comparisons. %\JQ{hyperparameter}

We test online approaches with their corresponding test sets. For offline approaches, we focus on two sources of $P$. One is \textbf{in-distribution names} representing speaker names from the corresponding $D_{tr}$. The other is \textbf{all-possible names} with more than 117 thousand names\footnote{\url{https://data.world/arunbabu/gender-by-names}}, which can reflect the models' performances in complicated real scenarios. 
For approaches with sampling operations, we construct data with 3 different random seeds. Results are averaged over the number of runs.
%, and 5 for testing, i.e., $T=5$ in Sec.~\ref{sec:quatification}

%The last one is \textbf{frequent names} which will be introduced in Section~\ref{sec:dda}.
%Other specific groups of speaker will be mentioned in Section~\ref{sec:unfairness}.

