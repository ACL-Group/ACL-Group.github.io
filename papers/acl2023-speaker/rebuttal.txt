General Response:

Our research is well-motivated because:
1) According to our recent tests, the speaker name sensitivity problem is still unsolved for large language models with hundreds of billions of parameters. 

Here is an example of dialogue summarization with outputs from ChatGPT:

Please generate a summary for the given dialogue within 20 words.

Dialogue:
Hollie(Missy): I'm home already, you can come over 
Derek(Neil): May I bring the dog? 
Sergio(Ashley): I'm on my way 
Hollie(Missy): sure you can, Derek(Neil) 
Hollie(Missy): do you want to stay overnight?
Derek(Neil): great!
Derek(Neil): I thought about it 
Hollie(Missy): nice!

Summary-1: Hollie invites Derek and his dog to stay overnight, Derek accepts. Sergio is on his way.

Summary-2: Missy invites Neil and his dog to come over and stay overnight, to which Neil agrees.

The two summaries are expected to be the same, modulo speaker names. However, the third speaker (Sergio/Ashley) is not even mentioned in Summary-2.

2) Even if large models can mitigate some of the sensitivity issues, small models will continue to exist in many resource-constrained scenarios and hence solving the sensitivity issues for them has its own merits.

3) Among all the models that can be fine-tuned with our limited resources, including T5 and GPT-2, BART is still the best and the most popular, therefore we pick BART as the target of this study. Our intention is to devote the limited paper space to a more in-depth analysis of the problem using a range of tasks.



Reviewer 1

Thank you very much for the comments.

Reject-3: Please refer to the general response.
 
Question 1A: 
* The name pool P is a set of candidate names to be substituted into the samples. The exact composition of P can change depending on the experimental setup, as specified in Sec. 4.3 and 5.5.

* The model's sensitivity results in divergent or incorrect content for different speakers (see Fig. 1). The motivation for changing speaker names is that we prefer the model to be insensitive to speaker names, i.e., when names are replaced, the results should be the same. 

* Yes, we replace p by p' by random sampling.

* Yes, the differences are among multiple outputs with different name replacements.

* The scores are computed in a task-specific way. We use Rouge-2 and BertScore for dialogue summarization, and Bleu and Rough-L for the other two tasks (see Table 3).
 
Question 1B: 
* Yes, the cost of computation grows proportionally with the size of the augmented training data, regardless of the size of the name pool. 

* Augmenting the data in the mini-batch will not make any difference as long as the size of the augmentation is the same. 

* Cross-attention loss encourages a stable attention distribution, instead of the latent representations, to the input tokens regardless of the speaker names. It will not cause the token embeddings to be similar. And our ablation tests in Table 5 show that cross-attention loss doesn't result in performance degeneration but instead improves the performance in most cases. 

Question 1C: 
* Vanilla is the baseline that simply fine-tuned the basic pre-trained model with the original dataset for different tasks. Ins is our proposed approach (Sec. 3.3). We will add the explanations in the caption of Table 2. 

* Yes, 19.71 is not significantly better than 19.21 in Table 4. Only the underlined scores are significantly better than all the baselines 
defined in the caption of Table 3.


Reviewer 2

Thank you for the precious comments. We will add some discussions about the following points in the revision.

reject 1:
Please refer to our general response. 

reject 2: 
In this work, we reduce the speaker name sensitivity by learning better model parameters. Changing the sampling strategy attacks the problem from a different angle during inference time, which may be profitably combined with our approach.

Question 2A: 
Yes, our approach can be generalized to reduce other sensitive factors so long as replacing the existing factor doesn't alter the correctness of the sample. For example, if the task is to query user reviews about the durability of electronics, the color of the product should be irrelevant and can be replaced.

Question 2B: 
Because methods such as CTRL generate text with some general controls such as styles or genres, whereas our method can be viewed as generating text with more strict controls (generating the *same* text regardless of the speaker names). Yes, these methods can be combined to enable better unbiased text generation.

Question 2C: Yes, our approach can be generalized into a "perturb-generate-evaluate" framework. The perturb phase can be implemented by bias factor replacement, or paraphrasing, etc. The evaluation phase can use not only text similarity metrics but also other metrics such as sentiments.

Reviewer 3

We appreciate the hard work and the comments.

Reject-1. Please refer to the general response.

Reject-2. The proposed method is simple but effective: the sensitivity scores are reduced by around 30%~50% while increasing the performance as shown in Table 3. For large language models, as long as one can fine-tune these models, one can also use our method, as it scales up roughly linearly with the size of data augmentation. In fact, the sensitivity already reduces substantially even when we double the training data.
 

---

R1: E

R1 asked some naive questions, such as "what's inside the name pool P", "why not just calculate the augmentation across the input mini-batch" and "whether the metric improvements e.g., 19.21 from ID to 19.71 to FreIns* are significant", which shows that they didn't have a deep understanding of our submission. As a result, some of his reasons to reject, such as "the proposed metrics have flaws", are not backed up with sufficient evidence.

Also, we suspect part of the reviews from R1 and R3 are synthetic because they include the exact same sentence: "The fairness issue may be already solved with a very larger corpus with larger parameters scaled up."




%%%%%%
%encoder-decoder models still outperform decoder-only models
%including ChatGPT (cite). Among encoder-decoder models, BART is still the
%stronger therefore our experiments only involve the comparison
%with encoder-decoder models.
%
%OPT is a decoder-only model while our approach is designed for encoder-decoder models. T5-flan is a concurrent work. We choose the most competitive model at that time, i.e. BART, as our basic model for extensive experiments.
%

%The augmentation is done by name replacements and such samples won't exist in the original input mini-batch. 
%loss是对attention做的，不是对representation做的。不是为了smoother
%pay less attention to name tokens才更有用

<<<<<<< .mine
Thank you very much for the comments.

Reject-3:
* It would not be fair to directly compare large language models with relatively small models. Small models will continue to exist in many resource-constrained scenarios and hence solving sensitivity issues for them has its own merits.

* Even for large language models with hundreds of billions of parameters, this problem is still unsolved. 
||||||| .r44235
Thank you very much for the comments.

Reject-3:
* Direct comparison between large language models and relatively small models
is unfair. Small models will continue to exist in many resource-constrained scenarios and hence solving sensitivity issues for them has its own merits.

* Even for large language models with hundreds of billions of parameters, this problem is still unsolved. 
=======
