\section{Related Work}

\textbf{Entity/Name Bias in Narrative Texts}: 
Previous work on entity biases shows that pre-trained language models are sensitive to changes in narrative text. Some works~\cite{zhang2018graph,zhang-etal-2017-position,wang-etal-2022-rely} for relation extraction mask entities in the context to prohibit learning spurious features between entities and relations. \citet{yan2022robustness} analyzes the robustness of models by entity renaming on reading comprehension. They all consider different kinds of entities, such as person and organization. However, the entities have the potential to be grounded in real life~\cite{smith2021hi}, and the background knowledge of these entities may be necessary for understanding. Besides, the context and the entities cannot always be well-separated, especially persons~\citet{yan2022robustness}. Thus, masking and switching operations are not always suitable for these entities.
In our work, we focus on speakers that are not grounded.  %don't consider entities mentioned in dialogues and 

Names that are not grounded have also been studied. Information such as age, gender and race can be reflected by a given name to some extent~\cite{girma2020black}, while models learned with statistical features may make wrong predictions about specific persons or bring unexpected stereotypes~\cite{bertrand2004emily}. \citet{romanov2019s} takes occupation classification as an example and discourages the model to predict an individual's occupation depending on his/her name. \citet{wang2022measuring} presents that machine translation models perform poorly on female names when translating into languages with grammatical gender and also have sentiment bias caused by names with sentiment-ambiguous words. Samples in all these works only have a single name each, while multiple speaker names are entangled in a single dialogue.


%entities:

%[2020EMNLP]"You are grounded!": Latent Name Artifacts in Pre-trained Language Models

%[2022NAACL]On the Robustness of Reading Comprehension Models to Entity Renaming

%[2022NAACL] Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis

%entities are likely to be grounded. speakers  are more ideal research target to 
%classification tasks , sequence generation tasks.

%names:

%[2022ACL]Measuring and Mitigating Name Biases in Neural Machine Translation, gender biases, sentiment biases


%[2019NAACL] What's in a Name? Reducing Bias in Bios without Access to Protected Attributes, occupation bias, protected attributes

%dialogues: more names in a single input



\textbf{Fairness of Dialogue Models}: 
Safety and fairness issues on generations from dialogue models are crucial for implementation in practice. Harmful differences in responses caused by different demographic personas are observed in well-known dialogue systems~\cite{sheng2021revealing,dinan2020queens}, including offensiveness, gender bias, race discrimination, etc. These unfairness phenomena also exist in dialogue systems without considering persons~\cite{liu2020does}, reflected by the politeness, sentiment, diversity and other aspects of a response. Recent work from~\cite{smith2021hi} shows dialogue models treat their conversation partner differently for different speaker names. Instead of analyzing differences in open-ended dialogue systems, we target on text generation tasks given dialogues and show that sensitivity/unfairness also exists among speakers.


%[2021]Revealing Persona Biases in Dialogue Systems.pdf:
%[2020COLING]Does Gender Matter? Towards Fairness in Dialogue Systems
%[2020]Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation

%[2021]Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models


%dialogue understanding tasks, multiple speakers with different genders or races are entangled.
