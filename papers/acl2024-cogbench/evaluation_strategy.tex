% \section{Experimental Setup}
\section{Evaluation Strategy of CogBench}

% \KZ{This section is a bit too detailed. Cut down. I think it can even be a
% subsection of Experiments section.} \XJ{omitted some details. Kept it as a section as someone may care about the evaluation strategy espeially for description task.}
In this section, we will introduce the evaluation strategy of tasks in CogBench.
% For CogVQA task, we use Accuracy as the evaluation metric. For CogID task, we use GPT-4 to evaluate the performance.

\subsection{Evaluation for Description Task}

For the evluation of Description task, we consider model performance comprehensively on two levels: low-level Recognition ability and high-level Cognition ability. 
Evaluation metrics for both levels are calculated based on recall score, referred to as Recognition Score and Cognition Score, respectively.
% \KZ{Instead of calling them recognition vs. cognition, I suggest calling them
% ``hard coverage'' and ``soft coverage''.} 
% \XJ{seems we did not emphasis soft and hard?}

% \subsubsection*{Recognition}
% \KZ{No point having 3rd level of sections.}
% For the Recognition part, we calculate recall score based on [Entities] as the Recognition Score. 
% We use the following method to calculate recall score:
% For the Recognition part, the recall score
% \textbf{Recognition} \quad
% \paragraph{Recognition}
The \textbf{Recognition Score} is calculated as the ratio of the number of recognized [Entities] to the number of annotated [Entities] in all images.
%  which is referred to as the Recognition Score.

First, we use SpaCy\footnote{https://spacy.io/} to extract nouns from the model generated description, and then calculate cosine similariy between embeddings\footnote{Implemented with sentence-transformers package (https://www.sbert.net) and \textit{all-mpnet-base-v2} is adopted as the model to encode [Entities] and nouns.} of annotated [Entities] and extracted nouns. 
% Sentence-BERT \cite{reimers-2019-sentence-bert} is adopted to encode the nouns and [Entities]. 
For each entity, if the cosine similarity score between the entity and any noun is greater than a threshold (0.5 in this paper), we consider the entity is recognized by the model. 
% The recall score of an image is the ratio of the number of recognized entities to the number of entities in the image.
% The micro-average score is finally calculated as the overall Recognition Score. 
% of each model on Description task.
% \MY{It's better to discuss the results with respect to the ``cognitive capability checklist'' at first, you can group them into recognition and cognition, but still stick to the framework you proposed. You can group in the annotation section in a more obvious way, saying that your recognition includes xxx while cognition includes xxx}

% For the Sentence-BERT adopted for evaluation, we implemented with sentence-transformers \footnote{https://www.sbert.net} and use \textit{all-mpnet-base-v2} as the model.

% \subsubsection{Cognition}

% For the Cognition part, 
% \paragraph{Cognition} 
% We evaluate the cognitive reasoning abilities of LVLMs from the eight aspects as we annoated.
% We calculate the Cognition Score of each CoR type to evaluate the performance of models on different reasoning capabilities.
For \textbf{Cognition Score}, we first calculate a score for each of the eight cognitive reasoning abilities, and then compute an overall score.
% Finally, we calculate micro-average score as the overall Cognition Score of each model on Description task.

We use GPT-4 \cite{openai2023gpt4} to help calculate the Cognition Score.
% evaluate the cognition ability demonstrated in description generated by models. 
To avoid the interpretability issues of GPT-based evaluation, instead of using a subjective evaluation method of directly comparing two descriptions, we use GPT-4 to perform a easier, more objective and fine-grained binary classification task, that is, judging whether the generated description contains the semantics of annotated CoRs.
Details and prompts of this GPT-based evaluation are shown in Appendix \ref{sec:cogid_eval_prompt}.

% we use GPT to perform a easier and more fine-grained task instead of a coarse-grained evaluation task to directly evluate the quality of the generated description. 

% For CoR types other than [Event Relationship Reasoning], we task GPT with determining whether the conclusion in each CoR is mentioned in the description. 
% For [Event Relationship Reasoning], we task GPT with determining whether the causal relationship between events (i.e. the whole CoR), as annotated, is present in the description.
% Prompts of the two subtasks mentioned above are shown in Appendix \ref{sec:cogid_eval_prompt}.

% \begin{figure}
%     \begin{tcolorbox}
%         % [title = {Reasoning Eval},
%         [colframe = blue!30!white,
%         colback = blue!2!white,
%         colbacktitle = blue!10!white,
%         colupper = black, collower = yellow!75!red,
%         coltitle = black!90!white]
%         \small
%         \textit{
%         Given a \textcolor{blue}{<DESCRIPTION>} and some \textcolor{green}{<KEY POINT>s}, please tell me if the description explicitly present the exact or similar semantic meaning of each key points. Note that instead of reasoning about whether each key point is possibly correct based on the description, you only need to determine whether the description mentions semantic information in the key point. If there is ``or'' in the key point, you just need to determine if the description mentions any of the situations listed in the key point. Assign a score of 0 or 1 to each key point, where 0 represents NO and 1 represents YES.} \\

%         \textit{\textcolor{blue}{<DESCRIPTION>:}} \\
%         \textit{\textcolor{blue}{\{Description generated by models.\}}} \\

%         \textit{\textcolor{green}{<KEY POINT>:}} \\
%         \textit{\textcolor{green}{1. \{Annotated key point.\}}} \\
%         \textit{\textcolor{green}{2. \{Annotated key point.\}}} \\
%         \textit{\textcolor{green}{3. \{Annotated key point.\}}} \\
%         \textit{...} \\

%         \textit{Please write your answers in this format:} \\
%         \textit{1. [ ]  2. [ ]  3. [ ]  ...} \\
%     \end{tcolorbox}
%     \caption{Evaluation of Reseasoning types.}
%     \label{fig:eval}
% \end{figure}

% After obtaining the score of each CoR, we calculate the Cognition Score by summing up the scores of all CoRs and dividing by the number of CoRs in the image.
% We calculate the Cognition Score of each CoR type to evaluate the performance of models on different reasoning capabilities.
% Finally, we calculate micro-average score as the overall Cognition Score of each model on Description task.

After obtaining the score of each CoR, we calculate a recall score for each reasoning capabilities.
The overall Cognition Score is calculated by summing up the scores of all CoRs and dividing by the total number of CoRs.
The effectiveness analysis of GPT-based cognition evaluation is shown in Appendix \ref{sec:eval_gpt_eval}.

% Note that it is similar to \citet{chan-etal-2023-clair}.


% Besides, traditional evaluation metrics are we also adopted as reference. We use BLEU-4, METEOR, ROUGE-L, CIDEr-D, SPICE, and BERTScore as evaluation metrics.


\subsection{Evaluation for VQA Task}

For Multiple-Choice Questions in VQA task, we use accuracy as the evaluation metric. 
As questions are generated based on CoRs, we can also calculate both the accuracy for each reasoning capability and the overall accuracy.
