\section{Conclusion}
This paper studies key-value restricted language model inference. To shed light on the effectiveness of existing eviction policies, we conduct comprehensive comparative analysis by decomposing eviction policy into importance score calculation and eviction scope construction. We identify the inconsistency and instability of prior policie and introduce RoCo, a robust cache omission policy with improved downstream performance. We also release EasyKV, the accompanying library for versatile key-value constrained LLM inference.

\section*{Limitations}
The first limitation of this work is its applicability to decoder-only Transformer models. For encoder-decoder style language models like T5~\cite{t5}, the KV cache in its encoder part involves bi-directional attention computation, which is not handled by existing cache eviction policies. We leave the adaptation to encoder-decoder models to future work as decoder-only LLMs are the mainstream and most capable models. Another limitation of this study lies in its practical implementation. The open-sourced implementation of this work is based on Pytorch and HuggingFace transformers library, which are not heavily optimized for GPU memory operation. Future iterations is dedicated to implementing cache operations with more efficient CUDA kernels.