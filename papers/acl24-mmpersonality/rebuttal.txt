ACL 24
Official Review of Paper3331 by Reviewer 45EX

We appreciate the valuable feedback provided by the reviewers. Our responses to Reviewer's questions are listed below:

**Q**: The claim that relationships are more important than personality is not particularly persuasive.
**A**: While we consider relationships to be a significant factor influencing personality, we do not assert that they are of greater importance than personal information. Moreover, there is no experimental evidence to support this hypothesis.\MY{We did not make this argument in our paper, we meant that it's important to acknowledge personality dynamics through interactions and relations between different people. One might reveal different traits when socializing with different characters.}

**Q**: What does the Personality item represent, and could you clarify the annotation process for this information?
**A**: Sorry for the misinterpretation of Figure 2. Actually, this explanatory text in Section 2.2 is for our sample Json file and not for this Figure. In fact, The personality annotations of our datasets were scraped from an open source website. 
And the personality information on this site is voted on by thousands of people who are familiar with that movie or television production. For our Figure 2 in Paper3331\MY{line? What do you mean paper 3331}, the Personality item refers to the four personality types of each character.

**Q**: However, if over 10% of the samples are incorrect, it might be necessary to manually revise the entire set again. This raises the question: is this data truly usable?
**A**: Thanks for this suggestion and we will do this manual revision to improve the performance of our annotation process. To be honest, we don't have such an experiment to claim that the relationships is usable for personality detection. 
To address your concern and to substantiate the usability of our dataset further, we are committed to conducting supplementary experiments aimed at evaluating the effectiveness of relationship annotations in detecting and understanding personality traits.

Official Review of Paper3331 by Reviewer z1jj 

We greatly appreciate your insightful feedback and the valuable references you provided. We acknowledge the oversight of not fully considering the extensive body of existing work in the relevant domain. We will incorporate these references into our paper to provide a more comprehensive background. \MY{you might want to add something saying that these works provide great foundations and do not diminish the novelty of our work.}
We intend to post the following content to our Intro Section:
Lepri et al. (2012) shows the importance of sociometric badges in capturing interpersonal interactions, 
which indicates the relationship might be a key factor to improve the personality prediction performance. 
Chittaranjan et al. (2011) have made an early attempt to classify personality traits using smartphone data, 
emphasizing the role of ubiquitous computing in personality research.
Vinciarelli et al. (2014) provides a comprehensive overview of the field of personality computing and stresses 
the importance of quality and quantity of training data. That makes MMPD an useful corpus to help other researchers 
to improve their works.

As for your comment about our strong claim in Sec 4.1, we admit that we used some strong words there without
adequate scientific support. We will soften this statement to emphasize the diversity of our dataset annotations rather 
than comparing it to real world data.

Official Review of Paper3331 by Reviewer EpPs

Thank you for your thoughtful and detailed feedback on our work. We appreciate your recognition of the potential utility of our dataset to the NLP and affective computing communities, as well as your candid critique of the aspects you found lacking. Below, we address each of your concerns and outline our plans to enhance the quality and clarity of our manuscript.

Evaluation of Crawled Labels:
We acknowledge the oversight in not conducting a thorough evaluation of the quality of labels obtained from external websites. To address this, we commit to implementing a rigorous quality assessment of the crawled labels, involving a manual review sample of the dataset. And the personality labels is voted on by 160 people on average who are familiar with that movie or television production, which is more convincible than finding 5 or 6 volunteers to vote. As a matter of fact, the median, max, min of the voters for each character are respectively 35, 10759 and 5.

Evaluation of ChatGPT Labels:
We appreciate your acknowledgment of our evaluation of the labels generated by ChatGPT. But we don't think we need to do this on real world data. Our research is designed to complement, not replace, the capabilities of LLMs, aiming to advance personality computing through the strengths of multimodal data. \MY{yet I don't think the reviewer is questioning that we are replacing LLMs, he's questioning the validity of using LLM to label it? Stress that we compared with other model-based methods where we didn't see any good results as the relationship is quite complexed. As mentioned in the paper, we invited annotators to manually validate and more rigorous process will be included in our next version?}



------------------------------------------------------------------------------------------------------------------

EMNLP 24 Rebuttal:
Official Review of Submission4172 by Reviewer VEiJ

1. The author mentioned the Dynamic Nature of Personality, but how the proposed PM solves this concern is not introduced well in the Introduction section. How and why does incorporating relationship networks into personality prediction models offer  a solution to this issue (It is more like a model-level limitation)? If the context information matters in personality recognition, how do other datasets fail to provide such context? And how does the proposed PM solve this limitation?

2. I think section 2.1.2 is confusing. Why these two relations are important and how do they exactly affect the personality? The content presented here is not quite related to the title of Section 2.1.2

3. Also, Section 2.2. This part can be put in the readme file of the dataset rather than a sub-section in a research paper...

4. In section 3.3, the author mentioned "Only text data are supposed to be processed" for social and emotional relations annotations. However, the authors already highlight the importance of multi-modality in personality recognition (the proposed dataset is also multi-modal), this should be equally important in emotion relation annotations. This part should be well explained. Besides, how to ensure the annotations from ChatGPT are correct?

Answer to Q1: We don't really introduce a solution for Dynamic Nature of Personality. It's a psychological truism that incorporating relationship networks into personality prediction allows for the capture of these dynamic changes by considering the context of interactions and relationships. So what we want to emphasize is there is a possible use of our dataset. We only did a preliminary study to show the phenomenon of Dynamic Nature of Personality. As for other datasets, they don't realize personality is not only static but also dynamic so they consider every person has only one character in any cases.
Answer to Q2: As we mentioned in Section 2 and Appendix B, Social relations are crucial as they provide a framework to observe and interpret personality in action. These interactions are fundamental in shaping and expressing personality traits through various domains of life. Emotional relations, on the other hand, depict the attitudes and feelings towards others, offering additional layers of context that are not captured by social relations alone. They help in understanding the underlying emotional currents that influence personality expression.
Answer to Q3: Thanks for the suggestion of this placement. We will improve our paper structure. 
Answer to Q4: We mentioned "Only text data are supposed to be processed" in section 3.3 which introduces the annotation process for relations labels. On one hand,  it's only about the relations recognition which doesn't need multi-modalities. On the other hand, our annotation tool (GPT 3.5) doesn't support multi modalities.


Official Review of Submission4172 by Reviewer m9WG:

Section 4.1.1 mentions a manual quality check, but who were the annotators and the annotation protocol?

A significant part of the paper is devoted to the evaluation of the dataset through some specific baselines, and discuss some examples from the relation data: that is not wrong, though not really the most interesting part. The actual contribution in annotating the data is instead quite limited: the bulk of the personality data is crawled from an external website, the data alignment is mostly engineering effort (and mostly left to the Appendix), the relationship labels are done through ChatGPT without the comparison with any other baselines.

Also related to the previous point, it seems that the real focus of this paper is more the relation annotation, evaluation and analysis than the collection and annotation of personality data.

The evaluation in section 4.2.1 is not so clear, accuracy is not defined (on which label?), and little details are given about the experimental setup.



Thank you for your thoughtful and detailed feedback on our work. We appreciate your recognition of the potential utility of our dataset to the NLP and affective computing communities, as well as your candid critique of the aspects you found lacking.\MY{not repeating this sentence for each reviewer. They will see all our replies if they want to.}
The annotators were a group of five human volunteers with backgrounds in filmography and literary, ensuring a high level of expertise in understanding the structure drama scripts. They are in their mid-twenties and had at least an undergraduate education. The protocol involves the following steps: 1. Annotators independently reviewed a randomly selected sample of 235 scenes from the dataset. By given relevant scripts and subtitle files, they are required to match each utterance in subtitle with corresponding names. 2. The annotations were compared against the results generated by our algorithm to evaluate accuracy. Discrepancies were discussed and resolved through consensus, ensuring consistency and reliability.
We recognize that our work is more about engineering, and our third contribution regarding relations is not the most significant but rather an initial case study. Nevertheless, we wish to emphasize that our dataset is indeed valuable and provides other researchers with a novel research angle to study personality.
In Section 4.2.1, accuracy refers to the correctness of personality and relationship predictions made by the models. It is calculated as the proportion of correctly predicted labels (personality traits) to the total number of labels.

The overall quality of writing in the paper is poor.\MY{use more concise language and paragraphs, i.e. *Written quality. - The organization of our paper can be a bit different from mainstream structure, however we consider this necessary since xxxx}
The motivation for building the dataset is unclear. There should be at least a full paragraph explaining why the authors chose to work on this topic, not just 1-2 lines.
There should be detailed information about the personality classes (number and labels available to annotators to choose from) in the initial sections. The paper jumps straight (in one of the last sections) into personality class dynamics and their relationships in Figure 7.
The term "diversity" can mean different things depending on the context. The authors need to clarify what kind of diversity they are referring to in line 129.
Instead of focusing on the motivation and properly defining the classes, the authors spend more time explaining why they used the JSON format in section 2.2. It would be more helpful to describe the dataset's attributes for the readers.
It's unclear how many total classes there are in the personality classification task. (results in Table 4)
The authors explain how they used ChatGPT to annotate relations, but they do not provide guidelines for annotating personality. What about inter-annotator similarity? This information is missing.
If the authors choose to release the dataset in the future, do they have permission from the source to do so? They are submitting data under copyright to OpenAI's APIs, and there is a high chance that OpenAI may record and use that data for their purposes. This seems unfair. I kindly request the (senior) area chair and editors to review this issue.

We greatly appreciate your insightful feedback and the valuable suggestions you provided. 
The motivation for building the dataset is driven by the need for comprehensive multimodal data to improve personality prediction models. We listed three limitations of existing datasets from line 74 to 89. That's why we build our dataset. 
The details of personality classes are all introduced in Appendix A because they are too wordy.
In our context, "diversity" refers to the content which includes variety of personality traits, genres of movies and TV shows, and types of social and emotional relationships included in our dataset. We mentioned it in line 121.
As for the number of classes. We introduced MBTI personality model in Appendix A as well as explaining each dimension of MBTI, and also mentioned there are 16 types in line 497.
We only use ChatGPT to annotate relations and we never use it to annotate personality labels. According to line 92 to 102, we crawled labels from an open source website so we don't need guideline for annotators. 
According to Article Vter item Four in Universal Copyright Convention, we clearly introduce the copyright concerns in the end of our paper. And we don't think we need to consider the OpenAI, because we only provide OpenAI the original and open source scripts. \MY{This paragraph is a bit aggressive, try softening the tone. We have provided xxx as seen from lines xxx, following xxx.}
