\section{Experiments}
\label{sec:exp}

\begin{comment}
\begin{table*}[htbp]
% \resizebox{0.8\textwidth}{!}{%
    \centering
    \small
\begin{tabular}{lcccc}
\toprule
\textbf{}             & \textbf{Emotional Reactions} & \textbf{Interpretations} & \textbf{Explorations} & \textbf{Average} \\ 
\midrule
\textit{naive}        & 1.3563                       & 1.7864                   & 1.9242        & 1.6890         \\
\textit{empathy CoT}  & 1.4953                       & 2.1008                   & \textbf{2.4837}        & 2.0266        \\ 
\textit{therapy CoT}  & 1.5283                       & 2.1404                   & \uline{2.3182}        & 1.9956        \\
\textit{filtered therapy CoT}  & \uline{1.6880}                       & \uline{2.2282}                   & 2.2764        & \uline{2.0642}        \\ 
\textit{ChatGPT}  & \textbf{2.1671}                       & \textbf{2.5980}                   & 1.9633        & \textbf{2.2428}        \\ 
\midrule
\textit{ground truth} & 1.5530                       & 2.0903                   & 2.4734        & 2.0389        \\
\bottomrule
\end{tabular}%
% }

//amount of xxx, similar 
\caption{Empathy Measurement Results Scored by GPT-4 across Three Dimensions Introduced in \citet{sharma-etal-2020-computational}. \KZ{Pls say what is underlined and what is bolded. I also find it a bit puzzling that ground truth is not as good as ChatGPT or even other kinds of CoT. If the human expert is not that good, then what's the point for us to be ``inspired'' by them? Or are we saying that these therapists who participated in the dataset collection are not really experienced?}}
\label{tab:gpt4score}
\end{table*}
\end{comment}

We demonstrated the effectiveness of our \textit{PsyMix} in this section through human evaluation and automatic analysis through GPT-4~\cite{openai2023gpt4}. 
% \MY{We demonstrated the effectiveness of our SupportChat (again, this name needs a change) on a sole public real counseling dialogue dataset.} Baichuan2-7B~\cite{yang2023baichuan}
\subsection{Dataset}
The dataset for supervised fine-tuning, named \texttt{Xinling}~\cite{li-etal-2023-understanding}, is a public real counseling dialogue dataset in Chinese. This dataset comprises 300 dialogues between counselors and seekers, gathered from an online welfare counseling platform.  Each session on this platform provides about 50 minutes (with an average of 78 turns) of free text-based counseling delivered by experienced counselors, highlighting the dataset's high quality.
\subsection{Experiment Settings}
The base model utilized for supervised fine-tuning is \texttt{Baichuan2-7B(Chat)}~\cite{yang2023baichuan}, currently one of the top-performing Chinese LLMs of moderate size. Nevertheless, our methodologies are also applicable to models and datasets in other languages. The hyper-parameters employed in tuning are detailed in Appendix  \ref{apd:experiment}.

\subsection{Methods for Comparison}
\label{sec:compare}
In our experiments, we primarily compare our proposed \textit{PsyMix} with two baselines and single-aspect CoP. 
Baselines includes \textbf{Naive}, which involves straightforward supervised fine-tuning without incorporating CoP, and  \textbf{ChatGPT}, referring to the responses generated directly by prompting ChatGPT as a counselor\footnote{We provide the prompt in Appendix \ref{apd:baseline_prompt}}. %To highlight the effectiveness of a mixture of CoT, 
We also trained models exclusively on one psychotherapy approach's analysis, respectively named \textbf{PCT CoP}, \textbf{CBT CoP} and \textbf{SFBT CoP}.

\subsection{Human Evaluation}
Human evaluation is widely considered the golden standard for dialog systems, particularly for counseling sessions where subjective satisfaction is the optimal goal. We recruited evaluators through online advertisements, resulting in 16 volunteers aged 19 to 51, 56\% of which were female.% and 56\% females. 
\paragraph{Evaluation Procedure} 
For counseling dialogues, immersion in the conversation is crucial for accurate assessment. Evaluators must fully engage to better understand the seeker's needs and assign more reasonable scores. Additionally, counseling sessions tend to be lengthy. Therefore, our chosen method of human evaluation involves having each evaluator rate an entire counseling dialogue sentence by sentence, from start to finish. This approach is efficient and enables evaluators to immerse themselves in the conversation. 
For each utterance from the seeker, we provide responses from the seven different models of counselors outlined in Section \ref{sec:compare} in a randomized order. Evaluators then assign scores from 1 to 5 to each response, based on how comfortable they feel and inclined to continue the coversation. %Notably, evaluators are instructed to award higher scores to responses that leave them feeling more comfortable and inclined to continue the conversation.


\begin{table}[th]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccc}
    \toprule
     Method & Avg. score & Avg. length & Satisfaction rate \\ 
    \midrule
    \textit{ChatGPT}  &  2.73 & 47.06 &  33.9\%   \\ 
    \textit{naive}     &   2.81   & 14.56 & 32.1\%\\
    \midrule
    \textit{PsyMix}  &   \uline{3.12}  & 25.97 & \uline{40.8\%}  \\ 
    \textit{CBT CoP}  &  3.04  & 24.91 & 38.6\%   \\
    \textit{PCT CoP}  &  2.99  & 25.67  & 37.5\%  \\ 
     \textit{SFBT CoP}  &  2.96  & 24.93 & 37.6\%\\
     \midrule
     \textit{ground truth} & \textbf{3.58} & 25.72 & \textbf{55.5\%}   \\
    \bottomrule
    \end{tabular}
    }
    \caption{Human-rated scores of responses generated by various counselor chatbots and human counselors. ``Satisfaction rate'' means the proportion of utterances scored 4 or 5 points by humans.}
    \label{tab:human}
\end{table}

\paragraph{Results}
We assessed a total of 348 utterances extracted from 12 dialogues, with each utterance rated by two evaluators. The average pairwise agreement\footnote{This metric is introduced in~\citet{liu2023alignbench}. The detailed description is in Appendix \ref{apd:agreement}.} among these evaluators is 0.723, indicating a relatively high level of agreement. The rating results are in Table \ref{tab:human}. 
We observed that the ground truth responses produced by human counselors achieved the highest average score. Among the responses generated with CoP, \textit{PsyMix}, which incorporates mixed CoP of three psychotherapy approaches, outperformed others that used only one type of CoP. Interestingly, we noticed that incorporating CoP led to a significant increase in response length and higher scores, compared to the naive approach. This improvement is because the greater specificity of CoP-enhanced responses by deeply understanding the seeker's situation\footnote{We provide specific examples generated by these models as case studies in Appendix \ref{apd:case}}.

\subsection{Empathy Analysis}
\label{sec:empathy}
To analyze the characteristics of responses generated by different models, we utilize an empathy measure framework proposed by \citet{sharma-etal-2020-computational}. This framework dissects empathy into three dimensions: \textit{Emotional Reactions}, \textit{Interpretations}, and \textit{Explorations}. We employ GPT-4 to assign scores (1-3) to each aspect of every response. Detailed explanations of these three dimensions and scoring criteria are provided in Appendix \ref{apd:thought_prompt}.
\paragraph{Results} %Since empathy is subjective, higher scores in these three dimensions do not necessarily indicate better overall empathetic responses. 
Human counselors' responses are regarded as ground-truth hence the more human-like, the better counselor chatbot is.
We therefore measure the empathy level of each generated response by calculating the Mean Square Error (MSE) between the response and the ground truth empathy scores. The results are displayed in Table \ref{tab:gpt4score-mse}. 
% \MY{Minor thing: I understand that you move this table before human results for layout concern, however when you put it forward the labeling is wrong, you will be citing Table 3 first.} 
Notably, our model, \textit{PsyMix}, closely aligns with the ground truth across most dimensions, demonstrating an appropriate level of empathy.

\begin{table}[ht]
\centering
\resizebox{0.8\columnwidth}{!}{%
\small
\begin{tabular}{lcccc}
\toprule
\textbf{}             & ER & IP & EX & Average \\ 
\midrule
\textit{ChatGPT}      &0.966       &1.019     &1.557     &1.181\\
\textit{naive}        &\textbf{0.648}       &1.073     &1.461     &1.061\\
\midrule
\textit{PsyMix}    &0.725       &\textbf{0.902}     &\textbf{0.947}     &\textbf{0.858}\\
\textit{PCT CoP}      &0.761       & 0.952     &1.1101     &0.941\\ 
\textit{CBT CoP}      &0.668       &0.961     &1.1749     &0.935\\
\textit{SFBT CoP}     &0.757       &1.103     &1.102     &0.987\\ 
\bottomrule
\end{tabular}%
}
\caption{The Mean Square Error (relative to the ground truth, the lower the better) of GPT4-rated scores for various counselor chatbots. ER = Emotional Reaction, IP = Interpretation, EX = Exploration.}
% \KZ{What's the point of underscoring the second best?}
\label{tab:gpt4score-mse}
\end{table}









