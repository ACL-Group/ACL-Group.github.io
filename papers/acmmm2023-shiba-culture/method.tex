\section{Methodology}
\label{sec:method}
To verify our assumptions that dogs from different human language environments bark differently and the barking difference is related to their host's language, we conduct classification-based experiments and analyze the Shapley value to find which features are important in distinguishing barks. %\KZ{What do you mean by interpret prominent features? Is it more like to use the Shapley value to measure how important an acoustic feature is? Pls be specific.} 

\subsection{To Answer the First Question: Pairwise Classification}

The most intuitive approach is to group all fine-grained barking clips by the contexts, and then
do a two-way (English or Japanese environment) classification in each group using only 
acoustic features.  Barks under the same context are considered clear of other 
confounding factors but differ only by their host language environments. 
Of course, this is just a best-effort approximation. Dogs may also differ by their
age and sex, but these attributes are hard to obtain from the YouTube data even manually.
Given the large size of our dataset, we believe a significantly better than random classification
accuracy will show that we can distinguish English dogs with Japanese dogs just 
by acoustic features.  

Unfortunately, due to the large number of combinations among the scenes, locations and 
activities, our initial investigation shows that there are not many clips to classify 
under the same context. Thus classification accuracy with such grouping is not statistically 
significant. Therefore, we adopt a 4-way classification problem instead. Specifically, we first pair the dog barks from the same context but under different language environments in 4 ways: En-En, Ja-Ja, En-Ja, and Ja-En. Two clips are considered to be from the same context if they have the same scene category, the same location, and their activity vectors with a cosine similarity of 0.95 or above. The problem is then defined as the classification of these pairs into any of the above four classes. Our classification models include xgboost, KNN, Logistic Regression, and Random Forest. Multiple acoustic features including spectral features (filterbank~\cite{strang1996wavelets}, PLP~\cite{hermansky1990perceptual}, MFCC~\cite{davis1980comparison}) and handcrafted feature-sets (eGeMAPs, GeMAPs~\cite{eyben2015geneva}, ComParE~\cite{schuller2013interspeech}) are utilized in the classification. If the classification accuracy is significantly above random guess, we can conclude that one can distinguish English and Japanese Shiba Inu dogs by their barking sounds.

\subsection{To Answer the Second Question: Correlation on Prominent Factors}
%\JY{Correlation Missed!}
To ascertain the influence of the host language on dog barks, we analyze prominent factors 
that distinguish Japanese and English dogs' sounds. Shapley value is commonly adopted 
to explain feature importance for a given machine learning model, which can help 
determine the prominent features influencing dog barking. %Python implements that in SHAP. 


%\JY{not talk GeMAPs here}
%\KZ{Given that you actually tested against a number of different audio feature
%sets in the experiment part 1, why do u pick GeMAPs here so early? Maybe here instead of
%fixing one feature set, just talk generically and only finalize the actual set after
%we have seen the result in Sec 4? Sec 4.1 will tell us which feature set is good,
%and then we we analyze that feature set in more details in 4.2? }
%GeMAPs\cite{eyben2015geneva} is a widely-used acoustic feature set consisting of statistical 
%computation on acoustic features, which are low-level descriptors that exhibit high 
%interpretability.

%For its universality and explainability, it is selected as the input features to compute %Shapley values and determine our prominent influencing acoustic features.

To compare the relationships between a dog barking and the host language, 
we also include features extracted from human speech (English and Japanese corpus). 
The speech sources include 8,000 clips from CommonVoice~\cite{ardila2019common}, 
which is an open source multilingual speech dataset contributed by volunteers around 
the world, and host speech from EJShibaVoice. We adopt two different sets of speech data 
because they provide distinctive features. 
Speech from EJShibaVoice has direct relation with barks, 
so that we can conduct Pearson value analysis between them, 
while CommonVoice is purer and more common to help us find universal 
feature of human speech. Similar procedures are conducted on human language 
and the prominent factors are later compared with those inferred 
from dog barking~(\secref{sec:prominentfactor}).
Furthermore, to ascertain the correlation between barks and their host speech in a statistical way, we analyze the Pearson correlation between them. In the meantime, the Pearson correlation between barks and random speech is shown to compare~(\secref{sec:prominentfactor1}).
