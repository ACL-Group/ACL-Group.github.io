To All:

A number of reviewers pointed out that the acoustic differences between English and Japanese dog vocals could be due to factors other than the host language: One factor is related to the recording devices and audio ambience however we would like to re-iterate that the first kind of factors/biases should be well mitigated with the large and diverse data source coming from either side of the Pacific that we curated on YouTube. The number of families for Japanese and English respectively stand at 219 and 275, much larger than any other similar studies previously reported. The other factor is related to social norms and customs in two different countries/cultures, which we believe are inter-related with the different languages that are in the question in this paper. Having correlation between dog vocals and human speeches doesn't nullify the correlation between dog vocals and social norms and cultural behaviors, and vice versa. In fact, these two kinds of correlations can strengthen each other, in our humble opinion.

While we are painfully aware that all reviewers treat this paper's scientific findings, which is the correlations between dog vocals and human host languages, with acute interest and well-deserved vigor, we would like to stress once again that the most important contribution of this paper is actually the fact that we have, for the first time, demonstrated that such an interesting scientific question can be explored in a unique data-driven and computational approach. We sincerely hope that the reviewers can take this paper with an open mind, and give us the opportunity to share our thoughts with the the audience at this prestigious venue.  

To reviewer 1:

Thanks for your encouraging comments. Here we definitely agree that there are multiple confounding factors, and that's one of the reasons why we chose to collect data from open-domain social media to rely on the large amount of data to reduce such biases. At the same time, We identified major documented factors that could give rise to dog sound change such as scenes and behaviors so that we have inferred the context (section 2.3) and paired the barking clips (for the purpose of comparing clips under controlled and paired context).

Regarding your concern about the processing pipeline, our models (mentioned in line 294) exclusively utilize images because visual modality is more informative than audio modality[1]. Image models have shown excellent performance in our case. In terms of the owners' influence, we believe that all individuals speaking in the video can affect the dogs. Thus, we calculate the average value of host speech.

[1] Kazakos E, Nagrani A, Zisserman A, et al. Epic-fusion: Audio-visual temporal binding for egocentric action recognition[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 5492-5501.)

To reviewer 2:

We find your comment extremely helpful. We are not claiming that the language environment is 
the only cause or influencing factor. Instead, our carefully-designed method and experiments revealed a preliminary finding that dogs vocal expressions are correlated with their host language, which could be a cornerstone for future research in this direction. The paper you cited points out that dogs in different language environments have behavioral differences. In fact, what we have discovered in this paper is exactly another kind of behavior difference. And the results in Table 5 shows that there is substantial correlation between dog barking and host speech indeed.

Additionally, a website at https://www.akc.org/dog-breeds/shiba-inu/ suggests that Shiba Inu dogs are purebred and have undergone minimal genetic changes resulting from animal hybridization.

While it would be ideal to raise dogs under controlled environments (everything being equal except for the linguistic environment), this concept poses significant cost and logistical challenges that make it impractical to implement in reality. Therefore, we made extensive efforts to mitigate confounding factors by collecting abundant data and carefully pairing vocal clips in similar contexts.

To reviewer 3: 
Thank you for your valuable feedback. As for your concern on Section 4.2, we want to clarify that we use pairing methods in our experiment because we need to pair the data according to their context, otherwise, the classification will be influenced by the confounding factor of context. Even in binary classification to tell whether two clips come from different countries or not, there is an accuracy of 96.68%.

To reviewer 4:
Thank you for your sincere feedback. Regarding your concern about the dataset source, we did consider conducting the experiment in a controlled environment. However, achieving complete control over all circumstances is impossible, and it would have significant drawbacks when it comes to obtaining a large amount of data involving biases given limited data acquisition methods. Instead, we resolve to mitigate bias by collecting diverse data from open media sources.

While it is possible to make genetic assumptions, we have concrete results presented in Table 5, which demonstrate a clear correlation between dog vocals and their host speech.

Regarding the influence of the dogs' owners, we believe that all individuals speaking in the video can impact the dogs. As a result, we calculate the average value of host speech to account for this influence. In Section 2.1, we manually evaluated the accuracy of the spoken language, which stands at 93.2%.
