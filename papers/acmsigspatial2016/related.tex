\section{Related Work}
\label{sec:related}

In this section, we'll review the related work on traffic prediction, data imbalance problem, transfer learning and map distortion.

\subsection{Traffic Prediction}

Much work has been done on predicting traffic conditions, because 
it has many real-world applications. Most commercial mapping services
such as Google Map and Baidu Map offer the functionality 
to predict traffic status of major roads in future, often based on
the historical traffic data in the past.  Prediction of brand new
area with no historical data is rare.
	
Research work has been done to tackle this problem from 
different aspects. The previous traffic prediction methods 
can be divided into two categories mainly: 
Simulation Models and Machine Learning Techniques. 

\subsubsection*{Simulation Models} 
Based on observed traffic data, Clark
et al.\cite{Clark2003Traffic} proposes a non-parametric regression 
model to predict traffic. Bierlaire \cite{Bierlaire1998DynaMIT}, 
described a real time dynamic traffic assignment system 
that provides traffic prediction and other services like travel 
guidance, which contains two important simulation tools: 
a Mesoscopic Demand Simulator and a Mesoscopic Supply Simulator. 
These two microscopic simulation models are both based on 
trajectories of individual vehicles to simulate overall 
traffic data and further prediction. In some other papers 
\cite{KDD2011Driving}, researchers use traffic data from 
GPS-equipped taxicabs to estimate the traffic flow of 
certain road segments.
	
The main limitation of such studies is that they all rely on 
very sporadic observations so that they are often restricted 
to synthetic and simplified data for simulation. 
Also, these simulation models cannot be applied to other 
road segments where there isn't any historical traffic data.
	 
\subsubsection*{Machine Learning Techniques}
In the recent years, real-time high-fidelity spatiotemporal 
traffic data have become available. Thanks to this increase 
in the availability and quantity of real-time traffic data, 
researchers are allowed to develop and apply data mining and 
machine learning techniques to forecast traffic based on various 
real-world datasets.
	 	
A very common idea is to consider changes to traffic condition
on any stretch of road as a simple time series. 
Much valuable work was done along this line. Since early 1980s, 
univariate time series models, such as Auto-Regressive Integrated 
Moving Average(ARIMA)\cite{ARIMA} and Exponential Smoothing(ES) 
models \cite{ES,ES2}, have been widely used in traffic prediction. 
In the last decade, more researchers turned to Neural Network(NN) 
models in forecasting of various traffic parameters such as 
speed\cite{Xiao2003Fuzzy,Ishak2004Optimizing}, 
estimated travel time\cite{Lint2005Accurate} and 
traffic flow\cite{Stephanedes1981IMPROVED,Messer1998Short}. 
Today, results of these models are often used as benchmark 
for short-term traffic prediction. 

More recently, researchers at Microsoft and MIT\cite{} 
have attempted to solve similar problems by proposing 
complex networks, machine learning and even deep learning methods. 
Colak et al.\cite{Colak2016} and Simini et al.\cite{Simini2012} 
investigated the underlying mobility pattern and 
its implication on congested travel in urban areas. 
Schinitzler and Qi \cite{Schnitzler2014,Qi2014} proposed the 
usage of Hidden Markov Model as well as Gaussian Process 
in traffic prediction. Ren et al. \cite{Ren2014} predicts 
commuter flows in spatial networks using a radiation model based 
on temporal ranges. Chan \cite{Chan2012Neural} proposes a 
novel neural network training method that employs the 
hybrid exponential smoothing method to improve the generalization 
capabilities. Xu et al. \cite{Xu2015,Xu2015a} used 
spatial-temporal traffic prediction method. 
Zhang et al.\cite{zhang2014hybrid} presents a hybrid model for 
multi-step ahead traffic flow forecasting in a freeway system 
with real-time traffic flow data. And later on, 
in \cite{Zhang2015Hybrid}, the same authors proposed a 
hybrid traffic speed modeling and prediction framework which 
takes multi-time-scale historical traffic speed data in 
Hangzhou,China as well as related events as inputs. 
Mchugh and Pan et al.\cite{Mchugh2015,Pan2012} utilize 
big data or real world transportation data for traffic prediction 
and visualization. Xu \cite{Xu2014} proposed accurate and 
interpretable bayesian MARS for traffic flow prediction. 
Horvitz et al. \cite{Horvitz2012} studied methods, designs, and 
study of a deployed traffic forecasting service, which is 
a system similar to ours except they use self-deployed data source. 
Petrlik et al. Petrlik et al. \cite{Petrlik2015Towards} 
uses a new support vector regression (SVR) method 
which simultaneously optimizes the meta-parameters of SVR 
model and the subset of its input variables.
		
However, there are two main shortcomings in most of these models: 
The first one is that most of them are still based on the naive idea. 
They treat traffic flow only as univariate time-series data 
and thus ignore all the other important information 
which can be valuable features of a single segment. 
In our model, we included many local features like weather, 
house pricing, POI and so on; The second one is that they 
tend to study the prediction problem for each segment individually 
and ignore the connection between different segments. 
To address this problem, we combined non-local features like 
routing with local features to train our model.

\subsection{Data Imbalance Problem}
Real-world datasets are predominately composed of normal examples with only a small percentage of abnormal or interesting examples, which will case misclassified problems. As our problem is based on real-world traffic information, the class imbalance is a serious issue. For our case, there would be more data representing the class of clear traffic than other classes, often to large ratio. 

Previous researchers have done a lot in this field. One is to assign distinct costs to training examples. The other is to re-sample the original dataset by under-sampling and up-sampling. Under-sampling of the majority class is a good way to increase the sensitivity of a classifier to the minority class. But it doesn't improve minority class recognition. And the general idea of the cost function based approaches is that we think one false negative is worse than one positive, in other words, we give more weights to the false negative than false positive, so the machine learning algorithm will try to make fewer false negatives compared to false positives. In case of SVM, different classes can have different weights on them, resulting desired loss penalty.


Chawla et al. \cite{Chawla2011SMOTE} shows a hybrid algorithm called SMOTE (Synthetic Minority Over-Sampling Technique). They combined over-sampling and under-sampling. Their method of over-sampling the minority class involves creating synthetic minority class examples. They performed experiments using C4.5, Ripper and a Naive Bayes classifier. And they evaluated the method using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy. Their experiments show that minority class and under-sampling the majority class can achieve better classifier performance than only under-sampling.


Recently, researchers found that class imbalance isn't a problem in itself and performance degradation is also associated with other factors related to the distribution of the data. One of these is the presence of noisy and borderline examples. Certain intrinsic limitations of SMOTE can aggravate the problem produced by these types of examples. 
S\'{a}ez et al.\cite{S2015SMOTE} proposes the extension of 
SMOTE through an iterative ensemble noise filter called Iterative-Partitioning Filter(IPF), which can overcome the problems produced by noisy and borderline examples in imbalanced datasets.

In our experiments, we use three classifiers. Firstly, we use a classifier to decide whether the point is a green point, to some extent, this is a method to over-sampling the yellow and red points. Then we use the second classifier to classify the non-green points, to check whether they are yellow points. At last, we will use the third classifier to distinguish the rest points into red points and others.     
	

\subsection{Transfer Learning}
Many machine learning are based on the assumption that training and future data are in the same future space and have the same distribution. However, in real-world applications, there are many cases that we have a classification task in one domain while the training data in another domain. One of the best ways to solve this problem is transfer learning.


Transfer learning has been studied by researches for a long time to solve this problem. Semi-supervised classification \cite{14,15,16,17}addresses the problem that the labeled data may be too few to build a good classifier, by making use of large amount of unlabeled data together with a small amount of labeled data. Variations of supervised and semisupervised learning for imperfect data sets have also been studied on how to deal with the noise problems.

Pan et al \cite{Pan:Survey} categorize transfer learning into 
three sub-settings, inductive transfer learning, transductive transfer 
learning and unsupervised transfer learning, based on different 
situations between the source and target domains and tasks. 
Taylor et al. \cite{Taylor:Survey} introduces the transfer learning 
problem in reinforcement learning domains. In transfer, 
knowledge from one or more source tasks is used to learn one or 
more target tasks faster than if transfer was not used. 
The paper distinguished methods according to five dimensions: 
Jumpstart, asymptotic performance, total reward, transfer 
ration and time threshold.


In our problems, we get the training data from Baidu Map. Because there are some rural places where Baidu Map cannot provide their traffic conditions, we lack the training data of these regions. So we need to do transfer learning. We use data that those areas have similar features as these to be the training data to do transfer learning. 


\subsection{Map Distortion}
OpenStreetMap is an open-source map which is easy for us to edit but has a small amount of POIs while Baidu Map has plenty of POI but not available for us to edit it. So we came up with the solution to transform the POIs in Baidu Map to OpenStreetMap. Here comes another problem of map distortion because Baidu Map uses the BD-09(a proprietary coordinated system used exclusively at Baidu Maps) standard and OpenStreetMap Map use WGS-84(the international standard) format. 


Researchers in China have tried to solve this problem. Traditional method is to use Bursa-wolf(seven parameters) model or Moloden-Sky model (three parameters)[WGS-84] to transfer the latitude and the longitude. The first step is to acquire the ellipsoidal earth level gap. The second step is using the two common points of known coordinate to get the convert parameters. We tried the traditional method at first, but we found that the calculation is very complicated and costs a lot of time. Also it is hard for us to get the accurate results.

Liao et al.\cite{psh} proposed a useful method by implementing polynomial fitting. Their method take advantage of polynomial fitting by using a flat surface coordinate position as the dependent variable three-dimensional coordinate conversion. Its major feature is the transition point plane coordinates (x, y) as the dependent variable without knowing the target ellipsoidal earth level gap. 


We found Baidu has provided a simple API to convert WGS-84 coordinates to BD-09. After that conversion, we could transform the longitude and latitude to the point coordinated of Baidu Maps, and then using simple formula to calculate the exact pixel.
