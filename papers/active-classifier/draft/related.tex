\section{Related Work}
\label{sec:related}
% \KZ{Don't use casual terms such as ``like'', or ``got''. You need to have an
% overview of the section first about what you gonna talk about. Is it
% fastText or FastText? Please be consistent.}
In this section, we mainly discuss where fastText is often used, the background and common methods of AL methods proposed for different model and scenarios as well as the originality of our experiment setting.

With the prevalence of fastText, a lot of downstream tasks have been done with it and obtained acceptable results. Bojanowski and Grave \shortcite{ftblog} claims that in text classification tasks, fastText is on par with other deep learning classifiers and also cut training time from several days to just a few seconds. In their experiments, they compare fastText with VDCNN as well as char-CNN on Yahoo and Amazon datasets and achieve best results among these models. Besides text classification, it is also widely used to learn word vector representations \cite{bojanowski2017enriching}. It has been designed to work on 157 languages, including English, German, French, Chinese and so on. Due to its time-saving property, fastText is the most suitable deep model to due active learning since AL involves iterating many times. 

There has been a considerable amount of empirical work on active learning and
particularly on sampling strategies. They include uncertainty sampling, 
query-by-committee \cite{gilad2006query}, expected model change \cite{sznitman2010active}, 
expected error or variance minimization and information gain~\cite{joshi2012scalable}. 
Among them, the most popular and widely used is uncertainty sampling. 
Also, there exists some approaches which specially cater to certain classifier, 
such as convolution neural network~\cite{sener2017active}, 
recurrent neural network~\cite{zhao2017deep} and 
support vector machine~\cite{tong2001support}, 
or downstream tasks such as 
natural language processing~\cite{olsson2009literature}, 
visual recognition~\cite{luo2005active} and semantic segmentation~\cite{vezhnevets2012weakly}.

Considering all the surveyed paper, most classifier used in 
AL research on text classification is simple. 
For example, Tong and Koller \shortcite{tong2001support} conduct 
their experiment on support vector machine, while McCallumzy \shortcite{mccallumzy1998employing} 
designed algorithm on EM. So, it is valuable for us to design for fastText. 
In addition, even in Zhao \shortcite{zhao2017deep} who adopts RNN for AL, 
the number of classes in the of datasets was not mentioned. Therefore, 
our study on the effect of number of classes on AL performance is important and
useful.
