\section{Usage}
\label{sec:usage}
\subsection{Prerequisites}
We use Python 3.6.8 as our developing language. The library needed is shown as follows:
\begin{itemize}
    \item numpy 1.18.0
    \item pandas 0.24.2
    \item jieba 0.40
    \item nltk 3.4.5
    \item sklearn 0.22
    \item gemsim 3.8.1
    \item tensorflow 1.14.0
    \item keras 2.3.1
\end{itemize}
\subsection{Running}
We implement random sampling, uncertainty sampling,  density weighted sampling, radius sampling methods. For uncertainty sampling,  density weighted sampling, radius sampling methods, we have extra added frequency, square root frequency tuning. In terms of models, we have constructed fastText, CNN, BERT as well as LSTM with attention. They have been implemented in fastText.py, CNN.py and LSTM.py.

We can run the code using the following command. 
python cnn.py -d yanjing -m 2 -fm 5 -ib 100

Parameters have the following meaning:
\begin{itemize}
    \item dataset(-d): which dataset to use. New dataset can be added to the data directory. Tokenized data and embedding should be provided.
    \item number(-n): figure out the data classes number you desire. The training data should be provided in the /data directory. Data should contain both data itself as well as  the shuffled index and mapping saved in pickle file. [can be deprecated]
    \item mode(-m): figure out the sampling strategy
    \item factor mode(-fm): figure out the frequency tuning strategy. [can be deprecated]
    \item initial batch(-ib):figure out the seed labeled data size.[can be deprecated. Default value is set to 100.]
    \item step size(-s): figure out the batch size, number of points to be labeled at each iteration. [can be deprecated. Default value is set to 100.]
\end{itemize}

\subsection{Extension}
In addition to the sampling methods we have implemented, you can add some new sampling strategy. You can add a new function, set a mode number and call it in \textbf{test\_all} function. In the meanwhile, you can try new models. The only requirement the model should meet is to give data hidden sentence vectors and prediction vectors. If you need to add new dataset, you can put it in data directory and figure out its embedding.