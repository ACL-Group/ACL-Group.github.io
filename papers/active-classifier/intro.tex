\section{Introduction}
\label{sec:intro}

Recently large neural models such as ELMo \cite{peters2018deep} and BERT \cite{devlin2018bert} 
have been popular in natural language processing. 
Despite their improved results, the cost of training is mounting. 
Hofst√§tter and Hanbury~\shortcite{hofstatter2019let} 
reported that BERT is more than 100 times slower than non-contextualized ranking models. 
However, Xu \shortcite{Liang2019} has tried different text classification methods 
on Zhihu dataset and found that fastText is ten times faster than other methods, 
while achieving similar results. Therefore, fastText \cite{joulin2016bag}, 
as a good substitute, is balanced in both training time and test accuracy 
and is thus widely adopted in many industry tasks. 

Unfortunately, even if we use fastText for classification, 
large amount of training data is still essential. Annotated data is expensive to obtain, 
notably in specialized domains where only experts can provide reliable labels. 
As we will show later in \secref{sec:results}, for a news headline dataset which contains
325,285 samples, to achieve 95\% of the accuracy trained on all samples, by random sampling,
we need at least 55,100 samples, which is still very costly by human labeling.
Active learning (AL)~\cite{settles2009active} 
is thus proposed to ease the labeling effort
by selecting the most informative samples to label, based on certain criteria. 
In this paper, we study the behavior of active learning using
fastText classifier to solve text classification tasks. 

In deep models, the classification process can be generalized in the form of: 
\[y^* = \argmax_y f(\mathcal{X})\] 
where $\mathcal{X}$ is 
the internal representation of $x$, 
$f(\mathcal{X})$ is the prediction vector and $y^*$ is the predicted label. However, after investigating previous literature on AL, we find that most research is designed for binary or few-class classification problem (see \secref{sec:related}) and little attention has been paid on many-class text classification. 

In this paper, we investigate into several traditional AL methods 
and apply them to many-class short text classification using 
fastText. Results show that the advantage of traditional AL methods 
over random sampling diminishes with the increase of the number 
of classes in the classification task (see \secref{sec:eval}).
We further make two observations:
First, in unbalanced and noisy datasets, current sampling strategies often
produce a lot of duplicates and are relatively biased. 
For example, by entropy sampling, 
a lot of very similar samples are chosen because they have identical entropy score. 
%Therefore, we have proposed a frequency tuning metric such that the model won't lay too much emphasis on one certain class and to some extent solve the issue.
Second, most AL methods leverage statistic properties of $f(\mathcal{X})$, 
calculating the variation ratio \cite{freeman1965elementary}, mean standard deviation \cite{kampffmeyer2016semantic}, or the mutual information between predictions and model posterior \cite{houlsby2011bayesian}, but they ignore the features in $\mathcal{X}$, which is closer
to the raw input.

In order to address these issues, we investigated into previous approaches by 
analyzing the properties of $\mathcal{X}$ and come up with a new
sampling strategy called {\em radius uncertainty}, taking into consideration of 
the underlying distrbution of the input data itself. We also propose to use
class frequency in the current samples to mitigate the duplicated or 
near-duplicated sample problem. This simple strategy can be applied to
almost all previous AL methods.

Our contributions are as follows:

(1) The proposed frequency adjustment feature shows improvement over all previous AL methods.
 
(2) The new radius uncertainty strategy is the new state-of-the-art sampling strategy
according to our experiments on 4 many-class short text classification tasks.
 
(3) We are the first to conduct comprehensive analysis on different AL methods on 
multiple many-class datasets and provide insights of property and difficulty of 
designing a many-class AL sampling method. 
