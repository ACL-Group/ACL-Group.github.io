\section{Related Work}
\label{sec:related}

% \KZ{AL on few class text classification problem.}

% \KZ{AL on long text classification problem.}

% \KZ{Limited work on AL on short-text classification problem.}

% \KZ{Different text classifiers. Why we picked these 4 classifiers to study
% in this work.}


In this section, we mainly discuss different classifiers used for text classification,  previous active learning research on text classification and limited work on AL on short-text classification problem. 

Text classification has been a long-discussed topic and a wealth of classifiers have been designed such as boosting and bagging, logistic regression, Naive Bayes Classifier (NBC), non-parametric techniques k-nearest neighbor (KNN), support vector machine (SVM), tree-based classifiers as decision tree and random forest, conditional random fields (CRFs). With the prevalence of deep learning, deep models have achieved state-of-the-art results provided with ample labeled data. Therefore, in our experiments, we select popular deep classifiers LSTM with attention, convolution neural network (CNN), BERT as well as fastText.

% With the prevalence of fastText, a lot of downstream tasks have been done with it and obtained acceptable results. Bojanowski and Grave \shortcite{ftblog} claims that in text classification tasks, fastText is on par with other deep learning classifiers and also cut training time from several days to just a few seconds. In their experiments, they compare fastText with VDCNN as well as char-CNN on Yahoo and Amazon datasets and achieve best results among these models. Besides text classification, it is also widely used to learn word vector representations \cite{bojanowski2017enriching}. It has been designed to work on 157 languages, including English, German, French, Chinese and so on. Due to its time-saving property, fastText is the most suitable deep model to due active learning since AL involves iterating many times. 

There has been a considerable amount of empirical work on active learning on text classification but most of them put emphasis on binary or few class classification. Tong~\shortcite{tong2001support} surveyed AL on SVM on 10-class Reuters. Different from our experiment, Tong designed 10 binary classifier for classification. Mccallumzy~\shortcite{mccallumzy1998employing} modified QBC on EM and tested on 5-class NewsGroup as well as 10-class Reuters. Hoi~\shortcite{hoi2006large} conducted batch AL on Fisher model and conducted experiment on 10-class Reuters, 6-class WebKB as well as 11-class NewsGroup.
% query-by-committee \cite{gilad2006query}, expected model change \cite{sznitman2010active}, 
% expected error or variance minimization and information gain~\cite{joshi2012scalable}. 
% Among them, the most popular and widely used is uncertainty sampling. 
% Also, there exists some approaches which specially cater to certain classifier, 
% such as convolution neural network~\cite{sener2017active}, 
% recurrent neural network~\cite{zhao2017deep} and 
% support vector machine~\cite{tong2001support}, 
% or downstream tasks such as 
% natural language processing~\cite{olsson2009literature}, 
% visual recognition~\cite{luo2005active} and semantic segmentation~\cite{vezhnevets2012weakly}.

Considering all the surveyed paper, most classifier used in AL research on text classification is simple. 
Even in Zhao~\shortcite{zhao2017deep} who adopts RNN for AL on short-class dataset provided by Zhuiyi, the number of classes was not mentioned. 
