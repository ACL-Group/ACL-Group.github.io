Dear reviewers,

Thank you so much for the precious comments. Below we itemize our responses and the revisions we have undertaken.

Reviewer #1
1.1 We have removed the references to Weibo in the abstract and introduction and only mention our efforts on Weibo and its infeasibility in Sec 2.2 data sources. 

1.2 The number 46 was a typo. It should be 79 and we have modified it as such. 

1.3 The wordings in the first column of Table 3 may be misleading. What we meant was "All features minus verbs before drugs (feature 1)", etc. What we wanted to show is how much F1/accuracy drops if one of the features is turned off. We have rephrased column 1 to be "Without feature 1", "Without feature 2", etc. Moreover, in Sec 2.3.2, we reformatted the list of features in a table for better readability.   

1.4 We put the algorithm in an algorithm environment and combined Sec 2.3.3 and 2.3.4 into one subsection with a section title "Automatic labeling by bootstrapping."

1.5 We plot an additional line in Fig 3, indicating the changes to training data size over the iterations.

1.6 We have added the results for pattern-based, HMM and CRF on auto labeled data and reorganized Table 4 accordingly. 
	
Reviewer #2
2.1 What we actually meant was spontaneous reports can only be submitted by medical practitioners, and not normal patients, hence the data obtained through these reports represents just one source of information. Whereas information from medical forums which is studied by this paper comes directly from patients and may be more diverse and comprehensive, thus has the potential to cover rare ADRs. We have modified the wordings in this part of the introduction section.  
  
2.2 In Sec 2.1.2, we have stated that we use the lexicon from Sougou to cover colloquial terms. We have also added some examples from Sougou to Sec 2.1.2 in the revision.

2.3 Table 5 actually shows the percentage increase in the number of sentences after homophone 
transform and NOT ADR extension. We have revised the caption of that table accordingly. Moreover, we have evaluated the precision/recall of our extended ADR lexicon and included this new result in Sec 3.4. The main contribution of this paper is the binary classifier so we focused more on the classification of relations between the drug and the medical condition. But as we enlarged our lexicon as much as possible by combing the 4 lexicons from different sources and adding the colloquial term, more symptoms could be detected. 

2.4 Please see response 1.1.

2.5 The threshold 55 is not intended to determine if an ADR results from taking a drug. Rather, if a drug name and an ADR are more than 55 words apart in the text, we simply do not consider this pair at all. If the pair is within the 55-word distance, whether it is an ADR relation is determined by our classifier.

2.6 Our initial training data consists of 300 positive and 300 negative samples (the rest of the labeled data being tuning and test tests, see Sec 2.2). This is clearly not big enough to train a comprehensive classifier for all possible ADRs. Therefore, we seek to automatically enlarge the training data set in a bootstrapping fashion. The process of automatic labeling is indeed a form of active learning, in which the package inserts provides feedback in the learning process. 

2.7 It is actually possible to have negative ranking scores. We have considered using the ratio between positive and negative evidences, but later we decided that it serves the same purpose. 

2.8 The major contribution of this paper is not only using the Chinese social media data, but also the bootstrapping framework to automatically enlarge training data. We believe this framework can also be applied to English data and benefit that part of the world as well.

Reviewer #3
3.1 It is a common practice to train a binary classifier with balanced data, because i) the distribution of the labels is often not known a priori; ii) we would have obtained a biased classifier if the data is not balanced. 

3.2 The original training data (300+300) was labeled by human and contains ADR pairs that cannot be found from package inserts. Subsequently, it is true that we use the ADR or indication information from package inserts to help us decide if an evidence is positive or negative when we automatically accumulate more training data. So indeed, during test time, if a pair of drug-ADR comes from a sentence with features similar to an indication relation, our classifier is inclined to predict false. Our classifier is not particularly trained to consider the "third class" which is no relation between the drug and the ADR, though such example may exist in the original negative training data. Having said that, we want to stress that our test data has been labeled by human who would label the "third class" as false. Therefore, there's nothing wrong
with our evaluation results. Furthermore, in the revised version, we modify Fig 1 to make our overall framework more comprehensible. We also change the title of Sec 2.4 to "Baseline classifier techniques"

3.3 We have modified Table 4ï¼Œto include more results for auto-labeled data (see response 1.6). 

3.5 The features we used in this paper have previously been used one way or the other in previous relation extraction/classification work. We use verbs and prepositions as our features here because they collectively form dependency relations such as dobj and pobj, which provide important signals when it comes to the distinction of relations or predicate classification.
 
3.6 Please see response 1.1. We also changed the caption of Table 2 to "Category of drugs studied" and the term "Disease" to "Category" in the table header.