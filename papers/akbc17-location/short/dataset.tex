\section{Datasets}
\label{sec:data}
\vspace{-10pt}
Our proposed vocabulary of single-word physical objects is constructed by the intersection of all entities that belong to ``physical object'' class in Wikidata and 
all ConceptNet concepts. 
We then manually filtered out some 
words that have the meaning of an abstract concept, which 
results in 1169 physical objects in total.

Afterwards, we utilize a cleaned subset of the Project Gutenberg corpus~\cite{lahiri:2014:SRW}, which contains 3,036 English books written by 142 authors.
% 
An assumption here is that sentences in fictions are more likely to describe real life scenes. 
We sample and investigate the density of \lnear~ relations in Gutenberg with other 
widely used corpora, namely Wikipedia, 
used by~\citeauthor{mintz2009distant}~(2009) and New York Times corpus, 
created by~\citeauthor{riedel2010modeling}~(2010) and 
used by~\citeauthor{Lin2016NeuralRE}~(2016),~\citeauthor{hoffmann2011knowledge}~(2011),~\citeauthor{surdeanu2012multi}~(2012). 
In the English Wikipedia dump, out of all sentences which mentions at least two
physical objects, 32.4\% turn out to be positive. 
In the New York Times corpus,
the percentage of positive sentences is only 25.1\%. 
In contrast, that percentage in the Gutenberg corpus is 55.1\%, much higher 
than the other two corpora, making it a good choice for \lnear~ 
relation extraction.
%\subsection{\lnear~ Sentences Dataset}
%\label{lsd}

From this corpus, we identify 15,193 pairs that co-occur in more than 10 sentences.
Among these pairs, we randomly select 500 object pairs and 
10 sentences with respect to each pair for annotators to label their commonsense~\lnear. 
%We do not distinguish between \lnear\ and \textsc{atLocation} relations,
%where the latter typically refers to objects which are adjacent to or
%contained by each other, e.g., {\em room} and {\em door}.
Each instance is labeled by at least three annotators who are college students
and proficient with English. 
The final truth label of a sentence is decided by a majority vote from the four annotators. 
The Cohen's Kappa among the three annotators is 0.711 which suggests substantial agreement. 
%This dataset will be used to train and test models for relation
%classification.
%We compare the statistics of our \lnear\ sentence
%dataset with a few datasets on other well known relations in 
%\tabref{tab:datasets}.
%One can see that our dataset almost double the size of those most
%popular relations in the SemEval task, and the sentences in our
%data set tend to be longer with more words.
{We randomly choose 4000 instances as the training set and 1000 as the test set for evaluating the first sentence-level relation classification task.}
%\begin{table*}[th]
%\centering
%\small
%\begin{tabular}{|l|c|c|c|c|} \hline
%Data set & Frequency & Percentage & Words per entry & Chars per word  \\ \hline \hline
%\lnear & 2,754 & 55.1 & 18.6 & 4.51  \\ \hline
%Not \lnear& 2,246 & 44.9 & 19.1 & 4.32  \\ \hline\hline
%Cause-Effect & 1,331  & 12.4 & 17.3 & 4.71\\ \hline
%Component-Whole &1,253 & 11.7 & 17.9 & 4.12 \\ \hline
%Others &1,864 & 17.4 & 17.8 & 4.34 \\ \hline
%\end{tabular}
%\caption{Comparison between our \lnear~ dataset and 
%the most popular relations from SemEval 2010 Task 8 dataset for 
%relation classification}
%\label{tab:datasets}
%\end{table*}
%\subsection{Commonsense \lnear~ object pairs}
%We randomly sampled 500 pairs of objects by the number of sentences they
%appear in. 
%This tends to give us pairs which are more popular.
For the second task, we further ask the annotators to label whether each pair of objects are likely to locate near each other in the real world. 
Majority votes determine the final truth labels.
The inter-annotator agreement here is {0.703}.  
Both datasets are made publicly available.\footnote{\url{https://adapt.seiee.sjtu.edu.cn/~frank/location_relation_data.zip}}