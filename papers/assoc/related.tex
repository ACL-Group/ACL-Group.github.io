\section{Related Work}
\label{sec:related}

In this section, we introduce a number of studies
in {\em semantic relatedness computation}
% (ESA, LSA, survey including
% many corpus based similarity measures, taxonomy based similarity)
and related work in {\em free association}.
% (original paper about FA, various FA norm data sets, prediction of FA).

Previous approaches to semantic relatedness pursue two main
directions, of using hand-crafted lexical taxonomies like
WordNet \cite{Miller1995} or Roget's Thesaurus \cite{Roget} as
semantic knowledge, or of employing probabilistic approaches to
decode semantics based on large corpora.

The first approach of using hand-crafted resources proposes
knowledge-based measures that tap into the properties of their underlying
structure to compute semantic relatedness
 \cite{Roget,Lin:1998,leacock1998,Hirst:1998,Jiang:1997,Resnik:1995,wu1994verbs}.
Though showing potential in such tasks like term relatedness
computation, this approach requires to construct manually curated lexical
resources and thus cannot easily scale to larger lexical coverage or
to a new language.

On the other hand, the second approach of using corpus-based
measures, instead of relying on human-organized knowledge, utilize
the contextual information and patterns observed in large corpus to
construct semantic profiles for words. Latent Semantic Analysis
(LSA) \cite{LSA} was an original approach to leverage word
co-occurrences from a large corpus of text, and ``learns''
its representation by applying Singular Value Decomposition to
the words-by-documents co-occurrence matrix. Explicit Semantic Analysis
(ESA) \cite{ESA} as well as Salient Semantic Analysis (SSA) \cite{SSA}
were proposed to incorporate large amounts of human
knowledge such as Wikipedia into word
relatedness computation. They both represent a word as a concept
vector, where each dimension corresponds to a Wikipedia concept.
Later, Temporal Semantic Analysis (TSA) \cite{TSA} considered that
words have different meanings over time and extended the concept
vector with a temporal dimension.
To bridge the corpus-based measures with
knowledge-based measures, Constrained LEArning of Relatedness (CLEAR) \cite{CLEAR}
was proposed to learn word
relatedness based on word occurrence statistics from large corpora
while constraining the learning process by incorporating knowledge
from WordNet.  Some recent works like \cite{mikolov} used machine learning techniques
to compute continuous vector representations of words from large datasets
, shown to perform better than LSA for preserving linear regularities among words.

Some models aim particularly at solving the similarity problem
between two sentences, or two short
texts \cite{WTMF,WTMF+PK,SPD-STS,LSA_STSS}. WSD-Based Sentence
Similarity \cite{SPD-STS} was proposed to compute the similarity
between two sentences based on a comparison of their actual meanings
by integrating word sense disambiguation. WTMF \cite{WTMF} was
proposed to model the missing words in the sentences as a typically
overlooked feature to address the sparseness problem for the short
text similarity task.

All the existing semantic relatedness models mentioned above, though
leveraging some useful signals from hand-crafted lexical taxonomies
or large corpus text, fail to actively take advantage of the human
perception signal in semantic relatedness computation. Our approach,
by effectively bridging this gap using signals in the well-studied
psychological process of free association, outperforms
state-of-the-art models in both word and short text relatedness
tasks.

%Free association is a task that requires participants to produce the
%first word to come to mind that is related to
%a presented cue word. This task is used in everyday activities as
%a means for ``collecting thoughts.'' For example, when one uses the
%Yellow Pages, free association from a needed product or service can be
%helpful in determining an effective search heading \cite{Nelson:2000}.
%As free association itself is a daily practice for humans to
Free association is a task requiring human participants to produce the
easily associated word for the given cue word, to tap into human perception acquired through
world experience \cite{Nelson:2004}.
Mining the signals contained in this cognitive process
is made possible by several collections of free association norms
 \cite{Nelson:2004,kent1910study,Minnesota,kiss1973associative},
which are typically collected by researchers in phychology and
cognitive science. As the Florida Norms is the
largest collection available, and also the most recent in time,
we choose to use it as our primary source of human perception
to be combined with signals from Wikipedia.

% Some researchers use Wikipedia to solve the relatedness measuring problem.
%  \cite{ZhangX:2010,Ito:2008,Gabrilovich:2007}
% Zhang et al.  \cite{ZhangX:2010}propose a method using a generalized maximum flow
% to measure implicit relations between two Wikipedia concepts, and argue that
% their method is able to measure the strength of a relation appropriately by using
% three important factors: distance, connectivity, and co-citation.
% They also argue that another important aspect of their method
% is mining elucidatory objects, that is, objects constituting a relation,
% which can help people understand relations between concepts.
% Ito et al.  \cite{Ito:2008}propose an approach that leverages global statistical information of the whole Wikipedia to compute semantic relatedness among concepts (disambiguated terms) by analyzing co-occurrences of link pairs in all Wikipedia articles. They argue that In Wikipedia, an article represents a concept and a link to another article represents a semantic relation between these two concepts. So the co-occurrence of a link pair indicates the relatedness of a concept pair. Then they design algorithms based the link information in Wikipedia to solve the accuracy and
% scalability problems existing in previous methods.
% The problem with these two methods described above is that they just use link information in Wikipedia, and the links is much more sparse compared with TB and SL used in our MindDrifter. In addition to incorporating more knowledge information, another advantage of our MindDrifter lies in that we simulate the real mind drifting process based on the transition of context, which was never considered in these two methods described above.
% Other kind of relatedness measuring is also searched. For example, Gabrilovich et al.  \cite{Gabrilovich:2007} focus on computing semantic relatedness
% of natural language texts. They propose a method that represents the meaning
% of texts as a vector in a high-dimensional space of concepts derived from Wikipedia,
% and then use conventional metrics like cosine similarity to
% assess the relatedness of two text vectors.

% Besides Wikipedia, other corpora are also used in corpus-based methods. \cite{Chen:2006,Bollegala:2011}
% Chen et al. \cite{Chen:2006} propose a web search with double checking
% model to explore the web as a live corpus for the associations' mining.
% Bollegala et al. \cite{Bollegala:2011}propose an empirical method to estimate semantic similarity using page counts and text snippets retrieved from a web search
% engine for two words. The optimal combination of different relatedness information they mined from the web is learned using support vector machines.

% Apart from corpus-based methods, the knowledge-based methods
% are also explored in this field.
% Resnik \cite{Resnik:1995}presents a new measure of semantic similarity
% in an is-a taxonomy, based on the notion of information content.
% The author argue that one of the advantage his method have over edge counting
% method is that it is not sensitive to the problem of varying link distances.
% In addition, by combining a taxonomic structure with empirical
% probability estimates, it provides a way of adapting
% a static knowledge structure to multiple contexts.
% Agirre et al. \cite{Agirre:2009} presents and compares WordNet based
% and distributional similarity approaches and then propose a combination of
% these two approaches.
% Morris et al. \cite{Morris:1991} argue that in text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning.
% These lexical chains are a direct result of units of text being ``about the
% same thing,'' and finding text structure involves finding units of text that are about the same thing. Then they a method to compute the chains and use it to know the structure of the text.
% Strube et al. \cite{Strube:2006}investigate the use of Wikipedia for computing
% semantic relatedness measures and shows that Wikipedia provides a suitable
% encyclopedic knowledge base for extracting semantic information.

% Alvarez et al. \cite{Alvarez:2007} present a novel algorithm for scoring the semantic similarity(SSA) between words. Given two input words $w_1$ and $w_2$, SSA exploits their corresponding concepts, relationships, and descriptive glosses available in WordNet in order to build a rooted weighted graph. The output score is calculated by exploring the concepts present in the rooted weighted graph and selecting the minimal distance between any two concepts $c_1$ and $c_2$ of
% $w_1$ and $w_2$ respectively.
% Li et al. \cite{Bollegala:2011}propose a lightweight and effective approach for measuring
% semantic similarity using a large scale semantic network automatically
% acquired from billions of web documents, which can accurately compute
% the semantic similarity between terms with multi-word expressions(MWEs) and ambiguity.
% Pirro et al. \cite{Pirro:2009} present a method exploiting some notions of the feature-based theory of similarity and translates it into the information theoretic domain, which leverages the notion of Information Content (IC). In particular, the proposed metric exploits the notion of intrinsic IC which quantifies IC values by scrutinizing how concepts are arranged in an ontological structure.
