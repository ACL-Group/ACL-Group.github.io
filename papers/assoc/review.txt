----------------------- REVIEW 1 ---------------------
PAPER: 1497
TITLE: An Association Network for Computing Semantic Relatedness
AUTHORS: Keyang Zhang, Kenny Zhu and Seung-Won Hwang

Significance of the Contribution: 6 (+ (slightly positive))
Soundness and Positioning with Respect to Related Work: 8 (+++)
Depth of Theoretical and/or Experimental Analysis (as appropriate): 7 (++)
Quality of Presentation: 9 (++++)
SUMMARY RATING: 2 (++)

----------- COMMENTS FOR THE AUTHORS -----------
This is a well-written paper focused on the problem of semantic similarity.

I thought that the idea of using florida data on free word association was a very clever
basis to serve as a  point of reference as well as to train the linear regression model.

The authors compute five different similarity metrics based on wikipedia. They do a good job
of explaining what they are and their approach for combining them into one score.  The experiments seemed to be well-designed and results convincingly argued.  Some of the improvements seem
rather incremental (e.g., in Table 3, 0.810 vs 0.812).

I have one major issue with the way the authors describe the related work. They seem to suggest
that the corpus based approach taken by them is somehow superior than 
hand-crafted lexical taxonomies
like WordNet(Miller 1995). They fail to acknowledge that Wikipedia information that they
are using is also hand crafted.  (Response to the rebuttal:  The correct comparison to make
is not between hand-crafted and corpus-based methods, but between hand-crafted sources
that are curated by one research group vs hand-crafted sources that are crowd-sourced.
Perhaps, another comparison could be drawn between a hand-crafted source that has not
explicit links to source text (ie, Wordnet for which the only explicit link is to glosses)
vs Wikipedia (where the representation is embedded as part of the article.).

Why not consider a similarity metric that combines what the authors
have created with a wordnet based method?


----------------------- REVIEW 2 ---------------------
PAPER: 1497
TITLE: An Association Network for Computing Semantic Relatedness
AUTHORS: Keyang Zhang, Kenny Zhu and Seung-Won Hwang

Significance of the Contribution: 6 (+ (slightly positive))
Soundness and Positioning with Respect to Related Work: 8 (+++)
Depth of Theoretical and/or Experimental Analysis (as appropriate): 8 (+++)
Quality of Presentation: 8 (+++)
SUMMARY RATING: 2 (++)

----------- COMMENTS FOR THE AUTHORS -----------
This paper combines psychological data with web data to compute an
approach to determine semantic relatedness.  First it computes a
relatedness graph from Wikipedia entries.  Then it uses linear
regression to adjust the weights wrt to the Florida free association
norms gathered by experiments.  Then it performs a series of
experiments showing how the combination performs better than
its competitors.  This is a novel approach.  I like the extensive experiments.

The paper's english is a bit sloppy in places.

"free association" in abstract is not correctly grammatically used.

Run a spelling checker.

In the conclusion "We propose"  I presume that is not true, i.e., this 
is a concept paper?


----------------------- REVIEW 3 ---------------------
PAPER: 1497
TITLE: An Association Network for Computing Semantic Relatedness
AUTHORS: Keyang Zhang, Kenny Zhu and Seung-Won Hwang

Significance of the Contribution: 6 (+ (slightly positive))
Soundness and Positioning with Respect to Related Work: 4 (--)
Depth of Theoretical and/or Experimental Analysis (as appropriate): 6 (+ (slightly positive))
Quality of Presentation: 6 (+ (slightly positive))
SUMMARY RATING: -2 (--)

----------- COMMENTS FOR THE AUTHORS -----------
This paper focuses on the problem of computing semantic relatedness between two words/texts. Their general approach is an existing one: to generate an association network from which relatedness can be quickly computed from the graph weights. Like several other papers in recent years, these weights come from mining Wikipedia. However, their primary contribution is two-fold and comes in how they compute these weights: (i) they introduce an expanded set of co-occurrence metrics that take advantage of the structure/content of Wikipedia and (ii) they combine these metrics via a weighted sum as calibrated by a relatively large free association data set. The paper evaluates this method on a number of appropriate data sets and concludes that both introduction of new metrics, as well as the weighting based on free association, lead to state-of-the-art performance.

++ Significance of the Contribution ++

The focus of this paper is more appropriate for an NLP/IR venue, and thus it is difficult for me to be too confident in my critique of the significance of the contribution. However, the new metrics and resulting evaluations seem to be incremental in flavor (i.e. co-occurrence is well used, and experimented with in various ways) and magnitude (referring particularly to the results of Table 1-4). As this approach was not contextualized within an actual application, it is not clear to me, for instance, the degree to which a difference of 0.04 in the Pearson correlation on the Li30 dataset will make in cognitive systems. Also, given that this appears to be a typical pipeline (i.e. batch analyze/train with Wikipedia; break down the input into a bag of words; compute similarity) it is unclear to me how this work would fit into the realm of cognitive systems, including issues of incremental learning (section 3.7 talks to execution time of extracting values from the network, but is !
 it possible to incrementally add new terms/concepts/weights? how long would that take?).

++ Soundness and Positioning with Respect to Related Work ++

The authors position the paper within an appropriate body of work, but claim that prior work fails to “actively take advantage of [the] human perception signal in semantic relatedness computation.” Rather than define what is meant by this signal, the authors claim that incorporating more sources of co-occurrence and training model parameters via free association data will bridge the gap. The results in the paper seem to suggest that there is merit to the approach, but the authors disappointingly do not attempt to unpack the homunculus that is “cognitive human perceptions of relatedness.” For example, in the approach section (particularly 2.2), the paper introduces each of the new co-occurrence sources, with examples, but fails to explain how/why these better access the “human perception signal.” Some of these sources make sense (e.g. looking to the categories and links), but have also been looked at before in context of NLP applications like word sense disambigua!
 tion (see Mihalcea, 2011: Using Wikipedia for Natural Language Processing).

++ Depth of Theoretical and/or Experimental Analysis ++

The paper’s evaluation section walks through several stages of analysis, including comparison to other methods, analysis of source-data size, word vs. text similarity, and with/without the novel co-occurrence sources. These seem appropriate benchmarks for such an approach. I feel that I would have liked to have seen an experiment showing just the effect of free association training, possibly with another approach (this seems to be a major source of “human perception”) as well as embedding this approach within an actual application to see more a more grounded metric for improvement.

++ Quality of Presentation ++

Much of the text of the paper is well-written and easily understandable, though there is a distinct shift in writing starting in Section 3 (suggested edits/typos below). The references seem to be inconsistent/missing information relating to conferences, providing only the abbreviation (sometimes with redundant years) and no proceedings page numbers.

As far as I can tell, the authors made little, if any, effort to contextualize this paper within the cognitive systems track.

Suggested edits:
* Section 1: “offer insignificant signals that match" -> “to match”
* Section 2.2, Lemma 1: “Each term t in T0 appear in” -> “appears in”
- Figure 1: The caption is not useful to parse the image. The reader must go back and forth to the text. I would recommend either enhancing the caption or embedding co-occurrence labels within the image).
* Section 2.2: “We defer the discussion of the choice of alpha in Section 2.4” -> “till Section 2.4”
* Figure 2: The caption is misleading. This isn’t about overall distribution of co-occurrence, it is a discrete comparison of the normalized weights of five example pairs of super nodes in a particular text.
* Section 3.1: “while Rubenstein & Goodenough… set” -> “while the Rubenstein"
- Throughout Section 3/4, there are cases in which words run into citations/parenthetical (e.g. STSS-131(O’shea) -> add a space. However, many parenthetical phrases also have a trailing space (e.g. (LSA )).
* Section 3.2: “We apply term relatedness algorithm” -> “We apply the term”
* Section 3.2: “As expected” -> I’m not sure why this would be expected... it relates to the core hypothesis of your paper.
* Section 3.3: “Our method is just mapping … and assign w(u,v) as predicted” -> “and assigning w(u,v) as the predicted”
* Table 3: add spaces before years, be consistent about significant figures
* Section 3.5: “built off from mere sentence-level” -> “built from sentence-level”
* Section 3.6: “we can observe that, though individual type of cosine” -> “though each individual type”
* Section 3.7: “ralative” -> “relative”
* Section 3.7: “the time and space consumptions can” -> “consumption”
* Section 4: “First approach of using” -> “The first approach”
* Section 4: “the second approaches of using … utilizes the” -> “the second approach”
* Section 4: “take advantage of human perception signal” -> “the human perception signal”
* Section 4: “associate related things” -> fix
* References: Wettler M should be Wettler, M

++ SUMMARY RATING ++

This paper may be an appropriate paper for a general AI or, more likely, NLP/IR, venue but not the cognitive systems track. The approach discounts any structure in source sentences (“we simply abstract text as a bag of super nodes”) and does not attempt to connect with modern work in incremental text understanding and/or associative memories.

++ Breadth of Interest to the AI Community ++

In the general AAAI community, I think such approaches are of more interest.

++ Easily Accessible Paper ++

In general, the approach follows a well-understood pipeline. I think the authors would improve accessibility by better contextualizing the results, possibly with an actual application.


----------------------- REVIEW 4 (additional)---------------------
PAPER: 1497
TITLE: An Association Network for Computing Semantic Relatedness
AUTHORS: Keyang Zhang, Kenny Zhu and Seung-Won Hwang

Significance of the Contribution: 4 (--)
Soundness and Positioning with Respect to Related Work: 7 (++)
Depth of Theoretical and/or Experimental Analysis (as appropriate): 4 (--)
Quality of Presentation: 3 (---)
SUMMARY RATING: -2 (--)

----------- COMMENTS FOR THE AUTHORS -----------
This paper suggests to compute semantic relatedness between terms and short texts by constructing a graph over text/wikipedia concepts and fitting edge weights with free association data.

I think there are two interesting things in this paper:
a. Use of the free association data for the semantic relatedness task
b. Good empirical results

However, I think the paper is lacking in several aspects

a) The authors claim that using free associations instead of co-occurrence is important, but then what they actually do is use text co-occurrence to predict free association labels. It is completely unclear to me why these features can approximate free association, and whether any other random set of features could have fit free associations better. So as far as I see, the data for free associations is useful for the task of semantic relatedness. But I have no clue whether the particular method the authors use to approximate free associations makes sense or is better than many other possible alternatives. 

So what is the main claim? That using free associations as supervision is important? Or that their particular way of predicting free associations is the right way to go? I am definitely not convinced of the second, which seems to be the main claim.

b) Their method is neither simple or intuitive. After constructing the graph there are many heuristics going on, like using 9 vectors in a 3x3 setting and so on. If the graph is good - why not just read similarity from the graph directly using the edge weights, random walks or something that uses the graph more directly. I think that having a simple or intuitive method is important, otherwise it is hard to learn how to generalize the results from this paper.

c) The paper is not written clearly in various points in my opinion, especially section 2.3 should be improved.

d) Personally I am not a fan of the semantic relatedness task since it is not clear why it is useful.


-------------------------  REVIEW 5 (additional, informal) ------------------------

I'm very surprised that there's no mention of the more recent line of work on word embeddings (a few papers from Tomas Mikolov for instance). Also recent papers in translation (from Yoshua Bengio's lab) showed that learning word embeddings for translation tasks yield very good semantic relatedness on all those tasks cited in the paper. Given that the paper is not even mentioning them I would recommend rejection...


-------------------------  REVIEW 6 (additional, informal) ------------------------
I recommend rejection.
The described algorithm looks messy and unprincipled, and the presentation is not good: many missing details, algorithm descriptions are not clear.
The basic component of the presented framework, the “free association network” is not a well-defined term, and to me it looks close to be a graph of semantic relatedness
(the term semantic relatedness is not well-defined by itself ).  Some of the experiments may be contaminted - for example, when testing with ANfree on WS-227, it is not
clear how many test pairs were directly connected in the original Florida data.  
The results of ANwiki are impressive, but are not sufficient for accepting the paper for two reasons:
1.  As I said, as the algorithm is so messy, it is difficult to see and understand what is the gist of the method - that contributed most to this result.
2.  When research on semantic relatedness, there were hardly any datasets available, so testing on two was sort-of acceptable.  Today I believe that this is far from being enough.   There are quite a few more around.  Actually, I believe it could be interesting to use the Florida data as a test dataset.