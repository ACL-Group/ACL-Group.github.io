\section{Introduction}
\label{sec:intro}

% \begin{enumerate}
% \item Motivation: application scenarios (with 1-2 running examples);
% \item Characteristics of the data sources and their challenges;
% \item Briefly introduce previous approaches to extract information
% from images including setting the document zone, and their limitations.
% \item General flow of our approach (may give a diagram here)
% \end{enumerate}
% scenary

Due to ever evolving hardware and software, many medical images
such as electro-cardio graphs (ECGs), X-ray or ultrasound images
are directly printed and stored in hard copy formats.
Examples are shown in \figref{fig:medicalImages}.
% These images often contain a mix of graphics and text, which
% include parameter settings of the hardware, test measurements or simple
% diagnosis.
These images often contain a mix of graphics and text, which
include technical settings of the hardware used, test measurements or simple diagnoses.
Recently, there has been a growing demand for digitizing such
medical information from paper media sources.
Apart from scanning the graphics into a digital format, extracting
the semi-structured textual information is also an important part of
building electronic medical records for patients.

\begin{figure}[!htb]
\centering
\subfloat[ECG]{
\label{fig:medicalimage:ecg}
\epsfig{file=figure/17_ori.eps, width=0.5\columnwidth}
}
% \hfill
\subfloat[X-RAY]{
\label{fig:medicalimage:xray}
\epsfig{file=figure/X-RAY.eps, width=0.5\columnwidth}
}
\hfill
\subfloat[MRI]{
\label{fig:medicalimage:mrt}
\epsfig{file=figure/MRI.eps, width=0.5\columnwidth}
}
\subfloat[EEG]{
\label{fig:medicalimage:eeg}
\epsfig{file=figure/EEG.eps, width=0.5\columnwidth}
}
\caption{Examples of Medical Images}
\label{fig:medicalImages}
\end{figure}

Optical character recognition (OCR) is
a traditional technique used to turn images of printed text into machine encoded
text. It is well researched and performs well on plain text
documents such as novels and reports, for a variety of languages.
For example, Tesseract, which is one of
the most popular open source multilingual recognizers, logs an error
rate of 3.72\% for English words and 3.77\% for simplified
Chinese characters\cite{smith2009adapting}.
Google Books \cite{googlebooks} and Gutenberg \cite{gutenberg} are
projects which have scanned a large number of paper books into text for free and open
access. These projects made exclusive use of OCR for this conversion and
achieved high accuracy \cite{vincent2007google} \cite{lebert2008project}.
% 99\% for Gutenberg project \cite{lebert2008project}.
% \KZ{Give the accuracy of google and gutenberg if available.}

\begin{figure}[ht]
\centering
\epsfig{file=figure/17_b.eps, width=0.8\columnwidth}
\caption{An ECG Image with Interesting Text Area}
\label{fig:ecgexample2}
\end{figure}

For a semi-structured medical image, such as
\figref{fig:ecgexample2}, we would like to extract the attribute-value
pairs (e.g., {\em Vent. rate = 63 bpm}) and possibly other values such as
date (e.g., {\em 18-Nov-2010}) and time (e.g., {\em 9:13:02}) since those values endow us with lots of information about the patient.
Existing OCR software cannot extract such structured information in a straightforward
fashion,
but instead it produces rather convoluted results from the whole image,
similar to those in \figref{fig:ocrre}, which was produced by Tesseract.

\input{xmlre1}

%However, OCR alone does not work well on semi-structured text and hence
%can't be directly used for information extraction from the aforementioned
%medical images. \KZ{Give the reason here, perhaps because OCR models are
%largely Markov based? So semi-structured data breaks the flow of text.}
%When a medical image is input to an ordinary OCR software, the spatial
%information of the text components is often lost or mixed with noises
%and errors.
%%The reason is OCR converts the whole images into text data, in which
%%useful information often mix with noises and errors.
%In this paper, we would like to extract the attribute-value pairs
%and possibly other values from \figref{fig:ecgexample1}
%and \figref{fig:ecgexample2}.
%% or medical ultrasonography report.
%Such images contain lots of non-textual information or noises.

% example & ref
%\begin{figure}[ht]
%\centering
%\epsfig{file=figure/46.eps, width=0.8\columnwidth}
%\caption{ECG Images From Printer1}
%\label{fig:ecgexample1}
%\end{figure}

% \begin{figure}[ht]
% \centering
% \subfloat[Printer1]{
% \label{fig:ecgexample:a}
% \epsfig{file=figure/46.eps, width=0.48\columnwidth}
% }
% \hfill
% \subfloat[Printer2]{
% \label{fig:ecgexample:b}
% \epsfig{file=figure/17.eps, width=0.48\columnwidth}
% }
% \caption{ECG images from two different printers}
% \label{fig:ecgexample}
% \end{figure}

There are a number of significant challenges involved in extracting the information from medical images or OCR results in XML form.

% First, medical images differ from pure text document in that them have
% layout information.
First, medical images differ from pure text documents in that
they contain layout information.
Although most current OCR engines attempt to reproduce the physical
layout of the text units (along with X-Y coordinates) and store them
in a special format such as XML
% (\KZ{Better in the previous example})
,
such spatial
information is approximate and sometimes inaccurate, which is why neighboring
text blocks in \figref{fig:ecgexample2}, such as ``Vent. Rate'' and
``63 bpm'' were not automatically combined into the same XML block, but were
rather far apart (shown in two different ``classes'') in \figref{fig:ocrre} made by OCR softwares.
Even for images produced by the same ECG printer,
the XML results can still be very different as the spatial layout is
sensitive to many factors, such as accidental spots on the prints, color and
contrast, or the angle of the scanner camera.
In this case, solutions for other application domains, for example, the web,
are not well suited for information extraction from printed documents \cite{bartoli2014semisupervised}. With such inaccurate
layout information produced by OCR,
it is not easy to write a simple wrapper program to extract useful
data from images, even if the images come from the same printer.
%Writing a wrapper for each
%individual image would be tedious and counter-productive. Therefore,
%a mechanism that makes use of the spatial locality of the
%text units in the image and
%accommodates slight variations in the spatial layout would make the extraction
%more accurate and fault-tolerant.

%For example, \figref{fig:ocrre} is the simplified OCR results for the ECGs in
%\figref{fig:ecgexample1} and \figref{fig:ecgexample2}. The results are in the XML format and have attritube named {\em class}
%for layout information. Although these two images share similar format.
%OCR engine generates different results in that it splits elements that
%should be in the same line into two lines in the second example.
%XML is sensitive to the layout results so it's hard to tolerate
%all the layout results.
%
% example check the term
% layout of ocr results can be restore, so why OCR engine don't restore the results
% using the similar methods as we do?
% or the way we handle the layout problem is quite simple

% Delete for TIP
% Second, exiting OCR engines make heavy use of Markov properties such as n-grams
% since they primarily target the transformation of large body of text
% \cite{kolak2003generative}.
% % \KZ{Needs some refs here.}
% Unfortunately, the semi-structured texts in medical images are often
% short and not even written in complete sentences, thus breaking Markov assumption. To make
% matters worse, medical images contain scientific language, which may be
% very different from the training corpora of these OCR engines.
% This explains why we see errors like ``Vcnt'' and ``rule''
% in \figref{fig:ocrre}.
% %can't guarantee a perfect performance, which means
% %there are errors and noises in the OCR results.
% %Many of them due to the fact that the data are no longer long, continous
% %sentences, thus breaking the Markov assumption made by many OCR algorithms.
% %In \figref{fig:ocrresub:b}, ``Vent." is misrecognized as ``Vcnt.".
% Without sufficient contextual information, OCR may also misrecognize a
% digit as an alphabetic character, or as another similar digit.
% Furthermore, the mix of text with images and formatting
% lines often confuses the OCR engine, which is more biased toward full
% text images.
% Exact pattern matching, as used in
% traditional information extraction, doesn't work with such noisy OCR output
% as it doesn't tolerate noises or errors in text.
% %It's hard to autocorrect these errors
% %because image quality is the most important affecting factor.
% %The text we are processing can be full of no meaning words or
% %strange numbers.
% A fuzzy matching strategy is more desirable in this case.
% % example, what are the traditional IEs

Second, there are many types of medical images, resulting from a variety of
medical tests. Even the same type of test can produce vastly different images
due to the use of different equipments. Writing individual extraction wrappers
for the OCR outputs of all these formats is tedious and inefficient,
not to mention that there are significant programming barriers for
writing these wrappers, especially for the medical professionals who are the
end users of these extraction results. A more user-friendly approach enabling
users to specify such extraction requirements would be preferred.
%There are various kinds of medical images, such as electrocardiograph report,
%medical ultrasonography report, etc.
%However the basic measures for each type of medical test (e.g., ECG),
%are very similar from machine to machine. Only the layouts are
%different.
% example medical images

Finally, most off-the-shelf OCR programs are pre-trained with specific recognition
models, which may not be suitable for the extraction of medical images.
Furthermore, changes in imaging equipment technology over time may produce
different formats, layout, or terminology, rendering existing OCR models
obsolete.
Re-training the models requires a large amount of labeled data, which may
not be readily available. Incremental training as more labeled data arrives
is currently not supported by any OCR product.

There have been some limited attempts to address some of the above challenges.
One solution is a plugin of an OCR program that allows the user to specify
target zones of interest in the image to be extracted. The zones specified for
one image can be applied to images with slight variations by adjusting against
a fixed reference point that is supposed to exist in all these images.
% \KZ{I think the problem is not so much with the zones, because we also
% have zones, but rather with the reference point.}
% \JY{}
% example products
% http://www.square-9.com/automated-data-extraction-optical-character-recognition
The problem with this solution is its high reliance on the OCR zones
established by the user. The performance of the results is affected by the
accuracy of the zones. If the zones are too big, the results will be full of
noise. If the zones are too small, results will miss something.

Another solution involves using the page layout analysis technique. The page layout
analysis technique is used to determine where the text
resides on a page \cite{o1993document},
% \KZ{This page layout analysis approach is not clearly described. I don't understand after reading this paragraph.}
% By using page layout analysis technique, the hierarchy of physical components
% can be generated and to match with the hierarchy of logical components, which
% is predefined.
this includes identifying and categorizing the
regions of interest in the scanned image of a text document.
Typically, the first step is to segment text zones from
non-textual zones and arrange them in their original order.
Then in order to analyze the logical roles of the text zones
(titles, captions, footnotes, etc.), logical layout analysis
is used for labeling the semantics of the text zones.
Generally, page layout analysis is used for documents. The problem with applying
such technique on medical images is that it creates so much noises
that performance is ultimately affected.
For medical imaging reports like ECG, useful information is often
found in the small components of the image, while most of the images are
read as noises.
% check paper and more description, weakness, ref

In this paper,
we propose a spatial data description language, which borrows its syntax from
PADS \cite{fisher+:pads}, an ad hoc data processing language,
for describing semi-structured data in medical images.
% ref
We call this language OCR description language, or ODL.
ODL is designed for extracting and parsing semi-structured text data
from images. We believe that  information extraction from those data in ODL form may be much easier than extracting information from rough data or data in XML form, which means that our preprocessing part proves to be necessary.
%An example ODL description for the image in
%\figref{fig:ecgexample2} is shown in
%\figref{fig:description}. \KZ{Make this description two column, and give
%some brief explanation of this description here.}
%The parsing result of this description is shown
%in \figref{fig:parsing result}. \KZ{Give some explanation of the results,
%otherwise don't show the result here. E.g., you need to explain what F, E, etc.
%mean. You want to say that even though rate has been recognized as rule,
%the bpm value was still extracted (but still wrong!).}
% \KZ{I removed the preprocessing part, cos it's not important. Talk about it in
% discussion sec.}
%The our approach starts by preprocessing the images for text results.
To use this framework, the user first describes the components in the image
that he or she is interested in extracting. This includes constant strings
and variables of different data types.
ODL allows the user to specify the approximate spatial layout and constraints on
the data, e.g., integers within
a certain range, real numbers with certain decimal points, etc.
%This information is then as the key component in our fuzzy matching strategy.
The system then automatically generates a parser for these medical images.
This parser uses the output XML from OCR with spatial information as an input,
and outputs a data structure with values extracted for each variables
in the description, unless there is an unrecoverable error during the parsing process.
In addition, approximate layout information and constraints are used in parsing process
to tolerate noises and small format variations in the input images. Specifically, this method could be called fuzzy matching, meaning that more candidates could be saved after the parsing process.  It's obvious that we may have a higher probability to obtain the accurate result if more candidates are kept so that fuzzy match should be used properly in our system.
%An autogenerated parser based on the ODL description can release us from
%repetitive work. In this way, we turn the task of writing complex parsers
%into describing information on images.


When users process many images of the same format, the system
automatically discovers parsing errors given the current model and
prompts the user to manually correct some of the frequent and prominent
errors, which effectively serves as an online labeling function.
These incrementally labeled data are then used to update the parsing model.


%It should be emphasized that the incremental learning model is very important in our whole system. Incremental learning is a machine learning paradigm where the learning process takes place whenever we have new examples or data added to our baisc data set, leading to a most striking difference between incremental learning and traditional machine learning: it does not assume the availability of a sufficient training set before the learning process. What incremental learning in our system is really impressive: it does not require a relatively good and stable training set at first time. In fact, it could improve the parsing result with even relatively rough training sets at first by absorbing new data or corrective information as time passes in dynamic systems. Besides, the process would be very effective when there are some new images coming in since training process would not learn from scratch, which might waste time and computation resource.

%At last, we propose an incrementally human correction framwork which can
%make the best use of human correction to handle the misrecognition problem.
% Base on our experiments on about 500 real life ECG images,
% our approach achieves p1 and p2 after p3 times human correction.
% experimental results

% \begin{figure}[h]
% \begin{lstlisting}
% Oenum str_month_t{
% 	"Jan", "Feb", "Mar", "Apr",
% 	"May", "Jun", "Jul", "Aug",
% 	"Sept", "Oct", "Nov", "Dec"
% };

% Ounion month_t{
% 	Oint(1,12)	num;
% 	str_month_t	str;
% };

% Ostruct time_t{
% 	Oint(1,31)	day;
% 	"-";
% 	month_t	month;
% 	"-";
% 	Oint	year;
% };

% Ostruct triple_t{
% 	"Vent.";
% 	hskip(\s)	skip1;
% 	"rate";
% 	Oint x;
% 	"bpm";
% 	vskip(\n)	skip2;
% };

% Oscource Ostruct entry_t{
% 	time_t(<-,-,-,0.3l>) t;
% 	triple_t(<0.1w,-,0.5w,->) d;
% };
% \end{lstlisting}
% \caption{Description}\label{fig:description}
% \end{figure}

In summary, this paper makes three main contributions.
\begin{enumerate}
\item We design a simple declarative spatial data description language
for describing both spatial and data constraints in medical images created by some specific graphical user interfaces,
which can be used to automatically generate parsers for information
extraction from these images (\secref{sec:syntax});
\item We propose a fuzzy match strategy which is created by OCR results
(typically in XML form) and the corresponding description in ODL during the parsing phase to
tolerate the noises and errors in the OCR results as well as inaccuracies
of input description (\secref{sec:semantics});
\item We propose an incremental correction framework, which
makes use of the previous parsers as well as user corrections
as labels; such a framework considerably increases
the productivity of the extraction
from the large number of medical images. To be more specific, the framework improves the traditional machine learning methods by using a incremental learning process to avoid starting from scratch when we are trying to apply human corrections in the system. That means the framework would be more effective than most corrective systems.
(\secref{sec:correction}).
\end{enumerate}
