% !TEX root = ../main.tex

\section{Related Work}
%In this section, we review related works covering three aspects, namely using LLMs on mental health, general benchmarks and specific mental health benchmarks.
\paragraph*{LLMs on Mental Health}
Currently, there is relatively limited research utilizing LLMs in the field of mental health. Some studies have delved into the capabilities of LLMs for sentiment analysis and emotion reasoning ~\citep{kocon2023chatgpt, qin2023chatgpt, zhong2023can}. Lamichhane~\citep{Lamichhane2023chatgptapp}, Amin et al. ~\citep{amin2023will}, and Yang et al.~\citep{yang2023evaluations} conducted assessments of ChatGPT's performance across various classification tasks, including stress, depression, and suicide detection. The findings indicate that ChatGPT demonstrates initial potential for mental health applications, yet there remains significant room for improvement.

\paragraph*{General Benchmarks for LLMs}
To evaluate the performance of LLMs across different tasks, several benchmarks have been proposed. C-EVAL~\citep{huang2023ceval} assesses the advanced knowledge and reasoning capabilities of foundation models in Chinese. AGI-Eval~\citep{zhong2023agieval} serves as an evaluation framework for assessing the performance of foundation models in human-centric standardized exams. MMLU~\citep{hendrycks2021measuring} aims to develop a comprehensive test for evaluating text models in multi-task contexts. Big-Bench~\citep{srivastava2023beyond} introduces 204 challenging tasks covering various domains, aiming to evaluate tasks beyond the capabilities of existing language models. HELM ~\citep{helm2023liang} offers a comprehensive assessment, evaluating LLMs across various aspects, such as language understanding and common-sense reasoning. 
These benchmarks, while diverse and comprehensive, primarily emphasize general capabilities and do not cater specifically to the intricacies of mental health.

\paragraph*{Mental Health Benchmarks for LLMs}
Apart from general tasks, specific benchmarks are designed for certain downstream tasks. MultiMedQA~\citep{singhal2023large} focuses on medical question-answering, evaluating LLMs in terms of clinical knowledge and QA abilities. Mental-LLM~\citep{xu2023leveraging} focuses on evaluating the ability of LLMs to predict mental health outcomes through the analysis of online text data. Dialogue safety~\citep{qiu2023benchmark} focuses on the understanding of the safety of responses generated by LLMs in the context of mental health support. Compared to these benchmarks, PsyEval (1) provides a more targeted and comprehensive evaluation of LLMs' capabilities in addressing the unique challenges and nuances of mental health-related tasks. (2) fully considers the differences between the field of mental health and other disciplines.
However, these benchmarks, while addressing specific aspects of mental health or related fields, do not fully encompass the multifaceted nature of mental health issues.

%Contrastingly, our proposed PsyEval benchmark distinguishes itself by offering a more targeted and comprehensive evaluation, specifically designed for mental health-related tasks. PsyEval goes beyond assessing basic understanding or response safety, delving into the complexities unique to mental health. It recognizes that symptoms of mental disorders are subtle, subjective, and highly individualized, requiring a level of expertise, empathy, and emergency response awareness that is not addressed in other benchmarks. This includes understanding nuanced emotional states, detecting subtle signs of mental distress, and providing safe, empathetic interactions. PsyEval, therefore, fills a critical gap in the evaluation of LLMs, positioning itself as a necessary tool for advancing LLMs in the nuanced field of mental health, and setting a new standard for benchmarks in this domain.