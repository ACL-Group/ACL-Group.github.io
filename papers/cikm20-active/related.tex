\section{Related Work}
\label{sec:related}
% \EVE{add \cite{lowell2019practical}. it also talks about AL is not practically useful.}
% \KZ{AL on few class text classification problem.}

% \KZ{AL on long text classification problem.}

% \KZ{Limited work on AL on short-text classification problem.}

% \KZ{Different text classifiers. Why we picked these 4 classifiers to study
% in this work.}


In this section, we mainly discuss different classifiers used for text classification,  previous active learning research on text classification and limited work on AL on short-text classification problem. 

Text classification has been a long-discussed topic and a wealth of classifiers have been designed such as boosting and bagging\cite{opitz1999popular}, logistic regression\cite{walker1967estimation}, Naive Bayes Classifier (NBC)\cite{rish2001empirical}, non-parametric techniques k-nearest neighbor (KNN)\cite{altman1992introduction}, support vector machine (SVM)\cite{cortes1995support}, tree-based classifiers as random forest\cite{ho1995random}, conditional random fields (CRFs)\cite{lafferty2001conditional}. With the prevalence of deep learning, deep models have achieved state-of-the-art results provided with ample labeled data. Therefore, in our experiments, we select popular deep classifiers LSTM\cite{hochreiter1997long}, convolution neural network (CNN)\cite{kim2014convolutional}, BERT\cite{devlin2018bert} as well as fastText\cite{joulin2016bag}.

% With the prevalence of fastText, a lot of downstream tasks have been done with it and obtained acceptable results. Bojanowski and Grave \shortcite{ftblog} claims that in text classification tasks, fastText is on par with other deep learning classifiers and also cut training time from several days to just a few seconds. In their experiments, they compare fastText with VDCNN as well as char-CNN on Yahoo and Amazon datasets and achieve best results among these models. Besides text classification, it is also widely used to learn word vector representations \cite{bojanowski2017enriching}. It has been designed to work on 157 languages, including English, German, French, Chinese and so on. Due to its time-saving property, fastText is the most suitable deep model to due active learning since AL involves iterating many times. 

There has been a considerable amount of empirical work on active learning on text classification but most of them put emphasis on binary or few class classification. Tong~\shortcite{tong2001support} surveyed AL on SVM on 10-class Reuters. Different from our experiments, Tong designed 10 binary classifier for classification. Mccallumzy~\shortcite{mccallumzy1998employing} modified QBC on EM and tested on 5-class NewsGroup as well as 10-class Reuters. Hoi~\shortcite{hoi2006large} conducted batch AL on Fisher model and conducted experiment on 10-class Reuters, 6-class WebKB as well as 11-class NewsGroup.
% query-by-committee \cite{gilad2006query}, expected model change \cite{sznitman2010active}, 
% expected error or variance minimization and information gain~\cite{joshi2012scalable}. 
% Among them, the most popular and widely used is uncertainty sampling. 
% Also, there exists some approaches which specially cater to certain classifier, 
% such as convolution neural network~\cite{sener2017active}, 
% recurrent neural network~\cite{zhao2017deep} and 
% support vector machine~\cite{tong2001support}, 
% or downstream tasks such as 
% natural language processing~\cite{olsson2009literature}, 
% visual recognition~\cite{luo2005active} and semantic segmentation~\cite{vezhnevets2012weakly}.

Considering all the surveyed papers, most classifiers used in AL research 
on text classification is simple. There are only a few works on 
deep learning models and most of them are on image classification. 
Gal~\shortcite{gal2017deep} employed a Monte-Carlo dropout 
(MC-dropout) technique for estimating uncertainty of unlabeled instances 
and experimented on MNIST as well as ISIC2016, while 
Wang~\shortcite{wang2016cost} applied the well-known softmax response 
(SR) idea with pseudo-labeling (selflabeling of highly confident points) 
for active learning on face detection as well as object detection. 
Sener and Savarese \cite{sener2017active}  and Geifman and 
El-Yaniv \cite{geifman2017deep}  focused on coresets of the neural embedding 
space and exploited the coreset loss of unlabeled points as a proxy 
for their uncertainty. Zhao~\shortcite{zhao2017deep} added representativeness 
feature to the uncertainty score and adopted RNN for AL on short-class dataset provided by Zhuiyi. As Lowell ~\shortcite{lowell2019practical} concluded, the the benefits of current approaches do not generalize reliably across models, especially deep models and tasks.
