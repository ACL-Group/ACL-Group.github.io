%
% File coling2020.tex
%
% Contact: feiliu@cs.ucf.edu & liang.huang.sh@gmail.com
%% Based on the style files for COLING-2018, which were, in turn,
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2020}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}


\usepackage{graphicx}
%\usepackage{soul}
%\usepackage[utf8]{inputenc}
%\usepackage{amsmath,amsfonts,amsthm}
%\usepackage{mathrsfs}
%\usepackage{booktabs}
\usepackage{url}
%\usepackage{color}
%\usepackage{latexsym}
%\usepackage{epsfig}
%\usepackage{booktabs}
%\usepackage{diagbox}
\usepackage{array}
\usepackage{multirow}
\usepackage{multicol}
%\usepackage{hhline}
%\usepackage{pbox}
%\usepackage{threeparttable}
%\usepackage{epstopdf}
%\urlstyle{same}
%
%\usepackage{titlesec}
%\usepackage{threeparttable}
%\usepackage{booktabs}
%\usepackage{listings}
%\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{CJKutf8}
\usepackage{tikz}
\usepackage{calc}
\usepackage{subfig}
\usepackage{stfloats}
\usepackage{float}
%\newcolumntype{P}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}
%\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\definecolor{mygray}{gray}{0.6}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\defref}[1]{Definition \ref{#1}}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\socvec}{SocVec}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newtheorem{example}{Example}
\newtheorem{lemma}{Lemma}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}} 
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
\newcommand{\KZ}[1]{\textcolor{red}{Kenny: #1}}
\newcommand{\XS}[1]{\textcolor{red}{Xusheng: #1}}
\newcommand{\YZ}[1]{\textcolor{red}{Yizhu: #1}}

%\setlength\titlebox{5cm}
%\colingfinalcopy % Uncomment this line for the final submission

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Noisy Data Augmentation to Enhance the Robustness of Pretrained Models}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Commonsense reasoning tasks aim to 
empower machines with the reasoning ability 
about ordinary situations in
our daily life. Nowadays, many reasoning tasks 
are in crisis of spurious cues existing in datasets.
Such cues cause 
models to pay attention to the information unrelated to reasoning, 
which weakens the reasoning ability and generalization ability of the model.
%\YZ{Use inference ability or reasoning ability?}
In this paper, we propose
noise data augmentation methods, 
which effectively guide the 
pretrained representation models to ignore superficial patterns and  
have better performance on adversarial test set containing challenging examples 
without any external knowledge.
\end{abstract}

	\input{intro}
	\input{approach}
	\input{experiment}
	\input{conclusion}


%The generated noise data can be seen as adversarial training set to weaken the 
%shallow heuristics which can be learned by the dataset. The models trained with this method are 
%more robust than before. 
%For this task, we require to use a more fair dataset which contains less leakage information or we have to change our model to get ride of using these information. The training and  testing data should both be changed. Since
%(given citations).
%\item Large-data driven pretrained models are the current trend in solving
%commonsense reasoning problems (give examples and their citations).
%\item While they seem successful, they actually can pick up spurious stats cues
%and not really understand the stories (give citations and our exp results).
%\item  Some previous work We propose to alleviate the bias by  
%creating a larger ``symmetric'' dataset for training. ``symmetric'' isn't  a strict balance 
%distribution of unigram or bigram words, but a balance  on high dimensional features 
%which can not be learned by most data driven models (Bert, DSSM and SKBC).
 
%\item We propose to use ConceptNet, which is
%symbolic and structured knowledge to assist the popular pure data-drive approach. (see approach)
%\item We use ConceptNet in two ways: 1) simplify sentence to remove extraneous
%words; 2) embed local structure knowledge into sentence representation to 
%generalize the meaning of the sentence. (see approach)
%\item Results show that when applying our two techniques on three popular
%pre-train language models (Bert, DSSM and SKBC), we can yield better
%results on the unbiased dataset. (see results)
%\item Further, when combined with SKBC, we achieved the new state-of-the-art.
%(see results)
%\end{itemize}

%\section*{Acknowledgements}

%The acknowledgements should go immediately before the references.  Do
%not number the acknowledgements section. Do not include this section
%when submitting your paper for review.

% include your own bib file like this:
\bibliographystyle{coling}
\bibliography{coling2020}

%\begin{thebibliography}{}

%\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
%Alfred~V. Aho and Jeffrey~D. Ullman.
%\newblock 1972.
%\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
%\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

%\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
%{American Psychological Association}.
%\newblock 1983.
%\newblock {\em Publications Manual}.
%\newblock American Psychological Association, Washington, DC.

%\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
%{Association for Computing Machinery}.
%\newblock 1983.
%\newblock {\em Computing Reviews}, 24(11):503--512.

%\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
%Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
%\newblock 1981.
%\newblock Alternation.
%\newblock {\em Journal of the Association for Computing Machinery},
%  28(1):114--133.

%\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
%Dan Gusfield.
%\newblock 1997.
%\newblock {\em Algorithms on Strings, Trees and Sequences}.
%\newblock Cambridge University Press, Cambridge, UK.

%\bibitem[\protect\citename{Rasooli and Tetreault}2015]{rasooli-tetrault-2015}
%Mohammad~Sadegh Rasooli and Joel~R. Tetreault. 2015.
%\newblock {Yara parser: {A} fast and accurate dependency parser}.
%\newblock \emph{Computing Research Repository}, arXiv:1503.06733.
%\newblock Version 2.

%\bibitem[\protect\citename{Borschinger and Johnson}2011]{borsch2011}
%Benjamin Borschinger and Mark Johnson. 2011.
%\newblock A particle filter algorithm for {B}ayesian wordsegmentation.
%\newblock In \emph{Proceedings of the Australasian Language Technology Association %Workshop 2011}, pages 10--18, Canberra, Australia.

%\end{thebibliography}

\end{document}
