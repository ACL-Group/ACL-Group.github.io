Response
Rebuttal Email and Response
Response:	1. Generalizability of our dataset in real-life settings
We agree that movie dialogues are different from real life dialogues in many ways. Despite this, we still feel this dataset is meaningful because, as supported by our references, movie dialogues show similar phenomenon such as entrainment as real ones, and the cognitive process of grounding, or understanding background story, is similar when watching movie and when overhearing limited real world dialogues.


2. Context features
We tried topic modelling(LDA) and its clustering results don't make much sense to humans, so we didn't use that feature in our model. However, we believe our frequent N-gram feature is a good way to implicitly include topic information when explicit topic modelling doesn't work well due to limited and heterogeneous data.


3. NN settings
We adopted basic LSTM(bi-directional LSTM layer + 3 feed-forward layers with 0.25 dropout) and CNN(character-level convolution layer with context size 5 + 3 feed-forward layers) structures. Thank you for the suggestion and we would make that clearer in the paper!


4. Binary classification instead of 13-class
We didn't attempt on the 13-class problem because in previous human survey we found that it's very hard even for humans to make such accurate prediction especially when dialogues are short and don't contain explicit relationship cues.
Time:	Dec 20, 15:38 GMT
Authors
can
update:	no
Email:	Dear Authors,

The rebuttal phase is now open so that you may respond to any questions in the reviews and clarify aspects of the paper that the reviewers may have misunderstood.

Entering a rebuttal is entirely optional; the system will only be open for rebuttals for until **Dec 20, 2019 (23:59 UTC-12)**.

To enter your rebuttals, login to EasyChair Web page for ECAI2020; access your paper details; and use a special text-box enabled for entering the response. The text-box will accept at most 800 words. Note that once you submit your response, you cannot edit it. So it is safer to edit your rebuttal offline and only submit the final version, since once submitted it cannot be changed.

Please keep in mind the following during this process:

* The response must focus on any factual errors in the reviews and any questions posed by the reviewers. It must not provide new research results or reformulate the presentation. Try to be as concise and to the point as possible.

* The rebuttal period is an opportunity to react to the reviews, but not a requirement to do so. Thus, if you feel the reviews are accurate and the reviewers have not asked any questions, then you do not have to respond.

* The reviews are as submitted by the PC members, without any coordination between them. Thus, there may be inconsistencies. Furthermore, these are not the final versions of the reviews. The reviews can later be updated to take into account the discussions at the program committee meeting, and the PC members may find it necessary to solicit other reviews after the rebuttal period.

* The program committee will read your responses carefully and take this information into account during the discussions. On the other hand, the program committee will not directly respond to your responses, either before the program committee meeting or in the final versions of the reviews.

* Your response will be seen by all PC members who have access to the discussion of your paper, so please try to be polite and constructive.

* Nearly all papers have 3 reviews. In the rare cases where they are fewer, the ECAI 2020 PC apologies and will make sure that additional reviews are obtained before decision is made.

Final acceptance/rejection decisions will be issued by January 15, 2020.

Regards,
Giuseppe De Giacomo
ECAI 2020 Program Chair
http://ecai2020.eu

[*REVIEWS*]
Time:	Dec 18, 17:25 GMT
Reviews
Review 1
Relevance:	
5: (excellent)
Significance:	
5: (excellent)
Novelty:	
5: (excellent)
Technical quality:	
3: (fair)
Quality of the presentation:	
4: (good)
Overall evaluation:	
1: (weak accept)
From my perspective, this is an interesting and novel work. The authors develop an approach to identifying the type of relationship in a dyadic relationship from the dialogue itself. Human annotators aren't always even able to do this task, but to the extent that systems can be trained to do so at least as well, then it could be used for tuning support systems. It is beneficial that they simplified the task down to just work vs family, as the scheme was to nuanced and difficult for human annotators.

However, I hesitate the accept the paper based on the selection of the dataset. These dialogues from movies may not match real dialogues in work and home settings. People may say very different things when acting that they say in real life. The generalizability to another corpus is very concerning in this case. I believe this limitation greatly detracts from the quality of this work.

Moreover, I wonder if context is missing. For example, the differences between work and family may depend on where the conversations between co-workers and family members, respectively occur (ie bar vs. office, house vs. public), as well as the topics possibly. The work did not seem to take into account context, and therefor it is limited. However, to the extent that GIS can identify location, or for more immediate development goals, the system can automatically learn topic, this information can be fed into the system to moderate the prediction by context. Context should also be considered, then, in the training data from the dataset of movies.
Review 2
Relevance:	
3: (fair)
Significance:	
2: (poor)
Novelty:	
3: (fair)
Technical quality:	
2: (poor)
Quality of the presentation:	
3: (fair)
Overall evaluation:	
-2: (reject)
The reported work aims at identifying personal relationship based on (narrative) dialogues with a claim that the method can generalize to real-world (non-scripted) dialogues. For the purpose of this research, personal relationships have been categorized into two dimensions: personal/family versus workplace/professional. The allocation of dialogues to either category is performed by logistic regression using a set of handcrafted features based on reference [31]. The dataset consists of 2770 training dialogues with an average number of turns 0f 8.44. The results are within reach of human classification and exceed some achieved with CNN or LSTM, although little detail is offered on the DL approaches.

The work is overall rather empirical in its approach to dialogue and linguistic aspects. It rests on the assumption that narrative dialogues are comparable to real-world dialogues, which is justified by a small number of references but might actually be called into question. Scripted dialogues are not natural dialogues and are influenced by clearly defined roles, movie genres and narrative functions in dialogue (persuasion, antagonism, relationship building ...). Their turn-taking might also differ from natural conversation as well. This distinction is clearly established when considering dialogue datasets for ML.
Serban, I.V., Lowe, R., Henderson, P., Charlin, L. and Pineau, J., 2018. A survey of available corpora for building data-driven dialogue systems: The journal version. Dialogue & Discourse, 9(1), pp.1-49.

The features selected appear both arbitrary and somehow obvious: address and levels of politeness and familiarity quite naturally map onto the predefined category and in that sense Table 1 does not uncover any counter-intuitive phenomena.
The annotation mechanism does not appear to follow best practice either with a very limited set of annotators.

The overall impression is that this 2-category classification is a fallback position from the original 13-class relationships. The problem here is that the authors might simply have uncovered trivial features (such as address, or BOW referring to family terms - the examples of Figure 4 being pretty informative in that sense) that readily differentiate between the two target category and for which logistic regression is appropriate enough - but this departs from ML work in dialogue.
Review 3
Relevance:	
4: (good)
Significance:	
3: (fair)
Novelty:	
4: (good)
Technical quality:	
3: (fair)
Quality of the presentation:	
3: (fair)
Overall evaluation:	
1: (weak accept)
This paper presents a number of contributions towards the automatic inferring of relationships in text dialogues. This is a novel problem and is quite relevant to ECAI.

Firstly, they create and introduce an interesting dataset based on film scripts and manual annotations; they describe a number of engineered features, including differential bag of words, LIWC features, addressing features, linguistic pattern features, and other statistics-based features on complexity and talkativeness. They then train a number of baseline models and compare them to a simple logistic regression using those features, which seems to outperform the baselines on the binary task of determining a workplace vs family relationship.

Overall the paper was very well written and easy to follow. The task is, to the best of my knowledge, novel, and this paper is a good initial attempt at exploring the classification of relationships from text. The dataset provided, in particular, will be a strong contribution to further explorations of this topic, if made available.

That being said, the approach is still quite preliminary. I was maybe a little disappointed that the authors did not also try the 13-class problem and here only examined the binary workplace vs family model. Further, the proposed classification models are barely described, and it is unclear to what extent the comparison between the LR model using engineered features and the "baseline" CNN and LSTM models is fair. I would strongly recommend the authors to describe these other models and the classification experiment in more detail.
Copyright © 2002 – 2020 EasyChair
