Review Report

Title:	Inferring Personal Relationships from Dyadic Dialogues
Authors:	Minxue Niu and Kenny Zhu
Status:	Reject
MetaReview
Comments: Reviewers generally agreed that this is an important problem, but felt as it is currently written, the paper doesn't do a good enough job of situating itself in the context of prior work and the experimental results were not convincing. The writing also has clarity problems.

Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper claims to infer personal relationships from short conversational segments (based on movie scripts). They have built a new data set and their model seems to achieve a result close to human performance.
Strengths:Interesting topic, potentially useful area of research. Weaknesses:There is need for more detailed literature review (especially on sociolinguistics & pragmatics) to explain the potential value of research question for the research community. Although the authors claim that their paper supports sociolinguistic research, the links are currently vague. References are not documented properly. Authors should check the guidelines for conference submission carefully.

Reasons to accept
Interesting topic & potentially useful for NLP community.
Overall Recommendation:	2.5
Questions for the Author(s)
There are quite a few unsupported claims in the introduction section. There are almost no references from pragmatics & sociolinguistics. The ones that appear the in the references section are outdated. Especially, research on subject pronoun use and its pragmatic functions will give the authors cues about address terms in formal & informal conversations.
Missing References
Please check the conference guidelines about citing references.

Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper's main theme is relationship inference from short dialogue segments. The authors present a dataset of dyadic dialogue transcripts of movie scripts which are annotated with categorical relations. The purpose is to identify and extract multiple lexical, syntactic and semantic features from the dialogues to train a binary (family/workplace) classifier. The authors claim a prediction accuracy close to human performance.
Strengths:

very important topic of interest to the NLP community;

if done properly, such work might be useful for future sociolinguistic research;

Weaknesses:

although the authors confess that they focused on family/workplace relations because "it’s hard to fully contextualize such short segment without background knowledge even for humans", it is still unclear how exactly do they define such relations; For example, I don't see why the utterance "It's an order" in Figure 1's criptic dialogue should necessarily indicate "a work relationship with disparity in ranking.".

the authors claim in the abstract that they classifier they built reaches near-human performance. However, in Section 1 they say "We feed our new features along with other previously studied features in related field, such as Linguistic Inquiry and Word Counts (LIWC) (Pennebaker et al., 2015), into Logistic Regression classifier, and achieve very reasonable accuracies on the binary classification problem."

it is not clear why the authors decided to choose the particular 13-class taxonomy of relationships? Why not more/less? I.e., how do the authors really define 'common ones'? Authors say that ".. this is not an all-round coverage of all possible relationships in human society and there may be overlaps between different categories, but we aim to cover those common ones in real life which may be of interest in interpersonal relationship research."

In fact, only on page 7, at the end of the paper, the authors admit:

"We pose the 13-class relationship inference task to human volunteers in a preliminary survey and the average accuracy is only 39%. This means for most dialogue segments we extracted, there is not enough information even for humans to determine between the fine-grained 13 classes. Therefore, we focus on the simpler family/workplace binary classification problem in this paper."

They authors say: "We require the annotator to only assign labels when the relationship is clear, relatively stable and typical." This will affect not only coverage, but also boost unreasonabley the inter-annotation agreement as well as the classifier's performance (since it will consider only clear-cut relations and instances);

The human annotators were given specific instructions, while the classifier did not have access to this info: "Our ground truth annotator was provided with the movie title, the pair of characters involved in the dialogue, movie synopsis from IMDb and Wikipedia for each movie, as well as complete access to the Internet, and was asked to choose between one out of 13 classes or “Not applicable (NA)” label."

It is not clear why the gold standard was defined this way. And even so, it seems that "Only 47:11% of the pairs received a specific label, while others are considered “not applicable”." More than half of the instances were not classified - this is a lot!!! This is wrong.

Reasons to accept
None.
Reasons to reject
It is clear that the research described in this paper is in its infancy. The experiments were not thought-out well enough and there are many problems with the task definition, classification task, and the annotation process. The authors are advised to revise and resubmit.
Overall Recommendation:	1.5
Typos, Grammar, Style, and Presentation Improvements
Typos/grammar:
1) "Therefore, in this paper, we instead use movie dialogues, which is easy to obtain and process computationally"


Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper considers a supervised task of identifying relationships between pairs of people on the basis of dialogues between them. The task is considered in the context of movie scripts, and the relationships are categorized into family/workplace (which aggregates a finer-grained taxonomy of social relations like parent, courtship, etc). The authors derive various linguistic features such as pronoun use and LIWC categories in building their models.
The basic premise of the paper is interesting -- having explicit labellings of relationship types would be very useful to this part of the NLP community, as would features that might convincingly infer these relationships. To live up to this, the paper would need a convincing taxonomy that might be interesting beyond movie scripts, and/or interesting features we’d expect to similarly port to a variety of settings.

As such, I’m not convinced by the paper. To the first point, it’s unclear what motivated the ontology they chose, especially since “workplace” could mean many different things across movies (my sense is that the authors were working off of a white-collar-office prior, in expecting somehow less casual language), and since the authors offer no basis for their binary classes beyond “it makes sense in life”. If the task reduces to classifying a crudely defined context (“family” or “work”) in which a dialogue occurs, then the task seems a bit vacuous, and could be completed without any substantive modeling of the dialogue. In evaluating the merits of their taxonomy, the authors may try to think of particular scenarios where an end-user of their dataset or model would need to characterize relationships without access to labels -- which types of distinctions would be unlabeled and hard to discover except through automated analysis of the dialogue? Here, the authors may want to make reference to other works which explore particular dimensions along which relationships can be characterized, like power -- would their taxonomy enrich the characterizations that these works provide?

(e.g: Prabhakaran, Vinodkumar, Ajita John, and Dorée D. Seligmann. "Power dynamics in spoken interactions: a case study on 2012 republican primary debates." Proceedings of the 22nd International Conference on World Wide Web. ACM, 2013.

Danescu-Niculescu-Mizil, Cristian, et al. "Echoes of power: Language effects and power differences in social interaction." Proceedings of the 21st international conference on World Wide Web. ACM, 2012. )

In a similar vein, the authors might consider better ways to set up the problem that would more convincingly orient the task towards characterizing relationships. In particular, some movies may have more family-related dialogues and some may have more work-related. As such, would a classifier learn about indicators in the dialogue that signal personal relationships, or indicators that signal the genre or topical content of the movie? A fix may be to see if a classifier can distinguish between two dialogues in the same movie, with different relationship types.

These types of confounds also undermine the author’s discussion of features -- are they indicative of relationship or the style in which a movie which has a preponderance of a particular type of relationship tends to be written? Without addressing these confounds, it’s (even more) implausible that movie scripts could be believably mapped to real-life dialogue, as the authors claim (perhaps as a misreading of the works they cite to that effect).

Many parts of the paper aren’t clearly written and could use a pass for grammar. In addition, the authors make numerous claims throughout which seem overstated or not obviously defensible, muddying their point: for instance, their grandiose statements about human relationships in the introduction, their claim that prior work had access to explicit labelings of relationships in narrative texts, and that movie scripts are fair reflections of real-life dialogues.

Reasons to accept
See above -- the task is interesting and a better implementation of it would certainly be of use to NLP researchers in this area.
Reasons to reject
See above -- the task is rather poorly set up.
Overall Recommendation:	1.5
Questions for the Author(s)
To phrase a few of my objections as questions:
Could the authors describe more how they arrived at the binary labels they went with? And to clarify, were these the labels the human annotators used, or did the human annotators use the more fine-grained taxonomy?

I find it somehow implausible that topic models did not turn up meaningful signal (l416) -- does this mean that the topics were meaningless, or that they didn’t correlate with their relationship labels?

Could the authors further clarify what’s subsumed by their categorization of “work” relationships -- given the diversity of “workplaces” represented in movies?

Comparing relative frequencies of words across corpora may be subject to frequency effects, especially given the difference-based measure the authors use. As discussed in this work, which may be another useful reference: Monroe, Burt L., Michael P. Colaresi, and Kevin M. Quinn. "Fightin'words: Lexical feature selection and evaluation for identifying the content of political conflict." Political Analysis 16.4 (2008): 372-403.

Missing References
see above
Typos, Grammar, Style, and Presentation Improvements
As stated above, the paper could use a pass for grammar, and to perhaps weed out a few less well-defended claims.
