\section{Evaluation}


\label{sec:experiment}

We first introduce some competing methods to be evaluated as well as
the datasets used for training and testing.
Then we present a preliminary analysis on the ROCStories dataset to invalidate
previous approaches that train or fine-tune their models on the validation
set and we reconstruct a new dataset for training. Finally we conduct a comprehensive
evaluation of the competing methods to verify the effectiveness of our simplification method and 
the structured knowledge we incorporate.
%In this paper, our task is to tackle commonsense reasoning problem in stories. 
%Given SCT as evaluation set, we can use any source data to learn human commonsense knowledge and reasoning ability. 
%We suppose the validation data has information leak for test set. The most classification methods which training or fine tuning with SCT validation set may learn the bias leak features, rather than commonsense reasoning ability

%We introduce baseline methods, dataset and the results for models with our simplification 
%and structure knowledge incorporation methods.

%to be evaluated as well as
%the datasets used for training and testing.
%Then we present a preliminary analysis on the ROCStories dataset to invalidate
%previous approaches that train or fine-tune their models on the validation
%set and we reconstruct a new dataset for training. Finally we conduct a comprehensive
%evaluation of the competing methods to verify the effectiveness of our simplification method and 
%the structured knowledge we incorporate.

%In this paper, our task is to tackle commonsense reasoning problem in stories. 
%Given SCT as evaluation set, we can use any source data to learn human commonsense knowledge and reasoning ability. 
%We suppose the validation data has information leak for test set. The most classification methods which training or fine tuning with SCT validation set may learn the bias leak features, rather than commonsense reasoning ability(\secref{sec:dataset}). We suggest to use a larger corpus with negative ending generated automatically. We also show our model parameter details in \secref{sec:details}, the comparison result with analysis in \secref{sec:result} and ablation study is in \secref{sec:ablation} .
\subsection{Baselines}
\label{sec:baselines}
%We evaluate our methods against two group of baseline methods.
%First, as mentioned in ~\secref{sec:intro}, pre-trained story representation 
%helps with choosing the proper ending of a story. 
%We apply our concept-based story representation techniques on three typical 
%models: DSSM, SKBC and BERT. 
%These models are pre-trained with different kinds of mechanisms
% and classification methods, and they are representatives of popular
%pretrained models today. 
%The other baselines which used the pre-trained 
% methods are similar to these three models.
The baseline models can mainly be divided into supervised and unsupervised: 

\textbf{Unsupervised}: DSSM~\cite{mostafazadeh2016corpus} and GMSA~\cite{guan2018story} 
calculate semantic similarity 
between a pair of strings by representing them in a continuous semantic space.
%In story ending prediction task, this model maps the four-sentence context and 
%the fifth sentence into semantic vectors respectively considering the raw count of letter-trigrams without the order .
%The context and an alternative are encoded through three hidden layers whose dimension are all 300. 
%During test, DSSM chooses the alternative 
%ending with larger cosine similarity between its semantic vector and context's semantic vector.

\textbf{Supervised}: CGAN~\cite{wang2017conditional} generates negative endings as training the 
discriminator. SKBC~\cite{roemmele2017rnn} and SIMP~\cite{srinivasan2018simple} 
uses Skip-thought~\cite{kiros2015skip}
%\footnote{\url{https://github.com/ryankiros/skip-thoughts}}  
with GRU-GRU
%~\cite{hochreiter1997long} 
structure to produce generic sentence representations. 
BERT~\cite{devlin2018bert} and TransBERT~\cite{li2019story} 
apply the bidirectional training of Transformer~\cite{vaswani2017attention} 
compared to unidirectional Transformer for GPT~\cite{radford2018improving} 
and ISCK~\cite{chen2018incorporating}. Except for the pre-training representation 
models, FES-JOINT~\cite{peng2017joint} and SeqMANN~\cite{li2018multi} make
fully use of various semantic features, like sentiment, to get better results. 

We choose to apply our methods on 3 typical models: DSSM, GPT and BERT. 
%can produce highly generic sentence representations and has been applied to many semantic classification problems.
%Skip-thought is treated as the framework of encoder-decoder models. 
%The architecture is GRU-GRU~\cite{hochreiter1997long} which 
%has been shown to perform well on sequence modeling tasks.  
%We apply the simplification method and concept encoding.
%We employ the same experimental 
%setting as detailed in SKBC 
%except for the addition of a dropout layer with 0.4 drop-rate before 1000-node GRU hidden layer. 
%We feed each simplified sentence of  a 5-sentence story into the dropout layer with 0.4 drop-rate. 
%Then we feed the dropout hidden state of each sentence 
%as a timestep into a single 1000-node GRU hidden layer.
%A binary cross-entropy objective function is applied 
%to maximize the probability of choosing the positive ending. 
%All experiments use a batch size of 
%200 and over 20 training epochs to optimize the model. 
%We use a 2400-dimensional language model~\cite{kiros2015skip}
%\footnote{\url{https://github.com/ryankiros/skip-thoughts}} 
%from 98,161 unannotated ROCStories
%to train the concept sequence representation 
%on BookCorpus dataset~\cite{zhu2015aligning} which contains text from
%11,038 books, primarily novels.


%\textbf{BERT}~\cite{devlin2018bert}, without encoder-decoder archichecture, exploits
%transformer block~\cite{vaswani2017attention} which is a popular basic computational unit. 
%There are several available pre-trained BERT models which differ in how many
%layers and parameters are used in the model (the basic version has 12-layer transformer blocks, 768 hidden-size, and 12
%self-attention heads, totally 110M parameters; the large version has 24-layer transformer blocks, 1024 hidden-size, and
%16 self-attention heads, totally 340M parameters). We choose basic version which is pre-trained on BookCorpus.

%The second group of baselines are feature-based, generative model or 
%models similar to the above three models including the 
%state-of-the-art methods: \textbf{FES-JOINT}~\cite{peng2017joint} , \textbf{SeqMANN}~\cite{li2018multi} 
%, 
%, , \textbf{TransBERT}~\cite{li2019story}.
%\KZ{I don't think we should introduce so many base lines here. Only present
%the baselines that you will improve on. I think u can have two sections,
%one is the set of baseline methods that uses the language model, which can be
%improved from our two techniques. The second is a set of other baselines that can
%be compared with us end-to-end, but these may not use language models.}

%\begin{description}

%\textbf{FES-JOINT}~\cite{peng2017joint} combines the features of frame, 
%entity and sentiment. This unsupervised joint model chooses the proper 
%ending by calculating conditional probability given the context.
%
%\textbf{SeqMANN}~\cite{li2018multi} takes multiple shallow features into account, 
%including POS tag, word embedding, character feature, 
%sentiment negation and SemLM~\cite{peng2016two} features.
%
%\textbf{GMSA}~\cite{guan2018story} generates the story ending through 
%multi-source attention and brings in ConceptNet neighboring information.
%We compare the similarity between the generated ending and the two alternatives.
%
%\textbf{CGAN}~\cite{wang2017conditional} uses an generative adversarial 
%network (GAN) which applies GRU to generate negative endings for 
%training data augmentation.
%
%\textbf{SIMP}~\cite{srinivasan2018simple} uses Skip-thought embeddings and 
%encodes the entire story using a multi-Dense-layer classifier to determine the right ending.
%
%\textbf{GPT}~\cite{radford2018improving} makes a big improvement by training a language model for text representation 
%with linear fine-tuning. GPT also uses Transformer block as unit like BERT.
%
%\textbf{ISCK}~\cite{chen2018incorporating} incorporates sentiment and 
%commonsense feature between the context and ending into 
%\cite{radford2018improving}'s text representation, which can get a little improvement.
%
%\textbf{TransBERT}~\cite{li2019story} utilizes not only 
%general language knowledge
 %from large-scale unlabeled data but also  three
%semantically related transfer tasks, including natural language
%inference, sentiment classification, and next action
%prediction, to pre-train and initialize BERT. 
%
%Our methods are applied on these models. 
%For simplification method, we empirically fix the extra interval $\lambda$ to 1, 
%because larger interval, while potentially helps discover more
%concepts, may introduce noise. 
%For example, in 
%``Sally went home and wondered about her parents' marriage'', 
%when $\lambda$ equals to 2 and 3, we will get ``go wonder'' and 
%``go about'' incorrectly.

%In addition, the structured knowledge representation takes the form of a 
%300-dimensional vector from Numberbatch. 
% \end{description}

%In \textbf{our model}, the sentence representation contains text sentence 
%representation and commonsense structured representation. 
%n fact, our model is based on SKBC which consists of sentence encoding and 
%GRU network for classification. 
%Some experiments~\cite{roemmele2017rnn} have explores different embedding-based representations of the stories and different methods for generating negative examples.
%For {\bf our method}, %because Roemmele~\cite{roemmele2017rnn} has shown that training 
%sentence representation using ROCStories is almost as effective as
%using BookCorpus. 
%We used the same code and default parameters available
%at the above GitHub page. 

%As mentioned in \secref{sec:sentence simplification}, 
%For the tuning hyper parameter on story ending classifier. 
%We combine the sentence representations of the context and 
%final sentence into one sequence. 
%Each sentence representation is fed through a dropout layer 
%(drop-rate of 0.4) and then fed into a single 1000-node 
%GRU hidden layer. 
%The final hidden state value of two samples with same 
%context are given to a top feed-forward layer composed of one node  
%with softmax activation. 
%A binary cross-entropy objective function is 
%applied to train the network. 

\subsection{Dataset}
\label{sec:dataset}

\begin{table}[th]
\small
\centering
\begin{tabular}{lcc}
\hline
\textbf{Model}& Endings of SCT(V) (\%) & Endings of SCT(R) (\%)\\
\hline
%~\citeauthor{end:predict}(~\citeyear{end:predict})&72.5\\
SIMP& 72.60 &59.86\\
SKBC&72.76&58.18\\
GPT& 77.77 &57.93\\
$\text{TransBERT}_\text{BASE}$&79.0&54.52\\
$\text{TransBERT}_\text{LARGE}$&75.84&54.30\\
\hline
Human& 62.40&62.40\\
\hline
\end{tabular}
\caption{SCT test accuracies of SOTA models trained from endings only in SCT(V) vs. 
endings only in SCT(R).}
\label{tab:end}
\end{table}

%All datasets used in the experiments are documented in 
%\tabref{tab:datasets}.
%BookCorpus and ROCStories have been used to pre-train
%language models for this task in the previous work.

%\begin{table}[htbp]
%\small
%\setlength{\tabcolsep}{0.8mm}{
%\begin{tabular}{lllll} \hline
%\bf{Methods} & LM Train & Pred. Train & Validation & Test \\ \hline \hline
%DSSM & ROCS & -  & - &  SCT(T)\\ \hline
%FES-JOINT& NYT+ROCS & -  & - &  SCT(T)\\ \hline
%SeqMANN & ROCS & ROCS*(Tr)& ROCS*(V)& SCT(T)\\ \hline
%SIMP & BC & BC(Tr)& ROCS*(V)& SCT(T) \\ \hline
%SKBC & BC & BC(Tr)& ROCS*(V)& SCT(T)\\ \hline
%CGAN & ROCS & - & - &  SCT(T)\\ \hline
%GMSA & ROCS & - & - & SCT(T)\\ \hline
%GPT & BC &  ROCS*(Tr)& ROCS*(V)& SCT(T)\\ \hline
%ISCK & BC & ROCS*(Tr) & ROCS*(V) & SCT(T) \\ \hline
%BERT & BC+WIKIPEDIA &  ROCS*(Tr)& ROCS*(V)& SCT(T)\\ \hline
%TransBERT & BC+WIKIPEDIA &  ROCS*(Tr)& ROCS*(V)& SCT(T)\\ \hline
%Ours(BC) & BC &ROCS*(Tr) &ROCS*(V) & SCT(T) \\ \hline
%Ours(ROCS) & ROCS &ROCS*(Tr) &ROCS*(V) & SCT(T) \\ \hline
%\end{tabular}}
%\caption{Datasets used by different methods for training
%language model and the ending prediction, for validation and
%for test
%(BC = BookCorpus, NYT = New York Times \protect\footnotemark ,
%ROCS = ROCStories, ROCS* = ROCStories annotated with positive  by AMT and 
%negative endings by our generation approach,
%SCT=Story Cloze Test, T = test dataset, V = validation dataset, Tr = training dataset)}
%\label{tab:datasets}
%\end{table}
%\footnotetext{Available at \url{https://catalog.ldc.upenn.edu/LDC2008T19}}

%To train the story ending prediction models, many previous works
%used the SCT validation split which contains 1871 annotated items,
%and achieved quite good results.
The SCT dataset comes with 101,903 5-sentence stories (first 4 as context and last as ending).
Human authors were asked to write negative endings for 3744 of these stories to create
cloze test instances. These 3744 instances were then split into validation set (SCT(V))
and test set (SCT(T)). The remaining 98,159 stories are called raw stories.
Previous work indicated that human-authorship bias~\cite{sharma2018tackling} exists
in SCT datasets, especially when the validation set of SCT is used for training.
In fact, in a ``stripped-down'' version of the SCT task, where one is supposed to choose
between two alternative story endings, without given the context, 
SOTA models all performed much better than human, after training from the endings-only data of
the SCT validation set. This shows that the models are not really capable of human-like reasoning,
but merely pick up cues from the endings in the validation set (see SCT(V) column of \tabref{tab:end}). 

We thus follow Roemmele et al.'s~\cite{roemmele2017rnn} to construct
a new training set called SCT(R) by Random and Backward sampling of 
negative endings for 98,159 raw stories. With the new training data, 
the same SOTA models performs reasonably worse than human in the ``ending-only'' test,
as shown in SCT(R) column of \tabref{tab:end}. Therefore in the remaining experiments,
we will use SCT(R) as the training data for all competing algorithms in both SCT and 
SCT$_v$1.5~\footnote{SCT$_v$1.5 was previously released on \url{https://competitions.codalab.org/competitions/15333\#participate-submit\_results}, with the goal of fixing some of the bias, but was later
found to have other problems and was subsequently closed for access.}
tests.
 
%\eve{further explanation of Backward method or give citation?} 
% method with 4:2 proportion.
%We re-evaluated several top-performing algorithms 
%by adapting them to train {\em only on the endings} on validation 
%set (SCT(V)) and new training data (ROCS*(Tr)). The result on SCT test set 
%is shown in \tabref{tab:end}. 
%As a baseline, we include the human performance, which is the average
%accuracy of 5 human annotators who are not
%trained on the validation set. %but use their commonsense only. 
%The fact is that some of the ``top'' algorithms perform substantially better than human which 
%indicates that these algorithms take great advantage of spurious information leak between 
%SCT(V) and test data.
%%are not using ``commonsense'' but rather
%%the patterns leaked in the training data on SCT. For 
%$SCT_v1.5$~\footnote{$SCT_v1.5$ is released on \url{https://competitions.codalab.org/competitions/15333\#participate-submit\_results} 
%but now it is closed and we can not obtain the full dataset with golden labels.
% The results we presented are acquired before the competition ended.} is proposed 
% to break the leakage by rewriting new stories. However, 
%\citet{sharma2018tackling} has shown the ending-only 
%result is still above human baseline. 
%%which are also highly above human baseline.
%%Besides, it only contains 
%% validation and test set whose sizes are even smaller than last version. 
%Therefore, in this paper, we choose to use ROCS*(Tr) which make models hard to 
%learn spurious features through ending only training (in~\tabref{tab:end}) as 
%new training dataset and 
%test both on SCT and $SCT_v1.5$.
 %by automatically augmenting ROCStories corpus with negative endings. 


%The fact
%that human score is substantially worse than some of the ``top'' algorithms
%indicates that these algorithms are not using ``commonsense'' but rather
%the patterns leaked in the training data. 
%$SCT_v1.5$~\cite{sharma2018tackling}
%\footnote{$SCT_v1.5$ is released on \url{https://competitions.codalab.org/competitions/15333\#participate-submit\_results},
%but now it is closed and we can not obtain the full dataset with golden labels.
 %The results we presented are acquired before the testing phrase ended.}
%is a recently released revised version in order to reduce the 
%human-authorship biases in SCT. However the ending-only results for
%$SCT_v1.5$ is still higher than human~\cite{sharma2018tackling}. Besides, it only contains validation and test set whose sizes are even smaller than last version. 
%Therefore this dataset does not necessarily solve the problem that gives a good source
 %for training and testing in the story cloze test.
%Nevertheless, we still show the result of our model on $SCT_v1.5$(~\secref{sec:result}).
%Analogously, research~\cite{niven2019probing}
%has shown that the result of BERT on Argument Reasoning
%Comprehension Task~\cite{habernal2017argument} entirely
%relies on exploitation of spurious statistical cues in the dataset. 
%The following preliminary experiment shows the existence of bias cues in SCT. 

%ROCStories and their negative endings in SCT are solicited
%on Amazon Mechanical Turk (AMT). We found that the endings
%contain significant information leak. For example, \tabref{tab:hate}
%shows a feature (the word ``hate'') that is overwhelmingly found 
%in the negative endings.
%This means that by extracting such features from two endings, an algorithm
%may be able to predict the correct ending with high accuracy. 

%SCT=Story Cloze Test, T = test dataset, V = validation dataset, Tr = training dataset

%\begin{table}
%\small
%\centering
%\begin{tabular}{lccc}
%\hline
%\textbf{Dataset}& Pos ending& Neg ending &Total\\
%\hline
%Validation set & 3  & 70 &73\\
%Test set          & 4  & 69 &73\\
%\hline
%\end{tabular}
%\caption{Frequency of word ``hate'' appearing in the positive and 
%negative endings of SCT Validation set (SCT(V)) and Test set (SCT(T))}
%\label{tab:hate}
%\end{table}

\subsection{End-to-end Results}
\label{sec:result}
We first show the end-to-end results of the three baselines 
with simplification and concept encoding methods. Then we 
evaluate other models trained with new dataset on SCT.
\begin{table}
 \small
\centering
\setlength{\tabcolsep}{0.7mm}{
\begin{tabular}{lccccc}
\hline
$\textbf{Model}$ & Test &Original (\%)&Simp(\%)&CE(\%)&Simp+CE (\%)\\
\hline
\hline
DSSM& \multirow{3}{*}{SCT}&54.04&\textbf{58.79}&54.0&\textbf{58.2}\\
SKBC&&64.70&\textbf{68.13}&\textbf{65.12}&\bf{69.7}\\
$\text{BERT}_\text{BASE(ours)}$ &&56.54&\textbf{57.34}&\textbf{59.43}&\textbf{60.24}\\
\hline
DSSM&\multirow{3}{*}{SCT$_v$1.5} &54.30&\textbf{ 57.83}&\textbf{54.35} &\textbf{58.53}\\
SKBC&&64.56& \textbf{67.30}&\textbf{65.45} &\bf{67.97}\\
$\text{BERT}_\text{BASE(ours)}$&&56.88&\textbf{58.02}&\textbf{59.79}&\textbf{60.97}\\
\hline
\end{tabular}}
\caption{End-to-end accuracy on SCT and SCT$_v$1.5 test sets. 
Original=baseline, Simp=simplification method, CE=concept encoding method
}
\label{tab:main}
\end{table} 
%\tabref{tab:main} and \tabref{tab:main1.5} show all the three typical pre-trained 
%story representation models benefit from our simplification 
%and concept encoding methods. 
In \tabref{tab:main}, SKBC and DSSM achieve significant 3.43\% and 4.75\% improvement with simplification. 
$\text{BERT}_\text{BASE}$ gains 0.8\% increase with simplification and 
2.89\% with concept encoding method (both compared with original model). 
%It is because BERT consists of Transformer unit which is attention mechanism. 
BERT may learn the informative weight from pre-training with Transformer unit. 
Our simplification can even help with reducing the weight of less informative 
words for BERT. DSSM+CE performs worse than DSSM 
mainly because DSSM is a bag-of-words model and inevitably loses the order information. 
%and it models the first 4 sentences as a whole. 
%When using CE, we have to sum the embeddings of all 4 sentences up and 
%then concatenate it with the output vector of DSSM as final representation. 
%By doing this, we inevitably lose the order information. 
We can also get the same conclusion from the results testing on SCT$_v$1.5
that simplification and graph embedding can benefit ending prediction.
% Then we apply two encoding aspects: 
%simplification language model encoding 
%on three typical pre-trained language models to 
%verify that each semantic aspect contributes to the ending prediction model.
%\begin{table}
% \small
%\centering
%\setlength{\tabcolsep}{0.7mm}{
%\begin{tabular}{lcccc}
%\hline
%$\textbf{Model}$ &Original (\%)&Simp(\%)&CE(\%)&Simp+CE (\%)\\
%\hline
%\hline
%DSSM& 54.30& 57.83&54.35 &58.53\\
%SKBC&64.56& 67.30&65.45 &\bf{67.97}\\
%$\text{BERT}_\text{BASE(ours)}$&56.88&58.02&59.79&60.97\\
%\hline
%\end{tabular}}
%\caption{End-to-end accuracy on $SCT_v1.5$ test set with simplification and concept encoding methods. 
%}
%\label{tab:main1.5}
%\end{table}

\tabref{tab:all-models} shows the results of other previous research on our new training data and 
test on SCT test set.
%and validation dataset 
%in ~\secref{sec:dataset}. 
%Most of the baselines are implemented in
 %strict accordance with the settings in the original papers and new training data we proposed. 
% $\text{BERT}_\text{BASE}$ and
% $\text{BERT}_\text{LARGE}$ which implement by us are finetuned with our training data and respect to the 
% original parameter settings for language model.
%SKBC has the best previously reported result, 64.7\% accuracy.
%$\text{BERT}_\text{LARGE}$ rank second among all baselines. 
%It indicates that BERT is strong at learning representations given large amount of text data. 
$\text{BERT}_\text{BASE(ours)}$ retrains the 
language model of  $\text{BERT}_\text{BASE}$  with BookCorpus.
Our $\text{BERT}_\text{BASE}$ performs worse than the basic version because we retrain the 
language model with less unsupervised data. Though larger corpus, such as Wikipedia, can lead to a better result,
we only expect to show the effective improvement of our simplification and concept encoding methods. 
%The great drop of BERT (above 90\% while training with validation data) 
%may be caused by less spurious patterns in training data. 
%This is consistent with the work~\cite{niven2019probing} which generates test set adversarially 
%and the result decline sharply.
SKBC with our methods achieves 69.7\% accuracy, which is the best among our experiments. 
It performs better than any other commonly-used models we tested. 
Notice that our experiments are not meant to demonstrate the superiority of a
particular algorithm but to show that the proposed story representation 
methods (i.e., simplification and concept encoding) work for
a variety of models. 
%Being state-of-the-art is not our first concern. 
%The improvement our proposed methods bring in is applicable and promising for a wide range of models.
%Maybe other models with our methods can gain higher accuracy.
%The ending prediction results with BC and ROCS are nearly identical  
%which is consistent with Roemmele~(\citeyear{roemmele2017rnn})'s research.
%We also get the SOTA performance of 68\% on the newly 
%released $SCT_v1.5$ blind test dataset by training the language model on BC, 
%which is not shown in this table.  
%Unless otherwise noted, our model is trained on ROCStories.
Human performance is 100\% and can be viewed as an 
upperbound~\cite{mostafazadeh2016corpus}. 
%The models are divided into two
%categories, either using 
%the generated negative endings for binary classification or not at all. 
All results are averaged from
5 independent runs. 
\begin{table}
 \small
\centering
\begin{tabular}{lc||lc}
\hline
$\textbf{Model}$ & Acc (\%))& $\textbf{Model}$ & Acc (\%))\\
\hline
\hline
DSSM& 54.04& SKBC&64.70\\
GMSA& 61.20 & CGAN& 60.90 \\
SeqMANN& 59.74 & $\text{BERT}_\text{BASE(ours)}$&56.54\\
SIMP&61.09 & $\text{BERT}_\text{BASE}$&61.46\\
FES-LM&61.60 & $\text{BERT}_\text{LARGE}$&64.67\\
ISCK& 62.21 & $\text{TransBERT}_\text{BASE}$&61.46\\
GPT& 63.46 & $\text{TransBERT}_\text{LARGE}$&61.89\\
\hline
SKBC+Simp+CE(ours)&\bf{69.7}&Human& 100\\
\hline
\end{tabular}
\caption{Results of story ending prediction on SCT~\protect\footnotemark}
\label{tab:all-models}
\end{table}
\footnotetext{Some scores are 
great lower than that of the published models because they are trained with validation data which contains 
spurious features and proved to be unable for training in~\secref{sec:dataset}.
 }
%Note that the numbers reported 
%are not directly comparable with those in literature~\cite{srinivasan2018simple,roemmele2017rnn,li2018multi,radford2018improving,chen2018incorporating} as we reconstruct 
%training and validation set for binary classification methods which has been
%described in \secref{sec:dataset}.


%DSSM achieves an accuracy of 58.5\%, it simply uses a deep structured semantic model to learn representations for both context and endings and chose ending by calculating the similarity between the context and ending representation. GMSA model is a generative model, which takes in structured ConceptNet knowledge. Though it is not used for choosing endings, we compute the similarity between the generated ending and two alternatives and get 61.2\% for comparison. The semi-supervised models all rely on negative endings. CGAN achieves 60.9\% accuracy by generating negative endings with generator in GAN. The goal of their generator network is to generate a fake sentence and deceive the discriminator to take it as real target. SeqMANN achieves 59.74\% accuracy incorporating with many language features,. GPT make great achievement training on SCT validation set and get 63.46\% accuracy training on Train-RD. This model use transformer train a language model and fine tune to train a classifier. ISCK which is the best when using SCT validation set for training further improves the performance to 62.21\%. It combines the sentence representation of GPT, sentiment feature and commonsense similarity feature between the context and endings. SKBC the state-of-art in former models we reproduce achieves 64.7\% accuracy with skip-thought sentence representation and RNN classifier. We suppose GPT and ISCK learn the representation of sentence itself. Skip-thought model encode the sentence representation with former and later sentence information. This kind of information is more efficient in commonsense reasoning. SIMP is even more simple for only using dense layer to classify the story lines on 61.09\% accuracy. 
 
%Our model outperforms all the baselines. In fact we 
%logged 5\% improvement over
%Similar to SKBC model, we represent the story sentence with 
%low-dimension vectors and train a classifier with GRU model. 


%Also, for the record, our model correctly predicts the question in
%\figref{fig:story} while other strong competitors such as SIMP, GPT and ISCK
%failed.

%\KZ{Add one or two example stories in which we did correct but other algorithms
%failed. Give a brief analysis to show why our method do better 
%than this example. Actually here it would be ideal if our driving example
%can be used. That is, indeed our method predicts correctly while the major
%competitors failed.}
 
%\subsection{Different Simplification Strategies}
%\label{sec:simplify}
%Previous work on predicting ending of a story mainly divided into two directions. The first direction aims to generate ending for given context with generative language model~\cite

%\begin{table}
%\small
%\centering
%\begin{tabular}{lcc}
%\hline
%\textbf{Event Type} & Acc w/o CE (\%) & Acc w/ CE (\%)\\
%\hline \hline
%All Words(SKBC)& 64.70 & 65.12\\
%\hline
%5-TUPLE&55.12 &57.83\\
%FES&60.12 &63.66\\
%No Events & - & 63.70 \\
%Simp(ours)& 68.13 &{\bf 69.70} \\
%\hline
%\end{tabular}
%\caption{The effect of using different types of events for simplification with SKBC
%on SCT w/o concept encoding}
%No Events = concept embedding only, 
%Simp = simplified sentence embedding}
%\label{tab:sse}
%\end{table}

%\begin{table}
%\small
%\centering
%\setlength{\tabcolsep}{0.7mm}{
%\begin{tabular}{lccc}
%\hline
%\textbf{Dataset}&Words&Concepts& Simplified words\\
%\hline
%Training   &10.03&3.79&5.17\\
%Validation&10.03&3.79&5.17\\
%Test         &9.57&3.59&4.90\\
%\textbf{Event Type}& Before & Events & After \\
%\hline
%5-TUPLE&10.02&1&3.43\\
%FES&10.02&1.52&8.95\\
%Simp(ours)&10.02&3.78&4.90\\
%\hline
%\end{tabular}}
%\caption{Effects of simplification using different types of
%events on the length of the word sequence. 
%Before = avg. num of words before simplify,
%After = avg. num of words after simplify, Events = avg. num of events
%extracted.}
%\label{tab:size}
%\end{table}


%\begin{table}[th!]
%\small
%\centering
%\begin{tabular}{lc}
%\hline
%\textbf{Types of information} & Acc (\%)\\
%\hline \hline
%SKBC (whole sentence embedding)& 64.70 \\
%\hline
%\hline
%\end{tabular}
%\caption{The effect of incorporating with concept embedding on SCT}
%\label{tab:ce}
%\end{table}

%Understanding a story requires understanding sequences of events. 
%It is thus vital to model mantic sequences in text. 
%To evaluate the ability of 
%simplification methods on story ending prediction task, we make comparison between
% two types of event representation, 5-TUPLE~\cite{pichotta2016learning} and FES~\cite{peng2017joint}
%  which rely on dependency parsing or SRL . 
  %We employee the same settings with \textbf{our model}, 
  %except for the sequence modeling methods.
%  5-TUPLE represents an event as a 5-component tuple $(v, e_s, e_o, e_p, p)
%  $\footnote{ROCStories is parsed with Stanford CoreNLP tools for 5-TUPLE.}, 
%  where $v$ is a verb lemma which can not be $null$, $e_s$, $e_o$ and $e_p$ are nominal 
%  arguments standing in subject, direct object and prepositional 
%  relations respectively and $p$ is the preposition relating $v$ and $e_p$. 
%The other event representation FES jointly models different aspects of 
%semantic knowledge: frames\footnote{Semantic frames 
%are derived from semantic role labeling annotations built upon PropBank frames. }, 
%entities and sentiments. 
%Different from the original paper, we represent all three aspects 
%as \textit{string} instead \textit{vector} (e.g. \textit{POSITIVE} 
%label vs one-hot vector for sentiment).

%\tabref{tab:sse} shows the effectiveness of these simplification methods trained 
%on the same model architecture of SKBC. 5-TUPLE only 
%achieves 55.12\% due to omitting too much information. In \tabref{tab:size}, 
%the average number of words is only 3.43. Though FES brings in more information, 
%the pipe-lined process of extracting frames is prone to error propagation.


%We conduct an ablation study on the proposed model to evaluate 
%the effectiveness of each feature. Results using only one type of 
%embedding for sentence representation are shown in \tabref{tab:ablation}. 
%The representation with simplified sentence achieves 3.43\% improvement 
%over the sentence embedding of the original sentence. It also shows that
%sentence simplification is a more effective feature than concept embedding. 
%\subsection{Effects of Concept Embeddings}
%\label{sec:ce}
%\tabref{tab:sse} also shows all event sequence representations can benefit from 
%the incorporation of concept encoding which is pre-trained on ConceptNet. 
%The results of Simp and Simp+CE show that the relation between concepts can introduce extra 
%knowledge which can not 
%be learnt from language model.

%\subsection{Training Data Size}
%\label{sec:datasize}

%\begin{figure}
%\centering\includegraphics[width=0.8\columnwidth]{pictures/trend}
%\caption{Accuracies over train data size}\label{fig:trend}
%\end{figure}

%In \figref{fig:trend}, we compare the models' capability with different methods given increasing
%amount of training data (ROCS*(Tr)). The first observation is that 
%\KZ{Simp and CE are not really models. They are just methods to represent
%stories... Be careful with the choice of words and consistency throughout the paper. Consider changing the labels of the lines in Figure 3.}
% our two methods (Simp and Simp+CE) both perform well even 
%with very little training data. In fact,
%given 10\% of the data, they already achieve higher accuracy than SKBC using
%the whole data. The second observation is that compared with SKBC, model with our methods
% improves more quickly as the training data grows, which is indicated by
%the steeper slope from 10\% to 50\% of the data. 
%Finally, when comparing the effects with or without the concept embedding, 
%we can see that without structured knowledge,
%the model is unable to take advantage of more training data. 

%\subsection{Training Time}
%\label{sec:time}
%Except for the improvement on end-to-end accuracy, the simplification 
%can also take a one-third reduction in training time.
%The improvement in efficiency comes from fewer tokens in sentences 
%and smaller vocabulary.
% \eve{what is 'various of data'? 'various' should be 'variousness'}
%There are 43,095 unique words in all ROCStories and 
%19,455 unique words in simplified key tokens extracted from ConceptNet. 
%This greatly reduces the vocabulary size.
%In addition, the structured commonsense feature 
%also brings in extra commonsense knowledge. 
%the best result for WSE is 64.7\% with all data. 
%Though accuracy is still growing, the growth is slowing down. 
%With the limited training data, our model performs much better. 
%The reducing of noise and variance improves sentence representation quality. 
%The concepts embedding from , though can't give all the sentence information to the representation, helps improve the accuracy to 69.7\% which is the state of art in model comparison. The results suggest neither of these two kind of sentence embedding is sufficient to represent the commonsense knowledge and the combination can give the best performance.
