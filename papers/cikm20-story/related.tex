\section{Related Work}
\label{sec:related}

Our work on story cloze task is inspired by three research fields: script learning, commonsense knowledge and annotation artifacts in text inference  which will be reviewed in the following.

%In this section,
%we first summarize the general techniques solving the SCT task,
%then discuss the human-authorship bias,
%and finally we discuss the works that incorporate the commonsense knowledge 
%in this task.
\subsubsection*{Story Cloze Task}
%Traditional approaches seek to uncover the shallow contextual semantics
%of the story via manually defined features. 
The story cloze task~\cite{mostafazadeh2016corpus} is provided 
to evaluate the ability for story understanding and 
commonsense reasoning. A series of baselines~\cite{mihaylov2017story,mostafazadeh2016story}
model the fitness of the candidate ending
by measuring the semantic similarity between context sentences 
and the ending in the vector space.
Various sentence representations can be applied,
such as averaging word2vec~\cite{mikolov2013distributed},
Sentence2vec~\cite{kiros2015skip} and DSSM~\cite{huang2013learning}.
In addition, stylistic features such as sentence length and
character n-grams are useful to distinguish
right from wrong endings~\cite{schwartz2017story}. 

Recently, various deep learning methods have been introduced for
solving the SCT task.
The majority of these neural models follow a 2-layer architecture,
which first constructs the representation of each sentence,
then aggregates into the whole story context representation.
The sentence representation can be either obtained from word embeddings
via LSTM, GRU and attention layers~\cite{wang2017conditional,zhou2019story},
or produced from pre-trained models such as
Sentence2vec~\cite{roemmele2017rnn,srinivasan2018simple} and
GPT~\cite{radford2018improving,chen2018incorporating}.
Similarly, the context representation encodes the entire semantics
of all the context sentences through recurrent layers~\cite{cai2017pay},
attention layers~\cite{li2018multi}
or simple concatenation~\cite{bugert2017lsdsem}.
Moreover, the task accuracy can be boosted by leveraging shallow features with DNN models.
%For example, \scite{chen2018incorporating} proposed a gating mechanism to
%dynamically integrate GPT-based representations with shallow sentiment 
%features.  Our proposed DNN model is an instance of such 
%hierarchical architecture, and the commonsense-based sentence simplification 
%has shown its effectiveness when facing data without annotated 
%negative endings.


\subsubsection*{Statistical Script Learning}
%The use of scripts in AI dates back to the 1970s~\cite{schank2013scripts}. 
$Scripts$ represent knowledge of predetermined, stereotyped event sequences that define a certain activity, 
which can facilitate text understanding~\cite{chambers2009unsupervised, regneri2011learning, schank1975scripts}.
Early works~\cite{schank2013scripts,mooney1985learning} learnt scripts 
via construction of knowledge bases from text. 
More recently, researchers used statistical models to extract different types 
of representation from amounts of data, including unsupervisedly learning 
narrative schemas and scripts
~\cite{chambers2009unsupervised,regneri2011learning} as well as event schemas and frames 
~\cite{chambers2011template,balasubramanian2013generating,sha2016joint,huang2016liberal}.
For reasoning over these knowledge, researches tried to solve event
 prediction problem by transforming it into a language modeling paradigm~\cite{pichotta2014statistical,rudinger2015script,hu2017happens}.
% pichotta2016statistical,pichotta2016using,pichotta2016learning,
For this story cloze task, the semantic language model (SemLM)~\cite{peng2016two},
which serves as the language model at frame level,
is able to represent event sequential semantics
\cite{li2018multi,chaturvedi2017story}. In addition, a rule-based 
method~\cite{lin2017reasoning} devises some rules for matching 
explicit events in the given context. Some work\cite{modi2014inducing,regneri-etal-2010-learning,modi2016event} also
map the events into semantic embedding representation which leads to 
data sparsity.

\subsubsection*{Commonsense Knowledge in Text Inference}
Commonsense knowledge has been shown to be effective across many
inference tasks,
such as reading comprehension~\cite{mihaylov2018knowledgeable}
and dialogue generation~\cite{liu2018knowledge}.
Recently, ConceptNet has been incorporated into SCT models.
For example, \cite{chen2018incorporating} proposes a simple and
effective commonsense feature based on the similarity of concept embedding.
\cite{guan2018story} extends the semantics of each concept token
in the sentence by aggregating the embeddings of adjacent concepts.
The commonsense knowledge in our work is reflected in two aspects:
guideline of the process of simplification,
and the extra semantic resource of each sentence.

\subsubsection*{Annotation Artifacts in Text Inference}
Annotation Artifacts has been reported to be existing widely in many text 
inference datasets, like NLI and Multi-NLI~\cite{gururangan2018annotation}.
Some research work on generating new datasets with additional annotation by human
~\cite{sharma2018tackling} or automatically generating with adversarial method~\cite{zellers2018swag}. 
~\cite{roemmele2017rnn} provides a simple but effective method to generate negative examples. 
Other work focus on bias avoiding models~\cite{clark2019don,zhao2018gender}. Though these methods 
may reduce the time consumption of collecting new dataset, it is not easy to transfer them to other models, which 
is a great limitation.

%Prior works have discussed the human-authorship bias
%introduced in the SCT validation set.
%\scite{cai2017pay} reports that a deep learning model on the endings
%achieves 72.4\% accuracy, when all the training and testing contexts
%are completely removed from the dataset.
%\scite{schwartz2017story} produces almost the same result using only
%\textbf{lexical} stylistic features from candidate endings.
% Therefore, the semantic-irrelevant bias brought by the annotated
% negative endings strongly affects the evaluation of the ability
% of a model to predict the next sentence given context semantics.
%\scite{sharma2018tackling} proposed SCT-v1.5 dataset which alleviates
%the bias by applying a more strict crowdsourcing process,
%but the testing accuracy is still nearly 65\%.
%For those approaches learning on training data only,
%\scite{wang2017conditional} applies conditional GAN for
%synthesizing wrong endings,
%and \scite{roemmele2017rnn}
%designs a variety of negative sampling methods.
%Our work follows their best sampling strategy.

%\textbf{Commonsense Reasoning} When people reading articles, they use commonsense knowledge to help them understand the texts. So commonsense knowledge bases should be useful for reasoning tasks.




% Predicting the ending of stories involves several research areas, such as reading comprehension and commonsense reasoning. In this paper, we mainly extends the work on the SCT test set~\cite{mostafazadeh2016corpus} which is created to evaluate the ability of story understanding and commonsense reasoning. The research on this ending prediction problem can be partitioned with classification task and generation task.
%
% Most of previous studies treat this task as a classification problem by training a classifier or just matching the features between an ending and context in a story to choose the right ending. Just as \secref{sec:intro}
% described, the classification studies can be separated into three lines: feature-based, neural models and neural models with language features. The feature-based models mostly extract shallow semantic features from the text. Except for the POS tag, char character et.al~\cite{li2018multi} we have referred. \citeauthor{chaturvedi2017story} \citeyear{chaturvedi2017story} enhanced feature-based story understanding by modelling the coherence of a complete story from three perspectives, namely event, sentiment and topic. In addition, some studies~\cite{event,zhou2019story} incorporate event features extract from the sentence with rule functions or pre-trained extractor.
% The second genre, neural models, are the models without using extra language feature but the story itself. The neural models~\cite{end:gpt,srinivasan2018simple,roemmele2017rnn,dssm,mostafazadeh2016story} which contains narrative sentence representation give a special feature for the story, the sequential sentence embedding trained from large amount of narrative corpus. Without relying on sentence representation, some studies \cite{wang2017conditional,cai2017pay} just use end-to-end model to get the story classifier. \citeauthor{wang2017conditional} applies GAN model in which generator is used for generating wrong ending, and discriminator is the story classifier.
%
% The models in third line is the combination of neural models and semantic features. \citeauthor{schwartz2017story}(\citeyear{schwartz2017story}) add sentence length, character and word n-gram features to LSTM language model. To improve the performance in this commonsense reasoning task, \citeauthor{chen2018incorporating}(\citeyear{chen2018incorporating})  compute the similarity between the key words which can be found in ConceptNet of story ending and context as commonsense feature. Combine the sentence representation, sentiment and commonsense feature together,this model performs best training on SCT validation set.
%
% Generative method is more complex in predicting the story ending. ~\citeauthor{guan2018story}(\citeyear{guan2018story} ) adopts an incremental encoding scheme to represent context clues which are spanning in the story context incorporating commonsense knowledge is applied through multi-source attention to facilitate story comprehension. This model evaluated by BLUE score with the right ending.


%\textbf{Reading Comprehension} is the ability to understand the text and answer the question based on the text. It's an important research field in NLP. There are many forms of evaluation tasks. The SQuAD dataset~\cite{rajpurkar2018know} gives the context document and a question. This task is to find the existing answer from the context or recognize unanswerable question. The CNN/DailyMail~\cite{hermann2015teaching} task is a cloze-style task. It needs to infer the missing entity in the question from the article.





%\textbf{Story Ending Selecting} Since the missing of the negative endings in the training set of  ROCStory Cloze Task, some  researchers proposed strategies to generate incorrect options. Wang~\cite{wang2017conditional} proposed a conditional generative adversarial network. Roemmele~\cite{roemmele2017rnn} tried four ways to generate fake endings. They are random, backward, nearest-ending and language model.

%Others trained their models on the validation set. The measures can be split into two parts. One is traditional machine learning methods. Msap~\cite{schwartz2017story} and HCM~\cite{chaturvedi2017story} tried to use extracted features to predict endings. Another part is neural network models.
