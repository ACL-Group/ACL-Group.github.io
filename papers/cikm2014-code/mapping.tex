%\vfill\eject
\subsection{Taxonomy Construction}
\label{sec:constr}

We build a hierarchical taxonomy graph from the tag systems of {\it Stack overflow}.

\figref{fig:sof} shows the tags of one post in {\it Stack overflow}. We can find 
that each question can holds several tags. There are some particular relationships 
in these tags, one is functional relatedness and the other is hierarchical relationship.
For example, ``malloc'' is a sub-area of ``c'' and ``casting'' and ``malloc''
are sometime in functional simalarity. 
\\
\begin{figure}[!h]
\begin{center}
\includegraphics[width=\columnwidth]{figure/stackoverflow.eps}
\caption{Stackoverflow}
\label{fig:sof}
\end{center}
\end{figure}

We extract the hierarchical relationships from the tag systems with the idea of
\cite{tag}, which build the hierarchy with the distance: 
\[W(v_i, v_j)=\frac{|A(v_i)\cap A(v_j)|}{\min\left\{|A(v_i)|, |A(v_j)|\right\}}\]
to define the distance between different tags.
We build a clusting algorithm to find groups in this taxonomy graph and extract
hierarchy in these groups. We define a parameter $Div$ to evaluate the divergence
of the taxonomy graph. We eliminate the edges of the tag graph with a greedy 
strategy by remove the least weighted one, and check the value of $Div$ when
$N$ is changed.
\[Div = N \prod _{i=1}^{N}{(e^{\cfrac { n_i }{ N }}-1)}\]
Algorithm\ref{alg:graph} shows the computing process.
\begin{algorithm}
\caption{Taxonomy Clustering}
\label{alg:graph}
\begin{algorithmic}[1]
\Require $Div$, $threshold$, $V$, $E$
\Ensure $E'$
\Function{CLUSTERING}{$G,~M,~T,~\theta$}
\State $L|PO|T\leftarrow$new queue$|$new list$|MBR(G,~M)$
\State $L.Push(T)$
\Return $PO$
\EndFunction
\end{algorithmic}
\end{algorithm}

\figref{fig:tagraph} shows a part of the final hierarchical tag graph we
obtains.

\begin{figure}[h]
\begin{center}
%\includegraphics[width=0.9\columnwidth]{figure/tagraph.eps}
\caption{Stackoverflow Tag Graph}
\label{fig:tagraph}
\end{center}
\end{figure}

\subsection{Mapping to Taxonomy}
\label{sec:map}

For each source code document, we want to map it to the node concept of the 
taxonomy tree. This becames a hierarchical classification problem. 
Hierarchical classification is a very common problem in Machine
Learning community. It requires given an instance the model should
be able to generate corresponding labels according to the instance's
features. In this way, we have to build a mapping approach of a source code 
file to the taxonomy concepts.

\subsubsection{Labeled LDA}
Labeled LDA is a probabilistic graphical model that describes a process for 
generating a labeled document collection. L-LDA incorporates supervision by 
simply constraining the topic model to use only those topics that correspond 
to a documentâ€™s (observed) label set.

L-LDA can introduce the topic to word distributions for each tag, which is the
word distributions $P(w|tag)$. With these taxonomy features, how to predict
the possible tags of source code files? Here we restrict the topic $z$ of repositories
that is depicted in the generation process of \secref{sec:method} to be the
topics of tags.
\[
\label{ztag}
p(w|z)=p(w|tag)\]
We apply \equref{ztag} into the inference of RMM model. \equref{equ:mstep1} in M-step
of EM algorithm will no longer necessory.\\
{\bf E step:}
\[p(z_k|w_i,d_m) = \frac { p(w_i|z_k)p(z_k|d_m) }{ \sum _{k}{p(w_i|z_k)p(z_k|d_m)} }\]
{\bf M step:}
\begin{align}
\label{equ:mstep2}
\begin{split}
&p(z_k|d_m) = \\
&\frac{\sum _{i}{\N_{w_i}^{d_m}p(z_k|w_i,d_m)+\lambda \sum _{l} \sum _{n}{\N_{w_n}^{c_u}\xi  _{l,m}p(z_k|w_n,d_m)}}}{\sum _{k}\left(\sum _{i}{\N_{w_i}^{d_m}p(z_k|w_i,d_m)+\lambda \sum _{l} \sum _{n}{\N_{w_n}^{c_u}\xi  _{l,m}p(z_k|w_n,d_m)}}\right)}\\
\end{split}
\end{align}
%\subsubsection{hESA}

% ESA(Explicit semantic analysis){\cite{gabrilovich2007computing} can calculate the 
% simalarity of any two text with the help of {\it tf-idf} weighted vector of Wikipedia-based 
%concepts. In ESA, the meaning of texts are represented in a high-dimensional 
%space of concepts derived from Wikipedia. Assessing the relatedness of texts in 
%this space amounts to comparing the corresponding vectors using conventional metrics 
%each word correponds to a conceptsvector with weights, which can indicate the 
%relatedness between the word and concepts.
%Algorithm\ref{alg:sim} shows the computing process.
%\begin{algorithm}
%\caption{ESA}
%\label{alg:sim}
%\begin{algorithmic}[1]
%\State
%\Procedure{Simalarity Score}{}
%\State Get $p(w_n|z_k)$, $p(z_k|d_i)$ through EM algorithm.
%\For{ ${i=1}$ to $N$}
%\State $P({\bf w}|d_i)=\sum _{top_k}{P({\bf w}|z_k)P({z_k}|d_i)}$.
%\State $P({\bf Con}|d_i)=\sum _{n}{P({\bf Con}|w_n)P({w_n}|d_i)}$.

%\EndFor
%\EndProcedure
%\end{algorithmic}
%\end{algorithm}

%Here the taxonomy concepts are organised in hierarchy, we build the ESA database in hierarchical
%relationship we call it hESA. The idea is straitforward.
%If a word appers in document $d$, it is also regarded as a member of the parents of $d$.

\subsubsection{HSLDA}

Hierarchically supersived LDA \cite{perotte2011hierarchically} is a model for 
hierarchically, multiply-labeled, bag-of-word data, which addresses the classification 
problem of learning topic with hierarchically labeled data. For our taxonomy mapping 
problem, the generative process is as following.
%\begin{enumerate}
%\item For each topic $k=1,\ldots,K$

%\begin{itemize}
%\item Draw a distribution over words $\boldsymbol\phi_{k}\sim{\rm Dir}_{V}(\gamma\mathbf{1}_V)$%,
%%where $\mathbf{1}$ is a vector of ones of length $V$ 
%\end{itemize}
%\item For each label $l\in\mathcal{L}$

%\begin{itemize}
%\item Draw a label application coefficient $\boldsymbol\eta_{l}\mid\mu,\sigma\sim\mathcal{N}_{K}(\mu \mathbf{1}_K,\sigma \mathbf{I}_{K})$  
%\end{itemize}
%\item Draw the global topic proportions $\boldsymbol\beta\mid\alpha'\sim{\rm Dir}_{K}\left(\alpha^{\prime}\mathbf{1}_K\right)$
%\item For each document $d=1,\ldots,D$

%\begin{itemize}
%\item Draw topic proportions $\boldsymbol\theta_d\mid\boldsymbol\beta,\alpha\sim{\rm Dir}_{K}\left(\alpha\boldsymbol\beta\right)$ 
%\item For $n=1,\ldots,N_{d}$

%\begin{itemize}
%\item Draw topic assignment $z_{n,d}\mid\boldsymbol\theta_d\sim{\rm Multi}(\boldsymbol\theta_d)$ 
%\item Draw word $w_{n,d}\mid z_{n,d},\boldsymbol\phi_{1:K}\sim{\rm Multi}(\boldsymbol\phi_{z_{n,d}})$ 
%\end{itemize}
%\item Set $y_{r,d} = 1$
%\item For each label $l$ in a breadth first traversal of $\mathcal{L}$ starting at the children of  root $r$

%\begin{itemize}
%\item Draw $a_{l,d}\mid \bar{\mathbf{z}}_d,\boldsymbol\eta_{l},y_{\mathrm{pa}(l),d}\sim\begin{cases}
%\mathcal{N}(\bar{\mathbf{z}}^{T}_d\boldsymbol\eta_{l},1)\\
%\mathcal{N}(\bar{\mathbf{z}}^{T}_d\boldsymbol\eta_{l},1)\end{cases}$ %\item Draw $a_{l, d} \ | \ z_{1:N_d,d}, \boldsymbol\beta_l \sim \mathcal{N} \left(\bar z_d^{T} \boldsymbol\beta_{l},1\right)$, where $\bar z_d=N_d^{-1}\sum_{n=1}^{N_d}z_{n,d}$ 
% 
%\item Apply label $l$ to document $d$ according to $a_{l,d}$ \[\hspace{-2cm}
%y_{l,d}\mid a_{l,d}=\begin{cases}
%\ \ \ 1 & \text{if \ensuremath{a_{l,d}>0}} \\% and \ensuremath{y_{{\rm \mathrm{pa}}(l),d}=1}}\\
%-1 & \text{otherwise}\end{cases}\]
% 
%\end{itemize}
%\end{itemize}
%\end{enumerate}

