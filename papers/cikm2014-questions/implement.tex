\section{Implementation Details}
\label{sec:implement}

In this part we give a general view on how we implemented Q-LDA. Since our model is based on L-LDA, we modified the corresponding algorithm in Stanford Topic Modeling Toolbox.

\subsection{Preprocessing}

As section \ref{subsec:learn} noted, instead of learning the full model during the training, we simplified the problem by making \emph{qinfo}-topic distribution $\epsilon$ observed. We did this in two steps:

\begin{enumerate}
\item We first parsed all document titles and then extracted the named entities using NER tools, and attached them as \emph{qinfo} with every document together with its labels both serving as a document's features;
\item Next we count the co-occurrences between \emph{qinfo} and labels, thus having a probability distribution of every \emph{qinfo} in terms of labels.
\end{enumerate}

\subsection{Sampling}

After acquiring \emph{qinfo}-topic distribution $\epsilon$, we can start the Gibbs sampling process. The major difference is that before deciding what topics to sample for a document, we calculated another distribution from all of its \emph{qinfo}: that is, we numerically summed the distribution vectors of each \emph{qinfo} and then renormalize it. And from this distribution we pick a certain number of topics (usually $max(|Q^{(d)}|, 5)$, as a empirical parameter) with highes probability in addition to the restricted topics specified by observed labels $\Lambda$. And then everyting follows the same as L-LDA.