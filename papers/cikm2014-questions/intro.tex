\section{Introduction}
\label{sec:intro}

This paper focuses on the question labeling issue on Internet Q\&A site. Currently most if not all questions are labeled by the users themselves or the website editors, which requires additional human resources and cannot guarantee the accuracy. Therefore this paper hopes to model the question texts (hereinafter collectively referred to as ``documents'', including the question's title and body) accurately and extract necessary features to label the documents as a multi-label classification problem. Taking into account that a considerable number of documents have already been manually labeled, we choose supervised learning to better utilize those information. On the other hand, since interdisciplinary document classification is too difficult, we restrict the target documents in a specific area (like programming or mathematical problems), such that the number of labels are under control.

Since the underlying task is a multi-label classification problem for large-scale documents, the probabilistic topic model seems a feasible idea. This paper provides attempts to improve the topic model to better express target documents which contain titles in addition to ordinary document bodies, trying to acquire knowledge beyond simple texts by analyzing its latent semantics to improve the labeling results.

The multi-label classification now becomes a popular topic in Machine Learning communities, such as gene prediction or news article tagging. Most approaches fall into two categories: problem transformation methods and algorithm adaptation methods.  The former approach tries to transform the problems to a set of binary classification problems while the latter tries to adapt the existing algorithms for single-label classification to ones that are capable of predicting multi labels, such as Multi-Label k Nearest Neighbors (MLkNN) \cite{zhang2007ml} based on kNN algorithm and Back-Propagation Multi-Label Learning (BPMLL) \cite{zhang2006multilabel} based on back-propagation algorithm.

Meanwhile the topic model aims to understand the texts by modeling its hidden semantics, which could be used to describe the text similarity, to avoid the impact of polysemy or synonyms, or to cluster the texts. Those latent semantics summarize and abstract the semantics of the documents, representing a underlying form of the document content. Afterwards, along with the development of statistical language models, latent semantic is interpreted as a probability distribution over the dictionary while every document corresponds to a probability distribution on the semantic space. Based on this notion, Probabilistic Latent Semantic Analysis (pLSA)\cite{hofmann1999probabilistic} and Latent Dirichlet Allocation (LDA)\cite{blei2003latent} have been proposed to depict the topic structure behind texts.

%Such semantic information is first proposed by Latent Semantic Analysis model, which considers the semantic dimension characterizes the main content of the documents, and the documents themselves are merely representations of that dimension. It also shows that by analyzing co-occurrences between words, such semantic dimension could be extracted from the documents, and then the documents could be represented in a lower dimension.

Also notice that question documents in Q\&A sites have some other interesting properties, and we'd like to emphasize the functionality of a document's title. Usually users on those sites tend to describe their problems succinctly yet informative, for example they would very likely tell how the problems behave, or in what software, or what the wrong message is. In addition, since the document set we used mainly focused on programming or mathematics, many question titles would contain crucial keywords (like some named entities), making the titles more valuable for labeling. For example if the noun ``stemming'' tends to appear in the title with the label \emph{nlp}, for all other questions having ``stemming'' in the title without label \emph{nlp}, the topic assignment for words in those documents could also have \emph{nlp} as their choice.

In this paper, we propose a model based on L-LDA for documents having structures like \emph{title-body-labels}, in which we treat titles separately and model them in another way. At first we believe the key information in document titles would be named entities (along with some useful nouns) which we would refer to as \emph{qinfo} hereafter in this paper. Then we consider \emph{qinfo} is sampled independently from words, and there is another distribution called \emph{qinfo}-label distribution which is used to describe the probabilistic relations between \emph{qinfo} and topics. Based on this idea, we implemented a prototype model and showed that the results have some relative advantages over baseline models and aforementioned L-LDA.
