\section{Introduction}
\label{sec:intro}
Commonsense causal reasoning, a central challenge in artificial intelligence,
has been actively studied by both linguists and computer scientists.
It aims at understanding the causal dependency
between events and actions in our daily life.
To illustrate the problem,
we present a question from
Choice of Plausible Alternatives (COPA) evaluation~\cite{roemmele2011choice}
%\ZY{which developed for validating the ability of discovering the key causal
%knowledge by data-driven approaches},
which consists of one thousand multiple-choice questions requiring
commonsense causal reasoning to answer correctly.
%\ZY{The authoring methodology of COPA ensured the
%question set with sufficient breadth of the topics. And the questions in COPA
%data set validated by two raters have a quite high inter-rater agreement. In addition, the incorrect alternative was set closer in content to the premise which makes this task more difficult for purely associative
%methods.}
%\ZY{The COPA evaluation judged the degree of plausibility for
%commonsense causal implication. }
Specifically, each question is composed of a premise and two
alternatives, where the task is to select the more plausible alternative as
a cause (or effect) of the premise.
%\ZY{ Note that, the COPA causal implications are judged not strictly positive
%or negative but in degrees of plausibility. Thus, it is more than causality
%extraction which also need to properly model causal strength amongst text
%spans.
%}

%\begin{itemize}
%\item[] Premise: \emph{The runner wore shorts.}. What is the
%cause?
%\item[] Alternative 1: \emph{The forecast predicted a hot day.}
%\item[] Alternative 2: \emph{She planned to run along the beach.}
%\end{itemize}

\begin{example}
\label{ex:copa}
\noindent
\begin{itemize}
\item[] Premise: \emph{I knocked on my neighbor's door.} What happened as an
effect?
\item[] Alternative 1: \emph{My neighbor invited me in.}
\item[] Alternative 2: \emph{My neighbor left her house.}
\end{itemize}
\end{example}

From the above example, we can observe that a key challenge is
harvesting common sense causal knowledge
that the action of knocking causes that of invitation.

Existing work can be categorized by how such knowledge is harvested.
First category is data-driven approach of harvesting causality from web corpus.
Best known results in this category leverage
Pointwise Mutual Information (PMI) statistics~\cite{Mihalcea2006:CKM}
between words in the premise and alternative, to identify the pairs with
high correlation.
In our example, we can expect that two words \emph{knock} and \emph{invite}
co-occur frequently in web documents, which indicate a potential causality.

Though PMI has been an effective indicator in prior literature, it
suffers from the following limitations: First, lexical co-occurrence
can be a false alarm. In our example, \emph{door} and \emph{house}
are also observed frequently together, but identifying this pair as
causality leads to falsely identifying the second sentence as a
result. This observation suggests that term causality from lexical
co-occurrence alone is somewhat noisy, while harvesting from causal
lexico-syntactic patterns may address this problem, as \emph{house}
and \emph{neighbor} are not observed in causal lexico-syntactic
patterns. Second, co-occurrence is undirectional, while direction is
crucial in causality. COPA task is directional such that our example
question can be asked in another direction, that is, asking what is
a cause of knocking. In this direction, \emph{call} can be a strong
cause, but it cannot be a result, though PMI statistics would model
the two directions equally likely.

Second category, pursuing the opposite emphasis of depth in
understanding sentences, seeks to overcome the limitation of
the first approach.
These approaches build on deeper lexico-syntactic analysis of sentences,
to identify knocking and inviting in our example as
\emph{events}, and determine whether causality between two events hold.
%\KZ{How? By rules curated by human? Give example work here?}
For example, ConceptNet~\cite{HavasiSALAM10} leverages human efforts
to encode causal events as common sense knowledge.
However, these approaches, building on human and heavy analysis,
inherently lack coverage, compared to the first category, which is
reported to outperform the second~\cite{gordon2012copa}.

In contrast, our goal is to pursue both breadth and depth in modeling
commonsense causality.
To pursue breadth, we propose a data-driven approach of harvesting
\emph{term causality network} from a large corpus.
To pursue depth, we conduct lexico-syntactic analysis of the sentences to
extract events and identify events that are strongly causal using the causality network
and other semantic resources such as WordNet~\cite{Miller1995}.
Our approach overcomes the two limitations of existing data-driven approaches,
with the following novel contributions:
\begin{itemize}
\item We harvest a term causality network, selectively from causal
lexico-syntactic patterns, effectively pruning out false causality observed
from lexical co-occurrence.
This network encodes both causal direction and strength between terms and
is first of its kind to the best of our knowledge.
\item We redefine causal strength $u \rightarrow v$ to reflect directions, by combining conditional probability
of $u$ being the cause of the pairwise causality and $v$ being its effect.
\item To quantify causality between phrases, we aggregate term causality
leveraging both syntactic and semantic understanding on the premise
and the alternatives. For syntactic understanding, we parse sentences to
extract \emph{event} from premise and alternatives, consisting of head words
in verb and objects. For semantic understanding, we leverage
semantic knowledge on each term in the event obtained from WordNet,
to properly discount causality from ambiguous terms.
\item We evaluate the strength of our proposed framework using COPA task,
in which the framework achieved $68.8\%$ in accuracy, outperforming
all existing state-of-the-art approaches. Further evaluation on causality
detection between phrases also demonstrate the advantage of the proposed
framework.
\end{itemize}

The rest of the paper is organized as follows. \secref{sec:approach}
established our 4-part commonsense causal reasoning framework.
\secref{sec:eval} evaluates the accuracy of the extracted causal network,
the effectiveness of causal reasoning between sentences as well as phrases,
and the ability to determine the direction of causality. \secref{sec:related}
discusses some related work before \secref{sec:conclude} concludes the paper.
%\ZYBEGIN{}We demonstrate the effective causal directional property of our
%extracted causal network. \ZYEND{} In addition, we validate the accuracy of our
%causality detection using manually labeled causal relations from ConceptNet as
%ground truth \ZY{to show the effectiveness of our causality network}.
