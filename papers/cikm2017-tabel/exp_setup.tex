\subsection{Experimental Setup}
\label{sec:exp-setup}

In this section, we introduce how we perform word embedding on Wikipedia,
and how the cross-lingual table linking dataset is constructed.

\textbf{Wikipedia and Word Embeddings}

\noindent
We use the Feb. 2017 dump of English Wikipedia~\footnote{https://dumps.wikimedia.org/enwiki/}
and Chinese Wikipedia~\footnote{https://dumps.wikimedia.org/zhwiki/}
as the text corpora for training word and entity embeddings.
The dumps contain 5,346,897 English and 919,696 Chinese articles (entities).

For the purpose of embedding, all the entities occurred in anchor texts are regarded as special words.
E.g., the anchor text ``Rockets'' in the sentence
``the \underline{Rockets} All-Star player James Harden ... ''
is replace by the special word ``[[Houston\_Rockets]]''.
The advantage is that, by learning embeddings of both common and special words
in a uniform vector space, each entity is represented by the embedding of its identical word,
which is more precise than the aggregation of word embeddings in the entity's name.
Besides, in order to enlarge the number of anchor texts in the corpora,
e automatically add more anchor texts to both Chinese and English Wikipedia:
for each article page, we simply find all the surface form of phrases exactly matching the article name,
and then transform these phrases into an anchor text, linking to the current article.
Next we adopt Word2Vec~\cite{mikolov2013distributed} to train word embeddings from both corpus respectively,
the embedding dimension sets to 100.


% Wiki-Chinese: Treat as unstructured text, rather than a Chinese knowledge base.

\noindent
\textbf{Table Linking Dataset}

The cross-lingual table linking dataset consists of 116 web tables with
Chinese mentions and linked English Wiki articles.
The original Chinese tables are created by Wu et al. 2016~\shortcite{wu2016entity},
which contains 123 tables extracted from Chinese Wikipedia, and each mention is labeled by
its corresponding Chinese Wiki article.
We transform all the Chinese entities into English via inter-language links of Wikipedia,
producing the labeled English entities for 78\% of the entire mentions.
In addition, we discard long-tail tables, if the shape is smaller than 5*3, 
or if the number of labeled English entities is smaller than its columns.
%The whole dataset comes from 2 sources. We collect xx tables from Wutianxing \shortcite{},
%and construct the remaining xx tables by human annotation: among xxx tables extracted from
%Chinese Wiki dataset, we randomly sample xx tables with at least x rows and x columns.
%For each cell in the table, if it should be linked to a Chinese Wiki page
%but the link is missing in the original table, we manually add the link to this cell.
%This work is done by x annotators in x hours.
%Then we transform all the Chinese concepts into English concepts via inter-language links in Wikipedia.
In total, We collected 3056 mentions from 116 tables,
with 2253 mentions been linked to English entities (19.42 mentions per table).
We randomly split the dataset into training / validation / testing sets (80 : 12 : 24 tables).
%The dataset is publicly available in http://xxxxxx.
We will publish the dataset later.

%\textbf{Datasets.} 
%Chinese web table from the the previous work by XXXX \shortcite{}.
%Size: consists of xx cell mentions with labeled ch-wiki concept from xx tables.
%By looking up inter-lang link from ch to En, we collect xx cells with labeled En concepts
%(x.xx cells per table).
%We split the dataset into training / validation / testing sets (xx : yy : zz).


%\noindent
%\textbf{State-of-the-art Comparisons}

\subsection{State-of-the-art Comparisons}
\label{exp:soat}

%To the best of our knowledge, we are the first work for cross-lingual entity linking 
%in web tables. 
Since there are no previous works that directly handle the cross-lingual table linking,
we select comparison systems from two perspectives.
The first perspective is mono-lingual table linking,
we compare with Bhagavatula et al. 2015~\shortcite{bhagavatula2015tabel}
and Wu et al. 2016~\shortcite{wu2016entity}.
We call their systems $TabEL_B$ and $TabEL_W$ in short.
In order to make a fair comparison in our bilingual scenario, we bridge the language gap as follows.
%For $TabEL_{En}$,
For both systems, we apply the translation procedure in \secref{sec:candgen},
converting each cell mention into the most likely English surface form,
then run the mono-lingual model on these translated English tables and produce the final linking results.
%For $TabEL_{Ch}$, we map each predicted Chinese concept to the English concept
%by looking up the inter-language link.
%As mentioned in \secref{sec:intro}, we never use inter-language link in the learning step,
%however, $TabEL_{Ch}$ leverages this gold mapping data without any translating error,
%which makes the experiment unfair to other systems.
%Due to this reason, we regrad this work as an oracle baseline.

The second branch is cross-lingual text linking, we compare with Zhang et al. 2013~\shortcite{zhang2013cross},
and call it $TextEL$.
This work aims at entity linking from foreign languages to English on unstructured texts,
and proposed the BLDA model, which models each foreign text and English Wiki article
as the probabilistic distribution of latent bilingual topics in a uniform space.
In this case, we traverse each mention in row order and flatten the whole table into a word sequence,
then mark the word intervals for mentions to be linked. 
By turning the table into unstructured texts, the previous work is able to learn 
more flexible context information, but it may hard to capture the correlation of entities
in the same column.
