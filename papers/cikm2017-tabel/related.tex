\section{Related Work}
Our task could be viewed as a special case of entity linking, with the following restrictions: 1) the queries are all Web tables; 2) the queries and KB entities are in different languages. In \secref{sec:el} we discuss about works on standard entity linking tasks; in section \secref{sec:wt} we discuss works about Web tables; and in \secref{sec:cl} we discuss works on cross-lingual entity linking systems.

\subsection{Entity Linking}
\label{sec:el}
Entity linking has been a popular topic in NLP for a long time as it is the basic step for machines to understand natural language and an important procedure of many complex NLP applications such as information retrieval and question answering. Entity linking requires a knowledge base to which entity mentions can be linked, the most popular ones including Freebase~\cite{bollacker2008freebase}, YAGO~\cite{suchanek2007yago} and Wikipedia, where each Wikipedia article is considered as an entity. Most works focus on linking to Wikipedia and thus the task is also named as Wikification. The typical procedure of entity linking contains two stages: candidate generation, where the surface forms (mention) in the query text which could be linked to certain entities in the KB are identified, and a set of candidate entities are proposed for each entity mention; candidate ranking, where the candidates for each mention are ranked (usually based on context) and the best one is returned as linking result. Due to its fundamental role in many applications, the task of entity linking has attracted a lot of attention, and many shared tasks have been proposed to promote this study~\cite{ji2010overview, cano2014microposts2014, carmel2014erd}.

Wikipedia was first explored by Bunescu and Pasca~\cite{pasca2006using}, where an SVM kernel was used to compare the lexical context of an entity mention to each candidate's Wikipedia page. Since each entity mention needed to train its own SVM model, the experiment was limited. Later, Mihalcea and Csomai~\cite{mihalcea2007wikify!:} proposed a system called Wikify! for the Wikification task. They applied word sense disambiguation to this task, and experimented with two methods to link detected candidates to a Wikipedia page: 1) comparing the mention's lexical context to content of disambiguation page; 2) training a Naive Bayes classifier for each ambiguous mention. 

Later approaches made use of the observation that entity disambiguation in the same document should be related. Cucerzan~\cite{cucerzan2007large-scale} maximized the agreement between the context data stored for each candidate entity and the contextual information in the document, and also the agreement among the category tags of the candidate entities. Milne and Witten~\cite{milne2008learning} took a similar approach but relied on unambiguous terms in the context. Han and Zhao~\cite{han2009named} constructed a large-scale semantic network from Wikipedia, then computed similarity between query and candidate entity based on the semantic network. Ratinov et al.~\cite{ratinov2011local} formalized this task into a bipartite graph matching problem and proposed a score function which considered both local similarity and global coherence. Zhang et al.~\cite{zhang2011wikipedia} employed a Wikipedia-LDA model and the contexts were modeled as a probability distribution of Wikipedia categories. The similarity score between candidates and entities were computed based on the category distribution. 
Cai et al.~\cite{cai2013wikification} proposed to first enrich the sparsely-linked articles by adding more links iteratively and then use the resulting link co-occurrence matrix to disambiguate the mentions in an input document. Yang and Chang~\cite{yang2016s-mart:} proposed a tree-based structured learning framework, S-MART, which is particular suitable for short texts such as tweets. 

Similar to our work, Sun et al.~\cite{sun2015modeling} used neural networks for entity linking. They used a Siamese-like network structure, where the mentions and candidates are separately embedded into vector space,  and contexts are modeled by a convolution neural network. 
A cosine-similarity score is outputted as the score of a <mention, context, candidate> triplet and trained with hinge-loss. On the other hand, our model jointly assign the mentions simultaneously.

\subsection{Web Table and Table Entity Linking}
\label{sec:wt}
Different from general entity linking tasks, table entity linking focuses only on entries in tables. The interest in web tables was inspired by Cafarella et al.~\cite{cafarella2008webtables}. They found that there were about 154 million tables on the Web that could be used as a source of high-quality relational data, and implemented several applications such as schema auto-complete and attribute synonym finding. Mu{\~n}oz et al.~\cite{munoz2014using} proposed methods to mine rdf triplets from Wikipedia tables, and Sekhavat et al.~\cite{sekhavat2014knowledge} proposed methods to enrich a knowledge base by leveraging tabular data on the Web. These works and other applications involving Web tables could all benefit from our table entity linking system.

Syed et al.~\cite{syed2010exploiting} proposed approaches to infer a partial semantic model of Web tables automatically with Wikitology~\cite{syed2008wikitology}. Limaye et al.~\cite{limaye2010annotating} proposed a framework to annotate table cells with entity, type and relation information simultaneously using a graphical model. Ibrahim et al.~\cite{ibrahim2016making} presented a probabilistic graphical model to capture coherence between cells in tables and candidate entities, concepts or quantities. They devised a system to map table entries and headers to concepts, classes, entities and uniquely represented quantities in the form of <measure, value, unit> triple.

Other works focused solely on table entity linking.
Bhagavatula et al.~\cite{bhagavatula2015tabel} argued that models which jointly address entity linking, column type identification and relation extraction rely on the correctness and completeness of KB, thus may be adverse for the performance of entity linking. They also exploited a graphical model, where cells in the same row or column are connected. They trained a model to rank the candidates of a given cell by its context, i.e. cells connected to the given cell, and the predicted entity for each cell are iteratively updated until convergence. Wu et al.~\cite{wu2016entity} constructed a graph of mentions and candidate entities for each query table, then use page rank~\cite{page1999pagerank} to determine the similarity score between mentions and candidates. Besides, they combined multiple knowledge bases in Chinese to enhance the system.


\subsection{Cross-Lingual Entity Linking}
\label{sec:cl}
Starting from 2011 the annual TAC KBP Entity Linking Track has been using the multi-language setting~\cite{ji2010overview, ji2014overview, ji2015overview}, where the languages involved are English, Chinese and Spanish. Most systems for this task were adaptations of mono-lingual entity linking systems: either first do entity linking on foreign languages first then translate the results to English via language links, which requires a comprehensive knowledge base in the foreign languages; or first translate the query into English by some machine translation tool then apply English entity linking algorithms, whose performance greatly relies on the machine translator. 

Some other systems tried to avoid the usage of such assumptions. McNamee et al.~\cite{mcnamee2011cross} first experimented with cross-lingual entity linking on documents. They first used a machine translation tool developed by Irvine et al.~\cite{irvine2010transliterating} to transliterate the detected query mentions into English and transform the task into a mono-lingual one. Then they extracted some features and ranked the candidates with SVM-rank. However, to train this model, parallel corporas which are well aligned at sentence level were required.

Most methods managed to bridge the language gap through language-independent spaces.
Fahrni et al.~\cite{fahrni2011hits} presented HITS' system for cross-lingual entity linking. Their approach consisted of three steps: 1) obtain a language-independent concept-based representation for query documents; 2) disambiguate the entities using an SVM and a graph-based approach; 3) cluster the remaining mentions which were not assigned any KB entity in step 2. 
Zhang et al.~\cite{zhang2011wikipedia} leveraged a modified version of Latent Dirichlet Allocation, which they call BLDA (Bilingual LDA) and bridged the gap between languages via topic space. They trained the topic model on English-Chinese Wikipedia page pairs (indicated by inter-language links) and disambiguated candidate entities by computing the inner product of the topic distributions of the query text and the entity Wikipedia page. Their approach does not require supervised learning and performs well with a conservative candidate generation stage. 
Wang et al.~\cite{wang2015language} proposed an unsupervised graph-based method which matches a knowledge graph with a graph constructed from mentions and the corresponding candidates of the query document.

Tsai et al.~\cite{tsai2016cross} trained a multilingual word and title embeddings and ranked entity candidates using features based on these multilingual embeddings. They used canonical correlation analysis~\cite{hotelling1936relations} to project the embeddings of two languages into the same space, whose goal is the same as the translation layer in our model.