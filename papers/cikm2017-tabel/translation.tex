\subsection{Translation Module}
\label{sec:translation}

As mentioned in model overview, the input of joint model are two tables, mention table and entity table, where each of them is represented as a table of vector embeddings. Since a mention or entity name typically contains up to three words, we simply represent them as the average of embeddings of words they contain. The word embeddings are trained on large scale text corpus. Since mention table and entity table are written in two different languages, we train word embeddings on two corpus of different languages separately. Thus, the embeddings of a mention and its referent entity are naturally incompatible and we can't directly compare or calculate them. To solve this problem, We employ a bilingual translation layer to map embeddings in one language space to another language space. Through this translation layer, a non-English mention embedding $v_m$ can be translated into an English mention embedding $\widetilde{v_{m}}$ roughly through $\widetilde{v_m} = W_{t} v_m + b_{t}$, where $W_{t}$ is the translation matrix and $b_{t}$ is the bias. Notice that $W_{t}$ and $b_{t}$ are model parameters and will be updated during training so that the translation step will be more and more accurate.

In order to find a good starting point to train the model and jump out of local optima, we train $W_{t}$ and $b_{t}$ in advance. We use a small number of bilingual word embedding pairs $\langle v_{wc}, v_{we} \rangle$ to train the parameters. The loss function is as follows.
\begin{equation}
\label{eqn:translation}
L(W_{t}, b_{t}) = \Arrowvert W_{t} v_{wc} + b_{t} - v_{we} \Arrowvert_2
\end{equation}
The list of bilingual word embedding pairs are called translation seeds. We learn a initial translation matrix by minimize the loss and then feed the weights into the model before training.