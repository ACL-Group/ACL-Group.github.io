\section{Related Work}
\label{sec:related}

Commonsense causal reasoning is the process of capturing
and understanding the causal dependencies amongst events
and actions. The most common dataset is 
the Choice of Plausible Alternatives (COPA), which 
contains a \textit{premise} and two \textit{alternatives},
as well as a prompt specifying the relation between them.
Previous studies can be roughly categorized into three
lines: feature based methods, co-occurrence based methods and neural methods. 
Both are applied to COPA and select
which alternative conveys the more plausible cause or effect 
based on the prompt of the premise sentence.

As for feature based methods, Goodwind~\shortcite{goodwin2012utdhlt} 
proposes the COPACETIC system developed
by the University of Texas at Dallas (UTD) for COPA. They take COPA 
as a classification problem and use features
derived from varied datasets.

For co-occurrence based methods, Roemmele~\shortcite{roemmele2011choice} 
and Gordon~\shortcite{gordon2011commonsense}
focus on lexical co-occurrence statistics  gathered from story corpora.
They use the Pointwise Mutual Information (PMI) statistic\cite{church1990word}
to compute the number of times two words co-occur
within the same context relative to their overall frequency. 
This co-occurence measure is order-sensitive. 
Luo~\shortcite{LuoSZHW16} proposes a framework that
automatically harvests a network of causal-effect terms from
a large web corpus, referred as the CasualNet.
They extracted sequences matching lexical templates that signify causality.

In neural methods, Roemmele~\shortcite{roemmele2018encoder}
presents a neural encoder-decoder model that learns to predict relations 
between adjacent sequences in stories as a means of modeling causality. 
Dasgupta~\shortcite{abs-1901-08162} solves a range of problems
that each contain causal structure.
It adopts the meta-reinforcement learning,
in which a RNN-based agent is trained with model-free reinforcement learning (RL).
Yeo~\shortcite{YeoWCCH18} studies the problem of 
multilingual causal reasoning in resource-poor languages.
They construct a new causality network (PSG) of cause-effect terms, 
targeted for the machinetranslated English, 
but without any language-specific knowledge of resource-poor languages.

Recently, neural-based sequence-to-sequence models have met a great success
in machine translation and summarization~\cite{BahdanauCB14,SutskeverVL14,PaulusXS17,SeeLM17,NallapatiZSGX16}, 
espcially CNN seq2seq models~\cite{gehring2017convs2s,LiuLZ18,FanGA18},
which is much faster than others.
It shows that the CNN seq2seq models can obtain the latent relation
between encoder input and decoder output. Inspired by this, we
identify the causality generation problem.
To the best of our knowledge,
our model initiates to generate cause or effect through cnn seq2seq model 
and causality attention mechanism. 
We propose a causality dataset consisting
of cause-effect sequence pairs for training.
The causality attention mechanism
is a hybrid between traditional attention and 
causal strength which is calculated by CausalNet~\cite{LuoSZHW16}.
We calculate an explicit switch probability $\lambda$
to tune the traditional attention distribution and causal strength distribution.
The causality attention mechanism has the ability to obtain causality between texts.
Thus, our model can generate causal sentences with input sentences.



