\section{Offline Evaluation}
\label{sec:eval}

In this section, we first introduce the dataset and experiment setup, including evaluation metrics and baselines.
Then we present the offline results and give some discussions.
Finally, we perform ablation tests to complete our experiments.

\subsection{Datasets}

Inferring e-commerce concepts a user potentially needs is a relatively new problem, 
there is no such public datasets for experiments.
To create large amounts of gold standard data to train our model, 
we collect daily log of our online system, 
where concepts are already integrated in the recommender system. 
In a module called ``Guess What You Like'' at the front page of Taobao app, concepts are displayed as cards to users among the recommended items.
There will be one concept card every ten items on average.
In the snapshot shown in \figref{fig:cloud}(a), concept ``Tools for Baking'' is displayed as a card, with the picture of a representative item.
%\KZ{How do did you get the concept on the taobao page in the first place. 
%It seems that each concept is associated with a set of items on the taobao page.
%If these concepts were created manually, then why not use them as positive
%data directly?}
Once users click on this card, 
it jumps to a page full of related items such as egg scrambler and strainer.
In order to alleviate the potential influence of the item picture on users' decision making,
we collect positive samples from those user-concept clicks
only if that user continues to click at least two related items after entering the concept card.
For the same reason, negative samples come from at least two exposes of the same concept (but different item pictures) without any clicks.
We collect samples for continuous four days during January 11 to January 14, 2019, and use the data of first three days for training and validation. 
We randomly select $10\%$ samples of the last day for testing.
The ratio of negative and positive is around $37:1$.
For user-item interaction data, we collect 30-days transaction records on Taobao platform for each user in our data.
Detailed statistics of our dataset is illustrated in \tabref{tab:exp_data}.
\begin{table}[th]
	\centering
	%\scriptsize
	\begin{tabular}{l|c|c|c}
		\hline
		  & Training & Validation & Testing \\
		\hline
		\# of samples & 32,496,827 & 328,251 & 1,237,506 \\
		\# of users & 16,120,600 & 323,544 & 1,121,475 \\
		\# of concepts & 4,760 & 2,935 & 3,176 \\
		\hline
		\# of items & 438M & 76M  & 141M \\
		\# of categories & 15,257 & 11,799 & 14,590 \\
		\# of brands & 1,434,659 & 428,036 & 1,088,480 \\
		\hline
	\end{tabular}
	\caption{Statistics of Taobao's dataset.}
	\label{tab:exp_data}
\end{table}

%The e-commerce concept net we use as the knowledge graph is incomplete at current stage.
%Ideally, all edges between concept and other node should be bi-directional. For example, the relation from a category to a concept is clearly not the same as its reverse. 
%In practice, due to the fact our concept net is still under construction, we only adopt edges from item, category and brand to concept in our experiments.
Based on years of e-commerce experience, we mainly select five meta-paths (\figref{fig:model}) in our experiments: ``\textbf{UIC}'', ``\textbf{UITC}'' and ``\textbf{UIBC}'' for behavior  paths; ``\textbf{UTC}'' and ``\textbf{UBC}'' for preference paths.
Longer paths are not selected since they are likely to bring noises.
%We do not include meta-paths taking concept as intermediate node such as ``\textbf{UCTC}'' for this reason.




\subsection{Experiment Setup}

\noindent
\textbf{Evaluation Metrics}

\noindent
We perform evaluation of different models in two experiment scenarios.
1) In click-through-rate (CTR) prediction, we apply the trained model to each sample of test set and calculate $AUC$ based on the output score to evaluate the overall performance; 2) In top-$N$ recommendation scenario, 
we use the trained model to select $N$ concepts with highest predicted scores for each user in the test set. 
we evaluate the results by Hit Ratio ($HR@N$), and Normalized  Discounted Cumulative Gain ($NDCG@N$),
which are widely used in recommendation tasks having very few ground-truth results \cite{huang2018improving,chen2018sequential}.
In order to make sense under the second scenario, 
we augment the test set mentioned above by removing samples where the user does not have any positive clicks,
and report averaged $HR$ and $NDCG$ across users.

\noindent
\textbf{Baselines}

\noindent
We compare with the following baselines:
\begin{itemize}
	\itemsep0em
	\item \textbf{BPR} \cite{rendle2009bpr} is the Bayesian Personalized Ranking model that minimize the pairwise ranking loss for implicit feedback.
	\item \textbf{Wide\&Deep} \cite{cheng2016wide} is the widely used recommendation framework, which jointly trains wide linear models and deep neural networks. We use embeddings of users, concepts and other entities to feed Wide\&Deep.
	\item \textbf{MCRec+} is based on MCRec\cite{hu2018leveraging}, which is a state-of-the-art HIN based model for recommendation. It treats the KG as HIN and extracts meta-path based features for modeling user-target interaction. 
	We feed e-commerce concept net as the HIN for MCRec. For fair comparison, extra information appeared in our problem such as sequential user behaviors, user profile and concept schema, 
	are also fed into MCRec in a compatible way.
	\item \textbf{KPRN+} is based on KPRN \cite{wang2018explainable}, another state-of-the-art knowledge-aware recommendation model, which aims to reason over KG by composing both entities and relations. Similar to MCRec+, we feed extra information to KPRN to get KPRN+.
%\KZ{Where is BPR in the results table? I think it is better if you can include
%some results on some standard datasets, rather than just private taobao data.
%Also, I think you need to release the code of all your implementation, including
%the baselines to be more convincing.}
\end{itemize}

\noindent
\textbf{Implementation Details}

\noindent
We implement our model using the python library of TensorFlow \footnote{\url{www.tensorflow.org}}. 
We set the length of user behavior sequence to $15$,
and sampled path instance within each meta-path to at most $50$.
The dimension of entity embeddings (item, concept, category, etc.) is set to $20$, 
and the dimension of output layer is set to $32$.
The hidden state size of GRU is set to $40$.
All parameters are randomly initialized with Gaussian distribution.
We perform a mini-batch log-likelihood loss training with a batch size of $512$ for $5$ training epochs.
We use Adam optimizer \cite{kingma2014adam}, and the learning rate is initialized to $0.001$.
%To prevent the gradient explosion problem for training deep networks,
%we set gradient clip-norm as $5$.
For all the comparison models, 
we refer to their original papers and tune the parameters using the validation set as well.
With the help of a powerful distributed TensorFlow machine learning system in Taobao, 
we use $4$ parameter servers and $20$ workers,
and the whole training process can be finished in $4$ hours.

\subsection{Results}
\label{sec:off_eval}

We report the experimental results in \tabref{tab:eval_main} and \figref{fig:topn}.
Our model outperforms all the baselines, improving the result by up to 2.4\% in $AUC$.
Improvements in $HR$ and $NDCG$ also reveal the superiority of our model.
BPR and Wide\&Deep perform comparably poorly than other baselines, since they do not incorporate extra knowledge from e-commerce concept net into the model, failing to leverage rich features from paths between users and concepts.
For knowledge-aware baselines, 
%we mainly focus on implementing the encoding of paths in different ways according to the ideas mentioned in their papers respectively.
the main difference is the encoding of paths and attention mechanism.
MCRec+ performs best among all baselines, 
since it also try to characterize a three-way interactions among user, paths and the concept.
Our model substantially outperforms MCRec+ to achieve the best performance,
which indicates the importance of modeling mutual attentive influence of three components simultaneously.
KPRN+ performs worse than MCRec+, since the relation name matters in their problem is relatively trivial in our concept net.
The last two lines of \tabref{tab:eval_main} further demonstrate the effectiveness of our proposed attention module. 
By comparing to a degenerated version of our model, which replaces attention cube with average pooling in each component, 
our full model achieves better performance.

\begin{table}[th]
	\centering
	%\scriptsize
	\begin{tabular}{l|c}
		\hline
		Model &   AUC \\
		\hline
		BPR  &  0.6005 \\
		Wide\&Deep  &  0.6137 \\
		MCRec+  &  0.6447 \\
		KPRN+  &  0.6417 \\
		\hline
		Ours (- att. cube) & 0.6403 \\ 
		Ours (full) &  \textbf{0.6612} \\
		\hline
	\end{tabular}
	\caption{AUC in CTR prediction on Taobao's dataset.}
	\label{tab:eval_main}
\end{table}

\begin{figure}[th]
	\centering
	\epsfig{file=figures/topn.eps, width=\columnwidth}
	\caption{HR and NDCG in Top-N recommendation.}
	\label{fig:topn}
\end{figure}



\subsection{Ablation Study}
In this subsection, we explore the contribution of various components of our model.
We report AUC on evaluation set to compare different variations in \tabref{tab:ablation}.


\noindent
\textbf{Behavior Paths vs Preference Paths}

\noindent
We first evaluate how different types of meta-path between users and concepts effect final performance. 
If we remove all paths, AUC drops by $6.8\%$, 
revealing the huge benefits brought by the concept net.
Between behavior paths and preference paths,
we can observe that AUC drops more severely when removing the former ones,
which indicates that behavior paths are more important than preference paths in our model.
It appears that recent clicks or purchases of items play a larger role 
in reflecting user needs than long-term preferences,
which may inflect that user needs are changeable and unstable, and they can be easily influenced.
\begin{table}[th]
	\centering
	%	\scriptsize
	\begin{tabular}{l|c|c}
		\hline
		Variation  & AUC   & Decrease (\%)\\
		\hline
		- behavior paths  &  0.6826 & 4.03\\
		- preference paths  & 0.6934 & 2.41\\
		- all paths & 0.6694 & 6.08\\
		\hline
		\hline
		- user behavior sequence & 0.7010 & 1.30\\
		- user profile & 0.6986 &  1.65\\
		- concept schema & 0.7031 & 1.00\\
		\hline
		\hline
		Full  &  \textbf{0.7101} & 0.0 \\ 
		\hline
	\end{tabular}
	\caption{Ablation tests on validation set.}
	\label{tab:ablation}
\end{table}

\noindent
\textbf{Behavior Sequence vs Side Information}

\noindent
Now we investigate the influence of user behavior sequence and side information in our problem,
where side information further includes user profile and concept schema.
Ablation towards these three components shows that they all contributes to the final inference result, 
while user profile information matters most ($1.65\%$ decrease in AUC).
It is observed that user profile seems more important than user behavior sequence.
The possible reason is that the attention cube degenerates to a matrix, if we remove user profile from the model.
This may lead to a decrease in final performance.



\section{Online Application}

The above offline experimental results have shown superiority of our proposed model for accurately inferring user needs.
Now, we deploy our model online and integrate it into a recommender system in Taobao with a standard A/B testing configuration to answer the following three questions:
\begin{enumerate}
	\itemsep0em
	\item Does our inference model still perform the best at online setting regarding both accuracy and novelty?
	\item Does user needs inference actually improve user satisfaction?
	\item Comparing to traditional item recommendation, does user-needs driven recommendation with concept cards bring extra value to e-commerce platforms? 
	
\end{enumerate}

\subsection{Experiment Setup}
The experiments are conducted in the online module introduced in \figref{fig:cloud} (a).
We integrate the inferred user needs (a.k.a. concepts) for each online user to our item recommender system, 
making recommendations of concept cards (one concept plus one representative item).
Two online metrics are used to measure the performance: 
click-through-rate (\textbf{CTR}) and category-discovery (\textbf{Discovery}). Detailed definitions are as follows:
\begin{equation}
\textbf{CTR} = \frac{\# \text{ concept card clicks}}{\# \text{ concept card exposes}},
\end{equation}
\begin{equation}
\textbf{Discovery} = \text{Avg}_u(\frac{\# \text{ new clk-cates in 15d}} {\# \text{ clk-cates}})
\end{equation}
where Discovery is a measurement of how many distinct categories of representative items in concept cards a user clicked today are newly discovered (not clicked in the past $15$ days in Taobao platform).
It is a temporary\footnote{Designing a proper metric to evaluate novelty in industrial recommendation is a hard and unsolved problem.} metric used in Taobao to evaluate the novelty of recommendation results.

We deploy the user needs inference module online 
and daily update our model.
When recommending a concept card, online recommender system first output a list of items as usual, 
then we pair the items in the list with inferred top concepts, and filter out those items which are not related to any concepts. In the meantime, items within top concepts will complement the list.
Followed by another ranking module, concept cards with highest scores will then be displayed to users.
\subsection{Results}

To answer the first question, we compare our model to the former strategy based on rules\footnote{Concepts are ranked by the counting number of their related items which are behaved by the user.} and the strongest baseline MCRec+ in offline setting.
Online results of A/B testing show that our model achieves highest CTR, 
which demonstrates that it can infer user needs more accurately. 
On the other hand, largest improvement on Discovery
shows our model is able to bring more novelty.

\begin{table}[th]
	\centering
	%	\scriptsize
	\begin{tabular}{l|c|c}
		\hline
		Strategy  & CTR   & Discovery\\
		\hline
		Rule-based  & - & - \\ 
		MCRec+  & +5.1\%   & +3.4\%\\
		Ours  & \textbf{+6.0\%} & \textbf{+5.6\%} \\

		\hline
	\end{tabular}
	\caption{Improvements on CTR and Discovery. }
	\label{tab:online}
\end{table}

%Online prediction time is important to real-world industry systems. 
%Comparing to other online deep models, 
%the additional time cost of our model mainly lies on the generation of different path instances between the user and the candidate concept.
%For efficiency consideration,
%we constrain the number of instances for each meta-path to a small number of $5$ in practice. Given a node, we offline filter out all the out-going nodes with a low priority score (mentioned in \secref{sec:path}), and build an alias table.
%In this way, the time cost of node sampling is $O(1)$.
%Further empowered by a specially optimized graph search engine in Taobao, 
%the average running time of user needs inference is $10$ ms, 
%which is shorter than following ranking module and can be tolerated by the recommender system.

To answer the second question,
we conduct a real in-app user survey on Taobao since standard metrics like CTR and Discovery may not directly represent user satisfaction.  
Due to limited resource and time, we can only finish three rounds of survey.
In each round, we randomly select 50,000 users and send them top 3 concepts inferred by model or by rule-based strategy. 
Each selected user is asked to answer a simple question: ``Are you satisfied with [X] as a recommended shopping need for you?'', where [X] is replaced by one inferred concept and the answer is YES or NO.
Around 9k users out of 50k actually answered at least one question in each round of survey.
The satisfaction rate is then calculated as the percentage of questions whose answer is YES in all answered questions, 
which is shown in \tabref{tab:survey}.
User satisfaction rate is improved by \textbf{20.6\%} if we use the proposed inference model (14.7\% for MCRec+), which demonstrates such user needs inference actually make users more satisfied.
As we can notice, the absolute number of satisfaction rate is only 41\%,
which is clearly not a large number. 
In fact, it is hard to know the true upper bound of user satisfaction rate, meaning there is ample room for us to continuously explore user needs understanding. 
%It is really hard to guess what a user really needs at the moment he comes to our platform.

\begin{table}[th]
	\centering
	%	\scriptsize7
	\begin{tabular}{l|c|c}
		\hline
		Strategy  &  Satisfaction Rate  & Improvement\\ 
		\hline
		Rule-based  &  34\% & -\\ 
		MCRec+  &  39\% & +14.7\% \\ 
		Ours  &  \textbf{41\%} & \textbf{+20.6\%} \\
		\hline
	\end{tabular}
	\caption{Satisfaction rate from real user survey.}
	\label{tab:survey}
\end{table}


To answer the last question, 
we compare recommending a concept card with 
recommending an item (traditional item recommendation)
at the same position on ``Guess What You Like''.
Online evaluation shows significant improvements \footnote{This comparison is not entirely fair due to the different display form between a concept card and an item. 
	But the large improvements still indicate the value of user-needs driven recommendation. Integrating user needs understanding to general item recommendation is included in our future work.} of recommending concept cards:
\textbf{5.3\%} in CTR and \textbf{9.6\%} in Discovery.
If we further consider the purchases of related items in the page guided by concept cards, total sales volume (GMV) is improved by \textbf{84.0\%}, which demonstrates the great value and potential of such user-needs driven recommendation.


\subsection{Case Study}
\label{sec:case}

A major contribution of our model is that we propose a attention cube to model three-way interactions simultaneously, aiming to distinguish different importances of different factors in an e-commerce interaction, 
which may inspire us to better understand user needs.
Therefore, we analyze the attention values from several perspectives during online inference. 
All the following analysis is based on one day's user log.

%In \figref{fig:att}, we visualize average attention weights ${\bm{\alpha_u}}_i$, ${\bm{\alpha_p}}_j$ and ${\bm{\alpha_c}}_k$ for each component of user profile, meta-paths and concept schema.
%We can observe that ``\textbf{UIC}'' path has the highest attention weight among all meta-paths, while preference paths ``\textbf{UTC}'' and ``\textbf{UBC}'' are less important.
%The result verifies the ablation test towards path variations in \tabref{tab:ablation}.
%For aspects of user profile and concept schema, 
%it is shown that ``gender'' and ``life stage'' have larger impacts than others,
%and the weight of ``kid's life stage'' in user profile is the highest, 
%we guess the reason is that a large portion of active users in Taobao are parents who have kids.

%\begin{figure}[th]
%	\centering
%	\epsfig{file=figures/att.eps, width=\columnwidth}
%	\caption{Visualization of attention weights for meta-paths, user profile aspects and concept schema aspects. Darker colors indicate higher weights.}
%	\label{fig:att}
%\end{figure}

%We further investigate the attention weights to reveal some interesting findings.
During inference, if the gender of a user matches to the gender constraint of a concept, the attention weights of ``gender'' in both user profile and concept schema become nearly twice larger than not matching.
This indicates our model can explicitly learn rules such as a young female user is more likely to need a concept ``Party for girls'' rather than ``Party for boys''.

\begin{figure}[th]
	\centering
	\includegraphics[width=\columnwidth]{figures/att_2}
	%\epsfig{file=figures/att_2.eps, width=\columnwidth}
	\caption{Visualization of attention weights for an anonymous user. Darker colors indicate higher weights.}
	\label{fig:att2}
\end{figure}

To see if the same user has different preferences on meta-paths regarding different concepts,
 we randomly pick a user as illustrative example shown in \figref{fig:att2}.
The anonymous user has two positive interactions of concept cards: ``Learning to Walk for Kids'' and ``Fishing in River''.
After digging into transaction data,
we find that this user recently clicks a lot of kids related items, resulting high importance of behavior paths shown in his attention distribution when facing concept ``Learning to Walk for Kids''.
On the contrary, he has few behaviors related to fishing. 
Accordingly, the attention weights of preference paths are much higher than average when facing concept ``Fishing in River'' since his long-term category preference is ``fishing equipments''.
%Besides, attention weights of ``life stage'' in ``Learning to Walk for Kids'' is higher than ``Fishing in River'' because the former one is constrained to ``teenager'' while the latter has no constraints.










