\section{Related Work}
Extractive distractor generation typically involves two steps: 
candidate set generation and distractor selection.
% candidate set generation
% \textbf{Candidate Set Generation} ~
In the common scenarios, only the key and the stem are known beforehand and the set of candidates need 
to be automatically generated. A solution used in previous work is to construct distractor 
candidate sets from domain-specific vocabulary, 
thesauri~\cite{sumita2005measuring,smith2010gap} 
or taxonomies~\cite{mitkov2009semantic}. 
These domain-specific candidate sources are still not large or general enough, however, 
to support open-domain distractor generation. In contrast, our proposed framework is able to utilize a broad spectrum of general purpose KBs and perform context-dependent conceptualization in an open-domain setting.
%\KZ{Need to compare these works with our work to put our work in perspective all the time.
%Can't just discuss other work without mentioning our work.}

% distractor selection
% \textbf{Distractor Selection}~
Previous approaches usually select distractors according to different metrics based on the key, including embedding-based similarities~\cite{guo2016questimator}, difficulty level~\cite{brown2005automatic,coniam2013preliminary}, WordNet-based metrics~\cite{mitkov2003computer} and 
syntactic features~\cite{agarwal2011automatic}.
% Another approach generates distractors that are semantically similar to the key 
% in some sense, but not in the particular sense in the stem~\cite{zesch2014automatic}. 
Some approaches also consider the semantic relatedness of distractors with the whole stem ~\cite{pino2008selection,mostow2012generating} with domain restriction. Other researchers~\cite{liang2017distractor, liang2018distractor} investigate applying learning-based ranking models to select distractors that resemble those in actual exam MCQs, and quantitatively evaluate the top generated distractors. The DS in our framework incorporates a wide range of similarity measures to account for the plausibility in various aspects.

% relaibility checking
% \textbf{Reliability Checking}~  
To generate reliable distractors, a supervised classifier~\cite{lee2007automatic} is trained to do this job where they have a limited list of potential target words and distractors. 
Another way to perform reliability checking is by considering collocations 
involving the target word~\cite{smith2010gap,jiang2017distractor}. 
% e.g. if the key is \textit{strong}, and we can find 
% a collocation \textit{strong tea}, then we can use \textit{powerful} as a distractor because it is semantically similar to \textit{strong}, 
% yet \textit{powerful tea} is not a valid collocation. 
This approach is effective, but requires strong collocations statistics to 
discriminate between valid and invalid distractors and may not be applied to
the sentence in \figref{fig:mcq} which contains rare word combinations. 
A web search approach is applied by Sumita et al.~\shortcite{sumita2005measuring} to discard words that can be found on the web search results of the stem with blank filled by the distractor. We instead propose a novel web-based reliability checking feature and integrate it into DS for more accurate selection.
