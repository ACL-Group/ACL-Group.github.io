\begin{abstract}
Few-shot relation classification seeks to classify incoming query instances after meeting 
only few support instances. % during testing.
This ability is gained by training with large amount of in-domain annotated data. 
In this paper, we tackle an even harder problem by further limiting the amount of data available
at training time. 
%In this paper, 
We propose a few-shot learning framework for %sentence-based 
relation classification,
which is particularly powerful when the training data is very small.
In this framework, models not only strive to classify query instances, but also seek 
underlying knowledge about the support instances
to obtain better instance representations. %models are trained not only by queries but also the support instances.
The framework also includes a method for aggregating cross-domain knowledge into models by 
open-source task enrichment.
Additionally, we construct a brand new dataset: the TinyRel-CM dataset, a few-shot relation 
classification dataset in health domain with purposely small training data
and challenging relation classes. Experimental results demonstrate that our framework brings
performance gains for most underlying classification models,
%\KZ{I think you are talking about underlying classification models?}
outperforms the state-of-the-art results given small training data, 
%(e.g., on TinyRel-CM dataset and FewRel-dataset \cite{han-etal-2018-fewrel} with 
%shrunken training data), 
and achieves competitive results with sufficiently large training data.
%(e.g., on FewRel dataset with full training data).
%on TinyRel-CM dataset and achieves competitive results on
%the FewRel dataset \cite{han-etal-2018-fewrel}, especially when extremely small amount of training data is given.
% \KZ{Don't mention MESDA everywhere, but only when you need to refer to it succinctly.}
\end{abstract}
