\section{Related Work}
%\subsection{Relation Classification}
%\KZ{You said meta learning for few-shot RC problem is largely inspired
%by methods from the CV community, and our method is inspired by Chen 2019.
%I think one para should be devoted to meta learning in CV and how our work
%is related to Chen.}
Relation classification task aims to categorize the semantic relation between two entities conveyed by a given sentence into a relation class.
In recent years, deep learning has become a major method for relation classification.
%\cite{RecursiveNNRC} fed the recurrent neural network (RNN) with a syntactic tree to introduce attention mechanism in relation classification tasks.
Zeng et al. \shortcite{zeng-etal-2014-relation} utilized a convolutional deep neural network (DNN) during relation classification to extract lexical and sentence-level features. Vu et al. \shortcite{vu-etal-2016-combining} employed a voting scheme by aggregating a convolutional neural network with a recurrent neural network.
%
Traditional relation classification models suffer from lack of data. To eliminate this deficiency, distant-supervised approaches are proposed, which take advantage of knowledge bases to supervise the auto-annotation on massive raw data to form large datasets. Riedel et al. \shortcite{NYTdataset} constructed the NYT-10 dataset with Freebase \cite{Freebase} as the distant supervision knowledge base and sentences in the New York Times (2005-2007) as raw text. Distant-supervised methods suffer from long tail problems and excessive noise. Zeng et al. \shortcite{zeng-etal-2015-distant} proposed piecewise convolutional neural networks (PCNNs) with instance-level attention to eliminate the negative effect caused by the wrongly labeled instances on NYT-10 dataset. Liu et al. \shortcite{liu-etal-2017-soft} further introduced soft label mechanism to automatically correct not only the wrongly labeled instances but also the original noise from the distant supervision knowledge base.

The lack of annotated data leads to the emergence of few-shot learning,
where models need to perform classification tasks without seeing much data.
Meta-learning is a popular method for few-shot learning and is widely investigated in computer vision (CV).
Lake et al. \shortcite{LakeHuman} proposed Omniglot, a few-shot image classification dataset and put forward the idea of \emph{learning to learn}, which is the essence of meta-learning.
Memory-Augmented Neural Networks \cite{Santoro2016} utilized a recurrent neural network with augmented memory to store information for the instances the model has encountered.
Meta Networks \cite{metanet} implemented a high-level meta learner based on the conventional learner to control the update steps of the conventional learner.
GNN \cite{gnn} regarded support and query instances as nodes in a graph where information propagates among nodes, and classified a query node with the information of support nodes.
SNAIL \cite{snail} aggregated attention into the meta learner.
Prototypical networks \cite{proto} assumed that each relation has a prototype and classified a query instance into the relation of the closest prototype.
Image deformation meta-networks \cite{chen-2019-image} utilized a image deformation sub-net to generate more training instances for one-shot image classification.

Few-shot relation classification is a newly-born task that requires models to do relation classification under merely a few support instances.
Han et al. \shortcite{han-etal-2018-fewrel} proposed the FewRel dataset for few-shot relation classification and applied meta-learning methods intended for CV, including Meta Networks \cite{metanet}, GNN \cite{gnn}, SNAIL \cite{snail}, and prototypical networks \cite{proto}, %with CNN and PCNN core
on the FewRel dataset.
Prototypical networks with CNN core turned out to have the best test accuracy among the reported results.
Models that are more applicable for relation classification tasks are further proposed.
ProtoHATT \cite{hatt} reinforced the prototypical networks with hybrid attention mechanism. MLMAN \cite{ye-ling-2019-multi} improved the prototypical networks by adding mutual information between support instances and query instances. Bert-Pair \cite{gao-etal-2019-fewrel} adopted BERT \cite{devlin2018bert} to conduct binary relation classifications between a query instance and each support instance and fine-tuned on FewRel dataset. Baldini Soares et al. \shortcite{baldini-soares-etal-2019-matching} trained a BERT-like language model on huge open-source data with a Matching the Blanks task and applied the trained model on few-shot relation classification tasks.

Meta networks perform worst among all the methods \cite{han-etal-2018-fewrel} and
is time consuming (about 10 times slower than other methods with even better results).
%\KZ{How time-consuming? Better give some numbers. And then say that's why
%we didn't include it in our experiments.} % due to the complex training strategy.
Matching the Blanks is high-resource and not comparable to other methods. So we do not adopt these two methods as baselines.
%\subsection{Meta-learning}
% in CV... 组会的papers!
%Meta-learning aims to .... It is first proposed by [??] where the authors ...image classification... and is widely researched in computer vision. One category of meta-learning methods trains a meta-learner above the traditional learner. The meta-learner guides the learning steps of the traditional learner[?? ??]. Another category bases on metric learning, which aims to find the underlying distribution among all the classes[?? ??]. Prototypical networks[??] is a typical framework of metric-learning based meta-learning. It proposes the concept of prototype, which is a centroid among all the support vectors within the same class, and uses the prototypes to represent the vectors of each relation. In [??], prototypical network is adopted to handle NLP tasks such as few-shot relation classification.

%The methods of updating models in previous works rely on the output of the query instances to a great extent, which lose sight of the significant and precious information within the support instances.
Previous methods lose sight of the significant knowledge within the support instances.
Chen et al. \shortcite{chen-2019-image} conducted one-shot image classification task with an image deformation sub-net. The sub-net is designed to generate more support instances to augment labeled data and is trained by a prototype classifier. The prototype classifier updates the deformation sub-net according to the classification results on prototypes of generated support instances, and aims to improve the deformation process.
Inspired by this work, we add a support classifier over each support instance during the training process. The support classifier helps with the update process of the parameters in both the support classifier and the encoder, and is scheduled with a fast-slow learner scheme. The support classifier aims to utilize knowledge within support instances to obtain better instance representations. Additionally, previous few-shot relation classification models are trained with sufficient training data despite the small support set size during testing, %violating the original intention of few-shot learning. 
%We propose a task enrichment method to introduce cross-domain knowledge and is extremely helpful with small training data.
we put forward a new challenge on few-shot relation classification by limiting the training data size.


%\subsection{Datasets}
Relation classification datasets have been released in past decades. Conventional ones include SemEval-2010 Task 8 dataset \cite{semeval8}, ACE 2003-2004 dataset \cite{ace}, TACRED dataset \cite{zhang-etal-2017-position} and NYT-10 dataset \cite{NYTdataset}. All these datasets encompass sufficient data to train a strong model.
Han et al. \shortcite{han-etal-2018-fewrel} released the first few-shot relation classification dataset, the FewRel dataset, which contains 100 relation classes with 700 instances per class. 
Although only few support instances are provided in each test task, the training data is sufficiently large. %Although the dataset is intended for few-shot relation classification, the training data is still sufficiently large.
%Additionally, the relation classes are highly distinguishable with distinct entity types, which is meaningless in practical applications.
% (the model meets 700 instances times 100 classes during training). % And the performance of models decrease if the training data decreases.
We propose TinyRel-CM dataset, %which is a \emph{real}
a brand new few-shot relation classification dataset with purposely small training data and challenging relation classes.
%Relation classes of each task in TinyRel-CM dataset share common entity types. The TinyRel-CM dataset is a much harder task than previous dataset.
The TinyRel-CM dataset is the second and first Chinese few-shot relation classification dataset.
%few-shot relation classification tasks. %In our paper, we also propose a method that is capable of handling few training data.
