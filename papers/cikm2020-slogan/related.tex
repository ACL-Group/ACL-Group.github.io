\section{Related Work}
In this section, we briefly review related literatures
and explain why we choose convolutional Seq2Seq framework as our basic model.
We start by discussing text generation.
Then we present related works on automatic text generation in E-commerce.
%We start by discussing text generation.
%Then we present related works on automatic text generation in E-commerce.
%
%\subsection{Text generation in E-commerce}
%%%%%%%%%%%%
%Most existing works for text generation in E-commerce 
%are based on templates or statistical methods.
%As for product description generation, 
%Langkilde et al.~\cite{langkilde1998generation} propose a 
%template-based grammar for language generation.
%Gerani et al.~\cite{gerani2014abstractive} also generates sumarization of product reviews
%using a template-based NLG framework.
%Wang et al.~\cite{wang2017statistical} construct template candidates, and instantiate them  
%based on statistical methods as description candidates,
%then adopt the top candidate after scoring with a ranking model. 
%Such methods are limited by the hand-crafted templates.
%In the case of product title generation, 
%Souza et al.~\cite{de2018generating} statistics about the item titles and
%recombine frequent n-grams as generated titles using a stack-based search algorithm. 
%However, such methods are suffered from incoherence and lack of diversity, which is insufficient to generate quality results.
%
%To alleviate above limitations, researchers adopt deep learning models 
%and introduce diverse conditions to the generation model. 
%Lipton et al. ~\cite{lipton2015capturing} proposed a method to generate reviews 
%based on the conditions of semantic information and sentiment with a language model, 
%and Tang et al.~\cite{tang2016context} 
%conditioned their generation on discrete and continuous information. 
%Most recently, Chen et al.~\cite{ChenLZYZ019} extend the Seq2Seq framework
%to product description generation and achieve better performance. 
%%%%%%%%%%%%%%%%%%%

\subsection{Text generation}
\label{sec:textgen}
Neural models have recently achieved great empirical success on text generation tasks,
such as machine translation~\cite{bahdanau2014neural,wu2016google},
summarization~\cite{chopra2016abstractive,nallapati2016abstractive},
dialogue response generation~\cite{serban2017multiresolution}.
At a high level, the technique has been to train end-to-end neural network models
consisting of an encoder model to produce a hidden representation of the source text, 
followed by a decoder model to generate the target, which is referred as 
encoder-decoder (or Seq2Seq) framework.
Especially, convolutional sequence-to-sequence (CNN Seq2Seq) learning~\cite{gehring2017convolutional}
has been successfully applied on various text generation applications
~\cite{wu2019pay,narayan2018don,baevski2018adaptive,dinan2018wizard}.
Therefore, the attention based CNN Seq2Seq model~\cite{ott2019fairseq} 
is a competitive baseline for text generation.

\subsection{Text generation in E-commerce}
We present related works on text generation in E-commerce.
Previously discussed models for text generation can be easily adapted to E-commerce scenario.

Most existing works are based on templates or statistical methods.
As for product description generation, 
Langkilde et al.~\cite{langkilde1998generation} propose a 
template-based grammar for language generation.
Gerani et al.~\cite{gerani2014abstractive} also generates sumarization of product reviews
using a template-based NLG framework.
Wang et al.~\cite{wang2017statistical} construct template candidates, and instantiate them  
based on statistical methods as description candidates,
then adopt the top candidate after scoring with a ranking model. 
Such methods are limited by the hand-crafted templates.
In the case of product title generation, 
Souza et al.~\cite{de2018generating} statistics about the item titles and
recombine frequent n-grams as generated titles using a stack-based search algorithm. 
However, such methods are suffered from incoherence and lack of diversity, 
which is insufficient to generate quality results.

To alleviate above limitations, researchers adopt deep learning models 
and introduce diverse conditions to the generation model. 
Lipton et al. ~\cite{lipton2015capturing} proposed a method to generate reviews 
based on the conditions of semantic information and sentiment with a language model, 
and Tang et al.~\cite{tang2016context} 
conditioned their generation on discrete and continuous information. 
Most recently, Chen et al.~\cite{ChenLZYZ019} extend the Seq2Seq framework
to product description generation and achieve better performance. 


%Neural models have recently achieved great empirical success on text generation tasks,
%such as machine translation~\cite{bahdanau2014neural,wu2016google}
%and summarization~\cite{chopra2016abstractive,nallapati2016abstractive}.
%Especially, convolutional sequence-to-sequence (CNN Seq2Seq) learning~\cite{gehring2017convolutional}
%has been successfully applied on various text generation applications
%~\cite{wu2019pay,narayan2018don}.

%\subsection{Pre-trained Language Model}
%TODO: talk about language models

%The most related work is E-commerce product title generation
%which aims to present products using titles to summarize product information.
%Existing works on title generation are mostly 
%selection-based and statistical-based.
%In the case of selection-based approaches, 
%Cillick~\cite{gillick2011elements} adapts the diversity-based ranking techniques in extractive summarization to title generation, pruning and selecting the title from a set of title candidates.
%Moreover, the systems~\cite{rosti2007improved,barrault2010many,mathur2017generating,suzuki2011automatic} can also learn how to pick the candidate title which is closest to the reference title.
%Souza et al.~\cite{de2018generating} statistics about the item titles and
%recombine frequent n-grams as generated titles using a stack-based search algorithm. 
%Another relevant work is the product description generation task in E-commerce.
%Most prior works are based on templates and traditional statistical frameworks
%~\cite{langkilde1998generation,wang2017statistical} which are insufficient to generate personalized and informative descriptions.
%%Product descriptions usually are longer than slogans, thus rely on 
%Recently, Chen et al.~\cite{DBLP:conf/kdd/ChenLZYZ019} extend the encoder-decoder framework
%to product description generation and achieve better performance. 
%
%%We formulate personalized slogan generation as a text-to-text generation task.
%%Owing to the tremendous success of
%Crafting a successful slogan for specific topic is tedious and highly time-consuming.
%Besides, previous related works are mainly use templates or statistical methods
%which are insufficient to achieve satisfying performance.
%On the other hand, convolutional sequence-to-sequence (Seq2Seq) learning has been achieved tremendous success in many text generation applications,
%such as machine translation~\cite{wang2017statistical,chen2018stable,song2018double,wu2019pay},
%abstractive document summarization~\cite{fan2017controllable,liu2018controlling,narayan2018don},
%language modeling~\cite{baevski2018adaptive},
%story generation~\cite{fan2018hierarchical,fan2019strategies}
%and dialogue~\cite{miller2017parlai,dinan2018wizard}.
%Owing to this success, we propose a \textbf{S}emantic kn\textbf{O}wledge enh\textbf{A}nced
%\textbf{P}ersonalized (or SOAP)
%slogan generation model for online topic-based recommendation.
%We extend the effective convolutional Seq2Seq framework~\cite{ott2019fairseq}
%to slogan generation.
