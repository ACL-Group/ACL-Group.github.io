\section{Introduction}
\label{sec:intro}
Deep neural models have shown to be effective in 
a large variety of natural language inference~(NLI)
tasks~\cite{bowman2015large,wang2018glue,mostafazadeh2016corpus,roemmele2011choice,zellers2018swag}. Many of these tasks
are discriminative by nature, such as predicting a class label or
an outcome given a textual context, as shown in the following example:

\begin{example}\label{exp:snli}
Natural language inference in SNLI dataset, with ground truth bolded.
\begin{description}
\item{Premise:} A swimmer playing in the surf watches a low flying airplane headed inland. 
\item{Hypothesis:} Someone is swimming in the sea.
\item{Label:} \textbf{a) Entailment.} b) Contradiction.  c) Neutral.
\end{description}
\end{example}

The number of candidate labels may vary. Humans solve such questions by
reasoning the logical connections between the premise and the hypothesis,
but previous work~\cite{naik2018stress,schuster2019towards} 
has found that some NLP models can solve the questions
fairly well by looking only at the hypothesis (or ``conclusion'' in some work)
in the datasets.
%It is widely speculated that this is because in many datasets, 
%the hypotheses are manually crafted and may contain artifacts that
%would be predictive of the correct answer. 
Thus it is widely speculated~\cite{gururangan2018annotation,sanchez2018behavior,poliak2018hypothesis} 
that statistical bias on linguistic features 
(e.g., sentiment, repetitive words and even shallow n-grams)
which have statistical correlation with specific labels 
in benchmark datasets can be predictive of the correct 
answer. We call such biases spurious \textit{cues} when
they appear both in the training and test datasets with a similar distribution
over the prediction values.  \figref{fig:cue_def} illustrates this
scenario. 


%Such ``hypothesis-only'' tests can identify problematic questions
%in the dataset if the question can be answered correctly without 
%the premise. While such a method to evaluate the quality of
%a dataset is theoretically sound, 
%it i) usually relies on training a heavy-weight model such as Bert, which
%is costly to evaluate, ii) does not provide explanation why the question is 
%a culprit, and iii) cannot be used to evaluate a model since a model that
%can make a correct prediction using only the hypothesis is not necessarily a
%bad model: it is just not given the complete data.  
%
%
%%argumentation~\cite{niven2019probing}, commonsense reasoning~\cite{}, 
%%reading comprehension~\cite{lai2017race}, question answering~\cite{talmor2019commonsenseqa} 
%%and dialogue analysis~\cite{lowe2015ubuntu}. 
%
%Inspired by black-box testings in software engineering, 
%CheckList~\cite{checklist2020acl} assesses the weakness of 
%models without the need to know the details of the model. It does so by
%providing additional stress test cases according to predefined 
%linguistic features. Unfortunately, to ensure the correctness of
%these additional cases, the templates must be carefully crafted
%with substantial restrictions, thus limiting the testing space and
%complicating the implementation. 
%Furthermore, with CheckList, you only get to know what the model 
%is incapable of doing but won't know what the model has 
%learned from the data.

\begin{figure}[th]
\centering
\includegraphics[width=0.8\columnwidth]{picture/cue_def.eps}
\caption{Example of a \textit{cue}.} 
\label{fig:cue_def}
\end{figure}

Several statistical metrics, such as frequency, conditional probablity and
LMI~\cite{schuster2019towards}, have been proposed to measure of 
severity of ``cueness'' of a linguistic feature, 
but they fell short of giving concrete evidence that
real-world NLP models actually exploit the cues thus computed. CheckList~\cite{checklist2020acl} assesses the weakness of 
models with a few predefined categories of features, but the assessment must rely on
manually crafted templates to generate test cases, which is restrictive and laborious.

In this paper, we propose two blackbox tests to assess if a natural language reasoning
model biases on a linguistic feature. \textit{Accuracy test} checks if see if there's
any substantial difference in prediction accuracy with test cases with a cue and the test cases
without a cue; \text{distribution test} checks if the the model makes biased prediction on cases
infected with cues in the same way as these cues are distributed in the training data. 
The above two tests are related but not equivalent and their
outcome complement and reinforce each other.
We are able to show on 10 datasets and 4 popular models, that cues computed by LMI from the 
training data are likely to be exploited by fastText~\cite{joulin2017bag}, ESIM~\cite{peters2018deep}, BERT~\cite{devlin2018bert}, RoBERTa~\cite{liu2019roberta} models on the datasets tested, 
hence confirming some of the claims made by previous work.

%which
%In this paper, we propose a light-weight framework called
%can evaluate both the {\em data set} and the {\em model} for
%discriminative NLI tasks {\em fully} automatically. 

%Once these cues are neutralized from the test data, 
%previously successful models may degrade substantially
%on it, suggesting that the model has taken advantage of 
%the biased feature and is hence not as robust as assumed 
%against with such cues.
%
%Inspired by the black-box testing in CheckList, 
%%whwith various of perturbation strategies, like typos, 
%we propose to generate test cases by just 
%filtering the original test data into new subsets according to 
%specific linquistic features, like word, NER, Negation, and sentiment, etc. 
%%in constructing the spurious cues. 
%We can show the cases distribution on each label indicates whether 
%the dataset is balance on this feature. In another word, we can 
%find the bias and cues intuitively. 
%Meanwhile,  we can prob what bias features the model really sensitive to.
%We even out the filtered test date on different labels. The even out version 
%can be used as a ``feature test'' to estimate whether a model consider 
%bias features in training data to make prediction.
%
%Our framework can be used to identify simple but effective biases and cues  
%in a broad range of multiple choice NL reasoning datasets.
%Though not all multiple choice questions in these dataset involve 
%all three components, i.e., a premise, a hypothesis and a label, 
%we will describe in~\secref{sec:formulation} how to normalize them into
%the standard form. 

%The relative size of hard part over the whole
%data indicates the quality of the dataset .
%\KZ{pls quickly finish the remaining part of intro. 
%The contributions part is critical.}
%Thus we can
%separate the original dataset into easy and hard part with 
%the bias score feature of cues
%any   dataset into easy and hard part
%and thus we can evaluate the ``biasness'' and ``quality'' of the dataset. 



%using simple and cost-effective linear classification models 
%on the simple cue features,
%which trained with unbalanced score of cues to several benchmark datasets across 
%various tasks and domains. 
%We evaluate the effectiveness of our methods with the 
%deviation between our results and random selection results. 
%Simultaneously, the test set can be divided into two parts: \textbf{easy} and \textbf{hard}. 
 %\textbf{balance} and \textbf{imbalance} are relative rather than absolute. 
%\KZ{Improving the neural models by splitting the training set.} 

%The training data can be separated into $n$ parts, test on one part and training 
%the model on the rest for each time. 
%The filtered part contains the instances 
%which can not be correctly chosen.
%The filtered data will be used to train a better model for target tasks. 

In summary, this paper makes the following contributions:
%\KZ{These contribs need to be revised right?}
\begin{itemize}
\item we propose two simple tests to quantitatively 
assess whether a given model has taken advantage of a
spurious cue when making predictions;

\item we evaluate the statisical bias issues comprehensively on
10 popular NLR datasets and 4 models and not only reaffirm
some of the findings from previous work but also discover some
new perspectives in these datasets and models;

\item we created an online demo called ICQ 
(``I-see-cue'')~\footnote{\url{http://anonymized.for.blind.review}}  to showcase
these results and invite users to evaluate their own datasets and models.
\end{itemize}
% (\secref{sec:result}).

%\item We filter the training data and get a better performance on the \textbf{hard} dataset.
%to get a high quality training dataset which
 %is possibly closer to the intended task. 







