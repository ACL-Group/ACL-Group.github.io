Review 1
Relevance to CIKM:	
4: (good)
Originality of the Work:	
4: (good)
Technical Soundness:	
3: (fair)
Quality of Presentation:	
3: (fair)
Impact of Ideas or Results:	
3: (fair)
Adequacy of Citations:	
4: (good)
Reproducibility of Methods:	
3: (fair)
Reviewer's confidence:	
3: (fair)
Overall Recommendation:	
-1: (Weak Reject)
Nominate for best paper:	
-
Comments to the Author(s):	this paper propose the framework which aims to evaluate the potential biases and cues in datasets and statistical cues. It could be regarded as novel to testing for bias in model. The proposed accuracy and label distribution are calculated as cueness metric test. However. it is not convinced to show the model weakness since it could be varied for different tasks and inputs. I would like to see more analysis to demonstrate this claim.
Summary:	The proposed testing for bias could be novelty but lacks of more experimental studies or theoretical analysis to demonstrate the model weaknesses.
Review 2
Relevance to CIKM:	
4: (good)
Originality of the Work:	
4: (good)
Technical Soundness:	
3: (fair)
Quality of Presentation:	
3: (fair)
Impact of Ideas or Results:	
3: (fair)
Adequacy of Citations:	
3: (fair)
Reproducibility of Methods:	
3: (fair)
Reviewer's confidence:	
3: (fair)
Overall Recommendation:	
-1: (Weak Reject)
Nominate for best paper:	
-
Comments to the Author(s):	To measure if a natural language reasoning model utilities certain biases in a dataset, this paper proposes two tests as well as experiments over several real-world models. 

Strengths: 
(+) This paper is generally clear and easy to follow. 
(+) Detailed and plentiful experiments are conducted and presented. 

Weaknesses: 
(-) First of all, the title of this paper mentions ``NLP Models’’, but in the paper content only the NLI models are discussed, which are limited and cannot represent all NLP models. To make things correct, the title should be modified. 
(-) In Section 2.2, some definitions seem strange, e.g., it says `` If a feature occurs more than once in a problem instance, it’s counted as one’’, is it means the highest frequency is 1? Besides, detailed definitions of some familiar concepts such as the conditional probability and PMI are unnecessary. 
(-) In Table 1, why the train/test set split ratio is so different among the datasets? Will this difference affect the evaluation results? Besides, the human rated accuracy is equally 80% over the first 3 datasets, is it correct? Why? 
(-) Minor: some typos, e.g., in abstract, ``many NLP models may learned from…’’, in introduction, ``Accuracy test checks if see if …’’.
Summary:	The paper is clear and easy to follow; however, the overall quality is limited by some problems that should be addressed.
Review 3
Relevance to CIKM:	
3: (fair)
Originality of the Work:	
4: (good)
Technical Soundness:	
4: (good)
Quality of Presentation:	
2: (poor)
Impact of Ideas or Results:	
4: (good)
Adequacy of Citations:	
4: (good)
Reproducibility of Methods:	
3: (fair)
Reviewer's confidence:	
4: (good)
Overall Recommendation:	
1: (Weak Accept)
Nominate for best paper:	
-
Comments to the Author(s):	It is now well-known that NLP models might pick up on spurious biases in the training data in order to make accurate inferences on test sets. Many of the methods developed until now to identify such spurious biases identify potential dataset biases, that are not necessarily picked up by the models in practice. This paper proposes a test framework that uncovers such spurious, statistical cues present in datasets, that models really pick upon to make inferences. Experiments allow to identify the most adapted types of metrics to use in the framework, and show that the framework identifies biases in models that seem to follow common knowledge about the tested models.

This work is fairly original as it follows the new works on NLP model testing, and on spurious bias identification, and relies on similar types of features and metrics. It provides a new contribution as it aims at making the connection between dataset biases and model biases. Yet, if extended, both the method and the results it provides could have a rather important impact on both the research community and developers on NLP models and datasets. It could serve both to benchmark datasets and models, and point out to typical biases to further investigate.

The quality of its presentation however sometimes prevents from precisely judging the technical soundness of the work. While the overall structure of the paper is clear and logical, and the discussions of results interesting, the ideas introduced are not always easy to follow due to issues in the logical flow of paragraphs and to the structure of certain sentences. Besides, certain concepts are used without having been introduced earlier, and similarly figures make use of notations that are not self-explainable and not explained in the legend. Due to these unclarities, it is sometimes hard to follow what is explained about the method. Finally, a few details are missing about the experimental setup, which are both hindering reproducibility and technical soundness.

Below, I detail certain of the issues mentioned above. 


Unclarities in the text:
• Mistakes were found all along the text, in terms of grammar, spelling, and syntax. E.g. “to measure of severity”, “Accuracy test checks if see if ”, “it’s” -> “its”. Certain sentences are rather convoluted and hard to parse, e.g. for the definitions of accuracy and distribution tests.
• The figures are not self-explainable, possibly due to unexplained notations. The explanations of the notations are scattered in the text, but, a figure should ideally be understandable without additional information. You might want to find a way to make the figures self-explainable without having to read the text, or simply explain the elements in the legend. E.g. in figure 2, what is f, S, etc.? In Fig. 1, what do (1), (2), (3) represent in the figure?
• Some mistakes confusing concepts hinder the direct understanding of the text. E.g. in Sec 2.2: is “l” really a feature or a label?
• Certain concepts are used without providing explanations about these in the text. Because of this, the reader has to try guess their meaning, which does not help the reading, and the understanding. E.g. In sec. 3.2, you use the term “MRR” for the first time. You might want to write it in full letters at least once before using its abbreviation. Also, what is a filtered test set? This is not a usual notion, so you need to explain it before using it for the first time in sec 2.3. I believe that it is in relation to the features that you test for, yet this does not appear explicitly. So, it is hard to understand the connection between the tests and the features?
• Other terms might refer to known concepts, but these terms used are unusual, again making the understanding of the text more complex. E.g. “flattening” the label distribution”: isn’t it that you are simply balancing your dataset? By copying random instances whose labels do not have as many instances associated to them compared to the majority label? If so, you might want to use the term “balancing” the dataset as it is more common. This is more of a question than an issue, but I wonder why in the introduction the authors chose to use the word “cue” and not “bias” which might be more common in the NLP literature. The definition of the spurious cues and the associated figure (Fig. 1) is not easy to parse at the beginning, but only after reading the text further. 


Besides such unclarities that hinder understanding, a few other issues are spotted. 
• Sometimes, the reasoning behind design choices is missing. E.g. could you explain your rationale for adding the items you mention in the list of linguistic cues? This seems central to your approach, so it would be interesting to have more background knowledge about the method used to establish it. That can also serve to understand how to extend it in the future. Similarly, you might want to explain the rationale behind each cueness metric.
• Please always add references when making statements, e.g. “This again is consistent with the community’s common perception of these popular models” -> can you link to a paper that reflects this common perception? 

As for the technical soundness, here are points that might help making it stronger.
• Sometimes, because of the above unclarities, it is simply hard to judge how strong is the method. For instance, it is unclear how the linguistic cues are used in the tests, and what are the filtered data in the tests. Do you think it is enough to use the test set as a test bed, or would you also need to apply your tests onto serving data to make sure of the representativeness of the data used for the actual, intended application of the models.
• In the experimental setup, some details are missing. For instance, how do you create the training and test sets, how large are they, do you also use a validation set? (these are important since your metrics rely on training/test data). Besides, how do you train the models? How did you choose the datasets? Are they known to be biased? For which kind of feature? Not having this information also make it hard to understand both the validity of the experiments, and hence also the validity/performance of the proposed approach. Are their constraints on the size of the datasets used in order to apply the tests?
• Still in the evaluation, it would be great to compare your approach to previous approaches that you mention in the text. It would help the reader to get a better understanding of the performance of your approach. Are you identifying different cues or the same ones? In case where you identify the same cues, is your approach more easily applicable?
• It could be interesting to study how useful your approach is in practice. It is often the case that models, even though they pick up on spurious biases, will make accurate inferences on test data. Yet, they might start to make mistakes on serving data that are often following a different distribution. Hence, I wonder how much your test framework allows to estimate which models will perform correctly/incorrectly on serving data that are different from the training/test set distribution. In that sense, it would be an experiment where you apply your different models trained on one dataset to the other 7 datasets of your experiments, and identify which ones have the highest/lowest accuracy. Is it related to the cues you identify?
• About the results: From table 2, it looks like many of the cues are simply dots, or words which do not bear strong meaning, e.g. “the”, “a”, “is”, “I”, etc. What do you interpret from that? Would it make sense to somehow remove these when processing your dataset before training a model, and repeat your tests in order to see whether more unusual terms might also appear?
• In the approach, you could try additional, more complex linguistic features, and see the extent to which they are relevant to different NLP tasks. For instance, I would imagine that hate speech classifiers might rely on different, spurious, types of features than the tasks you presented currently. Doing such qualitative analysis of the results of your test could also be an interesting contribution, or show the relevance of your contribution.
Summary:	While the paper makes an original and useful contribution to the field, it is sometimes hard to follow. It is hence sometimes hard to judge of the validity of the proposition and of the experiments. Besides, these experiments could be extended further in order to better show the performance of the proposed approach. That is why I believe a weak accept is relevant. If accepted, I would recommend the authors to clarify the list of points mentioned above, to make their contribution clearer and stronger.
Metareview
Metareview for paper 2969
Title:	Statistically Profiling Biases in Natural Language Reasoning Datasets and Models
Authors:	Shanshan Huang, Kenny Zhu and Haifeng Tang
Text:	Dear authors, 

Thank you for submitting your work to the short paper track of CIKM. 

While the reviewers found your work interesting and could see its merits, all three pointed at some aspects that need further clarification before acceptance can be warranted. Ultimately, none of the reviewers wanted to champion your paper. Hopefully, you will be able to address these comments before submitting your work again.
