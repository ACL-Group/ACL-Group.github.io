\section{Related Work}
% and incremental learning.
\subsection{Large-Scale Taxonomy classification}
% Original  
Standard hierarchical text classification tasks \cite{kowsari2017hdltex, sinha-etal-2018-hierarchical} are merely recognize the limited scope of text patterns, and stemming from this, the text classification with large hierarchy of classes attracts attention and has been studied with the evolving of LSHTC \cite{partalas2015lshtc} Challenge, which includes 
% a growing number of 
over 12000 categories. DiSMEC \cite{babbar2017dismec} devises one-vs-all linear classifiers 
% learned in large-scale distributed framework, 
with explicit control of model size. HyperIM \cite{chen2020hyperbolic} is proposed to encode both input and label text jointly in the hyperbolic space, then used a linear classifier to make decisions. Bonsai \cite{khandagale2020bonsai} develops a bunch of algorithms following tree-based approaches to obtain diverse and shallow trees. HiMatch~\cite{chen2021hierarchy} encodes the complex structure of the label hierarchy as well as the input text, to capture the text-label semantics relationship. These methods assume that label taxonomies are stable, neglecting that 
% an enormous amount of products emerges daily in multiple business lines and the 
taxonomy evolves gradually. 
\subsection{Product Categorization}
Product categorization is a hierarchical text classification problem assigning categories to product instances. 

Approaches in early times are centralized with text features and basic machine learning algorithms. \cite{ding2002goldenbullet} introduces KNN and Naive Bayes to the field of product categorization, while \cite{yu2012product} 
% compares different text preprocessing methods and 
conducts experiments using TF-IDF with an SVM classifier. Restricted by bag-of-words paradigm, these methods lack the ability to represent text with contextual semantics.

Neural network based methods prevail since 2013. \cite{ha2016large} proposes an end-to-end deep learning model composed of multiple RNNs and fully-connected layers, which exhibits a significant advantage over traditional bag-of-words approaches. 
\cite{das2016large} conducts a comparison between linear, CNN and gradient boosting models. 
Multi-CNN and multi-LSTM are applied in \cite{krishnan2019large} combining structured and unstructured attributes of products. 
\cite{chen2019fine} utilizes 
% character-level convolutional embedding layer and a spiral residual layer 
several convolutional approaches
for a better representation of words, and they further adopt literal matching between product content and category label texts to deal with new categories. However, they do not consider the more complicated category \textit{divide} situation. 

Recent studies follow the pretrain-finetune paradigm since the great success of BERT \cite{devlin-etal-2019-bert}. 
% Pretrained language models bring rich prior knowledge from self-supervised learning on large corpus, which benefits product categorization by enhancing classification model with pretrained contextual word embeddings. 
\cite{lee2020cbb} uses the CamemBERT pretrained on French corpus as text encoder in SIGIR 2020 
% Multi-modal Product Data Classification 
Challenge. \cite{yang2020bert} exploits BERT with a dynamic masking strategy and achieves the first place on 
% the Product Classification task in 
2020 Semantic Web Challenge.

Apart from end-to-end classification approaches, \cite{hasson2021category, li2018don} adopts  hierarchical 
% sequence-to-sequence generation 
Seq2seq
models for product categorization. Nonetheless, their models need to be re-trained whenever category taxonomy vocabulary changes.
\subsection{Retrieval-Rerank Framework}
In Question answering over knowledge bases (KBQA) domain, due to the large number of entities and relations in KB, \cite{wang-etal-2021-retrieval} leverages a \textit{retrieve-and-rerank} framework to narrow down the search space and hold the powerful Top-$k$ accuracy as well. Similarly, this ``\textit{candidate generation $\rightarrow$ ranking}'' pipeline is widely-applied in Recommendation System~\cite{liu2017related}, as the recommendation system has been indispensable in many industry areas, such as e-commerce products, news articles and movies. Different strategies and models are well-designed in two stages of the pipeline according to the previous literature. Besides, the proposed Sentence-BERT (SBERT)~\cite{reimers2019sentence} highlights the power of siamese network structures to reduce the effort of finding the most similar text pairs while maintaining the accuracy from BERT. 
\subsection{Incremental Learning}
Class incremental learning resolves the problem that the classes increase progressively in a stream, and the classifier should continuously learn the incoming classes while sustain the accuracy on the seen classes as well. iCaRL \cite{rebuffi2017icarl} is proposed to 
% dynamically maintain a set of exemplars for each class, which 
circumvent the catastrophic forgetting problem by storing the information of previous classes. 
% iCaRL enriches its class vocabulary with incoming new classes by partially updating the model, and use Nearest-Mean-of-Exemplars Classification algorithm to infer on the test samples. 
\cite{xu2019open} extends the incremental learning as an open-world learning problem, where a model in addition rejects unseen classes instead of assigning them into the seen class vocabulary. 
% They built a framework based on a ranker to retrieve class candidates, and a meta-classifier to decide whether to accept or reject the retrieved labels. 
However, in their open-world learning setting, other taxonomy evolving situations (like split and merge) and multiple taxonomies are not taken into consideration.