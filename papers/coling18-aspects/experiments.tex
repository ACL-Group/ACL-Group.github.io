\section{Experiments}
\label{sec:experiments}
We compare the ExtRA framework with a number of 
baseline methods on extracting aspect terms from user reviews. 
We first introduce the dataset and the competing models, then
show the quantitative evaluation as well as qualitative analysis for
different models.

\subsection{Dataset}
\label{sec:dataset}
We use the customer review corpora of 6 kinds of product and service
\footnote{The data is available from
\url{http://times.cs.uiuc.edu/~wang296/Data/} and 
\url{https://www.yelp.com/dataset}} collected from popular websites, 
including Amazon, TripAdvisor and Yelp. 
The number of hotel reviews \cite{Wang2011LearningOD} in the original
corpus is huge. 
Therefore, we randomly sample 20\% of the reviews to perform our experiments.
The statistics of the corpora are shown in~\tabref{table:dataset}.
\begin{table}[th]
\small
\centering
\vspace{-0.5cm}
\caption{Dataset statistics.} 
\label{table:dataset}
\vspace{-0.2cm}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Product type} & \textbf{Source} & \textbf{\#Reviews} \\ \hline \hline
hotel        & TripAdvisor & 3,155,765   \\\hline
mobile phone & Amazon & 185,980  \\\hline
mp3 player   & Amazon & 30,996   \\\hline
laptop       & Amazon & 40,744   \\\hline
cameras & Amazon & 471,113  \\\hline
restaurant   & Yelp & 269,000   \\\hline
\end{tabular}
\vspace{-0.2cm}
\end{table}

%\label{sec:evaldataset}
Existing published aspect extraction datasets~\cite{hu2004mining,popescu2007extracting,pavlopoulos2014aspect,ding2008holistic} 
include only fine-grained aspects from reviews,
which are not suitable for evaluating the performance of 
prominent aspects extraction. 
Therefore, we build a new evaluation dataset particularly 
for this task.
For each category, we ask 5 annotators who are proficient in English 
to give 5 aspect terms which they think are important to each product type.
In total, there are 25 aspect terms
for each product type (including duplicates).
%\KZ{We need to explain why the data set is for 5 aspect only, not 10, 15?}
Because our baseline models can only extract aspect words,
our annotated ground truths are all words by design.
For example, the one set of annotations for hotel is 
{\em room, price, location, service, utility}, and the annotations
for restaurant is
{\em location, price, food, service, cleanness}. \footnote{The 
complete labeled set of ExtRA are released at \url{https://www.dropbox.com/s/dh20nvcziqf67j0/eval-dataset.txt?dl=0}.
Source code and processed data will be released after the review period.}

%\begin{table}[th]
%	\small
%	\centering
%	\caption{Selected ground-truth labels.}
%	\label{table:labels}
%	\begin{tabular}{|c|l|}
%		\hline
%	Product type & Prominent aspects \\ \hline\hline
%		hotel
%		%			& price location room service bath staff \\\hline 
%		& room price location service utility \\ \hline
%		
%		camera
%		%			& battery image lens price appearance storage focus design mode memory carry operation \\\hline
%		& image lens battery memory carry \\ \hline
%		
%		restaurant
%		%			& food price location service environment cleans \\\hline
%		& location price food service cleanness \\ \hline
%	\end{tabular}
%\end{table}


%We aim to extract $K$ prominent aspects from review texts
%which are most important and representative
%for the given kind of products or services. 

\subsection{Baselines and ExtRA}
\label{sec:base}
We introduce three topic modeling based baselines for the task.
These are \textbf{LDA}~\cite{Blei2003LatentDA}, 
\textbf{BTM}~\cite{cheng2014btm} and  
\textbf{MG-LDA}~\cite{titov2008modeling}. MG-LDA is a strong
baseline particularly designed for aspect extraction.
We treat each review as a document and perform those models
to extract $K$ topics.
% Each topic is expected to model an 
%prominent aspect for the given product or service.
Then, we select most probable word in 
each topic as our extracted aspect terms. 
To prevent extracting the same aspects ($w$) from different topics, 
we only keep $w$ for the topic $t$ with the highest 
probability $p(w|t)$ value, then re-select aspects for the other 
topics until we get $K$ different aspects. 
For fair comparison among different models, the number of 
target aspects $K$ is set as 5. The hyper-parameter of 
MG-LDA (global topics) is set to 30 with fine-tuning.

Another syntactic rule-based baseline model 
\textbf{AmodExt} is from the first stage of our framework. 
After extracting the aspect candidates using
\emph{amod-rule} in \secref{sec:candidate}, 
we sort the aspect candidates by 
their counts of extracted occurrences. 
Then select the top $K$ 
candidates as the prominent aspects.

\textbf{ABAE} is a neural based model that can
to infer $K$ aspect types. 
Each \emph{aspect type} is a ranked list of representative words.
To generate $K$ prominent aspects, 
we first infer $K$ aspect types using \emph{ABAE}, 
then select the most representative word from each
aspect type. 
%We evaluate our framework as well as 
%the above baselines on the evaluation dataset.

%\paragraph{ExtRA Settings}
%\label{ourmodels}
For \textbf{ExtRA}, in the taxonomy construction stage, 
we use a two-stage K-means clustering method for synset matching task, 
and the cluster number
is auto-tuned using silhouette score~\cite{rousseeuw1987silhouettes}.
We use SkipGram~\cite{miko} model to train the embeddings
on review texts for k-means clustering. 
The dimension of the embeddings is set as 100. 
In the aspect ranking stage, 
we empirically set the teleport probability $\alpha$
as $0.5$ which indicates that the expected walk-length
from the seeds is $\frac{1}{\alpha}=2$.
%The comparison of quantitative results are shown in \tabref{table:comparison}.}

\input{eval}
