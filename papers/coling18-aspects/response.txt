R1:
The evaluation dataset is annotated for the reviews data listed in Table 1.
We ask five annotators to provide five prominent aspects for each product/service type.
The reviewer may misunderstand the evaluation metric. 
All the annotations from 5 annotators (25 terms in total) are considered when computing the 
hard/soft accurracy shown in Table 3.

R2:
Thanks for the careful reading and really detailed comments.
The aspect ranking algorithm is inspired by the observation that the hubs that have many supporters in the taxonomy tend to be more important. The instruction of the annotation is to pursue both maximum coverage and minimum overlapping for the annotated prominent aspects. We will restate a more detailed annotating instruction in the revised version.

R3:
It is a good idea to use more sophisticated phrase mining techniques for the aspects candidate extraction. However, for our problem, the prominent aspects are always modified by some adjectives.
The paper focus on the prominent aspects extraction problem and evaluate the performance on such task directly.
We will definitely fix the gramma and representation issues in the revised version.
