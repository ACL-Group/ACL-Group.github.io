\section{Task 1: Mining cross-cultural differences of named entities}
\label{sec:mcdne}

%We first explain how we obtain the ground truth from human annotators, then present several baseline methods to this problem, and finally 
%discuss our experiment results in detail.
\textbf{Task definition:}
	This task is to discover and quantify cross-cultural differences of concerns towards named entities.  
	Specifically, the input to this task is a list of 700 named entities of interest, two monolingual social media corpora; the output is the scores for the 700 entities indicating the cross-cultural differences of the concerns towards them between two corpora. 
	We map the labels to numerical scores from 1 to 5 and use the average scores from the annotators as the ground truth {scores}. 
%Thus, 

\subsection{Ground Truth Scores}
\label{sec:mcdne_truth}
\cite{harris1954distributional} states that the meaning of 
words is evidenced by the contexts they occur with. 
Likewise, we assume that the cultural properties of an entity 
can be captured by the terms they always co-occur within a large social media corpus. 
Thus, for each named entity, we present human annotators with two lists of 20 most co-occurred terms within Twitter and Weibo corpus respectively. 
We randomly select 700 named entities for annotators to label.
%,which
%are the most frequently mentioned both in Twitter and Weibo. 
Annotators are instructed to rate the topic-relatedness between the 
two word lists using one of following labels: ``very different'', 
``different'', ``hard to say'',  ``similar'' and 
``very similar''. We utilize this way for efficiency and avoiding subjectivity. 
As the word lists presented come from social media messages, the social and cultural elements are already embedded in their chance of occurrence.
\footnote{All four annotators are native Chinese speakers 
but have an excellent command of English. Two of them lived in the US extensively. 
Annotators are educated with many 
selected examples and thus have shared the understanding of the
labels. The inter-annotator agreement is 0.672 by Cohen's kappa coefficient, 
suggesting a substantial correlation.} 



 
\begin{table*}[t]
	\scriptsize
	\centering
	\caption{{Selected culturally different named entities, with Twitter and Weibo's trending topics manually summarized}\vspace{-15pt}}
	\begin{tabular}{L{1.5cm} L{5cm} L{8cm}}
		\textbf{Entity} & \textbf{Twitter topics} & \textbf{Weibo topics}
		\\ \hline
		Maldives & coup, president Nasheed quit, political crisis & holiday, travel, honeymoon, paradise, beach \\ \hline
		Nagoya & tour, concert, travel, attractive, Osaka & Mayor Takashi Kawamura, Nanjing Massacre, denial of history\\  \hline
				Quebec & Conservative Party, Liberal Party, politicians, prime minister, power failure & travel, autumn, maples, study abroad, immigration, independence   \\ \hline
				Philippines & gunman attack, police, quake, tsunami & South China Sea, sovereignty dispute, confrontation, protest  \\ \hline
		Yao Ming & NBA, Chinese, good player, Asian  & patriotism, collective values, Jeremy Lin, Liu Xiang, Chinese Law maker, gold medal superstar   \\ \hline USC & college football, baseball, Stanford, Alabama, win, lose & top destination for overseas education, 
Chinese student murdered, scholars, economics, Sino American politics \\ \hline
	\end{tabular}
\vspace{-10pt}
	\label{tab:mcdne_res_4}
\end{table*}
%\vspace{-10pt}
\subsection{Baseline and Our Methods} 
We propose eight benchmark methods for this novel task. 
The first three are \emph{distribution}-based, while the next two 
are \emph{transformation}-based. 
The last three, namely MultiCCA,
MultiCluster and Duong are three popular \emph{bilingual word representation models} for general use.   
\textbf{Distribution-based} methods compute cross-lingual relatedness between two lists of the words surrounding the input English and Chinese terms respectively ($\mathcal{L}_E$ and $\mathcal{L}_C$). 
The $\mathcal{L}_E$  and $\mathcal{L}_C$  in the BL-JS and WN-WUP methods are the same as the lists that annotators judge.
\textbf{Transformation-based} methods compute the vector representation 
in English and Chinese corpus respectively, and
then train a transformation.
\textbf{Bilingual word representations based} methods use the existing state-of-the-art models and then compute the similarities between two bilingual word vectors as $clsim$.

\noindent
\textbf{BL-JS}: \textit{Bilingual Lexicon Jaccard Similarity.}
BL-JS utilizes the bilingual lexicon to translate $\mathcal{L}_E$  to a Chinese word list 
$\mathcal{L}_E^*$ as a medium and then calculates the Jaccard Similarity between 
$\mathcal{L}_E^*$ and $\mathcal{L}_C$ as $J_{EC}$. Similarly, we can compute $J_{CE}$. 
Finally, we compute $(J_{EC}+J_{CE})/{2}$ as the cross-cultural similarity 
of this given named entity.

\noindent
\textbf{E-BL-JS}: \textit {Embedding-based Jaccard Similarity.} 
E-BL-JS differs from BL-JS in that it instead compares two list of words in the same way that gathered from the
rankings of word embedding similarities between the name of entities and all English words 
and Chinese words respectively. 

\noindent
\textbf{WN-WUP}: \textit{WordNet Wu-Palmer Similarity.} 
WN-WUP uses Open Multilingual 
Wordnet~\cite{wang2013building} to calculate the average 
similarities between $\mathcal{L}_E$ and $\mathcal{L}_C$ over all English-Chinese word pairs in the $\mathcal{L}_E$ and $\mathcal{L}_C$.

\noindent\textbf{LTrans}: \textit {Linear Transformation.}
We follow the steps in Mikolov et al.~\shortcite{Mikolov:2013tp} 
to train a transformation matrix between \textit{EnVec} and \textit{CnVec}, 
using 3000 translation pairs with confidence of 1.0 in the bilingual lexicon. 
Given a named entity, this solution simply calculates cosine similarity 
between the vector of its English name and the \textit{transformed} vector 
of its Chinese name. 

\noindent\textbf{BLex}: \textit {Bilingual Lexicon Space.}
This baseline is similar to \textit{SocVec} but it does not 
utilize any social word vocabularies and  uses bilingual lexicon entries as pivots instead.
 
\noindent\textbf{MultiCCA}
\cite{ammar2016massively}. This method takes two mono-lingual word 
embeddings and a bilingual lexicon as input and develop a bilingual word 
representations.  We use both the Microsoft bilingual lexicon (BL)
and the bilingual social lexicon (BSL) we constructed as the bilingual lexicon
to compare their effectiveness. Dimensionality is tuned from 
$\{50,100,150,200\}$ in all methods.

\noindent\textbf{MultiCluster} \cite{ammar2016massively}.
This method requires re-training the bilingual word embeddings from the two mono-lingual corpora with a bilingual lexicon. We also use our BSL as 
an additional test (MultiCluster-BSL). 

\noindent\textbf{Duong}
\cite{duong2016learning}. 
Similar to MultiCluster, this method retrains the embeddings from 
mono-lingual corpora with an EM style training algorithm. 

\noindent\textbf{Our SocVec-based methods.}  With our constructed \textit{\socvec},~given a named entity with its English and Chinese names, we can simply compute the similarity between their \textit{SocVec}s as its cross-cultural difference score. 
Note that our method is more efficient because it requires no re-training on original corpora. Thus, we do not have to obtain new bilingual word embeddings by re-training on the corpora, every time when we have a updated BL or BSL, which is very helpful for efficient examining different BSLs.
%Our method is only based on monolingual word embeddings and B(S)L, and thus does not need re-training on the corpora at all. 


\subsection{Experimental Results}

For qualitative evaluation, \tabref{tab:mcdne_res_4} shows some of 
the most culturally different entities mined by our method. 
The hot and trending topics on Twitter and Weibo are 
manually summarized to help explain the cultural difference. 
The perception of these entities diverges widely between English and
Chinese social networks, thus suggesting
significant cross-cultural differences.
%\footnote{Admittedly, some cultural differences are time-specific and platform-specific.
%However, we argue that the two microblog platforms are among the most popular ones in their languages, and therefore the common cultural differences and the temporal variations of cultural differences can be valuable and beneficial for social studies as well.}
%\footnote{Separating temporal influences from the social media corpora can be another interesting future research topic about our proposed task.}


\begin{table}[t]
	\scriptsize
	\centering
	\caption{{Comparison of Different Methods}}
	\begin{tabular}{c|c|c|c|c|c|c|c}
		\textbf{Method} & \textbf{Spearman} & \textbf{Pearson}  & \textbf{MAP} &\textbf{Method} & \textbf{Spearman} & \textbf{Pearson}  & \textbf{MAP} \\ \hline
		BL-JS& 0.276 & 0.265 & 0.644 & MultiCluster-BSL(dim=100)&0.391&0.425&0.713  \\ 
		WN-WUP  & 0.335 & 0.349 & 0.677 & Duong-BL(dim=100)&0.618&0.627&0.785 \\ 
		E-BL-JS & 0.221 & 0.210  & 0.571 & Duong-BSL(dim=100)&0.625&0.631&0.791 \\ 
		LTrans& 0.366 & 0.385  & 0.644 &SocVec:opn& 0.668 & 0.662   & \textbf{0.834}  \\
		BLex& 0.596 & 0.595  & 0.765 & SocVec:all& \textbf{0.676} & \textbf{0.671}  & \textbf{0.834} \\
MultiCCA-BL(dim=100)&0.325&0.343&0.651 & SocVec:noun & 0.564 & 0.562 & 0.756\\  
MultiCCA-BSL(dim=150)&0.357&0.376&0.671 & SocVec:verb & 0.615 & 0.618 & 0.779\\ 
MultiCluster-BL(dim=100)&0.365&0.388&0.693&SocVec:adj. & 0.636 & 0.639 & 0.800\\ 
	\end{tabular}
	\label{tab:mcdne_res_1}
\end{table}

\begin{table}[t]
	\centering
\begin{subtable}{0.45\linewidth}
	\centering
	\scriptsize
	\begin{tabular}{l|c|c|c}
		\textbf{Similarity} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline
		PCorr. & 0.631 & 0.625 & 0.806\\ 
		L1 + M & 0.666 & 0.656 & 0.824 \\  
		Cos & \textbf{0.676} & 0.669 & \textbf{0.834} \\ 
		L2 + E & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
	\caption{Evaluation of Different Similarity Functions}
\label{tab:mcdne_res_2}
\end{subtable}
~~
\begin{subtable}{0.45\linewidth}
	\centering
	\scriptsize
	\begin{tabular}{c|c|c|c}
		\textbf{Generator} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\  \hline
		Max. & 0.413 & 0.401 & 0.726\\ 
		Avg. & 0.667 & 0.625 & 0.831\\ 
		W.Avg. & 0.671 & 0.660 & 0.832 \\  
		Top & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
	\caption{Evaluation of Different Pseudo-word Generators}
\label{tab:mcdne_res_3}
\end{subtable}
\caption{Evaluation of different choices in parameters}
\end{table}

In~\tabref{tab:mcdne_res_1}, we evaluate the benchmark methods and our approach with three metrics: Spearman and 
Pearson, where correlation is computed between truth averaged scores and computed cultural difference scores from different methods; Mean Average Precision (MAP), which converts averaged score as a label, by setting 3.0 as the threshold. 
The \textit{BSL} of \textit{SocVec:opn} uses only OpinionFinder as English socio-linguistic vocabulary, while \textit{SocVec:all} uses the union of Empath and OpinionFinder vocabularies. 
The following parameters are used for \textit{SocVec} methods: 5-word context window,
150 dimensions monolingual word vectors, cosine similarity as the \textit{sim} function, 
and ``\textit{Top}'' as the pseudo-word generator.

\textbf{Lexicon Ablation Test.} To show the effectiveness of social-linguistic vocabulary versus other type
of words as the bridge between the two cultures, we also compare the
results using sets of nouns, verbs and adjectives within the 
same \textit{SocVec} framework.
All vocabularies under comparison are of similar sizes 
(around 5000), which also indicates that the improvement of our method 
is not just the result of sparsity.
Results show that \textit{SocVec} models, and in particular, the \textit{SocVec} model using the social words as cross-lingual media, performs the best. 

\textbf{Similarity Options.} We also evaluate the effectiveness of four different similarity options in 
\textit{\socvec}, namely, Pearson Correlation Coefficient 
(\textit{PCorr}.), L1-normalized Manhattan distance (\textit{L1+M}), 
Cosine Similarity (\textit{Cos}) and  L2-normalized Euclidean distance (\textit{L2+E}).
%It is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking.
From~\tabref{tab:mcdne_res_2}, we conclude that among these four options, \textit{Cos} and \textit{L2+E} perform the best. 
%Although it is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking, we can find that 

\textbf{Pseudo-word Generator.} 
\tabref{tab:mcdne_res_3} shows effect of using four pseudo-word generator functions, from which we can infer that ``\textit{Top}'' generator function performs best for 
it reduces the noise brought by the less probable translation pairs. 
