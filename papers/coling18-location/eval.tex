\section {Evaluation}
\label{sec:eval}

In this section, we first show our experimental setup, then evaluate the performance of our proposed baseline methods and the state-of-the-art general classification method on the sentence-level \lnear~relation classification task. 
Then, we will evaluate the quality of the new \lnear~triples we extracted with our proposed method.

\subsection{Experimental Setup}
\label{sec:experiment}
\noindent
\textbf{State-of-the-art General Relation Classification}\\
The state-of-the-art relation classification model 
(DRNN)~\cite{xu2016improved} leverages
a four-channel input, stacked deep recurrent neural network model 
to represent the original sentence with original words, POS-tags, 
WordNet Supersenses and grammatical relations/dependency roles separately. 
%A major differences between it and our proposed LSTM-based methods is that 
%i) it only uses the information of the shortest dependency path between the two objects; 
%ii) 
%it does not take out the two objects out of the original sentence;  \BL{frank work on this plz}

\noindent
\textbf{Hyperparameters Tuning Details}\\
For our proposed feature-based baseline methods, 
we trained our word embeddings on the Gutenberg corpus with window 
size of 5 and dimensionality of 100. 
The pre-trained GloVe~\cite{pennington2014glove} word embeddings 
is also 100-dimensional.
We use grid searching method to tuning the hyperparameters of the 
SVM classifier. The candidate parameters are shown in~\tabref{tab:grid}, 
we do 5-fold cross-validation under each candidate situation and 
choose the one with the highest precision.

\begin{table}[th]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{kernel} & \textbf{C} & \textbf{gamma} \\ \hline \hline
		linear & 1,10,{100},1000 & N/A  \\\hline
		\textbf{rbf} & 1,10,\textbf{100},1000 & $10^{-4}$,{$\mathbf{10^{-3}}$},$10^{-2}$ \\\hline
	\end{tabular}
	\caption{Candidate Hyperparameters of the SVM Model for Grid Searching; The bold settings are the best.}
	\label{tab:grid}
\end{table}

As for our LSTM-based methods, we initialize the weight of embedding 
layer for input tokens and positions with uniform distribution, 
except for the lemma words, which are initialized with the weight from 
the pre-trained embeddings. The dimensionality of input token embedding layer and position embedding layer are 100 and 5 respectively. For the original two physical object words, we also use the widely-used 100-dimension pre-trained GloVe word embeddings, the same as aforementioned SVM setting. 
We set both the input dropout rate as well as the recurrent state update dropout probability as 0.5 and the number of training epochs as 40, use the validation accuracy as monitor metric and early stop with the patience of 5 epochs.

We used the released code of the DRNN baseline, and due to smaller dataset we have, we manually tuned the learning rate to 0.2 down from 0.3 for best possible results.

\begin{table}[t]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		& Random       & Majority    & SVM  & SVM(-BW)    & SVM(-BPW) & SVM(-BAP)   & SVM(-GF)    \\ \hline
		Acc.  & 0.500        & 0.551       & 0.584                                & 0.577          & 0.556        & 0.563          & \textbf{0.605} \\ \hline
		P & 0.551        & 0.551       & 0.606                                & 0.579          & 0.567        & 0.573          & \textbf{0.616} \\ \hline
		R    & 0.500        & 1.000       & 0.702                                & 0.675          & 0.681        & \textbf{0.811} & 0.751          \\ \hline
		F1        & 0.524        & 0.710       & 0.650                                & 0.623          & 0.619        & 0.672          & \textbf{0.677} \\ \hline \hline
		& SVM(-SDP) & SVM(-SS) & DRNN & LSTM+Word    & LSTM+POS   & LSTM+Norm    &                \\ \hline
		Acc.  & 0.579        & 0.584       & 0.635                                & 0.637          & 0.641        & \textbf{0.653} &                \\ \hline
		P & 0.597        & 0.605       & \textbf{0.658}                       & 0.635          & 0.650        & 0.654          &                \\ \hline
		R    & 0.728        & 0.708       & 0.702                                & \textbf{0.800} & 0.751        & 0.784          &                \\ \hline
		F1        & 0.656        & 0.652       & 0.679                                & 0.708          & 0.697        & \textbf{0.713} &                \\ \hline
	\end{tabular}
	\caption{Performance of baselines on co-location classification task with ablation. (Acc.=Accuracy, P=Precision, R=Recall, ``-'' means without certain feature)}
	\label{tab:aprf}
\end{table}
\subsection{Sentence-level \lnear\ Relation Classification}
We evaluate the proposed methods against the state-of-the-art general domain relation 
classification model (DRNN)~\cite{Xu2016ImprovedRC}. 
The results are shown in~\tabref{tab:aprf}.
For feature-based SVM, we do feature ablation on each of the 6 feature types (\secref{sec:feature}). For LSTM-based model, we experiment on variants of input sequence of original sentence.
``LSTM+Word'' uses the original words as the input tokens, while ``LSTM+POS'' uses just the POS tag sequence as the input tokens. ``LSTM+Norm'' uses the tokens of sequence after sentence normalization. \footnote{Besides, we added two naive baselines: ``Random'' baseline
	classifies the instances into two classes with equal probability; 
	``Majority'' baseline considers all the instances to be positive.}

From the results, we find that the SVM model without the Global Features performs best, which indicates that bag-of-word features benefit more in shortest dependency paths than on the whole sentence.
We find that DRNN performs best (0.658) on precision but not significantly higher than LSTM+Norm (0.654). 
The experiment also shows that LSTM+Word enjoys the highest recall score.
In terms of the overall performance, LSTM+Norm is the best one. 
One possible reason is that our proposed the normalization representation reduces input sequences' token vocabulary size, while preserving important syntactical and semantic information. While LSTM+POS also reduces the vocabulary size, it loses too much information. 

Another reason is that~\lnear\ relation are described in sentence mostly with the prepositions/adverbs decorating them, which are the descendants of object word in the dependency tree, other than words merely along the shortest dependency path. 
Thus, DRNN cannot capture the information from the words belonging to the descendants of the two object words in the tree, while this information is captured by LSTM+Norm. For the rest of the experiments, we will use LSTM+Norm as the classifier of our choice.



\subsection{\lnear\ Relation Extraction}
Once we have classified the sentences using LSTM+Norm, we can extract \lnear\
relation using the four scoring functions in \secref{sec:mine}.
We first present the quantitative results. 
We use each of the scoring functions to 
rank the 500 commonsense \lnear\ object pairs described in \secref{sec:mine}. 
\tabref{tab:3m} shows the ranking results using
Mean Average Precision (MAP) and Precision at $K$ as metric. 
Accumulative scores ($f_1$ and $f_3$) generally do better.
%is always better when it concerns $m$, the number of the sentences mentioning \textless$e_i$,$e_j$\textgreater.
%To evaluate the performance of the scoring result 
\begin{table}[t]
	\centering
	\begin{tabular}{|cccccc|}
		\hline
		${f}$	& {MAP} & {P@50} & {P@100}  &  {P@200}& {P@300}\\ \hline
		$f_0$ & 0.42 & 0.40 & 0.44 & 0.42 & 0.38 \\ \hline
		$f_1$	& 0.58  & {\bf 0.70} & 0.60& 0.53 & {\bf 0.44}\\\hline
		$f_2$	& 0.48 & 0.56 & 0.52  & 0.49 & 0.42\\\hline
		$f_3$	& {\bf 0.59} & 0.68& {\bf 0.63} & {\bf 0.55} & {\bf 0.44}\\\hline
		$f_4$	& 0.56 & 0.40 & 0.48 & 0.50 & 0.42\\\hline
	\end{tabular}
	\caption{Ranking performances of the 5 scoring methods.\vspace{-10pt}}
	\label{tab:3m}
\end{table} 
\begin{table}[th!]
	\centering
	\begin{tabular}{|ccc|}
		\hline
		(door, room)  & (boy, girl)     & (cup, tea)      \\
		(ship, sea)   & (house, garden) & (arm, leg)      \\
		(fire, wood)  & (house, fire)   & (horse, saddle) \\
		(fire, smoke) & (door, hall)    & (door, street)  \\
		(book, table) & (fruit, tree)   & (table, chair)  \\ \hline
	\end{tabular}
	\caption{Top object pairs returned by best performing scoring function $f_3$}
	\label{tbl:toppairs}
\end{table} 

Qualitatively, we show 15 object pairs with some of the highest $f_3$ scores
in \tabref{tbl:toppairs}.
Setting a threshold of 40.0 for $f_3$, which is the minimum non-zero
$f_3$ score for all true object pairs in the \lnear\ object pairs 
data set (500 pairs), we obtain a total of 2,067 \lnear\ relations, with
a precision of 68\% by human inspection.

