\section{Experiments}
\label{experiments}

In this section, we first present evaluation metrics and experiment settings used in our experiments. Then we describe the results and analysis of three baseline approaches.

\subsection{Evaluation Metrics}
\paragraph{Automatic Evaluation.} 
Question classification and clarification trigger detection mainly solve classification problems. Accuracy is used to evaluate the TextCNN and TextRNN models. In the experiment of BLSTM-QuD, we consider three metrics to evaluate its effectiveness: F1-micro, F1-macro, and accuracy. In the experiment of BERT-QuD, we consider two metrics to evaluate its effectiveness: F1 and accuracy. Our choice of measures is motivated by the importance of achieving high accuracy for this task. In the question generation experiment, BLEU and Perplexity are used. BLEU \cite{DBLP:conf/acl/PapineniRWZ02} is widely used evaluation metric in statistical machine translation \cite{DBLP:journals/corr/ChenFLVGDZ15}. Similar to statistical machine translation, where a phrase in the source language is mapped to a phrase in the target language, in this task a context is mapped to a natural language question. Perlexity is also a commonly used metric of language model performance \cite{DBLP:journals/corr/abs-2006-04666}. We therefore believe that BLEU and Perplexity constitute reasonable performance metrics for evaluating the generated questions.

\paragraph{Human Evaluation.} 
We perform human evaluation in the clarification question generation experiment. 
We randomly select 500 clarification questions (along with their contexts)
generated by each method and assign them to two judges. 
Score on a scale of ``0'' to ``3''. A score of ``0'' indicates that the resulting sentence is 
not a question at all. ``1'' means that the generated sentence is a question but does not fit the
context at all. ``2'' means that the generated sentence is a question and is somewhat relevant
to the context. ``3'' means that the generated sentence is a question with contextual relevance and accurate expression.

\subsection{Experiment Settings}
In the question classification, we use the Gensim tool and select Skip- gram algorithm to train the 100 dimensional word2vec model on the dataset we crawl, and finally obtain the word vector representation of 9086 Chinese characters (including Chinese characters, numbers, punctuation, etc.), then apply it into the TextRNN and TextCNN. In the BLSTM-QuD, due to the the more complex 
dataset we use, the Chinese word vectors we use have been trained through 
a variety of different representations (dense and sparse), 
context features (word, ngram, characters, and more) and the 
corpus of Wikipedia\_zh. Finally, a 300-dimensional Word2vec model and 
a word vector representation of $\sim$350K Chinese characters are obtained. 
Other settings are the same as the TextRNN. In the BERT-QuD, 
we adopt the BERT-Base pre-trained model provided by Google. 
BERT-Base has a total of 12 layers, and the hidden layer is 768 dimensions. 
A 12-head model is adopted, with a total of 110M parameters. 
In the experiments using the six methods of Window-top, Window-last-q, Window-1, Window-3, Window-5 and Win-10, 280K, 280K, 300K, 280K, 240K and 240K datasets are used, respectively. Among them, 20K datasets are used as the test set and 10K pieces are used as the valid set, and the remaining number of datasets are used as the training set. In the training set, the positive and negtive datasets are equally divided. In the OpenNMT-py framework of the question generation model, our network parameters are as follows: Encoder and Decoder: Transformer; Encoder type: BRNN; RNN type:GRU; Word vector size: 200; Layers for encoder and decoder: 2; Attention heads: 16; RNN size: 200; Batch size: 8. 

\subsection{Results and Analysis}
\paragraph{Question Classification.} 
The accuracy on TextCNN and TextRNN is 89.1$\%$ and 96$\%$, respectively. The \tabref{tab:example4} shows two failed examples of classification using TextCNN. These two sentences should be divided into ``Q'', but TextCNN classifies them into ``A''. When we classify, if there are two sentences, such as $\textcircled{2}$, then if only one of them is a clarification question, the whole sentence is divided into ``Q''. Perhaps in cases where the stopping words are not obvious $\left( \textcircled{1} \right)$ or where there is a non-clarifying question after the clarification question$\left( \textcircled{2} \right)$, TextCNN fails to classify. In the following experiment, we use TextRNN to classify the question.

\begin{CJK}{UTF8}{gbsn}
\begin{table}
\small
\centering
\begin{tabular}[t]{cc}
\toprule
\textcircled{1}&\makecell[l]{说你身上有单纯疱疹病毒？\\(Do you have herpes simplex virus?)}\\
\hline
\textcircled{2}&\makecell[l]{是不是尖锐湿疣？你要发照片过来看。\\(Is it acuteness wet wart? You have to send a picture to look at it.)}\\
\bottomrule
\end{tabular}
\caption{Two failed examples of TextCNN.}
\label{tab:example4}
\end{table}
\end{CJK}

\paragraph{Clarification Trigger Detection.}
We apply the datasets from the six methods of Window-top, Window-last-q, Window-1, Window-3, Window-5 and Window-10 to the two classification models of BLSTM-QuD and BERT-QuD respectively. The results are shown in \tabref{tab:CTD} .

\begin{table*}[th]
\centering
\small
\begin{tabular}{cccccc} 
\toprule
\multirow{2}*{Context}&\multicolumn{3}{c}{BLSTM-QuD}&\multicolumn{2}{c}{BERT-QuD}\\
\cmidrule(r){2-4} \cmidrule(r){5-6}
&F1-macro&F1-micro&Accuracy&F1&Accuracy\\ 
\hline
Window-top&0.7151&0.7157&0.6631&0.6963&0.6787\\
Window-last-q&\textbf{0.7423}&\textbf{0.7456}&\textbf{0.6928}&\textbf{0.7538}&\textbf{0.7496}\\
Window-1&0.7041&0.7041&0.6671&0.7054&0.7090\\
Window-3&0.7028&0.7031&0.6596&0.7187&0.7122\\
Window-5&0.6963&0.6964&0.6448&0.7253&0.7260\\
Window-10&0.6776&0.6777&0.6107&0.6714&0.6692\\
\bottomrule
\end{tabular}
\caption{Results of Clarification Trigger Detection.}
\label{tab:CTD}
\end{table*}

From the above experimental data, using the BERT-QuD method is more accurate than using the BLSTM-QuD method. It shows that BERT pre-trained language model can better represent the semantic information of words than the traditional word vector representation. The word vector generated by BERT is context-dependent. Using BERT instead of BLSTM as an encoder can provide deeper levels and better parallelism. Besides, linear BERT is more free from the influence of mask tokens than BLSTM, and the weight of mask tokens can be reduced by self-attention. However, BLSTM is similar to the black box model, so it is difficult to determine its internal processing mode for mask tokens. Second, the accuracy of BERT-QuD on the method Window-last is higher than that of other methods. This suggests that it is easier for the expert $E$ in the consultantion dialogue to ask a clarification question based on the previous clarification question to the current contents.

\paragraph{Clarification Question Generation.}
We use the six methods to get the Q$\&$A pairs input into the OpenNMT-py model for training. BLEU and Perplexity are used to evaluate the language generation. In addition, for dialogue generation, the answer with the highest prediction probability is not the ultimate goal. Only the ability to generate contextual relevance and achieve semantic relevance should be the real goal of dialogue generation. Therefore, we use the method of manual evaluation to evaluate the generated clarification questions, and all the results are shown in \tabref{tab:CQG}.

\begin{table*}[!htbp]
\small
\centering
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}*{Context}&\multicolumn{4}{c}{Automatic Evaluation}&\multicolumn{4}{c}{Human Evaluation}\\
\cmidrule(r){2-5} \cmidrule(r){6-10}
&BLEU-1&BLEU-2&BLEU-4&Perplexity&``0''($\%$)&``1''($\%$)&``2''($\%$)&``3''($\%$)&Average\\
\hline
Window-top&\textbf{0.3600}&\textbf{0.2545}&\textbf{0.1117}&\textbf{1.5697}&0.4&19.2&55.6&24.8&\textbf{0.5120}\\ 
Window-last-q&0.3104&0.2090&0.0536&1.8643&0.8&22.6&\textbf{49.8}&\textbf{26.8}&0.5065\\ 
Window-1&0.0620&0.0079&0.0018&1.8248&3.6&\textbf{48.6}&35.8&12&0.3905\\
Window-3&0.2875&0.2048&0.0570&1.8854&0.6&31&45.4&23&0.4770\\ 
Window-5&0.3262&0.2260&0.0565&1.9737&1.4&34&43.2&21.4&0.4615\\
Window-10&0.3053&0.2122&0.0609&1.9686&\textbf{4.2}&37.4&41.4&17&0.4280\\  
\bottomrule
\end{tabular}
\caption{Results of Clarification Question Generation.}
\label{tab:CQG}
\end{table*}

As can be seen from \tabref{tab:CQG}, Window-top generates the least Perplexity and the 
highest BLEU score. In the results of human evaluation, although Window-last-p method has the highest proportion of ``3'', Window-top method has the highest average score. In addition, we find the method of Window-1 is better than Window-3, the Window-3 is better than Window-5 and the Window-5 is better than Window-10. It seems that the shorter the context, the better. But the Window-top is better than Window-last-q. This is because when the context is relatively short, the model is better able to grab the central semantics of the context and thus generate good clarification question. However, when the context is too long, such as to use Window-top method, the information of the context is relatively large, so more aspects can be considered to generate the question, so as to be more suitable to the reality.

\begin{CJK}{UTF8}{gbsn}
\begin{table*}[!htbp]
\small
\begin{tabular}{c|l|l}
\toprule
Context&\makecell[l]{$C$1:慢性鼻炎怎么治疗啊？\\($C$1:How to treat chronic rhinitis?)\\$E$1:您好,平时是什么症状呢？\\($E$1:Hello, what are the usual symptoms?)\\$C$2:鼻子不通气。\\($C$2:A stuffy nose.)}&\makecell[l]{$C$1:弱视属于视力明显下降吗？\\($C$1:Is amblyopia a marked decrease in vision?)\\$E$1:弱视是指视力要比健康眼差。\\($E$1:Amblyopia means that your eyesight is worse than\\ a healthy eye.)\\$C$2:我的孩子检查出是弱视。\\($C$2:My child was diagnosed with amblyopia.)\\ $E$2:孩子是一个两个眼睛视力不好还是一只眼呢？\\($E$2:Do children have poor eyesight in one eye or two?)\\$C$3:一只眼睛。\\ ($C$3:One eye.)}\\
\hline
Ground truth&\makecell[l]{$E$2:持续多久了？\\($E$2:How long did it last?)}&\makecell[l]{$E$3:方便提供孩子的裸眼和矫正视力吗？\\($E$3:Is it convenient to provide children with naked eye\\ and corrected vision？)}\\
\hline
Window-top&\makecell[l]{$E$2:您好,这种情况多久了？\\($E$2:Hello, how long has this been going on?)}&\makecell[l]{$E$3:已经持续一阵子了吗？\\($E$3:Has it been going on for a while?)}\\
\hline
Window-last-q&\makecell[l]{$E$2:您好,做过什么检查？\\($E$2:Hello, what kind of checks have you had?)}&\makecell[l]{$E$3:视力是多少呢？\\($E$3:What is your vision?)}\\
\bottomrule
\end{tabular}
\caption{Two Examples of Clarification Question Generation between an expert $E$ and a client $C$.}
\label{tab:example2}
\end{table*}
\end{CJK}

Since Window-last-q is the best method for clarification trigger detection, we use the methods of Window-top and Window-last-q in the same conversation to predict the clarification question. Two examples are shown in \tabref{tab:example2}. As can be seen from \tabref{tab:example2}, the two methods of Window-top and Window-last-q are different in terms of the clarification prediction and semantics due to the length of the context. We can see that in the first example, the question generated by using the Window-top method is closer to the ground truth. But in the second example, the question generated by using the Window-last-q method is closer to the ground truth.

Therefore, we propose the hypothesis: if it is a short context, it is better to use Window-top method to generate clarification questions; if it is a long context with five or more sentences, it is better to use Window-last-q method to generate clarification questions. 
Window-last-q works better for long sentences because it can ignore distant noises that are not related to the current generation question, such as the doctor's established diagnosis in this doctor-patient Q$\&$A environment. In the second examples, since a recent sentence has confirmed the diagnosis of ``amblyopia'', it has certainly been going on for a while, and it is not necessary to generate the question using the Window-top method. This is influenced by the noise that the doctor says in the first sentence to describe the symptoms, because symptoms are typically asked about their duration. This suggests that the expert $E$ in the consultantion dialogue can decide how to ask the clarification question based on the length of the conversation. 

Based on our hypothesis, we randomly select 1,000 dialogues. Half of the dialogues are short contexts, less than 5 sentences, and half are long contexts, more than 5 sentences. We can use either the Window-top method or the Window-last-q method to generate clarification questions. Then we conduct human evaluation. If the Window-top method is more appropriate to predict the classification question than Window-last-q method, a score of ``1'' is given; otherwise, a score of ``0'' is given. The results are shown in \tabref{tab:example3}. The results preliminarily prove our hypothesis that Window-top method is suitable for generating clarification questions in long context and Window-last-q method is suitable for generating clarification questions in short context. But there are a few counter examples. The reason for the existence of these counterexamples remains to be investigated. It remains to be seen under what circumstances and how to generate clarification questions.

\begin{table}[!htbp]
\centering
\small
\begin{tabular}{ccc} 
\toprule
Score&Context$\_$length\textless5&Context$\_$length$ \geq $5\\
\hline
1&75.2$\%$&30.6$\%$\\
0&24.8$\%$&69.4$\%$\\
\bottomrule
\end{tabular}
\caption{Human evaluation results of our hypothesis.}
\label{tab:example3}
\end{table}

One interesting thing to mention is that the Window-last-q method is better for trigger but the Window-top method is better for generation. This may be because the semantic information of the conversation content is considered when the question is triggered, and the length of the existing conversation content is considered when the question is generated. Because the description of patient information in the doctor-patient Q$\&$A is usually concentrated in the first sentence, the generation of the clarification question without considering the initial description will cause the problem of missing information.
