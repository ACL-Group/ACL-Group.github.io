\section{Related Work}
\label{related}

In previous studies on the dialogue system \cite{DBLP:conf/sigir/AliannejadiZCC19}, the system can clarify users' information needs by actively asking them questions, so as to improve users' satisfaction. Asking clarifying questions is especially important in a session because they can only return a limited number of results. In multi-turn free-style consultation between an expert and a client, the expert also can improve communication efficiency by raising clarifying questions in time.

The research on clarification questions has attracted extensive attention 
from NLP \cite{DBLP:conf/acl/DaumeR18}. Human conversations on question-and-answer 
sites have been studied to analyze the intent of each sentence \cite{DBLP:journals/corr/abs-1804-08759} and, more specifically, to clarify the question \cite{DBLP:conf/chiir/BraslavskiSAD17}. In the machine reading comprehension literature, generating questions whose answers appear in a given passage has been extensively studied \cite{DBLP:conf/emnlp/DuanTCZ17,DBLP:conf/naacl/HeilmanS10,DBLP:conf/nlpcc/ZhouYWTBZ17}. There is also a more relevant study that focuses on clarifying questions and pointing out the missing semantic information in the article. For example, Trienes and Balog \cite{DBLP:conf/ecir/TrienesB19} focus on identifying unclear CQA posts that need further clarification. Rao and Daum{\'{e}} III \cite{DBLP:conf/naacl/RaoD19} propose a model to generate a clarification to identify missing information in closed domain settings. They propose a reinforcement learning model, which maximizes the utility function based on the added value of the potential response to the clarification problem \cite{DBLP:conf/acl/DaumeR18}. In other contexts, such as dialogue system \cite{DBLP:conf/naacl/BoniM03,DBLP:journals/nle/BoniM05}, the clarification question is also studied, which is similar to how we clarify the clients' information needs by asking clarification questions in our work.

Research on end-to-end open domain dialogue generation is encouraged by the success of neural seq2seq models on machine translation \cite{DBLP:conf/nips/SutskeverVL14}. On top of the basic architecture \cite{DBLP:conf/acl/ShangLL15}, various extensions have been made to tackle the safe response problem \cite{DBLP:conf/acl/QiuLBZY19}; to model dialogue history for multi-turn conversation \cite{DBLP:conf/aaai/SerbanSLCPCB17}; and to learn with advanced machine learning techniques \cite{li-etal-2017-adversarial}. In the research of generating clarification question, we can use seq2seq model to get basic generating results.

Previous work has also done a lot of research on user interaction. 
For example, extracting questions and answers from product reviews improved business recommendations \cite{DBLP:conf/cikm/ZhangCA0C18}. 
An interactive system was designed to collect more detailed information about users' preferences in site recommendations \cite{DBLP:conf/kdd/Christakopoulou16}. 
User query was used to represent session history, and a deep reinforcement learning framework was proposed to build personalized session recommendation system \cite{DBLP:conf/sigir/SunZ18}.
What makes our work different from these studies is that we ask a clarification question in multi-turn free-style consultation.
