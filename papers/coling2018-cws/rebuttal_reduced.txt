1. We will release our code and medical FORUM data in the camera-ready. 
Due to privacy concerns, we cannot release EMR datasets.

2. "I still have some lingering doubts about how the model parameters were selected". 

Not sure what it means. If they meant how hyper-parameters are selected, we clearly describe it in Sec 5.5.


1. "..., but its impact outside this particular task ..."

Due to space and resource constraint, we only focus 
on transfering CWS models within the medical domain, which was sufficiently 
evaluated on several sub-domains, including Respiratory, 
Cardiology and Forum. We speculate that the methodology 
may be applied across other domains and other sequence tagging
problems like NER and POS tagging. These ideas will be
validated in future work.

2. "The authors argue that ... This should have been made clearer ..."

We will explicitly point out in camera-ready that the claimed advantage of our models is supported by evaluation results in the corresponding tables.

3. "In the discussion, the authors did not ..."

We will add discussion to weigh-in the quantity vs. quality when it comes
to the impact on transfer learning.

4. INIT works well between domains with low disparity because: (a) well trained model in source domain provides a good start point for training in target domain (b) the final model is fine-tuned against target domain only. Our method is disadvantaged in this scenario because: (a) model parameters are randomly initialized and are independent between two domains (except for shared parameters), thus it cannot inherit so much information as INIT does; (b) the final model is fine-tuned against two domains synchronously; thus noise from the source  may be introduced into the target. It's a research problem we want to tackle in future.

5. We will fix caption and spelling errors in camera-ready.


1. Due to space and resource constraint, we only do experiments over medical text. However, we conjecture that our method also works under other scenarios.

2. Using CRF as decoder is a recent common practice in CWS and sequence tagging problems. As listed below. Anyway, the keypoint of this paper is the transfer learning method and not the base model to solve CWS.

Adversarial multi-criteria learning for chinese word segmentation. ACL 2017.
Semi-supervised sequence tagging with bidirectional language models. ACL 2017.
Aggregating and predicting sequence labels from crowd annotations. ACL 2017.
