Review #1

Relevance (1-5):	5
Readability/clarity (1-5):	5
Clarity (a) - Hypothesis:	Yes, it is stated directly
Clarity (b) - Hypothesis tested:	Yes
Originality (experiment) (1-5):	4
Technical correctness/soundness (1-5):	4
Soundness (a):	Hypothesis and results clearly relate
Soundness (b):	Yes, it is explained
Soundness (c):	Datasets are clearly described and appropriate
Reproducibility (1-5):	4
Data/code availability (1-5):	2
Error analysis (1-5):	2
Meaningful comparison (1-5):	3
Substance (1-5):	3

This work focuses on the task of Chinese word segmentation. Specifically, the authors propose several approaches for domain adaptation, where the source domain is a high-resource domain (where several datasets are available such as PKU, MSR, and WEIBO) and the target domain is the text from electronic medical records. The proposed approach is an adaptive multi-task transfer learning framework. The authors analyze the performance of transfer learning under different levels of data disparity. The proposed method outperforms several strong baselines based on existing work.
The paper is generally well written. The authors mention that the data will be made available, which seems a little unrealistic given the nature of the data / privacy concerns. In general the paper presents solid work although I still have some lingering doubts about how the model parameters were selected.

Overall recommendation (1-5):	3

Review #2

Relevance (1-5):	5
Readability/clarity (1-5):	4
Clarity (a) - Hypothesis:	Yes, it is stated directly
Clarity (b) - Hypothesis tested:	Yes
Originality (experiment) (1-5):	4
Technical correctness/soundness (1-5):	4
Soundness (a):	Hypothesis and results clearly relate
Soundness (b):	Yes, it is explained
Soundness (c):	Datasets are clearly described and appropriate
Reproducibility (1-5):	4
Data/code availability (1-5):	2
Error analysis (1-5):	2
Meaningful comparison (1-5):	3
Substance (1-5):	4

This paper applies transfer learning techniques to Chinese word segmentation in medical texts. Comprehensive evaluations have been performed with respect to three dimensions: the source/target domain pairs, the model types (I, II, III), and the statistical distance functions (KL, MMD, CMD) used to compute J_{Adap}. This is a useful study, but its impact outside this particular task may be currently limited (the proposed future work may change this). The statement about dataset availability is vague; getting the data is likely difficult for other researchers.
There are in total six datasets; three of them (P, M, W) contain material from general domains, and three others (C, R, F) are in medical domains and are created by the authors. In the first group of experiments (Table 6), transfer learning within medical domains is tested. Note that the caption of Table 6 erroneously contains the PKU dataset, which represents a general domain. In the second group (Table 7), transfer learning is tested in the direction from each general domain to each medical domain. The authors argue that their models have a significant advantage when there is high disparity between the source and the target domain, which is supported by the fact that bold numbers only appear in the columns that involve high disparity. This should have been made clearer, as the last paragraph of subsection 5.2 and the last point of subsection 5.8 are confusing. In the discussion, the authors did not compare the effect of transferring from a general-domain dataset (which has the advantage of quantity) against that of transferring from a medical dataset (which is good at quality) either.

The other two dimensions (model types and distance functions) are relatively easy to understand. There are six baseline systems. The INIT baseline has the highest score on all 7 domain pairs which do not involve high disparity. The authors should consider discussing why the INIT baseline is so strong, and why their approach is disadvantaged in the easier scenario of transferring between similar domains.

Spelling mistake just below heading 4.4.1: “interrupted” -> “interpreted”. In addition, could you remove the footnote from the abstract?

Overall recommendation (1-5):	3

Review #3

Relevance (1-5):	5
Readability/clarity (1-5):	5
Clarity (a) - Hypothesis:	Yes, it is stated directly
Clarity (b) - Hypothesis tested:	Yes
Originality (experiment) (1-5):	5
Technical correctness/soundness (1-5):	5
Soundness (a):	Hypothesis and results clearly relate
Soundness (b):	Yes, it is explained
Soundness (c):	Datasets are clearly described and appropriate
Reproducibility (1-5):	4
Data/code availability (1-5):	2
Error analysis (1-5):	4
Meaningful comparison (1-5):	5
Substance (1-5):	5

What the work contributes?
This work explores ideas of combining joint learning (multi-task transfer) for segmentation of Chinese words. They present very thoughtful and novel idea of combining various model. They experimented with Medical text, which is harder to segment compared to common language and mis-segmentation can be expensive.

Major Weakness

(a) None

Minor Weakness

(a) Ideas presented for combining models, especially Model-II and Model-III are generic and very novel. It would have been interesting to see how they perform for general domain CWS. I understand that authors wanted to focus on the medical text; however, I think this model might provide value beyond medical data.

(b) A justification for sticking to CRF based model for the decoder is not provided. Do the models not transfer well if the decoder is also built using LSTM?

Strength

(a) Very clear writing with good background study.

(b) Comparison with very strong baselines.

(c) Use of adaptive loss is both intuitive and well defined. Exploring them in three different setting makes sense and provides a good understanding of a domain adaptation.

(d) Datasets are reasonable and challenging. Meticulous work with annotation of data should be appreciated.

(e) All hyper-parameters with respect to the model are thoroughly validated.

Overall recommendation (1-5):	5
