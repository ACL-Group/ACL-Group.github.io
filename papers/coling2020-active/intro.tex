
\section{Introduction}
\label{sec:intro}

%Recently large neural models such as ELMo \cite{peters2018deep} and BERT \cite{devlin2018bert} 
%have been popular in natural language processing. 
%Despite their improved results, the cost of training is mounting. 
%Hofst√§tter and Hanbury~\shortcite{hofstatter2019let} 
%reported that BERT is more than 100 times slower than non-contextualized ranking models. 
%However, Xu \shortcite{Liang2019} has tried different text classification methods 
%on Zhihu dataset and found that fastText is ten times faster than other methods, 
%while achieving similar results. Therefore, fastText \cite{joulin2016bag}, 
%as a good substitute, is balanced in both training time and test accuracy 
%and is thus widely adopted in many industry tasks. 
%
%Unfortunately, even if we use fastText for classification, 
%large amount of training data is still essential. Annotated data is expensive to obtain, 
%notably in specialized domains where only experts can provide reliable labels. 
%As we will show later in \secref{sec:results}, for a news headline dataset which contains
%325,285 samples, to achieve 95\% of the accuracy trained on all samples, by random sampling,
%we need at least 55,100 samples, which is still very costly by human labeling.
A classification problem can be defined as: $y^* = \argmax_y f(\mathcal{X})$ 
where $\mathcal{X}$ is 
the representation of input $x$, 
$f(\mathcal{X})$ is the probability distribution of the output classes
and $y^*$ is the predicted label. In order to learn a good classifier $f$,
one has to provide labeled data for training, which can be very costly
especially when the model is complex.  
Active learning (AL)~\cite{settles2009active} was proposed to ease the 
labeling effort by selecting the most informative samples to label, 
based on certain criteria. In text classification scenarios, most AL research is designed for 
binary or few-class classification problem 
and little attention has been paid on many-class classification. Lewis and Gale \shortcite{lewis1994sequential} proposed uncertainty sampling which is one of the most popular active learning strategies and experimented it on AP news belonging to 10 categories, while Tong \shortcite{tong2001support} designed an approach to perform active learning with support vector machine and conducted it on datasets 
with less than 10 classes. 

% \KZ{Expand the introduction of previous
% studies a bit here. \textbf{al binary class text classification}}
Nonetheless, with the prevalence of e-commerce and online communication, multi-class short text overwhelms in many application areas such as Instant Messages, Online Chat Logs, Bulletin Board System Titles, Internet News Comments, Tweets, etc~\cite{song2014short}. Classifying them becomes increasingly important in many scenarios including
topic recommendation, e-commerce chat robot and so on. However, since short text often contains misspellings, non-standard terms and noise~\cite{yan2009dynamic}, labelling and classifying are difficult tasks. 
% \KZ{Informally describe short text classification with many classes
% and say why it's a useful problem. Give some examples and scenarios.
% Use examples that is difficult to find labels and hence requires human
% labeling. Emoji prediction is not, cos there's ample labeled data.}
Unfortunately, even though short text classification have appeared to be a practical and general problem, it is still far less discussed than traditional text classification. Approaches such as semantic analysis, semi-supervised learning, ensemble models have been suggested to solve it. AL, as one kind of 
semi-supervised learning, has also been proposed, 
but Lowell\cite{lowell2019practical} previously suggested it was difficult to 
generalize AL for various classifiers and datasets.

% \EVE{`these approaches' means semantic analysis or AL approaches?}
% \KZ{Give reason why we say it's under-studied. Give statistics
% of previous text classification tasks (in terms of num of classes and
In this paper, we seek to study the effectiveness of active learning on many-class short text classification and answer the following
research questions:

\begin{enumerate}[label=(\alph*)]
\item \textit{What is the better underlying text classifier to use with AL for the task?}
\item \textit{Is AL truly useful for many-class text classification problem,
compared to random sampling?}
\item \textit{Which AL sampling strategy is more effective with many-class text classification?}
%\item \textit{To achieve better results, shall we go for more labeled data or a stronger classifier (trained more thoroughly with more epochs?}
\item \textit{Is there any correlation between the AL's performance and the
characteristics of the dataset?}
\end{enumerate}

We thus implement four well-known AL strategies, and additionally develop one strategy, based on the internal diversity of the sampled classes
(named as class radius), and one feature based on the frequency of
the sampled classes (named as class frequency) which can be combined with all AL strategies. 
We applied these AL strategies in conjunction with
four popular text classifiers, namely, fastText, CNN, 
LSTM, and BERT, and evaluate them on 8 different publicly
available short text classification datasets (5 English and 3 Chinese). 
Most of these datasets typically involves at least 10 classes, 
and the average length of the text is under 20 tokens. Detailed experiments
and analysis enable the following discovery, which will be useful for
practical application of AL in future similar tasks:

\begin{enumerate}
\item If AL is to be used, fastText is the classifier to use, which is simple
and outperforms the other three classifiers in most cases; (\Cref{fig:acc_all,fig:acc_all_bert,fig:acc_all_lstm,fig:acc_all_cnn}, \tabref{table:ratioOfClassifiers})

\item AL does work for many-class short text classification, increasingly so
for more classes; (\tabref{table:ratioOfDataset})
\item There is no clear winner when it comes to which sampling sampling
strategy to use, but if fastText is used as the classfier, our proposed
class frequency feature works better in general than other strategies; (\tabref{table:auc_ft})
%\item To achieve consistently good results, for any given classification
%model, it is better to train more epochs than to feed it with more labeled
%data;
\item There is not clear correlation between the AL's performance and the
length of the samples, the size of the dataset or the class distribution
in the dataset. (\tabref{table:correkationOfDataset})
\end{enumerate}

%Results show that the advantage of traditional AL strategies 
%over random sampling diminishes with the increase of the number 
%of classes in the classification task (see \secref{sec:eval}).
%We further make two observations:
%First, in unbalanced and noisy datasets, current sampling strategies often
%produce a lot of duplicates and are relatively biased. 
%For example, by entropy sampling, 
%a lot of very similar samples are chosen because they have identical entropy score. 
%%Therefore, we have proposed a frequency tuning metric such that the model won't lay too much emphasis on one certain class and to some extent solve the issue.
%Second, most AL strategies leverage statistic properties of $f(\mathcal{X})$, 
%calculating the variation ratio \cite{freeman1965elementary}, mean standard deviation \cite{kampffmeyer2016semantic}, or the mutual information between predictions and model posterior \cite{houlsby2011bayesian}, but they ignore the features in $\mathcal{X}$, which is closer
%to the raw input.
%
%In order to address these issues, we investigated into previous approaches by 
%analyzing the properties of $\mathcal{X}$ and come up with a new
%sampling strategy called {\em radius uncertainty}, taking into consideration of 
%the underlying distrbution of the input data itself. We also propose to use
%class frequency in the current samples to mitigate the duplicated or 
%near-duplicated sample problem. This simple strategy can be applied to
%almost all previous AL strategies.
%
%Our contributions are as follows:

