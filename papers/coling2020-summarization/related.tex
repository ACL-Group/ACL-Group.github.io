\section{Related Work}
\label{sec:related}
%Great progress~
%\cite{RushCW15,NallapatiZSGX16,SeeLM17,PaulusXS17,LiuL19,UniLM19}
%has been made recently on
%neural-based abstractive summarization.
Repetition is a persistent problem in the task of 
neural-based summarization. 
It is tackled broadly in two directions in recent years. 

One direction involves {\em information selection} or 
{\em sentence selection} before generating summaries.
Chen~\shortcite{P18-1063} proposes an extractor-abstractor model, which uses an extractor  
to select salient sentences or highlights and then employs 
an abstractor network to rewrite these sentences.
Sharma~\cite{SharmaHHW19} and Bae~\shortcite{SanghwanB19} are also use extractor-abstractor model 
with different data preprocessing methods.
All of them can not solve repetition in seq2seq model.
Tan~\shortcite{TanWX17} and Li~\shortcite{D18-1205,D18-1441} encode
sentences using word vectors
and predicts words from sentence vector in sequential order 
whereas CNN-based models are naturally parallelized. 
While transferring RNN-based model to CNN model, 
the kernel size and the number of 
convolutional layers can not be easily determined when
converting between sentences and word vectors. 
Therefore, we do not compare our models to those models in this paper. 

The other direction is to improve the 
{\em memory of previously generated words}.
Suzuki~\shortcite{SuzukiN17} and Lin~\shortcite{LinSMS18} 
deal with word repetition in single-sentence summaries, 
while we primarily deal with multi-sentence summaries with 
sentence-level repetition. 
There is almost no word repetition in multi-sentence summaries.
Jiang~\shortcite{JiangB18} adds a new decoder without attention mechanism.
In CNN-based model, attention mechanism is necessary to connect encoder 
and decoder.
Therefore, our model also is not compared with the above models in this paper. 
The following models can be transferred to CNN seq2seq model and
are used as our baselines.
See~\shortcite{SeeLM17} integrates coverage mechanism, 
which keeps track of what has been summarized, as a feature that helps 
redistribute the attention scores in an indirect manner,
in order to discourage repetition. 
Tan~\shortcite{TanWX17} uses distraction attention
\cite{ChenZLWJ16}, which is identical to coverage mechanism. 
Gehrmann~\shortcite{GehrmannDR18} adds coverage penalty to loss function
which increases whenever the decoder directs more than 1.0 of total attention
towards a word in encoder.
This penalty indirectly revises attention distribution and results in
the reduction of repetition.
{\c{C}}elikyilmaz~\shortcite{elikyilmazBHC18} uses semantic cohesion loss,
which is the cosine similarity between two consecutive sentences, as part of
the loss that helps reduce repetition.
Paulus~\shortcite{PaulusXS17} proposes intra-temporal attention \cite{NallapatiZSGX16} and 
intra-decoder attention which dynamically revises the attention distribution while decoding. 
It also avoids repetition at test time by directly banning the generation of 
repeated trigrams in beam search. 
Fan~\shortcite{FanGA18} borrows the idea from \cite{PaulusXS17} and 
builds a CNN-based model. 

Our model deals with the attention between encoders and decoders. 
Different from previous methods, 
\textit{attention filter mechanism} does not 
treat the attention history as a whole data structure,  
but divides it into sections (\figref{fig:model_main}). 
Previously, the distribution curve of accumulated attention scores 
for each token in the source tends to be flat, 
which means critical information is washed out during decoding.
Our method emphasizes previously attended sections 
so that important information is retained.
Given our observation that repetitive sentences in the source are
another cause for repetition in summary, 
which cannot be directly resolved by manipulating attention values, 
we introduce \textit{sentence-level backtracking decoder}. 
Unlike \cite{PaulusXS17}, 
we do not ban repetitive \textit{trigrams} at test. 
Instead, our decoder regenerates a sentence that is similar to previously generated ones.
With the two modules, our model is capable of generating summaries with a
natural level of repetition while retaining fluency and consistency.

\cut{%%%%%%%%%%%
In this paper, we aim at reducing repetition in abstractive summarization
and evaluate the
effectiveness of our repetition reduction technique, so we need to compare our proposed
approaches with others on the same seq2seq model, to be fair. 
We did not take all of the seq2seq models with better ROUGE score as baselines, because
these models do not deal with repetition problem in abstractive
summarization effectively. 
Since most seq2seq models use attention of some sort, and our model deals
with incorrect attention distribution, we can reasonably deduce that these models
will benefit from our techniques as well. 
}%%%%%%%%%%%


