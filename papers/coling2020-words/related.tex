\section{Related Work}
\label{sec:related}

There are several studies related to the analysis of vocabulary, including the correlation between frequency and word difficulty, the text-level classification based on vocabulary knowledge,  etc.
%There are several works focusing on the vocabulary knowledge analysis or the text-level classification based on vocabulary knowledge. 
We will discuss the related research in this section.
%.\JQ{rewrite. Besides, it sames that word difficulty is a part of vocabulary knowledge analysis?}

%\subsection{Complex Word Identification}
\subsection{Vocabulary Knowledge in Difficulty Analysis}
Both linguists and educators devoted themselves into the research of vocabulary knowledge. 
There are some previous papers~\cite{koirala2015word,breland1996word,kirkpatrick1949vocabulary} discussing the various features that influence word difficulty. However, most attempts takes the frequency as only feature for words. 
It is common sense that there is a negative correlation between the word frequency and word difficulty and it truly performs well in these researches.
%Previously proposed methods have discussed the various features that influence word difficulty.\\
%Most attempts used word frequency as a sole feature.
%There is a negative correlation between the word frequency and word difficulty, as the frequency decreases, the difficulty of word increases~\cite{koirala2015word, breland1996word, kirkpatrick1949vocabulary}. 
%Therefore, frequency can be regarded as an effective measurement for word difficulty estimating.
%\JQ{delete, hide by \%}
Frequency bands are also used to describe the word difficulty distribution.
A typical example is an online language scoring API provided to check the word difficulty by ranking the frequencies in a huge corpus\footnote{\url{https://www.twinword.com/api/language-scoring.php}}. 
% \JQ{what is the global vocabulary set}

Other research talked about the feature combinations that influence word difficulty.
Cesar Koirala~\shortcite{koirala2015word} used the quantity difference between difficult words and easy words to show the function of word length,  number of syllables and  number of consonant clusters.
Hiebert et al \shortcite{hiebert2019analysis} chose statistic method to discuss the features of words that distinguish studentsâ€™ performances in various grades, including word frequency, part-of-speech and word morphological family size.
Similarly, spelling rules and morphological features which consist of prefixes and suffixes are also considered in the study of difficulty in Japanese and German \cite{hancke2012readability,nakanishi2012estimating}.

In this paper, we use NLP methods to extract the features that previous work mentioned, such as using n-gram feature to cover the prefixes and suffixes information of morphological features.
What's more, we consider the universal dependency and word embedding features for the first time and have achieved remarkable results compared with previous work.
%For the morphological features mentioned in previous work, the prefixes and suffixes information has be included in our n-gram feature.
%What's more, the n-gram also covers the probability for forming a word.
%In this paper, the universal dependency and word embedding features are applied to the recognition of word difficulty for the first time and have achieved remarkable results compared with previous work.
In addition, traditional research mostly focus on theoretical analysis individually, without a comprehensive analysis of all the features. 
They also ignore the importance of developing a effective approach to do the difficulty division task instead of long-term human labors.
%However, tranditional research were more biased towards theoretical analysis or lack of comprehensive feature analysis of words.
%There lacks a computational means to replace long-term human work in predicting words difficulty accurately.
Our methods try to solve this bottleneck and use automatic classification to assign difficulty levels for words.
%The results of both classification and difficulty ranking tasks show it works in various language environments.
\subsection{Complex Word Identification}
%\SY{1.Permance of students on vocabulary study. 2. Text readability analysis based on Vocabulary knowledge.}
The most relevant study of word difficulty is Complex Word Identification (CWI).
The complexity of a word has different performance with different contexts,  CWI task is to detect the words in need of simplification in certain context.

Different from CWI which explores the word complexity in certain sentences, 
we motivate to learn the comprehensive features from a huge corpus 
and give the difficulty level to a word based on an existing standard for second language learners.
There are several public datasets for CWI, but they are formed as a sentence with the offset of complex words and can't give us a certain difficulty standard for word difficulties.
The usual solutions for CWI including feature-based methods and deep learning methods are related to a given context, but our task is to identify the difficulty of a single word.
Thus, due to the really different task definition and motivation, the work of CWI can not be applied in our task.






 