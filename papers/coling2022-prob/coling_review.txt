
Review #1
Relevance (1-5):	5
Readability/clarity (1-5):	4
Originality (1-5):	4
Technical correctness/soundness (1-5):	5
Reproducibility (1-5):	4
Substance (1-5):	4
Detailed Comments

This paper proposes a set of lightweight computations to identify spurious statistical cues which may overestimate model performance. Additionally, it shows performance improvement using models trained on a hard subpopulation. Overall this paper is well-written and thorough experiments have been conducted.
Strengths:

The proposed framework is lightweight and can be used to create easy & hard splits of datasets.

The cue metrics are simple to implement and easily interpretable.

Weaknesses:

For better replicability, some hyper parameters could be better mentioned - like training details of logistic regression as well as the versions of FastText used?

Authors mention FastText as an average of Glove embeddings? Unclear if FastText algorithm has been used or FastText embeddings have been learned?

More literature on "reliability testing” should have been cited.

Typos:

which is expensive the train -> which is expensive to train

In order to measure the severity of information -> mentioned twice

The following papers should be cited: : Reliability Testing for Natural Language Processing Systems : Automatic Construction of Evaluation Suites for Natural Language Generation Datasets

Overall recommendation (1-5):	4
Confidence (1-5):	4
Presentation Type:	Oral
Recommendation for Best Paper Award:	No

Review #2
Relevance (1-5):	4
Readability/clarity (1-5):	2
Originality (1-5):	3
Technical correctness/soundness (1-5):	3
Reproducibility (1-5):	4
Substance (1-5):	3
Detailed Comments

The paper addresses the problem of statistical cues in NLP datasets that models take advantage of and overestimate their capability. The paper proposes a lightweight framework to automatically identify word-level biases from classification datasets and further evaluate the robustness of any models targeting the dataset.
In the methodology, the authors introduce six lightweight cue metrics based on the individual words in the questions and propose aggregation methods to combine the metrics.

There are major issues with this paper:

First, It is not clear what is the contribution of the paper. The paper introduces 6 cue metrics in section 2.2 based on individual words in the question and their distribution in the training data w.r.t. the label. Later in Section 3.1, based on the empirical results the authors conclude that the best cue is the "conditional probabilityâ (Line 372) and just focus on using it. This whole process is obvious and not surprising at all: by focusing on individual words one is assuming the Naive Bayes (NB) assumptions, hence it is not a surprise that the conditional probability, which is the main component of the NB, is the best cue. So what is the contribution here? Why even bother with other cues?

Second, the paper claims its method is lightweight and fast, but no runtime evaluation results are provided to back this claim. So it is not possible to evaluate the claim.

Other Issues:

The Cue formulation in section 2.2 is vague and super verbose. First, the equations are not well defined/explained, for example, the "#()â notation is used in Line 162, but it is not defined until Line 172 in the next subsection. Second, some terms are defined multiple times with different symbols, for example, the conditional probability in equation 4 is redefined in equation 5 with a different symbol that is redundant.

Formulations in section 2.3 are also redundant and unnecessary. For example, g_average and g_max are defined but never used or g_linear is used in equation 16 but is not defined at all.

Paper is hastily written and is full of grammatical errors. There are multiple cases where the paper references something without defining it or defining it in subsequent sections, e.g. FastText is first mentioned in L366 but it is not discussed until L428.

Some claims are just unsupported by any result from the paper. For example in Line 579, the authors claim that "model-dependent methods are not stable enoughâ. Since the term "stabilityâ has not been mentioned in the paper before, it is not clear how the authors have made this claim or even what they mean by stability.

Overall recommendation (1-5):	1
Confidence (1-5):	3
Presentation Type:	Poster
Recommendation for Best Paper Award:	No

