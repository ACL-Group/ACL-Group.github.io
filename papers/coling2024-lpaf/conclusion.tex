\section{Conclusion}
In this paper, we discover that the full-rankness of fine-tuned language models is the fundamental bottleneck for the failure of the traditional matrix factorization approach. As a remedy, we employ first-order unstructured pruning to extract the low-rank subnetwork that maximally preserves the task-specific information. We then propose sparsity-aware SVD and mixed-rank fine-tuning as two optimizations to boost the compression performance. Thorough experiments demonstrate that LPAF can achieve better accuracy-compression trade-offs against existing approaches. 
When applied to already compact language models, our method can further achieve a 2x compression with minor accuracy degradation. Our work provides valuable insight on the intrinsic low-rank structure of task-specific knowledge within PLMs, paving the way for future research on more sophisticated compression techniques.