\section{Experiments}
%\KZ{You need a preamble to explain what experiments we are doing here. If you
%want to squeeze space, don't sacrifice the standard format and style
%of the paper, but focus on cutting unnecessary text.}
In this section, we present the experiments of LPAF for language model compression. We compare with state-of-the-art compression methods and perform detailed analysis of the results to provide guidance under different resource budgets.

\subsection{Experimental Setup}
In this subsection, we present the detailed experimental setup regarding the datasets, baselines, training details, and compression settings.
\subsubsection{Datasets} 
We evaluate our approach on general natural language understanding tasks from GLUE benchmark~\cite{glue}, as well as extractive question-answering tasks using SQuAD v1.1~\cite{qnliandsquad} and SQuAD v2.0~\cite{squadv2.0}. GLUE tasks include Recognizing Textual Entailment~(RTE), The Corpus of Linguistic Acceptability~(CoLA), Standford Sentiment Analysis~(SST-2)~\cite{sst2}, Microsoft Research Paraphrase Corpus~(MRPC), Quora Question Pairs~(QQP), Question NLI~(QNLI)~\cite{mrpc}, and Multi-genre Natural Language Inference~(MNLI)~\cite{mnli}.

Following previous work~\cite{pkd}, we evaluate under a task-specific setting, i.e., we utilize no external corpus but only assume access to the training data of each task.



\subsubsection{Baselines} We compare LPAF as well as its three ablated versions that remove each of the three steps  against four categories of methods with a perceivable reduction in model size and computation.

\paragraph{Pre-training Distillation: }DistilBERT~\cite{distilbert}, and TinyBERT~\cite{tinybert} are two widely adopted pre-training distillation models, which use large amounts of unlabeled corpus followed by task-specific fine-tuning.

\paragraph{Task-specific Distillation: }PKD~\cite{pkd} extends KD by intermediate feature matching;
Theseus~\cite{theseus} proposes a progressive module replacing method for knowledge
distillation; CKD~\cite{CKD} transfers the contextual knowledge via word relation and layer transforming relation; MetaDistil~\cite{metadistil} uses meta-learning for training the teacher to better transfer knowledge to the student.

\paragraph{Structured Pruning: } 
Iterative structured pruning~(ISP)~\cite{isp} removes attention heads in multi-head self-attention layer and neurons in feed-forward layer with the lowest sensitivity in an iterative manner; FLOP~\cite{flop} represents weight matrices as the sum of rank-one component and adaptively removes the least important ones during training; Block Pruning~(BP$_{\text{hybrid}}$)~\cite{block} shares pruning decisions for each 32x32 weight blocks in self-attention layer and for each row/columns in feed-forward layer; CoFi~\cite{sp-l0} jointly prunes attention heads, neurons, hidden dimension, and entire multi-head self-attention/feed-forward layer via Lagrangian multipliers.

\paragraph{Matrix Factorization: }SVD$_{\text{Ft}}$~\cite{svd} applies truncated SVD on a densely fine-tuned BERT and re-trains the factorized model to recover accuracy loss.

\begin{table}[t]
	\footnotesize
	\centering
	\begin{tabular}{l|cc|cc}
		\toprule
		%\hline
		& \multicolumn{2}{c|}{\textbf{\% of Params.}} & \multicolumn{2}{c}{\textbf{FLOPs}} \\ 
		\midrule
		%\hline
		Task         & \multicolumn{2}{c|}{All}          & GLUE    & SQuAD    \\ 
		\midrule
		%\hline
		BERT-base    & \multicolumn{2}{c|}{100\%}          & 7.4G      & 35.4G         \\
		%		$T_{sparse}$ & \multicolumn{2}{c|}{100}          & 7.4G      & 35.4G         \\ 
		\midrule
		%\hline
		LPAF-260     & \multicolumn{2}{c|}{50\%}           & 3.7G      & 16.1G         \\
		LPAF-130     & \multicolumn{2}{c|}{25\%}           & 1.9G      & 10.3G         \\
		LPAF-80      & \multicolumn{2}{c|}{16\%}           & 1.3G      & 7.9G          \\
		\bottomrule
		%\hline
	\end{tabular}
	\caption{Percentage of parameters and FLOPs for LPAF with different preserved rank.}
	\label{table:stats}
\end{table}

\begin{table*}[t]
	\centering
        \scriptsize
	\begin{tabular}{l|cccccc}
				\toprule
%		\hline
		Task &\textbf{RTE} &\textbf{MRPC}  & \textbf{SST-2}                                                  & \textbf{QQP}                                                    & \textbf{QNLI}                                                & \textbf{MNLI}                                                    \\
				\midrule
%		\hline
		\% Params.   &50\% ~25\%  ~16\% &50\% ~25\%  ~16\%      &50\% ~25\%  ~16\%             &50\% ~25\%  ~16\%            &50\% ~25\%  ~16\%            &50\% ~25\%  ~16\%        \\
		% \hline
\midrule
		\multicolumn{7}{c}{\textbf{Pre-training Distillation}}   \\
		% \hline
\midrule
		DistilBERT  &65.0~ 61.0~ 56.3 &85.8~ 77.0~ 72.5     & 90.0 ~88.9 ~86.4          & 90.8~ 89.4~ 88.0          & 86.0~ 83.8~ 81.6         & 81.7 ~76.4 ~71.3         \\
		TinyBERT    &67.7~ 67.2~ 64.6 &86.3~ 85.3~ 78.2    & 92.3 ~89.8 ~88.0          & 90.5~ 90.0~ 88.7          & 89.9 ~87.7~ 84.5          & 83.1~ 80.6~ 77.4           \\
		% \hline
	\midrule
		\multicolumn{7}{c}{\textbf{Task-specific Distillation}}   \\
		% \hline
  	\midrule

		PKD  &65.5~ 59.2~ 53.8 &81.9~ 76.2~ 71.3    & 91.3~ 88.1 ~87.2          & 88.4~ 88.5 ~87.5          & 88.4~ 82.7~ 78.0          & 81.3 ~75.7~ 72.7                                                  \\
		Theseus &65.6~ 62.1~ 58.8 &86.2~ 77.2~ 72.8   & 91.5~ 88.5~ 86.1          & 89.6 ~89.0 ~86.0          & 89.5 ~85.0~ 80.3          & 82.3~ 76.4 ~73.5                                         \\
		CKD   &67.3~ 66.5~ 60.8 &86.0~ 81.1~ 76.6   & \textbf{93.0} ~89.8~ 88.7          & 91.2~ 90.1~ 88.9          & 90.5~ 87.0 ~84.9          & 83.6 ~79.0~ 76.8                                \\
		MetaDistil  &69.0~ 66.7~ 61.0 &86.8~ 81.8~ 77.3    & 92.3 ~88.9~ 87.0          & 91.0~ 88.9~ 86.9          & 90.4~ 86.8 ~84.9          & 83.5 ~79.5~ 76.8                             \\
		\midrule
		\multicolumn{7}{c}{\textbf{Structured Pruning}}   \\
		\midrule
	% \midrule
		ISP  &66.4~ 65.0~ 63.9 &86.1~ 83.6~ 82.8   & 90.6~ 90.4 ~89.4          & 90.8~ 90.1 ~89.3          & 90.5~ 88.7~ 87.2          & 83.2 ~81.9~ 80.8                                                  \\
		FLOP &66.1~ 58.5~ 56.0 &82.1~ 80.1~ 78.4   & 91.4~ 89.7~ 89.4          & 91.1~ 90.1~ 89.1          & 90.5~ 88.5~ 87.1          & 82.6 ~79.9~ 79.4                                                  \\
		BP$_{\text{hybrid}}$ &66.4~ 64.3~ 63.9&84.1~ 83.8~ 81.1   & 90.8~ 89.8~ 89.2          & 90.8~ 90.1~ 89.8          & 90.2~ 88.7~ 88.1          & 83.2 ~80.6~ 80.1                                                  \\
		CoFi  &\textbf{69.3}~ 66.4~ 66.4 &84.6~ 84.3~ 83.6   & 91.6~ 89.7 ~89.2          & 91.0~ 90.2 ~89.9          & 90.8~ 88.8~ 87.6          & 83.5 ~80.8~ 80.5                                                  \\
		\midrule
		\multicolumn{7}{c}{\textbf{Matrix Factorization}}   \\
		% \hline
	\midrule
		SVD$_{\text{Ft}}$  &62.1~ 60.3~ 55.6 &79.9~ 70.1~ 70.0  & 90.8 ~88.9~ 85.3         & 91.3 ~90.0~ 87.9          & 91.0 ~86.1~ 83.8          & 83.0~ 79.9~ 76.6         \\
		LPAF  &68.2~ \textbf{68.0}~ \textbf{67.9} &\textbf{86.8}~ \textbf{86.5}~ \textbf{86.0}  & 92.4~ \textbf{90.7~ 89.7} & \textbf{91.5}~ \textbf{90.4}~ \textbf{90.1} & \textbf{91.3~ 89.3~ 88.6} & \textbf{84.6}~ \textbf{82.6}~ \textbf{81.7}  \\
		~~- Step-1   &64.2~ 32.1~ 21.1 &82.1~ 32.1~ 21.1    &91.2~ 89.9 ~88.4 &91.3~ 90.3~ 89.7 &91.2~ 87.8~ 84.8 &83.3~ 82.0~ 79.6  \\
		~~- Step-2  &65.3~ 32.1~ 21.1 &86.0~ 32.1~ 21.1    &91.2~ 89.2~ 88.8 &91.2~ 90.2~ 90.0 &90.9~ 89.0~ 87.9 &83.4~ 82.4~ 81.5 \\
		~~- Step-3   &65.0~ 32.1~ 21.1 &84.8~ 32.1~ 21.1    &91.4~ 89.5~ 88.8 &91.1~ 90.3~ 89.9 &91.1~ 88.9~ 88.1 &83.0~ 81.3~ 81.0 \\
%		\hline
\midrule
		BERT-base  &~~69.2~~~ &~~86.4~~~  &~~92.7~~~           & ~~91.5~~~          & ~~91.4~~~         &~~84.6~~~         \\
%		\hline
				\bottomrule

	\end{tabular}
	\caption{GLUE results~(average of 3 runs) of all compared baselines applied on BERT-base. The best results are bolded. Significance test is conducted using paired student t-test and $p$-value $<$0.05.}
	\label{table:all}
\end{table*}



\subsubsection{Training Details}
The sparsity-relevant hyperparameter $v$ in step-1 is tuned for each task in GLUE and SQuAD. We empirically search $p_{init}$ in \{0.7, 0.5, 0.3\} and decay it to zero after half of the total training steps. During training, we
fix the batch size to 32. The maximum input length is set to 384 for SQuAD v1.1, SQuAD v2.0, and 128 for other tasks in GLUE. We use the AdamW~\cite{adamw} optimizer and search learning rate in \{2e-5, 3e-5\}. We follow the official implementation of all compared baselines and run structured pruning and matrix factorization methods with a unified logits distillation objective for a fair comparison.


\subsubsection{Compression Setting}

We opt for BERT-base as the main target language model and compress it into various sizes. The original BERT-base has 12 Transformer encoder layers, and each of them is a stack of multi-head self-attention sublayer and feed-forward sublayer.
We apply our proposed LPAF to \{query, key, value, output, up-projection, down-projection\} matrices of all layers and refer to BERT-base compressed by  LPAF with preserved rank $k$ as LPAF-$k$. We select $k$ from \{260, 130, 80\}, which corresponds to \{50\%, 25\%, 16\%\} of original parameters. We use  Facebook fvcore to compute FLOPs for measuring the computation cost. See \tabref{table:stats} for details. We set the number of layers in distillation baselines to \{6, 3, 2\} and tune the sparsity-relevant hyperparameters in structured pruning baselines such that  their final remaining parameters corresponds to \{50\%, 25\%, 16\%\} of BERT-base's parameters and the FLOPs roughly equal to LPAF-\{260, 130, 80\}.

% \begin{table}[th]
% 	\scriptsize
% 	\centering
% 	\begin{tabular}{l|cc|cc}
% 		\toprule
% 		Task& \multicolumn{2}{c|}{Others} & \multicolumn{2}{c}{SQuAD v1.1} \\
% 		\midrule
% 		Metric & \% of Param.           & FLOPs               & \% of Param.      & FLOPs        \\
% 		\midrule
% 		BERT-base & 100               & 7.4G                & 100        & 35.4G        \\
% 		$T_{sparse}$ & 100               & 7.4G                & 100        & 35.4G        \\
% 		\midrule
% 		LPAF-120   & 22       & 1.9G          & 22  & 10.3G  \\
% 		LPAF-100   & 19      & 1.6G          & 19  & 9.1G   \\
% 		LPAF-80    & 15       & 1.3G          & 15  & 7.9G   \\
% 		LPAF-60    & 12        & 1.0G         & 12  & 6.6G   \\
% 		LPAF-40    & 8        & 0.7G        & 8  & 5.3G \\ 
% 		\bottomrule
% 	\end{tabular}
% 	\caption{Percentage of parameters and FLOPs. $T_{sparse}$ has identical parameters and FLOPs as BERT-base because it only sets certain weights to zero without actually eliminating them from the computation.}
% 	\label{table:stats}
% \end{table}






\subsection{Main Results}

\label{sec:main}


\tabref{table:all} and \tabref{table:squad} summarize the results on GLUE, SQuAD v1.1/v2.0. Under 50\% parameter budget, as the previous state-of-the-art algorithms in task-specific distillation and structured pruning, CKD, MetaDistil, and CoFi deliver the strongest performance on certain GLUE tasks~(i.e., RTE, CoLA, SST-2) respectively, while LPAF performs the best on the others. As the compression rate increases, all distillation methods suffer from evident accuracy declines compared to structured pruning and matrix factorization methods, suggesting the difficulty of knowledge transfer when the capacity of the student model is insufficient. Compared with ISP and CoFi which remove entire attention heads and neurons, LPAF operates at a finer-grained matrix level and is therefore more flexible. Compared with FLOP and BP$_{\text{hybrid}}$ which remove rank-1 component or consecutive blocks of weight matrices, LPAF can effectively utilize the accurate low-rank subnetwork identified by UP$_{\text{first}}$ and maximally recover task accuracy via the proposed optimizations. Through controlled ablation, we show that low-rank sparsity~(step-1) plays the most critical role in preserving task accuracy, while sparsity-aware SVD and mixed-rank fine-tuning yield further improvements via more accurate sparse matrix approximation and regularized training.


\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{l|cc}
		\toprule
		%		\hline
		Task &\textbf{SQuAD v1.1 }&\textbf{SQuAD v2.0  }                                           \\
		\midrule
		%		\hline
		\% Params.   &50\% ~25\%  ~16\% &50\% ~25\%  ~16\%    \\
		%		\hline
%		\midrule
				\hline
		DistilBERT  &85.8~ 78.0~ 66.5 &68.2~ 62.5~ 56.2    \\
		TinyBERT    &82.5~ 58.0~ 38.1 &72.2~ 85.3~ 78.2         \\
		%		\hline
%		\midrule
				\hline
		%		PKD  &65.5~ 59.2~ 53.8 &81.9~ 76.2~ 71.3                                           \\
		Theseus &84.2~ 72.7~ 63.2 &71.2~ 77.2~ 72.8                                    \\
		%		CKD   &xx.x~ xx.x~ xx.x &xx.x~ xx.x~ xx.x                           \\
		%		MetaDistil  &\textbf{69.0}~ 66.7~ 61.0 &86.8~ 81.8~ 77.3                            \\
		%		\hline
%		\midrule
				\hline
		ISP  &86.0~ 84.9~ 81.9 &76.9~ 74.1~ 71.8                                            \\
		FLOP  &88.1~ 85.7~ 81.5 &77.7~ 75.3~ 71.3                                            \\
		CoFi  &87.7~ 86.8~ 84.9 &77.3~ 73.9~ 72.4                                               \\
		%		DynaBERT &67.9~ 61.2~ ~~--~~~ &86.0~ 83.8~ ~~--~~~                                         \\
		%		\hline
		
				\hline
%		\midrule
		SVD$_{\text{Ft}}$  &87.8~ 85.5~ 81.1 &77.4~ 70.1~ 70.0     \\
		%		\hline				
		
		%				\bottomrule
		LPAF~(ours)  &\textbf{89.2}~ \textbf{87.2}~ \textbf{85.7} &\textbf{79.1}~ \textbf{77.2}~ \textbf{75.1}  \\
%				\hline
		\midrule
		BERT-base  &~~88.2~~~ &~~77.9~~~  \\
		%		~~-w/o Step-1   &xx.x~ 32.1~ 21.1 &xx.x~ 32.1~ 21.1  \\
		%		~~-w/o Step-2  &65.3~ 32.1~ 21.1 &86.0~ 32.1~ 21.1   \\
		%		~~-w/o Step-3   &65.0~ 32.1~ 21.1 &84.8~ 32.1~ 21.1  \\
		\bottomrule
		%		\hline
	\end{tabular}
	\caption{SQuAD results~(average of 3 runs) of all compared baselines applied on BERT-base. The best results~($p$-value ~<~0.05) are \textbf{bolded}.}
	\label{table:squad}
\end{table}
\subsection{Analysis}
\label{sec:analysis}
In this subsection, we conduct a comprehensive analysis to shed light on the effectiveness of each component in our proposed LPAF framework, namely first-order pruning, sparsity-aware SVD, and mixed-rank fine-tuning.


\subsubsection{Effect of Different $\text{T}_\text{sparse}$} 	
%Recall that we choose the low-rank sparse model $T_{sparse}$ with at least 97\% of the fine-tuned BERT's performance.

 We analyze how different $T_\text{sparse}$ impact the final task performance of LPAF without sparsity-aware SVD and mixed-rank fine-tuning.
The results on SST-2 are summarized in \tabref{table:diffsparse}. As we decrease $v$, $T_\text{sparse}$ becomes more sparse and its rank also monotonically decreases. We observe that for a fixed $k$, the performance of LPAF-$k$ resembles a unimodal distribution of the rank of $T_\text{sparse}$: as the rank gets too high, the increased approximation error overturns the benefit of improved accuracy; when the rank is too low, the drop of accuracy also overturns the benefit of decreased approximation error. Generally, the best performance of LPAF-$k$ for a larger $k$ is achieved at a higher rank of $T_\text{sparse}$ compared to that of a smaller $k$.

\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{ccccc}
		\toprule
		\multicolumn{2}{c|}{T$_{\text{sparse}}$}    & \multicolumn{3}{c}{LPAF}                                                   \\ 
		\midrule
		$v$  & \multicolumn{1}{c|}{rank} & $k$=260 & $k$=130                        & $k$=80                          \\ 
		\midrule
		0.50 & \multicolumn{1}{c|}{705}  & \textbf{91.3}    & 89.9                           & 86.8                            \\
		0.25 & \multicolumn{1}{c|}{557}  & 91.1    & \textbf{90.1} & 87.2                            \\
		0.10 & \multicolumn{1}{c|}{377}  & 89.7    & 89.5                           & \textbf{89.3 } \\ 
		\bottomrule
	\end{tabular}
   \caption{Effect of different T$_\text{sparse}$ on SST-2 dataset. Generally, the more aggressive the compression configuration is, the smaller the optimal $v$ will be located.}
   \label{table:diffsparse}
\end{table}

\subsubsection{Effect of Sparsity-aware SVD}

In our sparsity-aware SVD, the reconstruction error of each parameter $\bm{W}_{i,j}$ is weighted by its importance score $\bm{S}_{ij}$. To examine its effectiveness in factorizing sparse matrix, we experiment with two variants on SST-2 dataset: (1) $\bm{S}$ is replaced by coarse-grained binary score $\bm{M}$; (2) non-weighted vanilla SVD.

\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{l|lll}
%		\hline
\toprule
		& \multicolumn{3}{c}{Before$\rightarrow$After Step-3}               \\ 
		\midrule
		Strategy/$k$ & \multicolumn{1}{c}{260} & \multicolumn{1}{c}{130} & \multicolumn{1}{c}{80} \\ 
		\midrule
		\multicolumn{1}{c|}{w/ $\bm{S}$}    & \multicolumn{1}{l}{\textbf{81.4}$\rightarrow$\textbf{92.4}} & \multicolumn{1}{l}{\textbf{79.9}$\rightarrow$\textbf{90.7}} & \textbf{77.5}$\rightarrow$\textbf{89.7} \\ 
		\multicolumn{1}{c|}{w/ $\bm{M}$}    & \multicolumn{1}{l}{81.0$\rightarrow$92.1}     & \multicolumn{1}{l}{79.7$\rightarrow$90.4}     & 77.2$\rightarrow$89.3     \\ 
		\multicolumn{1}{c|} {Vanilla} & \multicolumn{1}{l}{79.1$\rightarrow$91.4} & \multicolumn{1}{l}{77.9$\rightarrow$89.2} & 75.9$\rightarrow$88.8 \\ 
%		\hline
\bottomrule
	\end{tabular}
	\caption{Ablation study of sparsity-aware SVD on SST-2 dataset. w/ $\bm{S}$ indicates the continuous importance score. w/ $\bm{M}$ stand for a simple binary weight strategy. Vanilla refers to the default sparsity-unaware setting.}
	\label{table:diffsvd}
\end{table}

 In \tabref{table:diffsvd} we show that by informing the sparse matrix factorization process with importance score, 
 more task-relevant information can be retained at the beginning~(Step-2).  After further re-training,  weighting by importance score yields the best results under all choices of $k$, and a simple binary weighting strategy using $\bm{M}$ also brings improvement compared to vanilla SVD. This means that our sparsity-aware SVD is still applicable even when $\bm{S}$ is unavailable.

\subsubsection{Effect of Mixed-rank Fine-tuning} In \tabref{table:wwomixedrank}, we examine the effectiveness of mixed-rank fine-tuning. Results show that mixed-ranking fine-tuning consistently brings improvement over standard fine-tuning under all choices of $k$. Adding the consistency objective $\mathcal{L}_{c}$  stabilizes training and leads to further improvement. 
\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{l|lll}
		\toprule
%\hline
		Fine-tuning Method & $k$=260& $k$=130 & $k$=80   \\
		\midrule
%		\hline
		mixed-rank &\textbf{92.4}     & \textbf{90.7}   & \textbf{89.7}  \\
		- w/o $\mathcal{L}_{c}$  &91.9  &89.8   &89.1  \\
		\midrule
%\hline
		vanilla fine-tuning &91.4 & 89.5   &88.8  	 \\
		\bottomrule
%\hline
	\end{tabular}
	\caption{Ablation of mixed-rank fine-tuning on SST-2 dataset. Mixed-rank fine-tuning consistently brings improvement under various choices of preserved rank $k$. Adding the consistency regularization objective $L_c$ leads to further performance gains.}
	\label{table:wwomixedrank}
\end{table}

We also study the effect of using different values of $p_\text{init}$ on the performance of mixed-rank fine-tuning. \tabref{table:diffp} reveals that: (1) for $T_\text{factorized}$ with smaller $k$, it prefers a relatively large $p_\text{init}$ because its model capacity is largely reduced and it can benefit more from mixed-ranking fine-tuning to improve generalization; (2) for $T_\text{factorized}$ with larger $k$, a smaller $p_\text{init}$ is more favorable because its higher capacity makes it less likely to converge into bad local minimum; (3) setting $p_\text{init}$ to zero makes our method loses regularization effect brought by gradient-level interaction between factorized sub-matrices and original sparse matrix, thus degenerating performance under all compression ratios.
\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{cl|lll}
		\toprule
%\hline
		&$p_\text{init}$ & $k$=260     & $k$=130 & $k$=80   \\
		\midrule
%\hline
		 & 0.7 & 92.1 & 90.2  & \textbf{89.7}  \\
		& 0.5& 92.1 & 90.5   & 89.5  \\
		& 0.3& 92.2 & \textbf{90.7}   & 89.0 \\
		& 0.1& \textbf{92.4} & 90.6   & 89.0 \\
		& 0.0  & 91.8   & 90.0   & 89.3 \\
		\bottomrule
%\hline
	\end{tabular}
	\caption{Ablation of  different $p_\text{init}$ on SST-2 dataset. Setting $p_{init}$ to 0 is equivalent to LPAF without mixed-rank fine-tuning~(but still benefits from regularized dropout~\cite{rdrop}).}
	\label{table:diffp}
\end{table}


\subsection{Applicability to Other PLMs}
To verify the general utility of LPAF,  we apply it to compress an already compact 12-layer and 384-dimensional pre-trained MiniLM~\footnote{\url{https://github.com/microsoft/unilm/tree/master/minilm.}}~\cite{minilm} model with 21.5M parameters into 50\% of original parameters and FLOPs. The results are shown in \tabref{table:roberta}. For LPAF, we observe a similar low-rank phenomenon~(281 on average) in the sparse model, demonstrating the general low-rank sparse pattern induced by Step-1 in the proposed LPAF, i.e., first-order unstructured pruning. LPAF performs better than or on par with SVD$_{\text{Ft}}$ and the strongest task-specific distillation method CKD on three representative GLUE tasks, which confirms its general applicability to pre-trained language models of different scales.
%\begin{table}[t]
%	\centering
%	\scriptsize
%	\begin{tabular}{c|ccc}
%		%		\toprule
%		\toprule
%		Task & SQuAD v1.1   & QNLI     & MNLI-m/mm         \\
%		%		\midrule
%		%		\midrule
%		\midrule
%		CKD       & xx.x          & 89.3          & 83.0/83.7        \\
%		
%		%		\midrule
%		\midrule
%		SVD$_{\text{Ft}}$           & 88.2          & 89.6          & 82.8/83.0        \\
%		LPAF~(ours)                  &\textbf{88.8}          & \textbf{90.5}          & \textbf{84.4/84.5}          \\
%		\midrule
%	    MiniLM           & 89.6          & 91.2          & 85.0/85.2         \\
%		%		\bottomrule
%		\bottomrule
%	\end{tabular}
%	\caption{Results~(average of 3 runs) of  compressing MiniLM into 50\% of original parameters and FLOPs.}
%	\label{table:roberta}
%\end{table}


\begin{table}[t]
	\centering
	\footnotesize
	\begin{tabular}{c|ccc}
		%		\toprule
		\toprule
		Task & \textbf{SST-2}   & \textbf{QNLI}     & \textbf{MNLI-m/mm}         \\
		%		\midrule
		%		\midrule
		\midrule
		CKD       & \textbf{91.2}          & 89.3          & 83.0/83.7        \\
		
		%		\midrule
		\midrule
		SVD$_{\text{Ft}}$           & 90.0          & 89.6          & 82.8/83.0        \\
		LPAF                  &91.1          & \textbf{90.5}          & \textbf{84.4/84.5}          \\
		\midrule
		MiniLM           & 92.4          & 91.2          & 85.0/85.2         \\
		%		\bottomrule
		\bottomrule
	\end{tabular}
	\caption{Results~(average of 3 runs) of  compressing MiniLM. Best results are \textbf{bolded}~($p$-value<0.05).}
	\label{table:roberta}
\end{table}


