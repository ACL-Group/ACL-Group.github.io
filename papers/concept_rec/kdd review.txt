SUBMISSION: 265
TITLE: Inferring User Needs in E-commerce Recommendation

-------------------------  METAREVIEW  ------------------------
I tend to agree with the reviewers that the authors are conflating the influence of explainable recommendations on user behavior with using some available meta-information to improve recommendation.

The work hinges on the availability of the "concept graph" which one can safely assume is non-trivial, yet while a sample of the concept graph is made available there is very little insight provided into how this was defined and gets attached to underlying items. 

Agree with the reviewers that offline and online experiments need to be better aligned. Why FTRL in online setting, why not have an implementation of it offline? As it exists, I doubt that the paper can be very useful to other practitioners who don't have access to their own "concepts".



----------------------- REVIEW 1 ---------------------
SUBMISSION: 265
TITLE: Inferring User Needs in E-commerce Recommendation
AUTHORS: Xusheng Luo, Yonghua Yang, Keping Yang and Kenny Zhu

----------- Does the paper belong to one of the two categories: Deployed or Evidential? -----------
SCORE: 1 (yes)
----------- Category DEPLOYED. -----------
SELECTION: yes
----------- Category EVIDENTIAL. -----------
SELECTION: no
----------- Please justify your answer for the category you chose -----------
The paper discusses a recommender systems already on AB test in a large e-commerce website. I'm not sure if it's deployed for an extended period of time though. It does solve a real-world problem.
----------- Is there a clearly defined audience / group of users that will benefit from the solution presented in the paper? -----------
SCORE: 1 (Yes)
----------- Please justify your answer on the paper's anticipated audience -----------
Broadly Web practitioners will find this work relevant. More specifically, e-commerce industry.
----------- Originality/Novelty -----------
SCORE: 2 (Minor improvements over existing solutions)
----------- Please justify your score for Originality/Novelty -----------
The novelty is not clearly flashed out in the paper.
----------- Technical Quality -----------
SCORE: 3 (Fair technical work, with some flaws in design and/or validation)
----------- Please justify your score for Technical Quality -----------
Both offline and online evaluations. Good implementation details in most parts. Good choice of baselines for offline part. The online part is lacking though.
----------- Impact/Outreach -----------
SCORE: 3 (Fair solution to an important problem, but there are alternatives around)
----------- Please justify your score for Impact/Outreach -----------
Recommendations on e-commerce are clearly important, but there is a variety of solutions around, also for providing explainability.
----------- Clarity of Presentation -----------
SCORE: 4 (Paper is understandable but minor changes would make it accessible to a broader audience)
----------- Reproducibility -----------
SCORE: 3 (Yes - Fair. The information provided represents a fair effort to make it possible for readers to reproduce the results.)
----------- Please justify your answer regarding Reproducibility -----------
Datasets are not public, but a fairly good amount of statistics is provided.
----------- Overall Evaluation -----------
SCORE: -1 (An OK paper, but likely not good enough for KDD (weak reject). I vote for rejecting it, although I would not be upset if it were accepted.)
----------- Overall Review -----------
The paper proposes an approach for recommender systems on e-commerce that takes advantage of a "concept" notion, which generalizes user needs. 
A graph embedding approach is applied to generate recommendations. Offline and online experiments at large scale on TaoBao show the superiority over baselines.

Strengths: 
+ The paper is highly relevant to the applied data science track 
+ Addresses real-world problem at large scale. 
+ Good level of implementation details.
+ Good connection of the knowledge graph, metapath, and embedding idea to e-commerce recommendations. 
+ The system has been deployed on a leading e-commerce site. 
+ Some state-of-the-art baselines have been used (although it is not clear the comparison is entirely fair in the current setup).

Weaknesses:
- I did not agree with the motivation. The authors need to tone down some of the statements. There have been many prior attempts in RecSys to address novelty and explainability. Many methods are based on latent relationships between users and items rather than plain CF.
- Related work is lacking. Especially relevant literature from the e-commerce domain. Similar notions to the "concept" idea have been defined for various purposes in this domain (browse nodes is one example)
- Novelty is not clear to me. The authors must flash this out. What do you claim as your contribution over the state-of-the-art? The methods used seem standard. Also, I did not agree with your statement that you are the first to seek to infer user needs in e-commerce. This statement is too broad.  
- Some details are missing from the process of concept definition, which is very basic to this work. How did you define the concepts? 
- I was not convinced that the explanations provided are indeed compelling as "promised" in the introduction. I did not see how they provide a a level of transparency beyond the common in this area. 
- How come purchase rate not used as an evaluation metric? Is it not the ultimate goal in e-commerce (rather than clicks)? Also, user engagement is not taken into account in the online experiment. Overall, in such a paper I though the online experiment can provide a much bigger contribution.



----------------------- REVIEW 2 ---------------------
SUBMISSION: 265
TITLE: Inferring User Needs in E-commerce Recommendation
AUTHORS: Xusheng Luo, Yonghua Yang, Keping Yang and Kenny Zhu

----------- Does the paper belong to one of the two categories: Deployed or Evidential? -----------
SCORE: 1 (yes)
----------- Category DEPLOYED. -----------
SELECTION: yes
----------- Category EVIDENTIAL. -----------
SELECTION: no
----------- Please justify your answer for the category you chose -----------
The concept recommendation is a realistic problem and online A/B test is conducted in the paper. The authors also claim that their model has been deployed in Alibaba recommender and search system.
----------- Is there a clearly defined audience / group of users that will benefit from the solution presented in the paper? -----------
SCORE: 1 (Yes)
----------- Please justify your answer on the paper's anticipated audience -----------
The users in the area of recommender systems will benefit from the solution presented in the paper.
----------- Originality/Novelty -----------
SCORE: 3 (Incremental)
----------- Please justify your score for Originality/Novelty -----------
The core idea of the proposed model is presented in Figure 3. The user embedding and concept embedding are trained by embedding lookup layer, path embedding is learned by aggregating multiple types of meta-path. Attention for three-way interaction is applied as the weights to aggregate different elements. These techniques are proposed by others already and the authors apply them with minor changes.
----------- Technical Quality -----------
SCORE: 3 (Fair technical work, with some flaws in design and/or validation)
----------- Please justify your score for Technical Quality -----------
The effectiveness of the proposed algorithm is evaluated, compared to some baselines. The attention mechanism is also evaluated by ablation study. However, the baselines in offline evaluation, and especially in online experiment are not very strong. The "Interpretability" is not well explained.
----------- Impact/Outreach -----------
SCORE: 4 (A good solution to a rather narrowly defined problem for a small target group of users)
----------- Please justify your score for Impact/Outreach -----------
By constructing "concepts", the recommender system, which aims to recommend concept, is able to recommend novel items related to the chosen concept. By constructing concept net, the model is able to learn user-concept relationship from user-item behavior and item-concept relationships.
----------- Clarity of Presentation -----------
SCORE: 4 (Paper is understandable but minor changes would make it accessible to a broader audience)
----------- Reproducibility -----------
SCORE: 3 (Yes - Fair. The information provided represents a fair effort to make it possible for readers to reproduce the results.)
----------- Please justify your answer regarding Reproducibility -----------
The model details and experiment setting provide some help to re-produce the model.
----------- Overall Evaluation -----------
SCORE: 1 (An OK paper, good enough for KDD (weak accept). Accept if possible, although I would not be upset if it were rejected.)
----------- Overall Review -----------
Positive:
1. The authors propose a framework to recommend so-called "concepts", instead of recommending items, to avoid recommending redundant items.
2. Path embedding is utilized to model the relationship between users and concepts, in which attention on three-way interaction is proposed to model the weights of different elements during the interaction modeling.
3. Offline and online experiments are conducted to evaluate the proposed model. Especially, online AB testing proves the performance of the model in real-world applications.

Negative:
1. Constructing "concepts" needs huge amount of efforts and may be very tricky. The paper does not provide any hint or suggestion on it. The meta-paths are still crafted manually, which needs heavy labor work.
2. The techniques applied in the model are incremental. The user embedding and concept embedding are trained by embedding lookup layer, path embedding is learned by aggregating multiple types of meta-path. Attention for three-way interaction is applied as the weights to aggregate different elements. These techniques are proposed by others already and the authors apply them with minor changes.
3. Some concerns in the experiments:
3.1. In offline evaluation, there are many deep models that are performed better than Wide&Deep. However, the authors do not choose such models.
3.2. In Figure 5, CptInfer(-att.cube) is not displayed and compared. Is there any specific reason for that?
3.3. In Table 4, why not use the performance on test dataset, in order to be consistent with the performance result in Table 3?
3.4. "Interpretability" is only used for analyse users, but still it is not used when recommending items, to improve users experience.
3.5. The baseline in online experiments is too week, i.e., FTRL. Maybe the authors should explain why the baseline is FTRL. Maybe FTRL is well tuned, so as to beat other models?
3.6. In online serving part, it is a little confusing when recommending concepts and recommending items.



----------------------- REVIEW 3 ---------------------
SUBMISSION: 265
TITLE: Inferring User Needs in E-commerce Recommendation
AUTHORS: Xusheng Luo, Yonghua Yang, Keping Yang and Kenny Zhu

----------- Does the paper belong to one of the two categories: Deployed or Evidential? -----------
SCORE: 1 (yes)
----------- Category DEPLOYED. -----------
SELECTION: yes
----------- Category EVIDENTIAL. -----------
SELECTION: no
----------- Please justify your answer for the category you chose -----------
the system is deployed in an e-commerce platform.
----------- Is there a clearly defined audience / group of users that will benefit from the solution presented in the paper? -----------
SCORE: 1 (Yes)
----------- Please justify your answer on the paper's anticipated audience -----------
Yes, the audience is recsys people in e-commerce.
----------- Originality/Novelty -----------
SCORE: 4 (Novel approach, using mature components in a novel way)
----------- Please justify your score for Originality/Novelty -----------
Very original approach in introducing new types of nodes for better transparency. However, the solution works with embeddings, which are latent and non-interpretable by humans. So the initial research question is not being addressed.
----------- Technical Quality -----------
SCORE: 3 (Fair technical work, with some flaws in design and/or validation)
----------- Please justify your score for Technical Quality -----------
The technical part in general is ok. But the introduction of latent features is not helping the interpretability by the user. Also the evaluation is flawed (see my omments below)
----------- Impact/Outreach -----------
SCORE: 3 (Fair solution to an important problem, but there are alternatives around)
----------- Please justify your score for Impact/Outreach -----------
there is a need for a better understanding of what's happening with the user. Plain digital behaviourism does not get very far. For example, from the digital behaviour Netflix camt distinguish between value and addiction. The paper attempts to address this but fails as it is just another behavioristic approach.
----------- Clarity of Presentation -----------
SCORE: 3 (Presentation is OK but non-experts would have difficulties reading this paper)
----------- Reproducibility -----------
SCORE: 3 (Yes - Fair. The information provided represents a fair effort to make it possible for readers to reproduce the results.)
----------- Please justify your answer regarding Reproducibility -----------
The descriptions are ok but the data is missing.
----------- Overall Evaluation -----------
SCORE: -1 (An OK paper, but likely not good enough for KDD (weak reject). I vote for rejecting it, although I would not be upset if it were accepted.)
----------- Overall Review -----------
Very original approach in introducing new types of nodes for better transparency. However, the solution works with embeddings, which are latent and non-interpretable by humans. So the initial research question is not being addressed.

The goal here should not be to improve stanadrd metrics such as AUC over recommendations but the user satisfaction, retention etc. Also, the interpretability described in Sect 5.5 is an offline approach and hence plain wrong. You need to ask real users if something is interpretable or not. 

Unfortunately also the online study has been conducted in a data-driven fashion. The research question that you posed at the beginning requires real user ansers and not observing the behaviour. Hence, such an AB testing approach is not for this research question
