\section{Conclusion}


Dialogue summarization is receiving increasing demands in recent years for releasing the burden of manual summarization and achieving efficient dialogue information digestion.
It is a cross-research direction of dialogue understanding and summarization.
Abstractive text summarization is a natural choice for dialogue 
summarization due to the characteristics of dialogues, including information 
sparsity, context-dependency, and the format discrepancy between 
utterances in first person and the summary from the third point of view. 
With the success of neural-based models especially pre-trained language models, 
the quality of generated abstractive dialogue summaries appears to be promising
for real applications.
This survey summarizes a wide range of papers on the subject.
In particular, it presents a hierarchical taxonomy for task scenarios, made up of two broad categories, i.e., open-domain dialogue summarization and task-oriented dialogue summarization.
A great many techniques developed in different approaches are categorized into three directions, including injecting pre-processed features, designing self-supervised tasks, and using additional data.
We also collect a number of evaluation benchmarks proposed so far and provide a deep analysis with valuable future directions.
This survey is a comprehensive checkpoint of dialogue summarization research thus far
and should inspire the researchers to rethink this task and 
search for new opportunities, especially with current LLMs. It is also a useful guide for engineers 
looking for practical solutions.
