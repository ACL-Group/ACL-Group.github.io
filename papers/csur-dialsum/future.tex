\subsection{Future Directions}
\label{sec:future}


We discuss some possible future directions and organize them into
three dimensions: \textit{task scenarios}, \textit{approaches} and \textit{evaluations}. 
%\KZ{Maybe explicitly say that some of the things are our own
%insight, instead of confusing them with summary of other people's future work.}


\subsubsection{More Complicated and Controllable Scenarios}

More newly explored scenarios such as multilingual dialogue summarization, multi-modal dialogue summarization, multi-session dialogue summarization are worth researching. In addition, we put forward {personalized dialogue summarization} as a novel future direction that pays special attention to speaker or reader-related information neglected by previous works.

\textbf{Multilingual dialogue summarization} is a rising topic with few related papers. It considers multiple languages existing in the dialogue and summary on three levels of granularity. First, the most fine-grained one considers interactions between peers who are fluent in multiple languages resulting in the intra-utterance multilingual phenomenon is called ``code-mixing'' strictly~\cite{mehnaz2021gupshup}. Second, dialogues happening among multinational participants where they use their mother tongue to communicate lead to the inter-utterance multilingual phenomenon is called ``code-switching''~\cite{mehnaz2021gupshup}, i.e., mix-lingual in~\cite{feng2022msamsum}'s work. Third, summarizing a monolingual dialogue in a different language is called ``cross-lingual'' in~\citet{wang2022clidsum}. Different multilingual summarization datasets have been constructed for these settings based on the SAMSum~\cite{gliwa2019samsum}, DialogSum~\cite{chen2021dialsumm}, MediaSum~\cite{zhu2021mediasum} and QMSum~\cite{zhong2021qmsum} by either human annotations~\cite{wang2022clidsum,mehnaz2021gupshup,chen2022cross} or automatic machine translation~\cite{feng2022msamsum}. Preliminary studies in these papers show the potential of end-to-end multilingual models, such as mBART~\cite{tang2021multilingual},  in this task and their weaknesses in low-resource languages, poor domain transfer ability among datasets~\cite{wang2022clidsum} and decreases in the performance when processing multiple languages with a single model~\cite{feng2022msamsum}. \citet{chen2022cross} proposed the cross-lingual conversation summarization challenge, paving the way for the prosperity of research in this direction. Our survey focuses on the taxonomy of approaches for monolingual dialogue summarization, which we expect to provide a backbone for this raising area.

%Moreover, code-switched dialogue summarization is also an important application scenario with only a single dataset named GupShup~\cite{mehnaz2021gupshup} released so far. It is sourced from SAMSum, containing Hindi-English conversations and annotated summaries in English and Hindi-English.  
%\textbf{GupShup}~\cite{mehnaz2021gupshup} is sourced from SAMSum for code-switched dialogue summarization. \JQ{ Summarization problem in various languages will be another topic worth mentioning and exploring (multilingual, translation, code-switching etc.).}


\textbf{Multi-modal dialogue summarization} refers to dialogues occurring in 
multi-modal settings, which are rich in non-verbal information 
that often complements the verbal part and therefore contributes to 
summary contents. Some early work did research on speech dialogue summarization. 
However, most of them only extract audio features from speech and text 
features from ASR transcripts independently to produce extractive summaries. 
There is also work on video summarization~\cite{hussain2021video} focusing 
on highlighting critical clips while a textual summary is not considered.
Fusing the synchronous and asynchronous information among modalities 
is still challenging. AMI and ICSI are still valuable resources for 
research on multi-modal dialogue summarization.
% This have been recommended by \citet{feng2021survey}. 


\textbf{Multi-session dialogue summarization} is required when conversations 
occur multiple times among the same group of speakers. 
The Information mentioned in previous sessions becomes their consensus and 
may not be explained again in the current session. 
The summary generated merely from the current session is unable to 
recover such information and may lead to implausible reasoning. 
A similar multi-session task has been proposed by~\citet{xu2021beyond}. 
This setting also has some correlations with life-long learning~\cite{shuster2020deploying,liu2021lifelong}.
Such multi-session dialogues are just open-domain dialogues in drama or TV shows, such as SubTitles~\cite{malykh2020sumtitles} and SummScreen~\cite{chen2021summscreen}. However, current approaches generally break down the long dialogue and summary into shorter chunks due to the limitation of current models. 
For task-oriented scenarios, it is also common in real life. For example, the customer may repeatedly ask for help from the agent for the same issue that hasn't been solved before. An updated summary considering all of the questions and the latest answers can remind participants of the long dialogue history and can therefore facilitate the negotiation process. As far as we know, none of the papers have discussed this problem before in the area of dialogue summarization.


%Multi-modal Dialogue Summarization

%Multi-session Dialogue Summarization

%Controllable Dialogue Summarization
%intent

%speech dialogue summarization: more challenges without transcripts 人物识别、句子识别等，与ASR等技术融合？目前已有的大多数是extractive的with complicated framework :Automatic Summarization of Open-Domain Multiparty Dialogues in Diverse Genres; Extractive Summarization of Meeting Recordings; Summarization of spontaneous
%conversations
%\KZ{A bit sudden to bring in personalized dialogues?} 

Recent work mainly focuses on summarizing the dialogue content but ignores the speaker-related or reader-related information. 
However, these are significant and negligible factors as reflected by diversified reference summaries by different annotators, both in the aspect of styles and the content selection.
Thus, we propose \textbf{personalized dialogue summarization}, which can be understood in two ways. 
On the one hand, a personalized dialogue refers to the consideration of 
personas for interlocutors in dialogues. For example, the character 
role-playing information is indispensable information for generating 
summaries given dialogue from CRD3~\cite{rameshkumar2020storytelling} in drama conversation scenarios.
On the other hand, it refers to generating different dialogue summaries for 
different readers or speakers. \citet{tepper2018personal} is a demo paper raising the 
requirements for personalized chat summarization. They did the first trial 
on this task considering the personalized topics of interests and social ties 
during the selection of dialogue segments to be summarized. 
Some task-oriented datasets, such as CSDS~\cite{lin2021csds}, contain summaries from both the user and agent aspect are similar
to problem here. Recent work from~\citet{lin2022other} solved this problem by adding the cross attention interaction and the decoder self-attention interaction to interactively acquire other roles' critical information. This work is designed only for scenarios with two roles.
Open-domain scenarios pose more challenges with a variety number of speakers and summary readers from different social groups, raising an
expectation for corresponding datasets and approaches, which is a possible interdisciplinary research orientation.


\subsubsection{Innovations in Approach} 
Approach innovations include four parts:  feature analysis, person-related features, generalizable and non-labored techniques, and the robustness of models.

%more high-level features: personality, emotion

%comparison of features within the same category or cross-category

%more trails on self-supervised tasks and transfer learning from other corpus


From \secref{sec:observations}, we notice that although tens of papers 
introduce different features for dialogue summarization, there is still 
a lot of work to do. Comprehensive experiments to \textbf{compare the} \textbf{features} and 
their combinations upon the same benchmark are still needed, 
for features both in the same category or across categories. 
One can consider unifying the definition of similar features, 
such as different classification criteria of discourse relations or different graphs emphasizing the phrase-level semantic flows. 
These analyses would help in designing features in new applications and interpreting dialogue models.
%It's also \KZ{substantial for explanations of dialogue modeling, and also helpful for feature selection} in new applications. 


More \textbf{person-related features} can be introduced to this task, 
such as speaker personalities~\cite{zhang2019consistent} and 
emotions~\cite{majumder2019dialoguernn}. Knowing the background of a speaker can help better understand the motivation behind each utterance and potentially influential in the selection of content to be summarized, especially for personalized dialogue summarization.
A plug-and-play mechanism on top of the decoder for persona-controlled summarization may be an ideal solution inspired by~\citet{DathathriMLHFMY20}.
 


%From the taxonomy of approaches in \secref{sec:approach}, 
\textbf{Generalizable and non-labored techniques} have attracted increasing attention on other dialogue modeling tasks, 
such as multi-turn response selection~\cite{xu2021learning} and 
dialogue generation~\cite{zhang2019consistent}. 
These works proposed different self-supervised training tasks, largely releasing the requirement on human labor.
From the distribution of technical papers for dialogue summarization in Figure~\ref{fig:technical}, we can see that approaches overwhelmingly 
rely on injecting pre-processed features. 
However, most of these approaches are labor-intensive since training a labeling tool requires human annotations of corresponding labels on some in-domain training data or needs trial and error to find the best hyper-parameters for transferring to the target summarization domain. Otherwise, it will suffer from error propagation and lead to poor performance.
Complicated features such as graphs summarized in Section~\ref{sec:graphs} tend to overfit specific domains or current datasets by human observations,
which have poor generalization ability and lead to tiring feature engineering works. More work exploiting the common nature among scenarios and exploring useful representations with language models is expected. 
Recently, large language models~(LLMs) with tons of billions of parameters like GPT-3~\cite{brown2020language} and LLaMA~\cite{touvron2023llama} have demonstrated drastically lifted text generation ability compared to previous pre-trained language models. Specifically, the decoder-only Transformer architecture is adopted as the fundamental backbone for almost all performant LLMs. Scaling decoder-only language models not only simplifies the architectural design dimensions but also enables a unified human-machine interface for various downstream language tasks. 
To accomplish summarization task, LLMs are typically prompted with instructions like ``\textit{Summarize the above article:}'' or chain-of-thought~\cite{cot,wang-etal-2023-element} methods that elicit LLMs to extract various features, e.g., entities, events, that are helpful to compose the final summary. Compared with traditional dialogue summarization systems, LLM-based methods largely alleviate the tedious human labor and can be more generalizable due to the removal of unintended annotation artifacts. Nevertheless, approaches that are previously applied to small pre-trained language models in this survey may also provide inspirations and be adapted to augment LLMs for better dialogue summarization performance.

Approaches nowadays are mostly built on the pre-trained language models, which are sensitive to trivial changes on other tasks~\cite{wang-etal-2022-rely,yan2022robustness}. 
Nevertheless, the \textbf{robustness of models} hasn't been widely-investigated in dialogue summarization. The only work from~\citet{jia2023reducing} proposes that switching an un-grounded speaker name shouldn't influence the models' generation. However, according to their experiments with BART fine-tuned on SAMSum, such changes can lead to dramatically different summaries with information divergence and various reasoning results and show over 14\% changes in Rouge scores. This may further result in unintended ethical issues by showing discrimination against specific groups of names. Thus, analysis of models' robustness and developing more insensitivity approaches are in urgent need for practical applications.


% %addtional data + self-supervised tasks for learning more effective feature with different data 
\subsubsection{Datasets and Evaluation Metrics}
Expectations on datasets and evaluation metrics for dialogue summarization are as follows.

Section~\ref{sec:observations} shows that \textbf{high-quality datasets} 
expedite the research. Besides the expectations on benchmark datasets for the above emerging scenarios, datasets for task-oriented dialogue summarization 
with privacy issues are also sought after. They can be in small sizes with 
real cases after anonymization or can be collected by selecting 
drama conversations in specific scenarios and annotated with 
domain experts. 

\textbf{Evaluation metrics} are significant which guides the improvement directions for upcoming models. However, 
widely used evaluation metrics in Sec.~\ref{sec:evalmetric} are all borrowed from document summarization 
tasks and their effectiveness is unverified. % and largely ignored dialogue characteristics.
Recent work from~\citet{gao2022dialsummeval} re-evaluated 18 evaluation metrics and did a unified human evaluation with 14 dialogue summarization models on SAMSum dataset. 
Their results not only show the inconsistent performances of metrics between document summarization and dialogue summarization, and none of them excel in all dimensions for dialogue summarization, but also raise a warning on rethinking whether recently proposed complex models and fancy techniques truly improve the basic pre-trained language model.
Considering that human evaluation results are difficult to reproduce due to variations of annotator background and unpredictable situations in the annotation progress~\cite{clark2021all},
automatic metrics specially designed for dialogue summarization are 
urgently needed.

Factual errors caused by the mismatch between speakers and events are 
common as a result of complicated discourse relations among utterances 
in dialogues. 
%However, both overlap-based metrics such as Rouge and 
%contextualized representation-based metrics such as BertScore suffer, 
%\KZ{because?}. 
Previous work~\cite{huang2021factual} on document summarization classifies 
factual errors into two types. One is intrinsic errors, referring to the fact 
contradicting the source document. The other is extrinsic errors, 
referring to unrelated facts.  %\KZ{What do you mean by neutral facts?}
This classification is also suitable for dialogue summarization. However, whether their proposed 
QA-based~\cite{wang2020asking} and NLI-based~\cite{falke2019ranking} 
automatic evaluation approaches can be directly transferred to 
dialogue summarization for comparisons between dialogues and 
generated summaries due to their format disparity is still doubtful without thoughtful evaluations.
\citet{tang2021confit} introduced a taxonomy of factual errors for abstractive summarization and did human evaluation based on this categorization without proposing new automatic metrics.
\citet{liu2021controllable} made the first attempt by inputting the 
dialogue and summary together into a BERT-based classifier and claimed 
high accuracy on their own held-out data. But there is still a lack of 
details and comparisons to other methods, such as using bi-encoder 
architectures for the dialogue and summary respectively.
\citet{wang2022analyzing} classified factual errors in a similar way to ~\citet{tang2021confit} and propose a model-level evaluation schema for discriminating better summarization models, which is different from the widely-accepted sample-level evaluation schema that scores generated summaries and can further scoring the model based on their corresponding outputs. They evaluated the model by calculating the generation probability of faithful and unfaithful summaries collected by rule-based transformations based on their taxonomy. The generalization ability for this work among different datasets and scenarios is doubtful, since a similar work for news summarization, FactCC~\cite{kryscinski2020evaluating}, which is a metric trained based on rule-based synthetic datasets shows a poor generalization ability by~\citet{laban2022summac}.
With the strong generation ability of current LLMs, there's also a doubt that whether the previous taxonomy of error types and evaluation metrics is still suitable.
In a word, both \textbf{meta-evaluation benchmarks} and \textbf{evaluation methods} call for innovations.



%Factual Error
%Content preservation

