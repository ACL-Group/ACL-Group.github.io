Referee: 1


Recommendation: Needs Minor Revision


Comments:
In summary, this is a well-written survey paper, focusing on dialogue summarization. In particular, it covers a board range of the dialogue summarization, from datasets to methods.


 However, this paper can be better organized and there are some minor mistakes that should be corrected in the paper.




1. Since in Section 2, Problem Formulation, the authors present categories of different scenarios for dialogue summarization, it can be better to put the Section 7.1 Dataset closely after the Section 2, which is also discussed under the different scenarios categorized in Section 2.
It can help readers better understand the task and different datasets and more readable.


2. It has been widely discussed in the dialogue summarization papers that automatic evaluation metrics cannot measure the model performance well, in particular for consistency (e.g., Zhong et el., (2021) [153]; Chen et al., (2021) [22]; Chen and Yang (2021) [17]). And it has been the most challenging problem that many methods try to address (e.g. Chen and Yang (2021) [17], Huang et al., (2021) [46]). However, such issues are relatively less discussed in this Section, and the current discussion lacks citations regarding the evaluation metrics.


3. in Table 1 (page 22), there should be ''DialogSum'' and ''MediaSum'' instead of ''DialSumm'' and ''MediaSumm'', respectively.


Additional Questions:
Is the information in the paper sound, factual, and accurate?: Yes


If not, please explain why:


Rate how well the ideas are presented (very difficult to understand=1), very easy to understand=5): 5


Rate the overall quality of the writing (very poor=1), (excellent=5): 4


Rate the technical quality (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 4


Rate the relevance to significant areas of research or practice (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 4


Rate the general level of interest (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 4


Does this paper cite and use appropriate references?: No


If not, what important references are missing?: In the Section 7.2 Evaluation Metrics, the authors discuss widely used human evaluation metrics such as informative, conciseness, consistency and coherence, and fine grained metrics.
However, all lack citations.


Should anything be deleted from or condensed in the paper?: No


If so, please explain:


Is the treatment of the subject complete?: Yes


If not, what important details/ideas/analyses are missing?:


Please help ACM create a more efficient time-to-publication process: Using your best judgment, what amount of copy editing do you think this paper needs?: Moderate


Most ACM journal papers are researcher-oriented. Is this paper of potential interest to developers and engineers?: Maybe




Referee: 2


Recommendation: Needs Major Revision


Comments:


Summary:
This paper taxonomizes abstractive dialogue summarizations based on the dialogue scenario, features, and also discusses the different approaches researchers use for pre-processing, data augmentation, and also compiled a list of the datasets.


Strengths:
The review dates back to when meeting summarization was first proposed. Many junior researchers might miss this aspect, so on this front, the review takes in sufficient historical context.


Thorough compilation of datasets and research papers.


Weaknesses:
While there is a thorough compilation of work in abstractive dialogue summarization, this paper does not provide insights that a high quality survey paper is expected to provide.


Evaluation of abstractive dialogue summarization is extremely challenging yet Section 7.2 “evaluation metrics” is very brief and only focused on very standard evaluation approaches in language generation.


Questions and Suggestions:
(1) As there are other summarization tasks such using speech as input, it may be better to use the “abstractive text summarization” instead of “abstractive summarization” in the Introduction.
(2) In Section 2.2 "Comparisons to Document Summarization", it will be good to see the advantages of abstractive vs extractive approaches for dialogue summarization.
(3) On page 7, Line 32, it will be good to see some description of "greedy search" and "beam search".
(4) The source dialogue input length is one factor that affects the choice and performance of abstractive models, authors can discuss it in the corpora and methods sections.
(7) Following question (6), it will be good to see the average source content length (word and utterance) and summary length in Table 1 and Table 2, if applicable. It is straightforward such information can be found in the papers.
(8) The text size in some figures is too small.
(9) When referring to “Transformer”, the word should be capitalized, such as "vanilla transformer layer" in Line 49.
(10) Correction in intro needed: “Medical consultation [140]”: [140] refers to email summaries, which don’t seem to be on medical topics.


(11) While the tables provide comprehensive references, in the inline text, references are not always cited. For example, when first mentioning that there are different scenarios in Section 2.3, limited references are cited when mentioning legal and medical domains.




Additional Questions:
Is the information in the paper sound, factual, and accurate?: Yes


If not, please explain why:


Rate how well the ideas are presented (very difficult to understand=1), very easy to understand=5): 3


Rate the overall quality of the writing (very poor=1), (excellent=5): 3


Rate the technical quality (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 2


Rate the relevance to significant areas of research or practice (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 3


Rate the general level of interest (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 3


Does this paper cite and use appropriate references?: Yes


If not, what important references are missing?:


Should anything be deleted from or condensed in the paper?: No


If so, please explain:


Is the treatment of the subject complete?: No


If not, what important details/ideas/analyses are missing?: See below for detailed comments


Please help ACM create a more efficient time-to-publication process: Using your best judgment, what amount of copy editing do you think this paper needs?: Heavy


Most ACM journal papers are researcher-oriented. Is this paper of potential interest to developers and engineers?: Maybe




Referee: 3


Recommendation: Needs Major Revision


Comments:
This article includes comprehensive research about various aspects in abstractive dialogue summarization. Overall the paper is well-structured, some areas which may require modifications include:
1. Although the main purpose is on abstractive summarization, it will be beneficial to add a little more about extractive methods, and explain why abstractive is more preferred especially in the case of dialogues.
2. On the approaches section, are there any other methods besides neural encoder-decoder seq2seq models?
3. It will be helpful to add a subsection about how to encode speakers information because this is one of the major differences between document and dialogue. e.g. simply encode like normal tokens, use specific tokens, do specific cross-speaker attention, etc.
4. In the evaluations part, it is mentioned that there are models and datasets in various languages. Summarization problem in various languages will be another topic worth mentioning and exploring (multilingual, translation, code-switching etc.).


Other than the above, another major concern is numerous grammar errors and typos. Please see the attached file for some grammar errors to be fixed.
In addition, it will be better to also refer the very first Transformers paper (Attention is all you need.)


Additional Questions:
Is the information in the paper sound, factual, and accurate?: Yes


If not, please explain why:


Rate how well the ideas are presented (very difficult to understand=1), very easy to understand=5): 4


Rate the overall quality of the writing (very poor=1), (excellent=5): 3


Rate the technical quality (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 4


Rate the relevance to significant areas of research or practice (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 4


Rate the general level of interest (very high=5), (high=4), (moderately high=3), (low=2), (very low=1): 4


Does this paper cite and use appropriate references?: Yes


If not, what important references are missing?:


Should anything be deleted from or condensed in the paper?: No


If so, please explain:


Is the treatment of the subject complete?: Yes


If not, what important details/ideas/analyses are missing?:


Please help ACM create a more efficient time-to-publication process: Using your best judgment, what amount of copy editing do you think this paper needs?: Moderate


Most ACM journal papers are researcher-oriented. Is this paper of potential interest to developers and engineers?: Yes