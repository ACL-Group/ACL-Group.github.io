\begin{abstract} 
A gradient boosting decision tree (GBDT), which aggregates a collection of 
single weak learners (i.e. decision trees), is widely used for data mining 
tasks. Because GBDT inherits the good performance from each element of its ensemble, 
much attention has been drawn to the optimization of this model. 
With its popularization, an increasing need for model interpretation arises. 
Besides the commonly used feature importance as a global interpretation, 
feature contribution is a local measure that reveals the relationship between 
a specific instance and the related output. This work
focuses on the local interpretation and proposes an unified computation 
mechanism to get the instance-level feature contributions for GBDT in any version. 
Practicality of this mechanism is validated by the listed experiments as well as 
application in real industry scenarios.
\end{abstract}
