\section{Introduction}
Machine learning has great success in modeling data and making predictions
automatically.
In many real-world applications, we need an explanation rather than
a black-box model.
For example,  when customers apply for a loan on credit, 
the loan officers will compute their credit scores  
based on their historical behaviors. In this case, it's far from enough to 
only show the customers the final scores, and the loan officers would better
give some detailed reasons. 
%A practical model should be 
%self-explanatory and show what the model means. 
%Moreover, while the aggregated statistics are meaningful to data scientists
%and statisticians, domain experts hunger for more straightforward, 
%in-domain explanations.
%Compared to general statistics, the interpretations that demonstrate
%the main reasons for a statistical score can give the data analysts 
%better confidence about the model. 
%They can learn from the in-pack information of models and develop 
%better modifications by inspecting 
%the model as a compaction of the valuable data. 
%
%As a summary, model interpretation is helpful in these aspects:
%\begin{enumerate}
%\item Check the correctness of models in an intuitive way, 
%cross validate with the domain expertise of analysts and 
%promote more trust on the model.
%\item Explain which features are responsible for the better results in 
%the prediction, on occasions where a reason is needed.
%\item Help the analysts to understand the relation between data and outputs, 
%dig into the phenomenons behind the prediction and mine new knowledge 
%from the models.
%\end{enumerate}
%
While most efforts in data mining have been made on improving the accuracy 
and efficiency, which results in better models, little attention is paid to 
model interpretation for these models.
Several common measures for the variable significance have been proposed. 
Gini importance is one of the commonly used importance measure for Random Forest, 
which is derived from the Gini index\cite{breiman2001random}. 
Gini is used to measure impurity between the parent node and
two descendent nodes of samples after splitting. 
The final importance is accumulated from the Gini changes for each feature 
over all the trees in forest. 
This general feature importance(FI), also known as 
{\em global interpretation} , shows the important factors of the target, 
which unpacks the general information in the trained models. 
However, it doesn't take any feature values of an instance into consideration, 
which is insufficient sometimes. 
{\em Local interpretation}, on the other hand, places particular emphasis 
on a specific case and reveals the main causes of each record. 
This type of interpretation makes up for the shortages of the global one.
One approach proposed to define the feature contributions(FC)~\cite{palczewska2013interpreting}
 , which is accumulated from label distribution changes, as a measure of the 
feature impact on the output. The value of feature contribution reveals how 
much a feature contributes and the sign represents whether it's a positive 
impact or not. 
%and saves the analysts from time consuming rule constructing. Different models are put forward to 
%handle a variety of data mining problems. Anti-spam models cope with 
%spams and present the normal emails to users.  Credit card fraud detector distinguishes the 
%abnormal transactions from massive deals and protects the users from losses. Recommendation
%system is a convenience tool to identify the potential purchases of users and makes use of
%the shopping cart data and browsing histories. Fancy models are even applied to face recognition,
%stock transaction and automatic drive which wrap us in all thinkable aspects. Comparing to the
%laboring handcraft, machine learning models could learn from the data automatically and give out  
%more generalized predictions on unseen instances. 
%Machine learning models are divided in many types by their fundamentals. 

%Now that models support human in lots of domains, it's import to ensure the model performance.
%Model evaluation plays the key role in a thorough experience of building the models. Usually, we build
%up a series of models with different algorithm and parameter settings and carry out a model selection
%process to find the best one. Aggregate statistics are the foremost things for model evaluation. As the 
%saying that all models are wrong but some are useful, these metrics could judge the model from 
%amounts of outputs and tell how well it does on the problem. For instance, 
%$accuracy$ is the proportion of the correct instances in positive predictions and $recall$
%is the proportion of identified ones in actual positive cases. A $confusion\,matrix$ defines simple 
%measures based on the statistics of gold labels and prediction classes  and more integrated metrics 
%like $ROC$ serve as supplements. The analysts choose from them based on different concerns 
%to get a general estimation of the outputs.


GBDT\cite{friedman2001greedy} is an ensemble model built on top of  
a bunch of regression decision trees. It has some appealing characteristics. 
For example, GBDT can naturally handle nonlinearity and tolerate missing values.
As a winning model in many data mining 
challenges~\cite{he2014practical,bennett2007netflix,chapelle2011yahoo}, 
GBDT is a good option for regression, classification
and ranking problems with well-known ability to generalize. 
Besides its wide range of applications, 
GBDT is also flexible in allowing users to define their
own suitable loss functions.
Furthermore, there are many 
implementations\cite{chen2016xgboost}\cite{ke2017lightgbm} 
and much work has been done to speed up the training process.

In most cases, GBDT outperforms linear models and random forest. 
Given the popularity and high quality of GBDT, it's important to uncover 
internals of the model. For GBDT, global feature importances calculation is 
widely used to do the feature selection. 
For example, Breiman proposed a method to estimate feature 
importance\cite{friedman2001greedy}. 
%First, it make an approximation of relative influences 
% of a split as the empirical improvements in squared-error. 
%By summing over all the non-terminal nodes in 
 %a decision tree for every split feature, a cumulative importance is obtained. 
%For a collection of boosting trees,
%the averaging feature importance among them is the global relative feature 
%importances. In this way, the changes of loss function are divided 
%into splitting features and obtain a feature measure related to
%the training process.
However, existing work has largely ignored
the exploration of local interpretations, which will be the focus
of  this paper. Specifically, we will study feature contributions for GBDT. 
%Though local model interpretation for random forest has been proposed\cite{palczewska2013interpreting} ,
%the better performance of GBDT over random forest appeals for a local interpretation in order
%to apply it to the scenarios like the loan on credit.
We starts from previous approaches of model interpretation for random forest\cite{palczewska2013interpreting} 
and update the definition of  the feature contribution. 
The proposed mechanism is flexible enough to interpret all 
versions of GBDT. The original definition based on label distribution 
change is proved to be a special case of ours under a particular loss function. 

The rest of the paper is organized as follows. Section \ref{secrel}
provides a brief review of related work on local interpretations. Section \ref{secpre} gives out the formal definition
of feature contribution as preliminary and presents the approach for calculating feature contributions
for random forests. In section \ref{secmech}, we describe the rationale 
behind as well as main actions in interpreting GBDT. 
Section \ref{secexp} contains experiment settings and the process 
to examine the proposed methodology. At the end, section \ref{seccon} concludes 
our work.
