\section{Related Work}\label{secrel}
%\KZ{This section can be trimmed down to half a page.}
%Model interpretation provides convincing reasons to the model outputs.
%In the occasion when a detailed explanation is needed for a prediction, 
%even a reduction in model performance is acceptable. This is the reason why
%some analysts turn to logistic regression for help. Logistic regression is a typical
%linear model with thorough study. It is likely that researchers will examine the 
%regression coefficients , which is global feature importance after the normalization 
%preprocess. The coefficient weights of different features are in the same scale, so that 
%the relative value between coefficients represents the relative importance between
%features.  Because of its linearity, the instance-level feature contribution is easy to 
%compute as the product of  actual feature value and its coefficient. Both the global
%and local interpretation of logistic regression is transparent from observing the 
%formula and reflect the impact on the function value of independent variables.
%Linear models is regarded as interpretable with good quality and speed. In the field
%like credit scoring, it is the default choice.

%Another type of 
Local model interpretation provides convincing reasons to the model outputs.
One type of interpretations prefer both the good performance of complex models and
interpretability of simple models. The pipeline of this type will first make use of
advanced models as a black-box and then extract useful information out of it with
the help of a more interpretable model. For example,
% \cite{craven1997using} is an
%interesting attempt to conduct the model interpretation for neural network. The 
%proposed learning-based method tries to retrain a decision tree that approximates 
%a trained network in order to get inductive rules.  And 
a novel approach in \cite{cui2015optimal} 
formally treats the interpretation of additive tree models as extracting 
the optimal actionable plan. It models the optimization problem as 
an integer linear programming and utilizes existing toolkit as the solver.
The constraints are based on both the output score and the objective function. 
%Finally, the linear functions over variables for leaf node flags and feature value spans are get  and they could be efficiently solved. 
Notice that, %these two kinds of methods 
this kind of approaches need extra training process especially 
for the interpretation and bring new models or tasks to solve.

Some other researchers come up with model-independent local interpretations. 
They mainly make changes to feature value and test the chain effect to 
performance loss of predictions.
The loss is then taken as the measure of local importance of 
feature\cite{lei2017distribution}. This method only relies
on the output evaluation and provides an unified way to check feature 
contribution for black-box models. 
%Leave-One-Covariate-Out(LOCO)  share the same spirit of feature 
%importance computation of random forest \cite{breiman2001random}. 
By replacing the actual
feature values with missing, zero or average values, the impact of a feature in predicting is
then removed. The instance-level contributions of all the features can be calculated separately
and compared with each other. Moreover, this method is also work for global feature importance.

%Different from the models with a continuous closed-form functions, tree-based models are discrete.
%Directly calculating the gradient as the feature change is not suitable  for this kind of models.
As a derivative of decision tree, the random forest goes further on model interpretation
than GBDT. The method in \cite{kuz2011interpretation,palczewska2013interpreting}
computes the feature contributions so as to show informative results
about the structure of model and provide valuable information for designing new compounds.
This method makes full use of the information, not only the training data
but also the model structure. It is natural to design the interpretations with the model
structures to get a more reasonable result.

This work 
proposes an easy way to get the feature contributions on 
the instance-level. 
%The interpretation process make full use of model structure and extra model is not need. 
Generally, it can be applied to all versions of GBDT implementations
with little preprocessing and modification to the prediction process.
