\section{Related Work}
\label{sec:related}
% \begin{enumerate}
% \item Approach 1. \cite{ishitani2002model}
% \item Approach 2. \cite{kameshiro1999document}
% \item Approach 3. \cite{ishitani2003document}
% \item Keyword searching. \cite{lu2004information}
% \item PADS, LearnPADS. \cite{fisher2011pads} \cite{fisher2008learnpads} \cite{zhu2012learnpads++}
% \item Next 700. \cite{fisher2006next}
% \item Introduction about an OCR engine. \cite{smith2007overview}
% \
% \item Error tolerant about document images. \cite{nagasaki2004document}
% \item Layout of document images. \cite{liang2002logical}
% \item Transforming papaer documents into XML. \cite{altamura2001transforming}
% \item \cite{lazzara2011scribo}
% \item OCR \cite{mori1992historical} \cite{broda2007correction} \cite{xu1999prototype}
% \item Latex \cite{lamport1986document}
% \item Human Correction \cite{von2008recaptcha}
% \item OCR Language Model \cite{kolak2003generative}
% \end{enumerate}

%In this section, we will talk about some related work.
% \KZ{The overview is a bit too long and fluffy, much of it will be said
% again in the individual sections.} 
% The target of our work is to extract information from medical images. 
%We have presented ODL spatial description language to describe the 
%semi-structured text components using different fuzzy matching 
%strategies to achieve the goal and made use of manual correction 
%to further improve the accuracy. 
In this section, we introduce related 
work in the data description language field, page layout analysis, 
optical character recognition and incremental manual correction. 
% The target of our work is to extract information from medical images. 
% We have presented ODL spatial description language to describe the 
% semi-structured text components using different fuzzy matching 
% strategies to achieve the goal. 
% The most related work to ours are data description languages.
% Such data description languages are designed to describe and 
% process the ad-hoc data. However, few of them have the ability 
% to tolerate errors and handle the spatial information. 
% ODL extends the previous languages with novel features 
% such as fuzzy matching, and spatial constraints. 
% Another area that our work is closely related to is the page layout 
% analysis. 
% Page layout analysis is used for automatically recognize the 
% pyhsical layout of a document. 
% In some previous work, 
% page layout analysis is widely used for the information extraction on 
% images. 
% Our work differs from the traditional page layout analysis 
% tachnique in that we make use of the spatial text boxes coming
% out of OCR output and organize them according to the input description. 
% In other words, we do not rely on the image processing for the analysis 
% of the structure. 
% OCR techniques are also related. Since it's necessary to 
% turn images into texts. 
% In our system, we also rely on traditonal 
% OCR techniques to turn images into texts. We choose to make use of 
% the existing OCR engine, tesseract. Some other related work are 
% about the use of human power to correct the errors that machine made, 
% and image autothresholding techniques in the preprocess part.

\subsection{Data Description Language}
%To describe the text information on medical images, we design ODL, 
%which is a declarative data description language for processing 
%text data on the images. 
In previous work, some declarative data 
descriptions were designed for a different purpose. For example, 
PADS \cite{fisher+:pads} is a data description language to 
handle the ad hoc data. 
% The descriptions of PADS are concise and flexible. 
With the help of the descriptions, a compiler can be 
used to parse and print the data. Further research including 
the LearnPADS \cite{Fisher:2008:DSF,fisher2008learnpads}, 
which provides a fully automatic 
system for generating the corresponding PADS descriptions. 
%ODL is inspired by PADS and uses the type system in programming 
%language integrated with fuzzy matching and spatial features 
%to handle the specific text data from medical images. 

%In ODL, in order to describe spatial information, 
%we enhance the syntax with spatial features in order to limit the 
%search area of the description and horizontal and vertical 
%skip to describe the spatial relation between the data. 
There has also been some 
previous work on describing the spatial information in the document, 
such as LaTeX \cite{lamport1986document} and 
PostScript \cite{taft1999post}. In LaTex, 
lots of spacing parameters and spacing commands are used. 
For example, ``$\backslash$vspace$\lbrace\langle$skip$\rangle\rbrace$'' and 
``$\backslash$hspace$\lbrace\langle$skip$\rangle\rbrace$'' 
are two general spacing commands. 
%The function of 
%``$\backslash$vspace$\lbrace\langle$skip$\rangle\rbrace$'' 
%is to allocate vertical space and the function of 
%``$\backslash$hspace$\lbrace\langle$skip$\rangle\rbrace$'' 
%is to allocate horizontal space. 
%There are also some particular cases of these two commands to leave a 
%vertical or horizontal space of some predefined amount, such as 
%``$\backslash$smallskip'', and ``$\backslash$enskip''. 
In ODL, 
we use similar descriptions for spacing functions, ``$\backslash$vskip e'' 
and ``$\backslash$hskip e''. But the way we interpret them is different. 
The reason is ODL is a fuzzy description language so that it's easy for 
humans to write in ODL. ODL will tolerate some error of the spacing 
command by using the fuzzy matching strategies described before. 
However, in LaTeX, spacing commands are interpreted as it is. 

Another language used to describe spatial information is PostScript. 
In PostScript \cite{taft1999post}, in order to manipulate the text, some 
operations are designed (for example, {\em ashow}, {\em widthshow}, 
{\em awidthshow}, {\em kshow}, {\em xshow}, {\em yshow}, or 
{\em xyshow} operators). These operations take an input text 
string and a separate specification for positioning the elements. 
In contrast with ODL, such operations will paint the strings identified by the elements 
of the strings on the current page starting at given point, and also 
provide some spatial information like the $x$, $y$ coordinates. Spatial 
information, such as coordinates, is used in the painting of latter strings. 
It's much more detailed than ODL as one can attain all the coordinates 
for different strings if needed. 
%In ODL, we only describe the relationship 
%between different data components, so it's easier to describe such 
%information in ODL than in PostScript. 
% Simpler building blocks include Chapman Flack's Markup \cite{markup}, 
% which can be used as a front end to carry out simple text setting on its own. 

\subsection{Page Layout Analysis}
A typical document analysis system consists of page segmentation, 
optical character recognition, and logical structure identification. 
The interest in the logical structure was inspired by the 
emergence and popularity of common representation standards such as XML. 
By using these common standards, we can encode structural information 
together with the contents. \cite{o1993document}. 
One of the key components used to help to understand a document is logical 
labeling. 
The task of logical labeling is to label segmented blocks on a document 
image as title, author, header, text column, etc \cite{liang2002logical}. 
% \cite{ishitani2002model}. 
The set of labels will depend on the document classes or applications. 
Logical labeling techniques can be roughly characterized as either 
zone-based or structure-based. Zone-based 
techniques classify zones individually based on the features of each zone 
\cite{altamura2001transforming} \cite{palmero1999structured}. 
Structure-based techniques incorporate global constraints such as the position 
of the text. 
In our work, instead of using zone or struct, 
we describe the logical layout by making 
use of the composition expressions. 

Some style-directed layout analysis algorithms 
also allow users to specify the physical layout~\cite{kanungo2003stochastic}.
In this case, a regular language, including terminal symbols, 
nonterminal symbols and production rules, is proposed to express 
the varieties of physical regions and help the physical layout analysis.
However, the basic terminal symbol is the text line instead of the word. 
% The reason is that noises will mislead the producting rules for word.
The reason is that it is hard to distinguish word from noises based on the 
image features alone, which result in inaccuracies when processing 
the production rules. In our work, we introduce constraints to 
handle such noises.  

Another related work about page segmentation is VIPS \cite{cai2003vips}. 
It's a vision-based page segmentation algorithm used to extract the semantic 
structure of a web page. 
% Such a semantic structure is hierarchical, 
% meaning that each node will correspond to a block. 
The vision-based content structure of a page is obtained by combing the DOM structure 
and the visual cues. In VIPS, the web page is, first, segmented into 
several big blocks and the hierarchical structure of this level is 
recorded. For each big block, the same segmentation process is 
carried out recursively until we attain sufficiently small blocks. 
In our work, the semi-structure output of the OCR engine is comprised of the 
structured XML files. However, VIPS can't be directly used to analyze 
these XML files since the errors in the recognition process will 
lead to errors in the XML files about the visual cues 
and the DOM structures. In this way, VIPS will be misled by 
the wrong visual cues in its analysis of the document. So 
we abandon use of the visual structure in the XML files and instead 
make use of the detailed coordinate information to reconstruct the 
structure. 

Different from the solutions for web pages, 
PATO \cite{bartoli2014semisupervised} 
is a system for extracting predefined items from 
printed documents in a dynamic, multisource scenario. It analyzes 
the text blocks in the printed documents and handles the fuzzy matching problem 
by calculating the matching probabilities of recognized blocks and elements in 
the schema, which are generated manually by selecting the 
items to be extracted with point-and-click GUI interface. By focusing on 
the parameters of the text blocks, it pays little attention to the 
relationship between different text blocks, that is, it is not able to
represent dependencies between text block, like our system does.

\subsection{OCR}
Optical character recognition (OCR) is the mechanical or electronic 
conversion of images of typewritten or printed text into machine-encoded 
text. 
It is widely used as a form of data entry from printed paper 
data records, e.g., invoices, bank statement, receipts, mail, or other documents. 
It is a common method 
of digitizing printed texts so that they can be electronically edited and searched. 
% This method also can be used in machine 
% processes such as machine translation, text-to-speech, key data and text mining. 
Early versions of the OCR system 
needed to be trained with images of each character, and 
worked on one character at a time \cite{mori1992historical}. 
More recent OCR systems have high rates of accuracy for 
characters in different fonts. 
Some systems can reproduce the formatted output that 
closely approximates the original page including images, columns, and 
other non-textual components \cite{smith2007overview}. 

Errors in the OCR text will greatly affect the effectiveness 
of other related tasks. For example, error correction is an important 
technique in OCR document retrieval \cite{darwish2007error} \cite{taghva1996evaluation}.
Much work has been done to improve the performance of the OCR. 
For example, a language model is used in the post process of OCR 
\cite{kolak2003generative}. 
In this work, the content information is used to do the correction 
and achieve a good result particularly on the dataset of a long text 
document. However, the data sources we are handling here are often 
very short and the language model cannot achieve similar results. 
Also, some work has been done to design a system capable of extracting 
textual information from semi-structured documents, like invoices \cite{cesarini1998informys}. 
In these works, an important part is to recognize the form. 
% And the graphs are used to describe the form layout. 
However, the data source we are handling can't be described 
easily with a form template. The reason for this is that the textual information 
on medical images can appear in different places and be combined 
with images. So our system is flexible enough to only describe the localized 
textual data on them instead of describing the entire structure in the forms. 
% \JY{What's more, it's important}

\subsection{Manual Correction}
An important part of our system is making use of human effort 
to improve the performance of the whole system. 
There have been some previous efforts to explore the human power in 
digitizing the printed materials. For example, reCAPTCHA \cite{von2008recaptcha} 
is focused on channeling human effort to recognize CAPTCHA 
on the world wide web to decipher scanned words from books that computerized 
OCR process failed to recognize. In reCAPTCHA, 
instead of using the standard CAPTCHA, words taken from scanned texts 
are displayed. The solutions entered by humans are used to improve the 
digitization process. 
% However, to meet the goal of a CAPTCHA, 
% the system needs to be able to verify the user's answer. 
% To do this, reCAPTCHA gives the user two words, 
% the one for which the answer is not known and a 
% second ``control'' word for which the answer is known. 
In case of discrepancies among human answers, reCAPTCHA sends the word to 
ask for more human inputs and picks the answer with the highest 
number of votes. 
% where each human answer counts as one vote and each 
% OCR guess counts as one half of a vote (recall that these words all 
% have been previously processed by OCR). 
The reCAPTCHA system achieved an accuracy rate 
of 99.1\% at the word level (216 errors out of 24,080 words), 
whereas the accuracy of the standard OCR was only 83.5\% (3976 errors). 
However, reCAPTCHA achieved such an accuracy rate by making use of lots of human votes. 
It provides a solution to make use of a large catalogue of human inputs. 
And in our system, we make use of each human correction efficiently. 

Our manual correction policy is an incremental one, which means 
that our correction model can be incrementally learned from human 
efforts. Similar methods can be found in LearnPADS++ \cite{zhu2012learnpads++}, 
which proposes an algorithm that incrementally infers PADS descriptions 
for ad hoc data sources. In LearnPADS++, records that fail to 
parse by the candidate description are collected. It then uses 
the original LearnPADS algorithm to infer descriptions for the 
aggregated portions of bad data, and merges these
new sub-descriptions into the transformed description to produce a new 
one. In our system, we have a similar process whereby we handle the errors 
in the extraction, we collect all the errors and correct them with 
the correction model that learns from manual correction. 
After one round of correction, the remaining 
errors will go through a similar process again. 

% \subsection{Preprocess}

% Our work mainly focus on the information extraction on medical images. 
% We also design a new description language. The most related work is \cite{kameshiro1999document}, \cite{ishitani2002model} and \cite{ishitani2003document}. 

% When a printed document is to be input to a computer system, the document must be converted to a computerreadable format. Then, the information required has to be recognized in documents and stored in the required format. In general, it is
% difficult to accurately extract required keywords and their relationship from printed documents, because such documents have unknown words such as proper nouns, com pound words, or incorrect words due to OCR errors. Furthermore, it is necessary to solve word segmentation problems for Japanese documents, because the boundaries between words are ambiguous.

% The required information may be extracted hierarchically from the texts that are obtained by the OCR process from a document image. Such texts are also obtained hierarchically from regions which are extracted by the layout analysis process. In this hierarchical analysis, some ambi guities cannot be reduced in the result obtained by a single functional process.


% To extract information from images rely on the OCR technique. 


% We choose to design a description language to describe the image data. 
% There are some related work, \cite{fisher2011pads}, \cite{fisher2008learnpads} and \cite{lamport1985i1}. 

