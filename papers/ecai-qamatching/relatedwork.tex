\section{Related Work}
\label{sec:relatedwork}


%\begin{enumerate}
%\item Works that targets QA alignment problems - talk about QA alignment
%in different kinds of data (dialogue first).
%\item Other approaches/techniques that can be used to solve the 
%QA alignment problems. These approaches were not designed to solve
%QA alignment.
%\item other kind of alignment problems and their solutions (this is optional)
%\end{enumerate}
 
Detection of QA pairs from online discussions has been widely researched these years. Shrestha and Mckeown~\cite{shrestha2004detection} learned rules using Ripper for detecting QA pairs in email conversations. Ding et al.~\cite{ding2008using}, Kim et al.~\cite{kim2010tagging} and Catherine et al.~\cite{catherine2012does} applied the supervised learning method including conditional random field and support vector machine. Cong et al.~\cite{cong2008finding} proposed an unsupervised method combining graph knowledge to solve the task. Catherine et al.~\cite{catherine2013semi} proposed semi-supervised approaches which require little training data. He et al.~\cite{he2019learning} used the pointer network to find QA pairs in Chinese customer service. 
However, the tasks mentioned above are all different from ours. 
We identify QA pairs from two-party dialogues on online discussion forum, 
and focus especially on long-distance QA pairs. Besides, our 
dialogue is constrained between two roles who can both utter questions and  answers.
%the question which always needs follow-up QA to get the answer.

There exists several methods in other tasks which can be adapted to 
our QA matching problem. Feature-based method is popular for solving 
many NLP problems. In the work of Ding et al.~\cite{ding2008using}, 
Wang et al.~\cite{wang2010modeling} and 
Du et al.~\cite{du2017discovering}, 
they examined lexical and semantic features in two sentences 
for QA matching. However, the features such as common question words 
and roles have already been explicitly annotated in our data. 
Besides, other features such as special word occurrence or time stamp are 
unavailable here. According to the data, we considered the distance as 
the most important feature and implemented this feature-based method as one 
baseline. Recent researches using deep neural networks have increased a lot. 
He and Lin~\cite{he2016pairwise} and 
Liu et al.~\cite{liu2016modelling} used the sentence pair interaction 
approach which takes word alignment and interactions between the sentence 
pair into account. Attention mechanism was also added for performance 
improvement~\cite{rocktaschel2015reasoning,wang2016learning,chen2017enhanced}. 
We also use word alignment and interactions to calculate the QA similarity. 
Specially, we adopt attention mechanism to solve the LQA cases.

There are other kinds of alignment problems such as temporal sequences alignment. Video-text alignment is one of the temporal assignment or sequence alignment problems. Previous work automatically provides a time (frame) stamp for every sentence to align the two modalities such as \cite{bojanowski2015weakly} and \cite{dogan2018neural}. Bojanowski et al.~\cite{bojanowski2015weakly} extended prior work by including the alignment of actions with verbs and aligned text with complex videos. Dynamic time warping (DTW) is anothor algorithm for measuring similarity between two temporal sequences. It's also widely used in video-text alignment task~\cite{dogan2018neural}, 
speech recognition task~\cite{vintsyuk1968speech}.% and big data time series addressing~\cite{rakthanmanon2013addressing}.


%Attention mechanism has been widely used in Natural Language Processing areas. It was firstly be applied in NLP by  \cite{bahdanau2014neural}. They applied the Neural Machine Translation(NMT) with attention mechanism by jointly learning to align and translate. They improved the performance of basic encoderâ€“decoder NMT architecture by proposing a soft-align method which solved the bottleneck coming from the use of a fixed-length vector. An end-to-end neural networks model called R-NET was presented by \cite{wang2017gated} to do reading comprehension and question answering. They matched the question and passage with attention-based recurrent networks and aims to answer the questions according to a given passage. They masked out irrelevant parts in passage and emphasized the important ones with attention-based recurrent networks by assigning different levels of importance to passage parts according to their relevance to the questions. In question answering over knowledge base(KB-QA) task, an end-to-end neural network based on cross-attention mechanism proposed by \cite{nguyen2017novel} represented the questions and their corresponding scores according to the various candidate answer aspects. To some extent, it alleviated the out-of-vocabulary(OOV) problem with cross-attention mechanism. 



%\KZ{This para is unclear. Rephrase all of them..}
%There exists work on extracting question answering pairs from different utterances. \cite{cong2008finding} finding question and answer pairs from online forums where the question is given and answers to the question from many followers are diverse. The goal is to choose the most appropriate one ( ). Some extracts question answering pairs from email conversations where the successive relationships between utterances is unambiguous ( ). Some also labeling question answer pairs in multi-party meeting corpus, where the questions are solved one by one (  ). 
