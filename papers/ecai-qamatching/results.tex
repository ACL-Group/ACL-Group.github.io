\section{Results and Analysis}
\label{sec:results}
%In this section, we first show the end-to-end results on QA matching and accuracy on variable distances. Then we do the ablation tests to show the specific architectural decisions.  
In this section, we show the end-to-end results and ablation tests for the specific architectural decisions.
\subsection{Overall Performance}
The main results of different models are shown in Table \ref{tab:mainResults}. The last row lists the human performance, regarded as the upper bound of this task. Note that
human performance is not perfect due to inherent ambiguities in the dialogues. 
%We make the following observations. 

\begin{table}

	\centering
	\begin{tabular}{p{1cm}<{\centering}p{1cm}<{\centering}ccc}
		\toprule[1.5pt]
		Models &P&R& F1\\
		\midrule[1pt]
		GD1&69.84&44.73&54.53\\
		GDN  &70.03&69.11&69.57\\
		GD1+J&70.38&50.40&58.74\\
		GDN+J&51.47&82.90&63.51\\
		\hline
		mLSTM&58.17&4.20&7.84\\
		Distance&71.57&69.34&70.44\\
		RPN&72.40&68.63&70.46\\
\hline
		DIS&78.46$^\star$&70.34&74.70$^\star$\\
		HTY&75.40$^\star$&76.42$^\star$&75.90$^\star$\\
		HDM&76.44$^\star$&78.44$^\star$&\textbf{77.43}$^\star$\\
		\hline
		Human &85.11&84.21&84.66\\
		\bottomrule[1.5pt]
	\end{tabular}
	\vspace{-0.25cm}
	\caption{The end-to-end performance of all methods on test dataset. 
Scores marked with $^\star$ are statistically significantly better than the RPN 
with $p<0.01$.} 
	\label{tab:mainResults}
\end{table}

 

The results of the rule-based methods are not bad, which indicates that questions 
are actually followed by their answers in many cases. The GDN increases the F1-score 
to 69.57\% compared with Greedy-1 because it can solve the case of simple fragmented 
answers. For GDN+J, the recall is the best among all the methods while accuracy and 
F1-score suffer. 
The reason is that GDN tends to match NQ with Q as much as possible, so many chit chats will be regarded as answers, which reduces the precision.

Model mLSTM underperforms because it is difficult to solve the QA matching problem with 
only two short texts without history. The word distribution between the questions 
and answers are quite different and maybe unrelated without background knowledge. 
Distance achieves good scores which shows that the distance is very important factor 
when identifying QA relations in dialogues. People tend to answer a question 
the moment they see it except in the case of incremental QAs. RPN obtains competitive 
results. It mainly benefits from taking the dialogue session as a whole which 
contains all the information in a session. 
 
Our proposed models achieve the best results compared with above models. 
The HDM improves the F1-score to 77.43\%, significantly better than RPN by t-test with
$p<0.01$. 
Although the recall of HDM is not better than GDN+J and the precision is 
lower than DIS, the overall quantity and quality of QA pairs identified 
are the best, shown by the highest F1-score. 
In addition, the comparable results achieved by HTY demonstrate that 
QA matching not only depends on 
the distance but also relies on the history information.
This shows that HDM model successfully combines both the distance and history 
information.

\subsection{Variable Distance Matching}

 According to the results above, we can find that the model Distance, RPN, DIS, HTY and HDM are competitive. Thus, we further analyze the accuracy of these five models on variable distances.

\begin{table}

	\centering
	\begin{tabular}{p{1.5cm}<{\centering}ccccc}
		\toprule[1.3pt]
		 Models &1&2&3&4&$\geq5$\\
		\midrule[1pt]
		Distance      &\textbf{100.0}&88.01&0.0&0.0&0.0 \\
		RPN  &89.37&69.37&50.12&36.96&13.10\\	
		DIS &96.23&\textbf{89.13}$^\star$&17.03&2.45&0.0\\
		HTY &94.37&78.89$^\star$&57.42&38.48&\textbf{28.17}$^\star$\\
		HDM &95.99&83.16$^\star$&$\textbf{59.37}^\star$&\textbf{40.44}&24.80$^\star$\\
		\bottomrule[1.3pt]
	\end{tabular}
	\vspace{-0.25cm}
	\caption{The Acc (\%) of matched QA pairs on variable distances.}
	\label{tab:longrangeResults}
\end{table}
Table \ref{tab:longrangeResults} shows that Distance and DIS work well on SQAs 
but deteriorate rapidly 
as the distance between QAs grows, indicating that relying solely on 
the distance information is insufficient. 
On the other hand, using dialog context or history, 
the matching accuracy of RPN and HTY is generally lower on SQAs but higher on 
LQA. Our full model (HDM) actually is a good trade-off 
by incorporating both distance and history information. This also accords 
with the decision process made by human annotators.

\subsection{Ablation Tests}
We justify the design of our model in the following two aspects.

\subsubsection{Different definitions of history}

To show the effectiveness of using the turns between Q and NQ 
as history, we devise the following variants on the HDM model 
for comparison:
\begin{itemize}
	\item \textbf{Q-history Model (QH)} has the same structure of HDM where the history is all the turns before Q.
	\item \textbf{A-history Model (AH)} has the same structure of HDM where the history is all the turns before NQ. 
	%\item \textbf{Q-history Model (QH)} where history refers to all of the turns before Q.%has the same structure of HDM where the history is all the turns before Q.
	%\item \textbf{A-history Model (AH)} where history refers to all of the turns before NQ.%has the same structure of HDM where the history is all the turns before NQ. 
\end{itemize}

The main results with different choices of history are shown in 
Table \ref{tab:historychoice}. %\footnote{The complete results are shown in Appendix due to the space limitation.}.%Due to the space limitation, we only listed the significantly different results. The complete results are shown in Appendix.} 
Our final model (HDM) outperforms QH and AH, indicating that the turns between Q and NQ are significant when figuring out the relation of Q-NQ pair. The turns before Q is actually not that important for matching Q and NQ. 
Although there is an overlap between the history we defined and 
the turns before NQ, 
the turns before Q brings more noises than benefits for
the end-to-end performance. This suggests that our definition of 
history as the turns between Q and NQ is reasonable and effective.

\begin{table}
	
	\centering
	\begin{tabular}{p{1.5cm}<{\centering}|c|ccc}
		\toprule[1.3pt]
		Models &F1&Acc@3&Acc@4&Acc@$\geq5$\\
		\midrule[1pt]
		QH&74.56&21.53&21.53&0.79 \\	
		AH&73.84&15.81&10.78&4.76 \\
		HDM&\textbf{77.43}&\textbf{59.37}&\textbf{40.44}&\textbf{24.80}\\
		\bottomrule[1.3pt]
	\end{tabular}
	\vspace{-0.25cm}
	\caption{The matching results on different choices of history.}
	\label{tab:historychoice}
\end{table}

\subsubsection{Different ways of attending to the history}

To evaluate the effectiveness of mutual attention for aggregating the 
history, we devised variants of the HDM model for comparison:
\begin{itemize}
	\item \textbf{Non-mutual Model (NM)} has the same structure of HDM 
	where $Q$ attends to $H_{RQ}$ and $NQ$ attends to $H_{RNQ}$.
	\item \textbf{Identical history model (ID)} has the same structure of HDM where $Q$ and $NQ$ attends to the same history $H_{RQ}\bigcup H_{RNQ}$.
	%\item \textbf{Non-mutual Model (NM)} where $Q$ attends to $H_{RQ}$ and $NQ$ attends to $H_{RNQ}$.%has the same structure of HDM where $Q$ attends to $H_{RQ}$ and $NQ$ attends to $H_{RNQ}$.
	%\item \textbf{Identical history model (ID)} where $Q$ and $NQ$ attends to the same history $H_{RQ}\bigcup H_{RNQ}$.%has the same structure of HDM where $Q$ and $NQ$ attends to the same history $H_{RQ}\bigcup H_{RNQ}$.
\end{itemize}

The main result in Table \ref{tab:historyways} reveals that our choice 
of separating the history by role label and mutually attending to each other
does work. The full model (HDM) consistently outperforms both NM and ID. 

%When making use of the union of the history from both parties, 
%the combined history confused the model with too many turns to consider. 
%The results of NM is slightly better than ID especially on LQAs, probably 
%due to the better representations of the flow of semantic information on 
%individual parties. However, QA relations focus more on interactions 
%between parties. In a word, HDM performs better. 

For the ablation test on different ways of attending to the history, we conclude that
NM is better than ID. It's due to better understanding on individual speakers which can
help the understanding of the dialogue, similar to the idea in 
the AAAI2019 paper "A Deep Sequential model for Discourse Parsing on Multi-party Dialogues". 
Our work targets the QA relations in dialogues which are more related to the interactions 
between speakers. As a result, our full model is better than NM. 

The difference between our full model HDM and ID is that in ID both Q and NQ attend to
all turns in the history regardless of who uttered those turns, whereas HDM employs a
mutual attention mechanism that distinguishes turns by their speakers. Specifically, the
Q only attends to those turns uttered by the NQ speaker, while the NQ attends to those
turns by the Q speaker. This resembles to some extent the firm attention in Amplayo's paper\cite{amplayo2018entity}.
This will help the model to focus on the interactions between speakers. 

We also conduct a Z-test on the 
results to show that the improvements on the LQAs are statistically 
significant even with a small sample size.

\begin{table}

	\centering
	\begin{tabular}{p{1.5cm}<{\centering}|c|ccc}
	\toprule[1.3pt]
	Models &F1&Acc@3&Acc@4&Acc@$\geq5$\\
	\midrule[1pt]
	NM&75.81&57.18&37.50&20.04 \\	
	ID&75.46&54.50&28.43&11.70\\
	HDM&\textbf{77.43}&\textbf{59.37}&\textbf{40.44}&\textbf{24.80}\\
	\bottomrule[1.3pt]
\end{tabular}
	\vspace{-0.25cm}
	\caption{The results on different ways of aggregating history.}
	\label{tab:historyways}
\end{table}
%\caption{The matching results on different ways of aggregating history.}


\subsubsection{Example Outputs}

\begin{CJK}{UTF8}{gbsn}
To provide a better understanding of the behavior of our models, we include an example 
output in Table \ref{tab:case1}. It contains both LQAs and SQAs. 
In this case both HYD and HDM predicts QA relations better than the baseline RPN. 
As for SQAs, all of the models perform well. However, the DIS model is 
obviously not capable of matching LQA pairs. It indicates that the distance information sometimes hurts the performance of HDM on matching LQA pairs.
	
	
	%\usepackage{ctex} it changes the row space! what should I do? And translation
	\begin{table*}
		%\begin{table*}[!hp]
		\small
		\centering
		\begin{tabular}{p{1.5cm}<{\centering}cccccc}
			\toprule[1.3pt]
			Ground Truth &RPN&DIS&HTY&HDM&Role&Utterances\\
			\midrule[1.3pt]
			\multicolumn{5}{c}{Q1}&P&\makecell{Boy, 4 months. He tried a little yolk yesterday and shat that night \\but haven't shat until today what's wrong????}\\
			\hline
			O &O &O &O &O &D&\makecell{Hello}\\
			\hline
			O &O &O &O &O &P&\makecell{Hello}\\
			\hline
			\multicolumn{5}{c}{Q2}&D&\makecell{Is he four months old}\\
			\hline
			A2&A2&A2&A2&A2&P&\makecell{Yes}\\
			\hline
			A1&A1&O &A1&A1&D&\makecell{Eat too early}\\
			\hline
			A1&O&O &A1&A1&D&\makecell{Not advise}\\
			\hline
			A1&O&O &A1&A1&D&\makecell{Difficult for digestion}\\		
			\bottomrule[1.3pt]
		\end{tabular}
		\caption{A correct case of predictions and human annotations in our dataset.}
		\label{tab:case1}
		%\end{table*}
	\end{table*}
	
\end{CJK}
