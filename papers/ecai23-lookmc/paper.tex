\documentclass{ecai}  % use option [doubleblind] for double blind submission and hiding the authors section

\usepackage{graphicx}
\usepackage{latexsym}

%\ecaisubmission      % inserts page numbers. Use only for submission of paper.
                      % Do NOT use for camera-ready version of paper.



\usepackage{graphicx}
\usepackage{latexsym}

%\ecaisubmission      % inserts page numbers. Use only for submission of paper.
                      % Do NOT use for camera-ready version of paper.

%\paperid{123}        % paper id for double blind submission
\usepackage{microtype}
%\usepackage{natbib}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{multirow}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{color, colortbl}
\usepackage{url}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{verbatim}
%\usepackage[linesnumbered, boxed, ruled]{algorithm2e}
%
\usepackage[noend]{algpseudocode}


\usepackage{algorithmicx,algorithm}
\usepackage{bbding}
%\hypersetup{
%	colorlinks=true,
%	linkcolor=blue
%}

\newtheorem{example}{Example}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\Roy}[1]{\textcolor{red}{Roy: #1}}
\newcommand{\crosssymbol}{{\color{red} \XSolidBrush} }
\newcommand{\checksymbol}{{\color{green} \Checkmark} }

\usepackage{makecell}
%\usepackage[table]{xcolor}
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{138} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}
\definecolor{Gray}{gray}{0.9}


\begin{document}

\begin{frontmatter}

%\title{Short Circuits in Multiple-Choice Natural Language Reasoning}
%\title{Short Circuit Solutions: Enhancing NLU Reasoning Models with Crossover and Mutation}
\title{Combating Short Circuit Behavior in Natural Language Reasoning: Crossover and Mutation Operations for Enhanced Robustness}

\author[A]{\fnms{Shanshan}~\snm{Huang}}
\author[B]{\fnms{Siyu}~\snm{Ren}}
\author[C]{\fnms{Kenny Q. Zhu}\thanks{Corresponding Author. Email: kenny.zhu@uta.edu.}}

\address[A]{Shanghai Jiao Tong University, China}
\address[B]{Shanghai Jiao Tong University, China}
\address[C]{University of Texas at Arlington, United States}


\begin{abstract}
%For multiple-choice natural language reasoning problems, 
%we investigate a recent speculation that models can make a correct 
%choice without referring to the context of the choices in the
%questions, which we call ``short circuit'' in this paper. 
%We propose a biologically inspired black-box testing
%operation called ``crossover'' 
%that can effectively detect such behavior in any models.
%Together with another operation ``mutation'', the two
%can further be used to augment training data to teach the existing
%models to overcome short circuit problems and be more robust 
%in their tasks.
In this study, we delve into the ``short circuit'' phenomenon observed in multiple-choice natural language reasoning tasks, where models tend to make accurate choices without properly considering the context of the question. To better understand this phenomenon, we propose white-box and black-box proxy tests as investigative tools to detect short circuit behavior, confirming its presence in fine-tuned NLU reasoning models.
To tackle the short circuit issue, we introduce biologically inspired ``crossover'' and ``mutation'' operations. These operations are applied to augment the training data for popular models such as BERT, XLNet, and RoBERTa. Our results demonstrate that these data augmentation techniques effectively enhance the models' robustness and mitigate the short circuit problem.
%Our research contributes by proposing white-box and black-box methods to detect short circuit behavior, confirming its presence in fine-tuned NLU reasoning models, and introducing two operations for data augmentation, which effectively enhance model robustness and performance on both stress and original test data.
\end{abstract}



\end{frontmatter}
\input{intro}
\input{method}
\input{experiments}
\input{related}
\input{conclude}
\bibliography{ecai}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
