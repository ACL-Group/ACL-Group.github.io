\section{Related Work}
\label{sec:related}

\newcommand{\QS}[1]{\textcolor{magenta}{[Qingyu: #1]}}

%In this section, we present the recent developments in classification or clustering
%of images as well as document search results. In addition, we will also mention
%some of the recent work on image context extraction.
%which are related to key components of our work.
% FIXME: rewrite this paragraph.

%\subsection{Image Clustering}
We divide existing image clustering methods into three categories:
content-based, context-based and the combined approaches.

Content-based image clustering approaches \cite{FergusFPZ05,FanGL07,GaoFLS08}
rely on visual signals.
For example, Fu et al.\cite{Fu2011} gave a constraint propagation
framework for multi-model situations. They constructed multiple graphs,
one for each visual modalities
such as color histogram, SIFT descriptors \cite{Lowe99}, etc. The nodes are images
while the edges are similarities between the images by a particular visual modality.
A random walk process is employed on these graphs.
%During this process, the
%walker has some probability of walking to the same node in other graphs. Finally,
%they could obtain some propagated constraints, which could be used for
%constrained clustering. And there are other content-based approaches.
%Zhong et al.\cite{ZhongLL11} designed a deep learning architecture and algorithms.
%Some of these techniques have been used in Content-Based Image Retrieval (CBIR)
%systems \cite{Chang1984,Datta2008,ChenWK03,ChenWK05}.
All of the above work uses low-level visual signals of images such colors,
gray scales, contrasts, patterns, etc. These signals are insufficient
to capture high level semantics of the images. This is evident from our
experiments on Fu's algorithm which heavily relies on basic visual signals.
There has been some development on high level visual object recognition
and semantic annotation \cite{Li09scene}, but even the state-of-the-art
techniques in this area suffer from low accuracy and unreliability.

With the difficulty in content-based clustering,
some researchers turn to signals coming from the context of the images,
such as file name, alternate text and surrounding text.
Cai et al. made some progress in this respect.
They represented a web page segmentation algorithm named VIPS \cite{VIPS},
which works by rendering the web page visually and detecting
the important visual blocks in the page. And they subsequently proposed three
kinds of representations for images \cite{Cai2004}:
visual feature based representation, textual feature based representation and
link graph based representation, and proposed a two-level clustering algorithm
which combined the latter two.
Jing et al. \cite{Jing2006} introduced a novel method named IGroup for image clustering.
Instead of clustering on returned images directly, they first search the query
on normal web search engine, and cluster the titles and snippets from the search results.
They then construct a new query string to represent each of the cluster, and
send these query strings to the image search engine to get images for each
cluster. To construct the query string, they used an algorithm
proposed by Zeng\cite{Zeng2004}.
%The main problem with the above approaches is that they model the image context
%by bag-of-words or n-gram models. These are low-level signals which are again
%inadequate for understanding the complex semantics of surrounding text.
These bag-of-words approaches are inadequate for understanding the semantics of
the context. Relying on bag-of-words or n-grams can easily confuse noise with meaningful
signals. Our approach, on the other hand, leverages co-occurrence information on
high level concepts mined from Wikipedia, a
comprehensive knowledge source, and most importantly,
is able to disambiguate entities using
this knowledge. Hence, we are able to achieve better results.

Recently there are many attempts on combining visual features and textual features in
image clustering. Feng et al.\cite{Feng2004} used the surrounding text of images
and a visual-based classifier to build a co-training framework.
Gao et al.\cite{Gao2005} represented the relationship among low-level visual features,
images and the surrounding texts in a tripartite graph. Wang et al.\cite{WangMZL05}
reinforced visual and textual features via inter-type links and inversely uses those
features to update these links. The visual features, text features and inter-type
links are represented as three matrices. Three linear formulas is defined to iteratively
update the three matrices. Ding et al.\cite{DingLL08} proposed a
hierarchical clustering framework.
%Given a query, the framework first achieves ranked documents from textual
%search engine. Phrases are extracted from the returned documents and clustered.
%And then the framework forms a new query string for each cluster
%to perform image search.
%Visual features are used to cluster the returned result again to obtain more specific
%class.
Leuken et al.\cite{LeukenPOZ09} investigated three methods for
visual diversification of
image search results in their paper.
Tsai et al.\cite{Tsai11} proposed a technique based on visual synset
for web image annotation.
% which is used by Google Image's
%``by-subject'' function \cite{googleimagesubject,jing2012google}.
They applied affinity propagation clustering on a set of images associated
with a query term based on both visual and textual features.
Each cluster represents a visual synset,
and is labeled by related query terms.
However, this query-based/term-based labeling approach has
two limitations: 1) it cannot produce related concepts
to the clusters like our system does
(e.g. ``Teddy'' for Cluster 1 in \figref{fig:demo-bean});
2) the related query terms themselves can be ambiguous and are not suitable
for representing a visual synset.
In our paper, we represent each cluster with high related concepts which are
Wikipedia concepts without ambiguity.
%For each synset,
%they collected the query terms related to the images in the synset,
%and used TF-IDF to select top query terms to characterize the visual synset.
%%They further used SVM to assign an image to visual
%%synsets and compute the representative
%%query terms to annotate the image.
%The main contribution of that work is that, they annotate
%the visual synset(clusters) with query terms which serve as tags for the
%clusters. However, different from our approach, these annotations are
%query terms and not entities.
The main challenge with the above hybrid approaches is the
semantic gap between visual signals and textual signals. There is no easy way
to combine the two kinds of similarity measures into one unifying measure.


%In a way, the framework proposed in this paper is a hybrid approach
%too except we focus more on text context and only use visual cues
%as complementary information
%to merge clusters that are left out due to lack of textual signals.

%The most popular image retrieval method is content-based image
%retrieval (CBIR), which organizes and classifies images
%by their visual content or visual signals only \cite{Datta2008}.
%There have been many important attempts on CBIR. Some of the pioneering work was done
%by Chang et al.\cite{Chang1984}. They introduced a
%picture indexing and abstraction approach for pictorial database retrieval.

%Since then, CBIR has been successfully commercialized in
%image search engines, such as VisualSEEk \cite{Smith1996}.
%\KZ{This para is too simple and i'm not sure what you want to say? The next para
%doesn't follow naturally.}

%More recently, CBIR has been successfully applied to image
%classification.
%Zhong et al.\cite{ZhongLL11} designed a deep learning architecture and
%algorithms. %The architecture is the same as the
%multi-layer physical structure of the human visual cortex, which is associated
%with many cognitive abilities for human beings. There are three stages in their
%algorithm: bilinear discriminant initialization, greedy layer-wise
%reconstruction, and global fine-turning. With this algorithm, they could
%differentiate various kinds of picture from each other,
%such as airplanes, motorbikes, faces, tall buildings and so on.
%\KZ{We say so much about Zhong but we didn't compare with him in the experiments.
%What's the point? Unless we can kill his idea directly with a strong argument.}
%Fu et al. \cite{Fu2011} gave a constraint propagation framework for multi-model
%situations. They constructed multiple graphs, one for each visual modalities
%such as color histogram, SIFT \KZ{cite here}, etc. The nodes are images
%while the edges are similarities between the images by a particular visual modality.
%Then a random walk process is employed on these graphs. During this process, the
%walker has some probability of walking to the same node in other graphs. Finally,
%they could obtain some propagated constraints, which could be used for
%constrained clustering.

%All of the above work uses low-level visual signals of images such colors,
%gray scales, contrasts, patterns etc. These signals are often insufficient
%to capture high level semantics of the images. This is evident from our
%experiments on Fu's algorithm which heavily relies on basic visual signals.

%\KZ{What about work on high level visual semantics like Li Feifei's work, etc.?
%The problem in this space is that high level object recognition in computer vision
%is still immature with relatively low accuracy (evidence?).}

%But for the results from search engine, we have some textual signals, such as the
%surrounding text of picture, the search keyword and the URL information.

%Because of the challenges in high level visual recognition, other researchers
%turns to signals coming from surrounding text of web images,
%often by combining textual signals with the visual signals.
%Cai et al. made some progress in this respect \cite{Cai2004b,Cai2004}.
%They represented a method to bridge the gap between
%visual and text information by a web page segmentation algorithm named
%VIPS\cite{VIPS} which works by rendering the web page visually and detecting
%the important visual blocks in the page. They subsequently proposed three
%kinds of representations for images: visual feature based representation,
%textual feature based representation and link graph based representation,
%and proposed a two-level clustering algorithm.

%Since
%they think this it's an open problem for extracting semantic information by
%visual feature of pictures, they only use textual feature and link graph to
%finish their two-level clustering algorithm.
%And there are even more researches:%, such as \cite{Feng2004,Gao2005}.

%Along the same line, Feng et al.\cite{Feng2004} used the surrounding text of images
%and a visual-based classifier to build a co-training framework.
%Gao et al.\cite{Gao2005} represented the relationship among low-level visual features,
%images and the surrounding texts in a tripartite graph.

%Jing et al.\cite{Jing2006} introduced a novel method to return clustered image
%search result which is similar to the goal of this paper.
%Instead of clustering on returned images directly, they first search the query
%on normal web search engine, and cluster the titles and snippets from the search results.
%They then construct a new query string to represent each of the cluster, and
%send these query strings to the image search engine to get images for each
%cluster. To construct the query string, they used an algorithm
%proposed by Zeng\cite{Zeng2004}, which will be mentioned later.
%There is no visual signals used in their algorithm, and the surrounding texts
%are even not used too. The major weak point of their work is, the result
%relies on the image search engine.

%However, in all these researches above using textual information, they just
%extracted the surrounding text of images, or used web page abstract text in\cite{Jing2006},
%and there is no further processing on the surrounding text as
%well. This surrounding text
%should has a lot of noises which prevent us from understanding the meaning of
%the surrounding text.
% FIXME: too many "surrounding text"s here!

%In our work, we use \textit{Wikification} method to help us extract useful
%information from surrounding text. Some related work of \textit{Wikification}
%is mentioned in Section \ref{wikification}.
% TODO: Is there any other algorithms we used? More description about our
% algorithm needed here.

%\subsection{Web Document Search Results Clustering}
%Because our clustering algorithm focuses on text signals,
%next we survey a few representative pieces of work on clustering web documents.
%%Besides image search results clustering, there are also many researches about
%%web document search results clustering. The primary difference between them is
%%that there is no image information in web document search results clustering
%%methods. That is to say, we cannot extract the low-level visual signals and
%%surrounding texts in web document search results clustering.
%Two main techniques
%used in search results clustering are information retrieval and machine
%learning\cite{Leouski1996,ZamirE98,ZamirE99}.
%The most relevant work to this paper is done by
%Zeng et al.\cite{Zeng2004} who introduced a web search results clustering algorithm
%based on machine learning. First, they extracted all possible phrases from the
%contents by n-grams, and extracted features from each phrase. Then they used
%a regression model learned from training data, and applied on the features to
%get a \textit{salience score} for each phrase. The phrases with top score were taken
%as \textit{salience phrases}. These \textit{salience phrases} are actually the
%names of candidate clusters which are merged later.
%Moreover, Hearst and Pedersen\cite{Hearst1996} presented a cluster-based document
%browsing method in 1996. They used pairwise cosine similarity between document
%vectors and the fractionation clustering algorithm.% in their Scatter/Gather system.
%%The fractionation algorithm needs the number of clusters as an input, and the
%%complexity is $O(kn)$, where $k$ is the number of clusters, and $n$
%%is the number of documents.
%
%%\cite{vivisimo}
%
%%\KZ{Summarize the above approaches and compare/connect with our approach?}

%\subsection{Image Context Extraction}
%
%Work specific to image context extraction is limited.
%Alcic and Conrad \cite{Alcic2010} measured some context extraction methods, including
%N-terms-environment \cite{Coelho2004}, the Monash extractor\cite{Fauzi2009},
%siblings extractor which simply selects the sibling text nodes of the image node
%in the DOM tree, and the VIPS \cite{VIPS} algorithm which we mentioned earlier.
%%Since it's hard to define the exact the context of a web image
%%and it's a little overly
%%restrictive for testing with a too strong criterion, they consider the result
%%as a correct one when there is partially accordance between the output and the
%%test set. \KZ{Don't understand the above sentence.}
%%They used an F-score which is defined as
%%$$F_{score} = 2 \cdot \frac{P \cdot R}{P + R}$$
%%to represent the final result, where $P$ is the precision and $R$ is the recall.
%And they computed the standard deviation of F-score to measure the stability
%of all the algorithms.
%It is interesting that their experiments concluded that extracting context
%based purely on the DOM structure performed better than
%visual-based approach with VIPS being the worst performer.
%Since the sibling based extractor strikes a good
%balance between accuracy and time cost, we adopt a context extractor
%which is modified from the sibling extractor in our work.

%\subsection{Entity Disambiguation}
%\label{wikification}
%
%%In our work, we used a \textit{Wikification} method as a
%%\textit{Word Sense Disambiguation} (WSD) algorithm. As it's an open problem,
%%there are a lot of previous works on WSD \cite{GuthrieGWA91,Li1998:wcd,ChungKML01,Stokoe2003:WSD,Fernandez-AmorosGSS10}.
%There is large body of work on entity disambiguation. Due to space constraint,
%we limit our discussion on work that specifically leverages the knowledge from
%Wikipedia which is what we do in this paper.
%Techniques in this space are collectively called ``wikification'', which links
%terms in a text document to Wikipedia articles, effectively giving each term
%an explicit label (the Wikipedia concept).
%
%Mihalcea and Csomai \cite{MihalceaC07} are among the first to try
%automatically linking terms in a document to Wikipedia concepts. Strube and
%Ponzetto\cite{StrubeP06} propose three kinds of methods to measure the semantic
%distance between two Wikipedia concepts, including path based, information
%context based and text overlap based method, which help to decide the relation
%between two Wikipedia concepts.
%Kulkarni et al. \cite{kulkarni2009collective} propose a wikification method
%taking use of two factors, which are the compatibility between a term and a
%concept and the relatedness between two concepts. A hill-climbing approach
%is used to combine these two factors to get an approximate result.
%In Cucerzan \cite{cucerzan2007large} and Ferragina et al. \cite{ferragina2010tagme}'s
%work, they build a model for each Wikipedia concept. When wikifying a document,
%they merge the models of all the terms' candidate Wikipedia concepts to get
%an average model of the whole document. And then for each term, the candidate concept
%having the best similarity with the average model is picked up to link that term.
%However, merging all the concept models will generate a lot of noise.
%Meij\cite{Meij12} proposed a machine learning approach based on
%n-gram features, concept features, and also Twitter-related features to
%link terms in microblogs to Wikipedia. This work largely depends on
%some special characteristics of microblogs (e.g. hashtag) which makes
%it inapplicable to ordinary plain texts.
%
%Most of the above work uses bag-of-words model to represent Wikipedia concepts.
%%However, using bag-of-words (or even bag-of-terms) model can not capture the
%%semantic information well because
%%1) words and phrases can both be ambiguous and 2) when words are combined into phrases,
%%their original meaning changes.
%We, however, use co-occurrence
%frequency between two MWE concepts to measure their relatedness, which
%attacks avoids the drawbacks of BOW models.
%When wikifying a document, instead of comparing the concept model with
%the document model, like many previous methods do,
%we calculate the best combination of concepts according to
%the mutual co-occurrence frequency among them, and then assign concepts inside
%the combination to the corresponding term, which avoids the noise in
%Cucerzan and Ferragina et al.'s work.
%
%%Mline and Witten \cite{lui2011generation} applied a machine learning
%%method in their work.
%
%%While most of the Wikification methods are supervised,
%%which needs a lot of training to find some good features, we employ
%%an unsupervised method in our work. Our method depends only on the concept
%%co-occurrences which can be mined from the Wikipedia corpus automatically.
%%\KZ{ZY: summarize the above and give a verdict.}
%%Different from the above work,
%%we set a window and try to find the best combination of concepts in this window,
%%instead of merging all the candidate concepts together.
