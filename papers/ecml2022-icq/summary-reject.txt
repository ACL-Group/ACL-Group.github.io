View Reviews
Paper ID2386
Paper TitleStatistically Profiling Biases in Natural Language Reasoning Datasets and Models
Reviewer #1
Questions
1. Should this paper be summary rejected?
No.
2. If your answer to Q1 is Yes, then please select at least one checkbox for the rejection:
This work is not sufficiently positioned with respect to prior work. (Optional to give details in Q3)
This work is mostly incremental and/or the novelty does not warrant IJCAI publication. (Optional to give details in Q3)
This work is not supported with sufficient empirical evidence, or important baselines are missing. (Optional to give details in Q3)
3. (Optional) If your choose at least one of last four reasons in Q2, welcome to give details.
Please see feedback.
4. (Optional) Welcome to write brief comments to feedback to authors.
This work regards an important, million dollar question: what statistical biases affect NLU models? This paper aims, I think, to make a sort of "GLUE benchmark" for stress tests. The paper provides few novel results. For example, we already know that hypothesis only baselines do well on SNLI and MultiNLI, that overlap heuristics matter (Naik et al. 2019), that negation is predictive of "contradiction" (e.g. Gururangan et al. 2019), etc. Every single heuristic provided by this paper has been proposed by someone else, which removes the contribution of new heuristics. The paper's metrics appear to be novel, but there's no discussion of how they relate to other existing metrics for discovering statistical biases. For example, how does your cueness metric improve upon simple PMI (from Gururangan et al.)?

For your related work, do eventually include: Sammons et al 2010, Weston et al. 2016, Glockner et al. 2018, Nie et al. 2019 (Analyzing compositionality-sensitivity of NLI models), Jeretic et al 2020, Saha et al 2020, Williams et al. 2020 (ANLIzing ANLI).

Related works in QA, a sister task to NLI (e.g., Jia and Liang 2017), could also be included. As well as phenomenon-specific platforms, like SyntaxGym and, the new RobustnessGym (though I don't expect a citation to that one).

The selection of datasets is somewhat unexpected. Some are standard, some are not, I think they are unified by each one having been saturated? It would be nice to see inclusion of the new NLI dataset (Adversarial NLI) as opposed to just the current saturated datasets. (I'm not sure what "NLR" refers to, perhaps NLU, but without QA and MT? This term is nonstandard).

All that constructive criticism being said, I really like the topic, I love automatic cue discovery. My favorite section is 4.4, and my favorite tables are Table 3 (although it needs some bolding to give a quick gist from a single glance) and Table 5. I think the potential usability alone of a tool that outputs Table 5, could make this contribution worth accepting. There's something to be said for genuinely intuitive tooling.
Reviewer #2
Questions
1. Should this paper be summary rejected?
No.
4. (Optional) Welcome to write brief comments to feedback to authors.
The paper proposes framework aiming at evaluating potential biases and cues in NLR multiple choice datasets. It also aims at shedding light on the exploration of models from the perspective of statistical cues. The question is how the approach may be more efficient than human annotating.
Reviewer #3
Questions
1. Should this paper be summary rejected?
No.
Reviewer #4
Questions
1. Should this paper be summary rejected?
No.
Reviewer #5
Questions
1. Should this paper be summary rejected?
No.
Reviewer #6
Questions
1. Should this paper be summary rejected?
Yes. (If yes, please answer Q2-Q6)
2. If your answer to Q1 is Yes, then please select at least one checkbox for the rejection:
This work is mostly incremental and/or the novelty does not warrant IJCAI publication. (Optional to give details in Q3)
Reviewer #7
Questions
1. Should this paper be summary rejected?
Yes. (If yes, please answer Q2-Q6)
2. If your answer to Q1 is Yes, then please select at least one checkbox for the rejection:
This work is mostly incremental and/or the novelty does not warrant IJCAI publication. (Optional to give details in Q3)
4. (Optional) Welcome to write brief comments to feedback to authors.
Multiple choice comprehension tasks have proven to be bad for testing comprehension, because of the problems highlighted in the paper. The paper should provide stronger justification why such datasets should be further probed and changed.

Reviewer #8
Questions
1. Should this paper be summary rejected?
Yes. (If yes, please answer Q2-Q6)
2. If your answer to Q1 is Yes, then please select at least one checkbox for the rejection:
There are technical flaws. (Optional to give details in Q3)
This work is mostly incremental and/or the novelty does not warrant IJCAI publication. (Optional to give details in Q3)
3. (Optional) If your choose at least one of last four reasons in Q2, welcome to give details.
The methods proposed in this paper detect clues that may help a classifier predict the right output: however, what is an artificial spurious clue vs what is a relevant clue is not well defined. It is not clear therefore whether the detected clues are actual biasses, or whether for a model to detect such clues is intrinsically an issue.
The paper reports experiments, but should also state how to interpret their results, i.e., provide lessons learnt from these experiments: should researchers avoid using some of the tested datasets, or use them is a specific way, should they favor some models with respect to others?
Reviewer #9
Questions
1. Should this paper be summary rejected?
Yes. (If yes, please answer Q2-Q6)
2. If your answer to Q1 is Yes, then please select at least one checkbox for the rejection:
The paper is not clearly written and/or the presentation has to be significantly improved.
This work is not sufficiently positioned with respect to prior work. (Optional to give details in Q3)
This work is mostly incremental and/or the novelty does not warrant IJCAI publication. (Optional to give details in Q3)
Reviewer #10
Questions
1. Should this paper be summary rejected?
No.
4. (Optional) Welcome to write brief comments to feedback to authors.
It would be interesting to compare the biases recovered by ICQ to the (manually observed cues in) instances filtered by adversarial filtering methods such as AFLite: https://openreview.net/forum?id=H1g8p1BYvS.

