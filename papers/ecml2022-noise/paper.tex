% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{subcaption}
%\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{courier}
\usepackage{color}
\usepackage{epsfig}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multicol}
%\usepackage{threeparttable}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{multirow}
%\usepackage{subfigure}
\usepackage{ragged2e}
\usepackage{xassoccnt}
%\usepackage{amsmath,amsfonts,amssymb,amsthm,amsopn}

%\newcommand{\ft}[1]{\textsc{#1}}
%\newtheorem{example}{Example}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}
%\newcommand{\citealp}[1]{\citeauthor{#1}~\shortcite{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand\BibTeX{B\textsc{ib}\TeX}
\usepackage{amsmath}
\newcommand{\sgn}{\text{sgn}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}} 
\usepackage{tabularx}
\usepackage[figuresright]{rotating}
%\newcommand{\sgn}{\operatorname{sgn}}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%
\begin{document}
%
%\title{Noise Data Augmentation to Enhance the Robustness of Pretrained Models}
\title{Enhancing the Robustness of Reasoning Models by Data Noising}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
%\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Princeton University, Princeton NJ 08544, USA \and
%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%\email{lncs@springer.com}\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}


\maketitle
\begin{abstract}
Commonsense reasoning tasks aim to 
empower machines with the reasoning ability 
about ordinary situations in
our daily life.
% \KZ{Rephrase: Crisis??: Nowadays, many reasoning tasks 
%are in crisis of superficial patterns existing in datasets.}
%Nowadays, many reasoning tasks suffer from the superficial patterns in datasets.
%Such patterns cause 
%models to pay attention to the information unrelated to reasoning, 
%which weakens the reasoning ability and generalization ability of the model.
%\YZ{Use inference ability or reasoning ability?}
%In this paper, we propose
%noise data augmentation methods, 
%which effectively guide the 
%pretrained representation models to ignore superficial patterns and  
%have better performance on adversarial test sets containing challenging examples 
%without any external knowledge.
However, 
%many models suffer from the superficial patterns in datasets.
previous work has shown that BERT-based NLP modelsâ€™ reasoning capabilities are 
over-estimated due to spurious cues in training datasets.
%being easy to learn and that are not correct label predictor features.
%-- just happened to be unbalanced among the different labels. 
For limiting the bias towards 
spurious cues leaking 
%from training to testing datasets,
%123 adversarial instances are generated as ground truth.
we propose three new dataset augmentation techniques in this work, 
which augment a dataset with new generated adversarial instances labeled as a new ``noise'' class. 
Results show that models
trained with the augmented data become more robust
against adversarial test sets containing challenging examples. 
%and original test
%data, beating the strong back-translation baseline
%have better performance on adversarial test sets containing challenging examples 
%without any external knowledge.
%The assumption is that jointly learn to perform reasoning tasks and to classify 
%data as noise will make it harder for the model to use noise as good predictors for reasoning tasks. 
%Three different augmentation techniques are proposed (balanced noise, overlap noise and hypothesis noise). 
%In order to evaluate the proposition while limiting the bias towards 
%superficial patterns leaking from training to testing datasets, 
%adversarial instances are generated as ground truth.

\keywords{natural language processing  \and spurious features \and data augmentation.}
\end{abstract}

	\input{intro}
	\input{approach}
	\input{experiment}
	\input{related}
	\input{conclusion}




\bibliographystyle{splncs04}
\bibliography{coling2020}

\end{document}
