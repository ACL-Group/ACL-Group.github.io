============================================================================

---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                         Relevance (1-5): 4
               Readability/clarity (1-5): 2
                       Originality (1-5): 2
   Technical correctness/soundness (1-5): 2
                   Reproducibility (1-5): 4
                         Substance (1-5): 2

Detailed Comments
---------------------------------------------------------------------------
This paper builds upon the observation that BERT-based NLP models’ capabilities are over-estimated due to superficial patterns in training datasets being easy to learn and that are not correct label predictor features -- they just happened to be unbalanced among the different labels. In this work, new dataset augmentation technique is proposed, which augment a dataset with new generated instances labeled as a new "noise" class. The assumption is that jointly learn to perform reasoning tasks and to classify data as noise will make it harder for the model to use noise as good predictors for reasoning tasks. Three different augmentation techniques are proposed (balanced noise, overlap noise and hypothesis noise). In order to evaluate the proposition while limiting the bias towards superficial patterns leaking from training to testing datasets, adversarial instances are generated as ground truth. 

In that respect, would it be an easiest (and perhaps better?) way to train models on the training dataset A and then evaluate it on the testing set of dataset B?
Another concern I have is about the motivations of that research: as illustrated in Section 3.1.1, it is actually quite easy to spot (and therefore correct) superficial patterns in existing datasets. Isn't it the same for any other datasets we need to fix therefore alleviating the need for augmentation?

In the evaluation section, the concept of ratio needs to be further explained. It is said to be "the ratio of results over the case on original training data". I am afraid I don't get it though it seems quite important in the result table and discussions. It is also not clear how the different augmenting techniques complement each other’s: what about plugging all augmentation techniques and provides an ablation study per technique to see if some (let's say "balance noise" and "overlap noise" do not correlate too much). 

In Section 3.3.2, I find the analysis quite wrong: I do not see in Table 3 how Roberta raised its score by 10% on lexical overlap on entailed test type (98.84 vs 98.44, right ?)

As this paper discusses the Clever Hans effect of NLP models [1], that is the surprising super-performance of BERT-based models in test datasets due to its exploitation of spurious statistical cues in the training dataset. As such, the paper is probably lacking citation of Niven et al work [2]. 

Overall, I also find the manuscript very hard to follow as it is plagued with massive grammatical errors. No manuscript is perfect, especially for us non-native speakers. Yet, in that instance, it needs proof reading since it massively affects the ability of the reader to understand what is offered in the paper. 

Finally, the paper tries to bring simple solutions to a complex one which is an excellent initiative and also because too many datasets contains those leaks between training and testing sets which hinders the research in NLP. That being said, while the augmentation techniques seem harmless (Section 3.3.1), they is harder to tune that what it first seems (noise proportions should be indeed hyperparameters) and it doesn't bring a significant improvement either (see overall row in Table 3). Adding that the manuscript could be further polished, I really think that the initiative and first steps are interesting, but I don't feel it is mature enough for publication this time. 


[1] https://thegradient.pub/nlps-clever-hans-moment-has-arrived/ 
[2] Timothy Niven, Hung-Yu Kao, "Probing Neural Network Comprehension of Natural Language Arguments", pp. 4658–4664, ACL 2019
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
            Overall recommendation (1-5): 1
                        Confidence (1-5): 5
                       Presentation Type: Poster
     Recommendation for Best Paper Award: No


============================================================================
                            REVIEWER #2
============================================================================

---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                         Relevance (1-5): 5
               Readability/clarity (1-5): 2
                       Originality (1-5): 3
   Technical correctness/soundness (1-5): 4
                   Reproducibility (1-5): 3
                         Substance (1-5): 3

Detailed Comments
---------------------------------------------------------------------------
I would like to thank the authors for their interesting work.

The authors propose automatic data augmentation with noisy examples that are substantially different from the original examples (i.e. they are ungrammatical and parts of the input are masked out) and are labeled with a new "Noise" label. They generate 3 types of examples based on shallow patterns in NLI and fact-checking datasets, such as word frequency, to augment 3 datasets - SNLI, MNLI,  FEVER. Last, they fine-tune BERT and RoBERTa on the augmented data and show performance gains on 4 challenge datasets -Stress Test, HAN, Adversarial-NLI,  FEVER's adversarial dataset - for specific augmentation configurations.

Reading the introduction, my intuition was that because it seems to be very easy to distinguish between the noise data and the real data, a model could just learn to distinguish between the two types of data and therefore, would still learn the superficial patterns the noise data is intended to mitigate (e.g. to predict "contradiction" when seeing the word "nobody"). I was not convinced by the results that the noise data solves this issue. 
Applying the proposed method in an effective way seems to be complex and not always work effectively - one should identify the types of issues in the training data and select the augmentation method based on that. Moreover, it is required to run a hyperparameter tuning to find the best augmentation configuration, since some configurations might actually hurt robustness and, in general, models seem to be sensitive to the augmentation configuration. Also, performance gains on Diagnostic NLI Test and Generated FEVER Test (Table 5) seem marginal.

The suggested method, specifically the hypothesis-only augmentation, as well as the experiments, are specific to tasks of the premise-hypothesis format. Arguably, the proposed method could be extended beyond NLI and FEVER, but perhaps the title should be more clear about that.

Another concern is that the paper is difficult to read, due to many clarity issues (see details and examples below). 

Questions:
- Did you try training on a mix of noise examples of different types? If one is interested in using this method, would the best way is to try all training options? This sounds pretty costly.
- How the models perform on the noise data? Do they get 100% accuracy? 

Suggestions:
- Consider adding an example for balanced noise to Figure 3.
- How many noise examples were generated in total? I couldn't find exact numbers.
- For reproducibility, did you use the cased or uncased version of BERT and RoBERTa?

Clarity issues:
- Tables are packed with numbers and it is very difficult to read. Is there a way to simplify them? Maybe report results for the best augmentation configuration and move the full details to an appendix?
- I believe it supposed to be "Figure 3" instead of "Table 3" in section 2.2.2.
- A mistake in equation (3), first transition - it should be L' and not L
- Many sentences are unclear or have grammatical issues, for example:
	- "Most reasoning classification data are unnaturally, since..."
	- "But it consumes a lot and very hard."
	- "Different with"
	- "...we random replace some words..."
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
            Overall recommendation (1-5): 2
                        Confidence (1-5): 4
                       Presentation Type: Poster
     Recommendation for Best Paper Award: No


============================================================================
                            REVIEWER #3
============================================================================

---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                         Relevance (1-5): 5
               Readability/clarity (1-5): 4
                       Originality (1-5): 4
   Technical correctness/soundness (1-5): 4
                   Reproducibility (1-5): 5
                         Substance (1-5): 3

Detailed Comments
---------------------------------------------------------------------------
The paper develops a method to improve the performance of pre-trained language models such as BERT and RoBERTa on common sense reasoning tasks such as MNLI, SNLI, and FEVER. The crucial insight is that these models struggle when evaluated on adversarial data or problems out of domain because they rely too heavily on superficial patterns in the training datasets. In order to overcome this problem and enhance robustness, the paper proposes to augment these datasets by adding noisy items. Three strategies to add noise are explored: balanced distribution noise, overlap noise, and hypothesis-only noise. Based on a large battery of experiments, the paper reports a number of interesting findings, both positive and negative. On the positive side, the paper shows for instance that the proposed noise data augmentation methods succeed in forcing BERT to rely less on superficial cues and more on semantic knowledge, increasing its robustness to spurious overlap features in ''Entailed pas!
 sive'' examples. On the negative side, the paper shows that none of the proposed noise data augmentation methods succeed in improving the results of BERT on ''Antonym'' examples. Overall, I find the paper interesting and well structured. Unfortunately, the quality of the English is extremely low (the paper seems to have been written in a different language and then automatically translated into English).
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
            Overall recommendation (1-5): 4
                        Confidence (1-5): 1
                       Presentation Type: Poster
     Recommendation for Best Paper Award: No

-- 
COLING 2020 - https://www.softconf.com/coling2020/papers
