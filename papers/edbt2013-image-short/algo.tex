\section{Framework}
In this section, we introduce a novel image clustering approach
using text features. 
First, we extract relevant context of an image
using a modified {\em sibling algorithm} \cite{Alcic2010}. Second, with those 
high quality context, we perform a \emph{conceptualization} 
process on the context to gather its abstract semantics. 
Finally, we cluster images using a tri-stage clustering approach.

%The framework of our system is shown in \figref{fig:frame}.
%\begin{figure}[th]
%\begin{center}
%\centering
%\includegraphics[width=\columnwidth]{framework.eps}
%\caption{Framework of Image clustering by Conceptualization}
%\label{fig:frame}
%\end{center}
%\end{figure}

%\figref{fig:frame} shows two scenarios of image clustering. The traditional image clustering scenario is based on
%an assumption that we have all of the images, while the incremental scenario can handle daily incrementing new images.
%Different from the traditional clustering scenario, incremental clustering form and refine the clustering results in
%clustering process. We treat clustering results as classifiers, given a new image, we first use the classifier to judge
%if the new image should be assigned to some cluster or be treated as an independent cluster, then update the classifiers.
%Our framework can handle both of the scenarios. In the following subsections, we introduce the main components of our
%image clustering system step by step.

\input{context}

%\subsection{Conceptualization}
%In most of the previous text based image clustering approaches,
%they use traditional \emph{bag of words}(BOW) method. BOW
%have drawbacks such as losing semantics of phrases and inefficient signals in
%short texts. %{\color{red}KQ: an example to explain the drawbacks of BOW and advantage of BOC}
%We use Wikipedia/Probase concepts to represent the context.
%We call this representation \emph{Conceptualization}.

\subsection{Conceptualization}
Once we gather the necessary context of a web image,
the next step is to convert the context into a list of {\em weighted concepts}
in a process called {\em conceptualization}.
We do this because concepts provides abstraction and therefore high level
understanding of human language. Therefore concepts are better at
capturing the semantics of the context. In this work, we use Wikipedia
as an external knowledge source to provide the domain of all concepts.
The current version of Wikipedia contains over 4 million articles, 
each of which describes a concept which is usually the title of the
article.
%The goal of Wikipedia conceptualization is 
%to use a set of Wikipedia concepts to represent a given text. 
We conceptualize extracted context in two steps: \emph{Wikification} and 
\emph{Scoring}.

Wikification \cite{cucerzan2007large}
is a process that links the noun phrases in a 
plain text to the corresponding Wikipedia articles. 
This is similar to word sense disambiguation in that it assigns a
concept (sense) to a noun phrase.
Because links can be sparse in Wikipedia data, 
we add links to as many unlinked terms as possible in Wikipedia using an 
iterative method.

%We use Wikification to detect Wikipedia concepts
%which can explicitly point out the exact sense of 
%ambiguous phrases in the text.
%Unlike the previous works\cite{cucerzan2007large}\cite{kulkarni2009collective}\cite{ferragina2010tagme},
%we proposed a method based on the co-ocurrences of Wikipedia concepts.
%{\color{red}KQ: an example of Wikification}

Once the context is ``wikified'', it can be represented by the list of
Wikipedia concepts. Next we compute a score called {\em CF-IDF} 
(concept frequency and concept inverse document frequency) to present
the relative importance of each concept with respect to the context.
{\em CF-IDF} is similar to the well-known \emph{TF-IDF} except we compute
the frequency of a concept in the Wikipedia corpus rather than the frequency
of the surface forms (terms). 
To obtain the concept \emph{IDF}, we scan all of the articles in 
Wikipedia corpus, and for each concept, we count the number of documents in
which the concept appears as a link. 
%{\color{red}KQ:Describe the method of adding links to Wikipedia}
\emph{CF-IDF} is computed as follows:
\begin{equation}
\mbox{CF-IDF}(c, d) = cf(c, d) * log\frac{N}{df(c)}
\end{equation}
where $c$ is a Wikipedia concept, $d$ is the given document and 
$N$ is the total number of
Wikipedia articles. $cf(c,d)$ stands for frequency of $c$ in $d$, 
and $df(c)$ for the number of Wikipedia articles
in which $c$ occurs.

%\textbf{Expansion}
%%In many cases, the context of an image is limited. {\color{red}KQ: add examples}
%is used to avoid insufficient signals in the context.
%We expand the concepts we obtained from Wikification, to get more information.
%Expand concepts will have risks of expanding the noise. We only expand top K concepts
%ranked by \emph{CF-IDF} score, to control the expansion of noise. We get the top K concepts,
%from links in the Wikipedia. %{\color{red}KQ: example of concepts expansion}
%
\subsection{Tri-Stage Image Clustering}
We propose a tri-stage image clustering method to process the conceptualized
contexts. The three stages are tag context clustering, 
text context clustering and expansion clustering. 
Each stage form bigger clusters by merging the clusters formed in
previous stage. 

\textbf{Tag context clustering:}
The first stage clusters images by the most reliable signals, i.e., the
tag context because <IMG> tags directly describes the images in question.
We analyze the URL to obtain
Wikipedia concepts. Since the URL may contain random strings sometimes, 
we train a classifier to filter the random strings. 
Then we detect Wikipedia concepts from the URL and the image
label to build a concept vector, in which each dimension is a Wikipedia concept with CF-IDF as its weight. Finally we cluster the vectors using
Hierarchical Agglomerative Clustering(HAC) algorithm with cosine similarity.

\textbf{Text context clustering:}
The tag context clustering forms many small and tight clusters.
Next, we concatenate the text contexts of all images from a cluster 
to form the text contexts for clusters. This stage also
employs HAC algorithm to further merge conceptually similar clusters.

\textbf{Expansion clustering:}
The above two clustering steps are based on the exact concepts extracted
from the web pages. To discover more hidden signals, we 
expand our contexts by taking the top-k ranked concepts (by CF-IDF score) 
of each cluster, and replace each of the top concepts by the  
linked concepts in their Wikipedia articles. The result will be an 
expanded concept vector for each cluster. 
We then further merge some of the clusters by the new concept vectors.

