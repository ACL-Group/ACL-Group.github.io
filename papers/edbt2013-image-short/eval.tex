\section{Preliminary Results}
We present some preliminary results on web image clustering.
In the following subsections, we first introduce the data set,
then the evaluation metrics and finally compare the accuracy
of our approach with the baseline bag-of-word context clustering
as well as a state-of-the-art hybrid clustering method.

\subsection{Data Set}
We obtain a benchmark data set of web image data
from \emph{Google Image Search},
by querying 56 ambiguous entity names, such as \emph{kiwi}, \emph{pluto} and
\emph{explorer}. For each entity, we download the top 100 images along
with the original web pages of the images.
%Due to the fact that some of the web pages are not available,
%we drop the invalid web pages and keep downloading 100 pages for each
%entity.
The whole data set contains 5,600 web pages/images in total.

\subsection{Evaluation Metric}
We adopt two metrics to evaluate the result of image clustering:
\emph{$F_1$} and \emph{NMI}.
$F_1$ score combines \emph{Purity} and \emph{Inverse Purity} of the clusters.
Similar to the $F_1$ score used in information retrieval task,
$F_1$ score is computed in the following way:
\begin{equation}
F_1(C,L)=\frac{2\times Purity(C,L)\times IPur(C,L)}{Purity(C,L)+IPur(C,L)}
\end{equation}
where $IPur$ stands for \emph{Inverse Purity}.
\emph{Purity} measures the quality within a cluster. It is computed as follows:
\begin{equation}
purity(C,L)=\frac{1}{N}\sum_i{\max_j{|c_i\cap l_j|}}, c_i\in C\;and\;l_j\in L
\label{equ:purity}
\end{equation}
where $C$ is the clusters and $L$ is the set ground truth labels.
\emph{Inverse purity} is
computed by inverting the ground truth and the resulting clusters in
\equref{equ:purity}, i.e. $purity(L,C)$.

NMI (Normalized Mutual Information) describes the
amount of common information between the resulting
clusters and the ground truth.
\equref{nmi} shows the definition of NMI score, where \equref{mi}
stands for mutual information $I(C,L)$.
\begin{equation}
\label{nmi}
\mbox{NMI}(C,L)=\frac{I(C,L)}{(H(C)+H(L))/2}
\end{equation}
\begin{equation}
\label{mi}
I(C,L)=\sum_i{\sum_j{\frac{|c_i\cap l_j|}{N}log\frac{N|c_i\cap l_j|}{|c_i||l_j|}}}
\end{equation}
$H(C)$ and $H(L)$ are entropy of clusters $C$ and ground truth $L$ respectively.


\subsection{Image Clustering Accuracy}
We manually label 14 entities from the data set for testing.
\tabref{tab:result} shows the comparison of
the tri-stage clustering(TSC) algorithm against the hybrid clustering method
MMCP \cite{Fu2011} and
a baseline bag-of-words(BOW) approach.

BOW has very poor F1 and NMI score, since only considering the
concepts appears in
the context(sometime the context is very short) without external knowledge will have insufficient signals
to merge the small clusters. Compared to BOW, the TSC algorithm bring
much more signals to identify the similarity between two clusters.
As shown in the table, F1 and NMI score have around 0.3 and 0.18 improvement respectively.

In our experiment, MMCP algorithm was implemented using two modalities: color histogram and
textual feature (bag of words). As shown in \tabref{tab:result},
our system outperforms this state-of-the-art framework by significant
margins. We thus argue that conceptualization of the image context
is more powerful than combination of simple text and visual features.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[th]
\centering \scriptsize
\caption{Results on different queries with different methods}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
    {\bf } & \multicolumn{ 2}{|c|}{{\bf TSC}} & \multicolumn{ 2}{|c|}{{\bf MMCP}} & \multicolumn{ 2}{|c|}{{\bf BOW}} \\
\hline
{\bf Query} &   {\bf F1} &  {\bf NMI} &   {\bf F1} &  {\bf NMI} &   {\bf F1} &  {\bf NMI} \\
\hline
    Amazon & {\bf 0.87} & {\bf 0.49} &       0.46 &       0.42 &        0.3 &       0.23 \\
\hline
     Apple &       0.72 &       0.29 & {\bf 0.75} &  {\bf 0.5} &        0.3 &       0.25 \\
\hline
      Bean & {\bf 0.92} & {\bf 0.85} &       0.47 &       0.51 &       0.61 &       0.58 \\
\hline
  Emirates & {\bf 0.82} &        0.2 &       0.43 & {\bf 0.42} &       0.52 &        0.3 \\
\hline
  Explorer & {\bf 0.93} & {\bf 0.67} &       0.62 &       0.42 &       0.76 &       0.47 \\
\hline
     Focus &  {\bf 0.9} & {\bf 0.69} &       0.53 &       0.43 &       0.43 &       0.32 \\
\hline
      Kiwi & {\bf 0.84} & {\bf 0.48} &       0.53 &       0.42 &       0.53 &       0.34 \\
\hline
    Malibu & {\bf 0.89} & {\bf 0.69} &       0.64 &       0.44 &        0.7 &       0.48 \\
\hline
     Pluto & {\bf 0.86} & {\bf 0.45} &       0.61 &       0.31 &       0.61 &       0.25 \\
\hline
      Polo & {\bf 0.87} & {\bf 0.63} &       0.56 &       0.48 &       0.74 &        0.5 \\
\hline
  Sante Fe & {\bf 0.87} & {\bf 0.55} &       0.49 &       0.37 &       0.64 &       0.36 \\
\hline
      Tick & {\bf 0.78} &  {\bf 0.4} &       0.65 &       0.38 &       0.31 &       0.27 \\
\hline
      Time &      0.59  &       0.32 &       0.59 &  {\bf 0.7} &       0.41 &       0.22 \\
\hline
    Tucson & {\bf 0.94} & {\bf 0.71} &       0.53 &       0.41 &       0.67 &       0.38 \\
\hline
{\bf Avg.} & {\bf 0.84} & {\bf 0.53} &      0.56  &      0.44  &      0.54  &      0.35  \\
\hline
\end{tabular}
\label{tab:result}
\end{table}

%\subsection{Performance}
%Our system is efficient on
