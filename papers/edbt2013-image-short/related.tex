\section{Related Work}
\label{related}

In this section, we will introduce current achievements in the field of
image search result clustering, and then some methods we used in our project,
including context extraction and knowledge bases.
% FIXME: rewrite this paragraph.

%\subsection{Image Search Results Clustering}

The most popular image retrieval method is content-based image
retrieval (CBIR), which means any technology helps to organize images and
classify them by their visual content \cite{Datta2008}. There are several
previous work about CBIR. A previous work \cite{Chang1984} is a picture indexing
and abstraction approach for pictorial database retrieval. And there are a lot
of commercial CBIR image search engines, such as \cite{Smith1996}.

Recent CBIR work has already obtained some result in the field of image
classifying. Zhong et al. \cite{ZhongLL11} designed a deep architecture and
learning algorithms in their paper. The architecture is the same as the
multi-layer physical structure of the human visual cortex, which is associated
with many cognitive abilities for human beings. There are three stages in their
algorithm: bilinear discriminant initialization, greedy layer-wise
reconstruction, and global fineturning. With this algorithm, they could tell
various kinds of picture from each other, such as airplanes, motorbikes, faces,
tall buildings and so on.

Fu et al. \cite{Fu2011} gave a constraint propagation framework for multi-model
situations. They construct multiple graphs for each of the dataset modalities,
then define a random walk process on them. In this random walk process, the
walker has probabilities of walking to the same node in other graphs. Finally,
they could obtain some propagated constraints, which could be used for
constrained clustering.
% TODO: this para may not be completed at all.

% But for search engines like \textit{Google Image}, they usually use the original
% webpage to get the image search result. How can we make use of these text
% information?
% (Move to introduction)

All these researches above use some low-level signals of images. But for the
results from search engine, we have some textual signals, such as the
surrounding text of picture, the search keyword and the URL information.

To use these textual information, Cai et al. made some progress. In his paper
\cite{Cai2004b,Cai2004}, he represent a method to reduce the gap between
visual and text information, use a page segmentation algorithm named VIPS to
extract text information, which is mentioned in \cite{VIPS}. They gave three
kinds of representations for images: visual feature based representation,
textual feature based representation and link graph based representation. Since
they think this it's an open problem for extracting sematic information by
visual feature of pictures, they only use textual feature and link graph to
finish their two-level clustering algorithm.

And there are even more researches, such as \cite{Feng2004,Gao2005}.
Feng et al. \cite{Feng2004} use the surrounding text of image and a
visual-based classifier to build a co-training framework. And Gao et al.
represented the relationship among low-level features, images and the
surrounding texts by building a tripartite graph in \cite{Gao2005}.

Jing et al. \cite{Jing2006} introduced a novel method to return clustered image
search result. Instead of clustering on image search result, they clustered the
abstract webpage search result from Google, and extract an accurate keyword
from each of the cluster. Then they use these accurate keywords to search on the
image search engine.

However, for all these researches using textual information above, they just
extract the surrounding text of images (or using webpage abstract text in
\cite{Jing2006}), and there is no further processing on the surrounding text as
well. For the search results from image search engine, the surrounding text
should have a lot of noises which prevent us from understanding the meaning of
the surrounding text.
% FIXME: too many "surrounding text"s here!

In our work, we use \textit{Wikification} method to help us extract useful
information from surrounding text. Some related work of \textit{Wikification}
is mentioned in Section \ref{wikification}.
% TODO: Is there any other algorithms we used? More description about our
% algorithm needed here.

\subsection{Web Document Search Results Clustering}

Besides image search results clustering, there are also many researches about
web document search results clustering. The primary difference between them is
that there is no image information in web document search results clustering
methods. That is to say, we cannot extract the low-level visual signals and
surrounding texts in web document search results clustering.

Hearst and Pedersen \cite{Hearst1996} presented a cluster-based document
browsing method in 1996. They use pariwise cosine similarity between document
vectors and the Fractionation clustering algorithm in their Scatter/Gather
system. The Fractionation algorithm need number of clusters as an input, and the
complexity is $O(kn)$, while k is the numberof clusters, and n is the number of
documents.

Leouski and Croft \cite{Leouski1996} evaluated some of the search results clustering
algorithms. According to their achievement, the mainly two clustering techniques
used in search results clustering are information retrieval and machine
learning.

Zeng et al. \cite{Zeng2004} introduced a web search results clustering algorithm
by machine learning. Firstly they extracts all possible phrases from the
contents with n-grams, and learn some properties for each phrase. Then they use
a regression model learned from training data, and apply on the properties to
get a \textit{salience score} for each phrase, take the phrases with top score
as \textit{salience phrase}. These \textit{salience phrase} are actually the
names of candidate clusters which are merged later.

% TODO: more algorithms

\subsection{Image Context Extraction}

There is not so much work about image context extraction. In \cite{Alcic2010},
Alcic and Conrad measured some context extraction methods, including
N-terms-environment \cite{Coelho2004}, the Monash extractor \cite{Fauzi2009},
siblings extractor which simply select the sibling text nodes of the image node
in the DOM tree, and the VIPS \cite{VIPS} algorithm which we mentioned before.

Since it's hard to mark the exact image context and it's a little overly
restrictive for testing with a too strong criterion, they consider the result
as a correct one when there is partially accordance between the output and the
test set. They used an F-score which is defined as

$$F_{score} = 2 \cdot \frac{P \cdot R}{P + R}$$

to represent the final result, where P is the precision and R is the recall.
And they compute the standard deviation of the F-score to measure the stability
of all the algorithms.

To our surprise, the algorithms based on DOM have the best F-score result, while
VIPS performs worst. Since the siblings extractor have good F-score as well
as good time performance while the VIPS algorithm is slow, badly-resulted and
unstable, we finally use an optimized siblings extractor in our work.

% TODO: more context extraction algorithms?

\subsection{Wikification}
\label{wikification}

In our work, we used a \textit{Wikification} method as a
\textit{Word Sense Disambiguation} (WSD) algorithm. As it's an open problem,
there are a lot of previous works on WSD \cite{GuthrieGWA91,Li1998:wcd,ChungKML01,Stokoe2003:WSD,Fernandez-AmorosGSS10}.

However, Mihalcea and Csomai \cite{MihalceaC07} are the first to try
automatically linking terms in a document to Wikipedia concept. Strube and
Ponzetto \cite{StrubeP06} provided a method to measure the semantic distance
between two Wikipedia concepts. This is greatly useful for Wikification. There
are three kinds of measure methods they mentioned: path based, information
context based and text overlap based. But in our work, we use a totally
different method, which is the co-occurrence between two concepts.

Kulkarni et al. \cite{kulkarni2009collective} propose a method to map a term to
a certain Wikipedia concept. There are two factors used to determine the
Wikipedia concept, one is the compatibility between term an concept, the other
is relatedness between concepts. They have 12 features to determine the
compatibility, and use the category which the concept belongs to and the
co-citation for the relatedness, then combine them together and use a
hill-climbing appoach to get the approximate result.

In Cucerzan \cite{cucerzan2007large} and Ferragina et al. \cite{ferragina2010tagme}'s
work, they both use other terms' candidate Wikipedia concepts in the text to
help the term concept disambiguation. However, while this will bring a lot of
noise, Mline and Witten \cite{lui2011generation} applied a machine learning
method in their work. In fact, most of the Wikification methods are supervised,
which needs a lot of training to find some good features. To avoid this, we
choose a unsupervised method in our work. Different from \cite{cucerzan2007large,ferragina2010tagme},
we set a window and try to find the best combination of concepts in this window,
instead of merging all the candidate concepts together.
