Dear Reviewers:

Thanks for your earnest comments and advice. Below are our responses.

#R1

A1: c_k = {c_0, c_1}, where c_0 means non-causal class, and c_1 means the causal class.
	
A2: In the first iteration, we directly classify the triples by pmi feature and 
take the results as initial labeled corpus. Then, we estimate the P(...|c_k) from this 
automaticlly labeled data.
	
A3: We classify the triples using Naive Bayesian Classifier with parameters updated by 
EM methods. The initial parameters in NB Classifier come from the first iteration 
mentioned in A1.

A4: We do mentioned Do's worthy work in Section 2. Our results of noun-noun pairs are difficult
to compare with Do's work which extracts casuality expressed by event pairs.

A5: Our dataset is released under the url in Section 4.1. Sentences in this dataset contain
cue verbs and hence are compatible with all the competing methods in the paper.

#R2

A1:
SemEval dataset is not exactly compatible with our extractions framework because
may of the training and test sentences do not contain any cue verbs, which is required
by our method. Also, pmi and tbs features cannot be accurately calculated for SemEval, 
as it doesn't provide complete corpus from which dataset are drawn from.

A2: The "prob(np1, np2)" is the probability of the np1, np2 appearing in the same sentence.
	
A3: The 71 cues suggested by Girju are all verbal patterns. Non-verb POS weakens 
the pattern, and penalizing it is useful for some ambiguous patterns such as
start, spark, etc. We use labels like SemEval.
eg. Year <e1>1 BC</e1> was a common year starting of the <e2>Julian calendar</e2>.
Non-verb POS of "start" weakens the causal motive.
	
A4: We treat the cue words in the text span as valid causal marker, and everything else 
in the span may be noise and weaken the causal association. Therefore we measure
the proportion of the valid markers in the span by this dividing the total length of
the text span.
	
A5: The pmi feature can be directly obtained from the corpus, and it alone achieves 
the precision of 0.59. P(ti(fj)|ck) expresses how we transfer the numeric features into
probabilities. We divided each feature into 10 intervals and count triples that fall into them.

A6: Inter-annotator agreement: If np1 is the cause of np2 or np2 is the effect of np1, 
we say the two nps form a causal pair. We make this judgement just by looking at the
two nps, regardless of the context. Two annotators manually judged and labeled 
the instances. Only those pairs with the identical labels from both annotators
are added to our training/test dataset.

#R3

A1: Our work just focuses on obtaining nominal causal pairs from [NP, causal_verb, NP] 
patterns. This is a bit different from the problem attacked by the two mentioned papers 
which are really nice work. The mentioned work extracts not only noun phrases but also
unary predicates; and not only causal pairs and also inhibitory pairs. Therefore,
we chose not to compare with that work directly.
