\section{The Approach}
\label{sec:approach}


Causal relation exists between an event and another event or factor and phenomenon.
Generally, cause and effect can be changes, events, processes, properties, processes, facts, states and so on.
Therefore the parts of speech may be verb, noun, adjective or adverb.
We focus on finding the causality among all types of words mentioned before.
Our approach simply follows three steps.
First, we extract candidate pairs which include the cause and effect word as well as the times the pair has ever appeared in corpus.
Second, we extract some features including sementic, graphic and statistic features from the pairs and the statical data.
And finally, we employ a learning algorithm to classify each of the pair to either a true causal relation or not.

%Causal relation exists between two events, one being the cause, and the
%other being effect. We focus on noun phrase causality which
%both the cause and the effect are noun phrases. Our approach generally
%follows three steps. First, we extract candidate triples which include the
%cause, the effect, and a text span that contains the causal cue patterns
%between the two events. The {\em text span} is defined as the string between
%the two noun phrases. Second, we extract some crucial features from
%the triples as alternate representations. And finally,
%we employ a learning algorithm to classify
%each of the triples to either a true causal relation or not.

\subsection{Causal Candidates Extraction}
\label{sec:extract}
In order to extract causal candidates from open domain text, several cue patterns are used here.
{\em Cue patterns} which is composed of discourse marker and predicate pattern are the words that imply the probability of existence of causal relation.
{\em Discourse marker} is always the connectives. It splits a sentence into two complete sub sentences such as \textquotedblleft because\textquotedblright \, and \textquotedblleft if \ldots then \ldots \textquotedblright.
{\em Predicate word} is always a verb or verb phrase, such as \textquotedblleft lead to\textquotedblright \ and \textquotedblleft give rise to\textquotedblright \ . Sentences contain predicate words are usually separated into two incomplete parts.
Patterns that we used are shown below and are considered to be strong and comprehensive enough to indicate the existence of causal relation.\\
Our training data is extracted from 1/10 of the Bing \textquotedblleft Tier0\textquotedblright \  corpus.
Snapshot is generated in February, 2013 and the total amount of webpages is about 1.6 billion whose size is nearly 10T.
Sentences containing those patterns are extracted first.
Each sentence is then divided into two parts by the pattern. Actually, we just remain 10 words in each part because we argue that words far away from the cue pattern carry little information about causal relation.
We assume that the former part contains the cause word and the latter one has the effect word.
Words are splitted and filtered because some words are useless and may even bring noises such as \textquotedblleft about\textquotedblright, \textquotedblleft after\textquotedblright, \textquotedblleft else\textquotedblright, which we call {\em stop words}.
Finally, every word before the cue pattern is roughly connected to every word after it to form the candidate pairs.
The corresponding cue pattern and the count of the pair is also recorded and accumulated.
For example, a record is saved like this,
\begin{description}
\item[R1] lack\quad reduce\quad35:28:17:\ldots:34:4:3:2 \quad 1187
\end{description}
The word \textquotedblleft  lack \textquotedblright \, and \textquotedblleft reduce\textquotedblright \, correspond to cause and effect word respectively. Counts related to different cue patterns are spplitted by \textquotedblleft :\textquotedblright. Since we use 53 patterns, 53 number is recorded for each pair. The last number of the entry is the sum of the former counts meaning total frequency of the pair.


%we first employ the open information extraction system Reverb
%\cite{BankoCSBE07} to extract all relation triples that satisfy one of
%the 71 causal cue patterns which were previously reported by
%Girju et al.\cite{girju2003automatic}. We extract causal text span
%and noun-phrase chunkers together. Then we filter out those chunkers whose
%head word are not included in WordNet\cite{wordnet}.
%At this point, we obtain the causal candidates from text.
%The candidates are the triples in the following form:
%\triple{$NP1$}{$cue$}{$NP2$}, where {\em cue} stands for the
%causal cue pattern such as ``cause'', ``lead to'', etc. For example,
%from sentence A-2 in Section
%\ref{sec:intro}, we can get the causal candidate triple as follows:
%\triple{acid\_rain}{contribute\_to}{corrosion\_of\_metals}.
%We may get more than one triple from each sentence.


\subsection{Feature Extraction}
\label{sec:features}

In Section \ref{sec:extract}, we obtained the causal candidates by causal cue patterns in which case we take both the correlation and causality into consideration.
Based on that, We then extract features from three angles: statistic, graphic and semantic.
Statistic features focus on the mathematical process such as frequency, standard deviation and so on.
Graphic features concentrate on the relations among pairs. Some common structures are used as feature here.
Semantic features pay more attention to the words' meanings since the causal relationship exists in the semanteme itself.

\subsubsection{Semantic Features}
It is natural to think that causality is more concerned with semantic meaning rather than grammatical rules.
Three reasons are shown here that why we extract semantic features:
\begin{itemize}
\item A word can have a lot of different meanings. In many cases, whether a pair of word has causality relationship depends on the context to a great extent. When judging a causality relationship, we need to combine the word with the specific environment to determine which meaning the word is. There is no doubt that the word's meaning will make a big difference in deciding the relationship.
\item The POS of words may also influence the judgment. Our goal is to find causality relationship in whole text. Not only the causality between nouns but also the causality between verbs, adjectives, adverbs and between each other. Therefore, it is rather important to fetch the POS information of words, which can also be classified into semantic feature.
\item Different categories of words have differences in causing different thing. From this point of view, we need to classify all nouns into several categories and the certain category for a certain word can be a simple and crude symbol which can be used to represent the feature of meaning. Besides the categories also provide us with another point of view to evaluate the confidence of the causality between a pair of words.  For example, \textquotedblleft motivation\textquotedblright \ and \textquotedblleft film\textquotedblright \ are two categories in our final set of category. Compared with \textquotedblleft motivation\textquotedblright \ , \textquotedblleft film\textquotedblright \ is much more likely to cause changes in \textquotedblleft emotion\textquotedblright \ . On the contrary, \textquotedblleft motivation\textquotedblright \ will contribute more to a result of \textquotedblleft action\textquotedblright \ than \textquotedblleft film\textquotedblright \ . In a word, probability for causality is associated with the category that cause and effect words belong to.
\end{itemize}

Senmantic information is an important feature but hard to extract. In our experiment, WordNet and Google N-gram are two main corpuses used here.

WordNet has a tree structure and there is always a shared hypernyms for any two words.
The higher level the word is, the more words will converge on it.
Therefore, we investigate the structure of the tree and find out 31 top-level words to represent all categories which are showed below:
The 31 categories can cover most of the words in WordNet which in turn means most of the words can be classified into these 31 categories.


Most semantic information for noun can be extracted from hypernyms directly while the semantic feature for verbs, adjectives, and adverbs needs to be found in context. We believe that the object and subject for a verb or the word modified by an adjective or an adverb contains more useful and significant information. Google 2-gram is used here to help find those significant words by the following steps:
\begin{itemize}
\item For verbs: We extract all the objects and subjects of a certain verb. To extract objects, two situations must be included. The first is direct object, the noun phrase which is the accusative object of the verb. For example, from the sentence \textquotedblleft Smoking causes cancer.\textquotedblright \ we can know the cancer is a direct object of cause.  The second situation needs to be taken into consideration is the passive nominal subject. Once again, let¡¯s take \textquotedblleft smoke\textquotedblright \ and \textquotedblleft cance\textquotedblright \ as example. This time the sentence is changed to \textquotedblleft Cancer can be caused by smoke\textquotedblright \, the cancer is the passive nominal subject and it's actually the object of cause. For subject, the main situation needs to be considered is just the same as object.
\item For adjectives: We extract the modified nouns of the adjective to have a better understanding about where the adjective is mostly used. For example, we will get the word \textquotedblleft algorithm\textquotedblright \ for adjective \textquotedblleft powerful \textquotedblright \ in the 2-gram \textquotedblleft powerful algorithm\textquotedblright \ .
\item For adverbs: We extract the modified words for adverb just like adjective. For example, we will get the core modified word \textquotedblleft speaking\textquotedblright \ for adverb \textquotedblleft generally \textquotedblright \ from the 2-gram \textquotedblleft generally speaking\textquotedblright \ .
\end{itemize}


Those significant words share some similarities. First, there is a dependency between the significant words and the original one. Second, those words are all nouns and can be classified into 31 categories.
The categories of the significant words represent the categories of the original word. Frequency is also recorded because one word can have many significant words so it can be classified into several categories.

When a new word comes from original corpus, our processing procedure is showed below:
 In general, WordNet is mainly used to classify words into different categories and extract the information concerning the meaning of the word while Google N-gram, on the other hand, is mainly used to get the information of the position and possible position for a word in the context, which can be used to analyze different combination and collocation for each certain word.
we can say that Wordnet is used to extract features for nouns while Google N-gram is used for verbs, adjectives and adverbs.

\subsubsection{Graphic Features}
Though the most intuitive impression of causality relation comes from the context, the statistic data and the graphic features are of great importance in preventing the result from being affected by the subjective impression and make the most use of original corpus.

Suppose we imagine the words as nodes and the causal relations between two words as lines, all those above form a huge net.
It is natural to believe that some structures do have meanings to the causal relation.
We combine the simple \{0, 1\} label together with the frequency label to describe the feature.
\begin{enumerate}[1)]
\item {\em Sibling Structure}\\
Sibling structure considers the situation where pairs share the same cause or effect.
There are actually two types of sibling structure as is shown below:
For example, pairs $<$lack, reduce$>$.
One feature The features for Sibling Structure Type I of this pair is [1, 9575] where 1 means this pair does exist in such structure and 9575 means the total number of effects when the \textquotedblleft lack\textquotedblright \ is used as cause. It is reasonable that some words have high possibility to be a cause word or effect word. So, the neighbour amount of the other word will relatively be larger. In another word, the second feature of this pair here evaluates the cause word's weight.
\item {\em Loop Structure}\\
Loop structure also consists of two types. It can exist between 2 words or among 3 words. The following figure shows the two types of loop structure:
The line with arrow starts from the word that we think is cause word and ends to the word which we believe to be an effect word.
Obviously, Type I describes the loop relationship between two words.
It is acceptable that few causal pairs may exist in such structure where A causes B and B causes A. Some examples that show no causal relation is as follows: $<$lack, controversy$>$, $<$lack, projection$>$, $<$lack, music$>$ and so on.
As the same, Type II depicts the relation among three words. A few more pairs may exist in such structure but we still prefer to believe pairs in such structures are less reliable.
\item {\em  Closure Structure}\\
Closure structure exists among three words. Any two words of the three are supposed to have causal relation. Its structure is shown below:
There are three pairs here: $<$Word1, Word2$>$, $<$Word2, Word3$>$, $<$Word1, Word3$>$. It is an ideal structure because if there exists causal relation in the former two pairs, we may strongly believe that the third pair also has the same relation. The causal relationship is enhanced here. It is worth mentioning that different positions of the pairs in the triangle may have different meanings so we use three-bit feature to describe it.
\end{enumerate}


\subsubsection{Statistic Features}
Before we extract the statistic features, we propose three hypothesis and extract top 142 pairs according to their total frequency along with 1000 random pairs to prove the validity. The two sets of pairs are marked causal or non-causal manually by three persons.
\begin{description}
\item[H1] The frequency positively affect the possibility of causal relation.
\end{description}


\begin{description}
\item[H2] The smaller standard deviation of pattern distribute, the more possibility of causality.
\end{description}
It is apparently that one pair is strongly causal if most patterns contribute to \textquotedblleft prove\textquotedblright \, the causality. For example, the distribution of different pattern of pair $<$disease, death$>$ is 1100 : 410 : 270 : 548 : 9 : 2 : 0 : 0 : 12 : 29 : 55 : 23 : 1 : 0 : 2 : 6 : 159 : 404 : 48 : 220 : 16 : 86 : 135 : 301 : 334 : 58 : 35 : 23 : 413 : 0 : 0 : 12 : 8896 : 0 : 5 : 20 : 3 : 0 : 45 : 0 : 12 : 5 : 8 : 5 : 0 : 2638 : 1532 : 7969 : 948 : 23358 : 17 : 15 : 11. Almost all patterns are found to be in sentences together with the word \textquotedblleft disease\textquotedblright \, and \textquotedblleft death\textquotedblright. So the pair is more likely to have causal relation.
Thus, we use the deviation to represent the distribution of pattern frequency.

\begin{description}
\item[H3] Different cue patterns makes different contribution to the causality of the pair.
\end{description}
 We do a rank of patterns according to the previous marked 142 pairs. If one pair has causality, the existence count with one pattern is added to corresponding pattern weight, otherwise, subtract from it. After computing all weights of the patterns, we design a feature called {\em pair weight} which is defined using the formula:
\begin{equation}
pair\_weight = existence\_count * pattern\_weight
\end{equation}
Clearly, each pair has 53 pair\_weight features.

Above all, we extract frequency, standard deviation as feature and pattern score features.





%The features extracted in this paper are based on the
%{\em causal association intuition}. That is, all the causal pairs
%are correlated, but not all correlated pairs are causal.
%If we only use association metrics to detect causal pairs,
%it will not be sufficient. However, we obtained the causal candidates
%by causal cue patterns in Section \ref{sec:extract}, then
%the association metric can be additional hint to identify the valid
%causal pairs. Based on these considerations, we extract four
%numeric features from each candidate. All these features are simple
%but effective.
%
%The most intuitive feature is the pointwise manual information (PMI)
%value  $pmi$
%between two noun phrases of each candidate triple:
%\[pmi(np_1, np_2) = \log \left(\frac{prob(np_1, np_2)}{prob(np_1) prob(np_2)}\right).\]
%This feature presents the events distribution on original corpus.
%
%Then, we analyze the \emph{cues} in candidate triples as well as their
%original text spans and define another three features.
%We propose a feature $vbs$ to punish the association metric score
%if the cue patterns contained in the text span is other part of speech
%than verb. This feature can disambiguate candidates which contain
%low quality cue patterns, such as ``start'', ``spark'', etc.
%
%We employ OpenNLP for basic NLP analysis and processing.
%We further analyze the causality using statistics of noun concepts.
%Some nominal sequences with high or medium frequency (depending on the corpus)
%tend to co-occur with certain causal cue patterns.
%This tendency can help to measure causal correlation between two
%noun phrases. For triple \triple{$np_1$}{cue}{$np_2$},
%we define the {\em balanced tanimoto score (bts)} feature as follows:
%\[bts = \frac{{shr}}{{n_1}} + \frac{{shr}}{{n_2}}\]
%where $n_1$ and $n_2$ are the number of causal cue patterns
%that have co-occurred with $np_1$ and $np_2$ respectively, and $shr$ is the number of causal cue patterns which co-occurred with both $np_1$ and $np_2$.
%
%The last feature $rdist$ is to measure the relative distance between
%$np_1$ and $np_2$, we just divide the length of \emph{cue} by
%the length of text span that contains it.

So far, we extract all the three types of features,
which can be used to represent the candidate pairs.

\subsection{Learning Algorithm}
\label{sec:learning}
In this part, we build a classifier to identify true causal relations
in the candidate triples.

%\subsubsection{Data Set}
%
%We use the Wikipedia articles as our corpus. After the preprocessing and
%filtering step in Section \ref{sec:extract}, we obtained 5754 candidate triples.
%We select 200 instances and manually classified them as ``causal'' or
%``non-causal''. We take 150 out of 200 instances as the training set,
%and take the other 50 instances as the test set.

\subsubsection{Supervised Learning Method}

Our first attempt of a supervised learning method uses a linear model
to combine
the numeric features. The linear classifier is defined as follows:
\[s(t) = \sum\limits_{i = 1}^4 {w_i*f_i} \]
where $s$ is the metric value of the triple $t$, $f_i$ is the feature
of the triple $t$, and $w_i$ is the weights of $f_i$.
To solve the model, we only need to get the weights $w_i$ and the
threshold $m$. We use the threshold to classify triples. For triple
$t=$\triple{$np_i$}{$cue$}{$np_j$}, if $s(t) \ge m$, we take
\pair{$np_i$}{$np_j$} as causal pair, otherwise we reject this pair
as non-causal. To this end, we use the simulated annealing algorithm
\cite{KirkpatrickGV83}
to search for a reasonable solution ($w_1$, $w_2$, $w_3$, $w_4, m$).
%We train the parameters with the training set built in Section 3.3.1, and evaluate the model on the test set.

Since the features we extract are all numeric, we further adopt
an implementation of Logistic Regression model\cite{witten2005data} to build the classifier.
%We compare our proposed models to previous work
%(Girju et al., 2003; Chang and Choi, 2006) on the same dataset.
%The results are showed in table1.

\subsubsection{Unsupervised Learning Method}

The annotated work for supervised models is time consuming and cannot scale up.
We next propose an unsupervised method to solve this problem.
Our inspiration comes from Chang and Choi\cite{chang2006incremental}.
Generally speaking, we embed our causal association metrics model
into an EM algorithm. During each iteration, we update the model parameters
in E-step, and classify the candidates in M-step until the parameters converge.
 We built the initial classifier with $pmi$ feature only.
The top 45\% of triples are classified into ``causal'', and the rest are
classified into ``non-causal''. Then, for the following classification of
each iteration, we reconstruct a Naive Bayesian Classifier
with new parameters. The class $c$ of triple $t$ is computed as follows:

\[c = \mathop {\arg \max }\limits_{c_k} P(c_k|t_i) = \mathop {\arg \max }\limits_{c_k} \frac{{P(c_k)P(t_i|c_k)}}{{P(t_i)}}\]

We assume that all the causal features are independent.
In our model, $P(t_i|c_k)$ can be rewritten as:
\[P(t_i|c_k) = p(ccpt_i|c_k)\prod\limits_{j = 1}^4 {P(t_i(f_j)|c_k)} \]
where $t_i(f_j)$ gives the $j$th numeric feature value of triple $t_i$,
and $cppt_i$ is the causal cue phrase of triple $t_i$.
We define the $P(t_i(f_j)|c_k)$ as follows:

\[P(t_i(f_j)|c_k) = \frac{{freq(\forall t_x(f_j) \in \theta (t_i(f_i)))}}{{freq(\forall t_x \in c_k)}}\]
where $t_x$ presents the arbitrary triple, \(\theta (t_i(f_i))\) gives an interval which $t_i(f_i)$
belongs to. The range of each feature is pre-divided into 10 intervals,
so that they would not change in each iteration.

