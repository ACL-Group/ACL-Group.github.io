\section{Experimental Results}
\label{sec:eval}

In this section, we first show how we prepare the data for argument
conceptualization. Then, we use some example concepts
generated by our algorithm to show the advantage of our algorithm
against selectional preference, FrameNet and ReVerb~\cite{FaderSE11}.
We also quantitatively evaluate the accuracies of our algorithm and SP
on two taxonomies. Finally, we apply our algorithm to
an NLP task known as argument
identification~\cite{Gildea2002:NPP,Abend2009:UAI,Meza-Ruiz2009:JIP}
and show that concepts generated by our algorithm can be used
to achieve promising accuracy compared to selectional preference,
Reverb and a state-of-the-art semantic role labeling tool.

\subsection{Experimental Setup}
\label{sec:preprocess}

We use our algorithm to conceptualize subjects and objects
for 1770 common verbs from Google syntactic N-gram using
Probase and WordNet as isA taxonomies.
\footnote{All evaluation data sets and results
are available at \url{http://202.120.38.146/ac}.}
From 1770 verb set, we also
sample 50 verbs with probability proportional to
the frequency of the verb. This set of 50 verbs (Verb-50) is
used for quantitative experiments including evaluating the
accuracy of argument concepts and the accuracy of argument identification.

All argument instances we use in this work come
from the combination of {\em Verbargs} and {\em Triarcs} packages
of the Google syntactic N-gram data.
The data set, which was extracted by parsing millions of books,
contains 2 billion ngrams with dependency labels.
From the labeled dependency trees, we extract
subject-verb dependency pairs (nsubj, agent) and
object-verb dependency pairs (dobj, nsubjpass).
%Because the subject or object
%of a verb is just a word, we further recover the whole phrase by
%taking the subtree rooted at that word and find a best matching
%instance term from the taxonomy by a sliding window.
Since the dependency only specifies the head word of subject/object,
to extract the whole subject/object phrase in the dependency tree, we
retrieve the subtree rooted on the head word to construct a candidate
subject/object phrase. In order to make use of the knowledge bases,
the subject/object phrases should be recognizable by Probase/WordNet.
Thus, we use a sliding window on the candidate phrase to further
extract the most probable (longest) Probase/WordNet term.

For the system parameters, we set the maximum overlap
threshold between two concepts to 0.2, and the number of concepts
$k$ to $\{5, 10, 15\}$ to obtain argument concepts for
different granularity.


\subsection{Conceptualization Results}
We compare the concepts learned by our algorithm with FrameNet elements, Reverb
arguments, and concepts learned by SP.
ReVerb is an open information extraction system
that discovers binary relations\footnote{ReVerb extracts general relations 
instead of verb predicates, e.g., XXX heavily depends on YYY.} from the web
without using any predefined lexicon. ReVerb data contains
15 million subject-predicate-object triple instances without
any abstraction or generalization.

Table \ref{tab:results} shows 5 example verbs and their argument concepts (AC),
FrameNet semantic roles (FN), ReVerb argument instances (RV) as well as
selectional preference (SP) concepts for the verbs' subjects and objects.
$k$ is set to 5 for AC, and the top 5 instances/concepts are showed for
RV and SP.
We can observe that the FN semantic roles are too general, while RV instances
are the opposite. Both inevitably lose information:
FN is a manually constructed lexicon by experts thus
cannot scale up well, while ReVerb is automatically extracted from massive
English sentences and hence comes with abundant errors (e.g., {\em ive} for
``enjoy''). SP does not consider semantic overlap between argument concepts.
Compared to the other methods, our algorithm generates concepts with
variable granularity and low semantic overlap. These concepts are
more accurate and comprehensive.

To quantitatively compare our algorithm to SP, we ask three human judges to
annotate whether the concepts generated by our algorithm (AC) and
selectional preference (SP) are the correct abstraction of the verb's
arguments.
The majority votes are used as the ground truth. We compute the percentage of
correct concepts as accuracy, and report the accuracy of AC and SP in \tabref{tab:precision}.
AC generates more accurate concepts than SP mainly because AC
considers the quality of argument instances extracted from dependency.
Noise from dependency parser could be large when they follow
some specific patterns such as ``time'' in ``eat this time''. SP doesn't
distinguish noises from correct arguments and thus may generate
incorrect object concepts, e.g., ``time'' for ``eat''.

%AC generates more accurate concepts than SP mainly because it
%considers the correctness of argument instances generated by dependency
%parser.
%and the overlap of two candidate concepts.

\begin{table}[th]
\centering
\small
\caption{Accuracy of AC and SP concepts}
\begin{tabular}{cIc|c|c|c}
\whline
\multirow{2}{*}{k} & \multicolumn{2}{c|}{Subject} & \multicolumn{2}{c}{Object}\\
\cline{2-5}
& AC & SP &  AC & SP \\
\whline
5 &\bf 0.90 & 0.62 &\bf 0.98 & 0.72 \\
\hline
10 &\bf 0.90 & 0.61 &\bf 0.97 & 0.77 \\
\hline
15 &\bf 0.90 & 0.64 &\bf 0.96 & 0.77 \\
\whline
\end{tabular}
\label{tab:precision}
\end{table}

\begin{table*}[th]
  \centering
  \scriptsize
  %\small
  \caption{Example subject/sbject concepts from 4 lexicons}
    \begin{tabular}{cIr|l|l|l|l}
    \whline
    Verb  &       & \multicolumn{1}{c}{Argument Concepts} & \multicolumn{1}{|c}{FrameNet} & \multicolumn{1}{|c}{ReVerb} & \multicolumn{1}{|c}{SP Concepts} \\
    \whline
    \multirow{2}[4]{*}{accept} & Subj  & \tabincell{l}{person,community, \\ institution,player,company} &  \tabincell{l}{Recipient,Speaker, \\ Interlocutor}  &  \tabincell{l}{Student,an article, \\ the paper,Web browser, \\ Applications} & \tabincell{l}{world,group, \\ person,term, \\ safe payment method}   \\
          \cline{2-6}
          & Obj   & \tabincell{l}{document,payment, \\ practice,doctrine,theory} & Theme,Proposal  & \tabincell{l}{the program,publication, \\ HTTP cookie,the year, \\ credit card}  & \tabincell{l}{topic,concept,matter, \\ issue,word}   \\
    \hline

    \multirow{2}[4]{*}{cause} & Subj  & \tabincell{l}{factor,disease, \\ event,agent,technique} &  Actor  &  \tabincell{l}{The root,HIV, \\ Car accident,Suicide, \\ Cardiovascular disease} & \tabincell{l}{word,factor,condition, \\ complication,symptom}   \\
          \cline{2-6}
          & Obj   & \tabincell{l}{disease,effect, \\ challenge,emergency,defect} & Event  & \tabincell{l}{the problem,AIDS, \\ Poverty,death, \\ Heath problems}  & \tabincell{l}{symptom,complication, \\ condition,disease,factor}   \\
    \hline

    \multirow{2}[4]{*}{earn} & Subj  & \tabincell{l}{person,service,factor, \\ organization,product} &  Earner  & \tabincell{l}{just 250 new users,\\ People,Women, \\ three,No points}  & \tabincell{l}{currency unit, \\ monetary unit, \\ group,coin,denomination}   \\
          \cline{2-6}
          & Obj   & \tabincell{l}{incentive,income,award, \\ currency,credential} & Earnings  &  \tabincell{l}{\$12000,money, \\ men,Run,current order}  & \tabincell{l}{currency,coin,qualification, \\ benefit,integrity issue}  \\
    \hline

    \multirow{2}[4]{*}{enjoy} & Subj  & \tabincell{l}{group,community,name, \\ country,sector} &  Experiencer  &  \tabincell{l}{people,ive,Guests, \\ everyone,someone} & \tabincell{l}{world,stakeholder, \\ group,person,actor}   \\
          \cline{2-6}
          & Obj   & \tabincell{l}{benefit,time,hobby, \\ social event,attraction} & Stimulus  &  \tabincell{l}{life,Blog,Breakfirst, \\ their weekend,a drink}  & \tabincell{l}{benefit,issue, \\ advantage,factor,quality}   \\
    \hline

    \multirow{2}[4]{*}{submit} & Subj  & \tabincell{l}{group,community, \\ name,term,source} &  Authority  &  \tabincell{l}{no reviews,Project, \\ other destinations, \\ HTML,COMMENTS} & \tabincell{l}{large number,number, \\ stakeholder,position,group}   \\
          \cline{2-6}
          & Obj   & \tabincell{l}{document,format,task, \\ procedure,law} & Documents  & \tabincell{l}{one,review, \\ a profile,text, \\ your visit dates}  & \tabincell{l}{document,esi online tool, \\ material,nickname, \\ first name}   \\
    \whline
    \end{tabular}
  \label{tab:results}
\end{table*}

\subsection{Argument Identification}

In the argument identification task, we use the inferred argument concepts
to examine whether a term is a correct argument to a verb in a sentence.
In this paper, we focus on identifying the objects of a verb. To evaluate
the accuracy of argument identification, for each verb in Verb-50,
we first extract 100 sentences containing the verb from Engish Wikipedia
corpus. We then extract \pair{verb}{obj} pairs from these 5000 sentences.
Apart from parsing errors, most of these pairs are correct
because Wikipedia articles are of relatively high quality.
We roughly swap the objects from half of these pairs with an object of
a different verb, effectively creating incorrect pairs as negative examples.
For example, if we exchange ``clothing'' in ``wear clothing'' with the ``piano''
in ``play piano'', we get two negative examples ``wear piano''
and ``play clothing''.
Finally, we manually label each of the 5000 pairs to be correct or not,
in the context of the original sentences.  As a result, we have a test
set of 5000 \pair{verb}{obj} pairs in which roughly 50\% are positive and
the rest are negative.
%To create roughly equal amount of negative examples,
%Since using wrong argument in human writing is a rare case in
%daily life, especially for the high quality online Encyclopedia, the
%negative examples of \pair{verb}{obj} pairs are dominated by the positive
%ones, which makes it difficult to observe the differences in accuracy
%among different methods.
%We thus generate artificial negative examples as follows.
%for each verb in verb-50, we randomly swap the objects from the above
%``positive'' examples with
%we sample 100 sentences that contain the verb
%from Wikipedia and extract argument instances from them.
%Then, we toss an unbiased
%coin to decide whether we exchange the arguments in the sentences with arguments of some other verb.

\begin{figure}[th]
\centering
\small
\fbox{\parbox{.95\columnwidth}{
accept
arrange
ban
blame
borrow
carry
cause
change
close
connect
contact
cut
damage
define
delay
destroy
disturb
divide
earn
ease
eat
enjoy
heat
imagine
imply
import
judge
kiss
mark
match
need
notice
operate
overcome
print
prove
punish
release
report
select
sell
serve
shoot
sing
store
submit
survive
test
view
watch
}}
\caption{Verbs in Verb-50}
\label{fig:verb50}
\end{figure}

We compare our argument concepts (AC) with SP, ReVerb and Semantic Role Labeling (SRL) as follows:
\begin{itemize}
\item {\bf AC \& SP}: Check if the object is an instance of the top $k$ concepts (IsA relation) of the target verb.
\item {\bf ReVerb}: Check if the object is contained in the 
verb's list of objects in ReVerb.
\item {\bf SRL}: SRL targets at identifying the semantic arguments of a predicate in natural language
sentence, and classifying them in to different semantic roles. We
use ``Semafor''\cite{chen2010semafor}, a well-known SRL tool,
to label semantic arguments in the sentences,
and check if the test term is recognized as a semantic argument of
the target verb.
\end{itemize}

We set $k = 5, 10, 15$ for AC and SP.
The accuracies are shown in \tabref{tab:argumentidentify}.
In the table, AC(PB) and SP(PB) represent the results on Probase,
while AC(WN) and SP(WN) represent the results on WordNet.

\begin{table}[th]
  \centering
  \small
  \caption{Accuracy of argument identification}
    \begin{tabular}{cIc|c|c|c|c|c}
        \whline
        \multirow{2}{*}{k} & \multicolumn{2}{c|}{Probase} & \multicolumn{2}{c|}{WordNet} & \multirow{2}{*}{ReVerb} & \multirow{2}{*}{SRL} \\
        \cline{2-5}
             & AC & SP & AC & SP &  &  \\
        \whline
            5 & \bf 0.61 & 0.58 & 0.50 & 0.50 & 0.47 & 0.50\\
        \hline
            10 & \bf 0.61 & 0.58 & 0.52 & 0.52 & 0.47 & 0.50\\
        \hline
            15 & \bf 0.61 & 0.59 & 0.53 & 0.53 & 0.47 & 0.50\\
        \whline
    \end{tabular}
  \label{tab:argumentidentify}
\end{table}

From Table \ref{tab:argumentidentify}, we observe that
the accuracy of AC is higher than that of SP, ReVerb and SRL.
Due to its limited scale, ReVerb cannot recognize many argument
instances in the test data, and thus often labels true arguments
as negative. SRL, on the opposite side, tends to label everything
as positive because the SRL classifier is trained based
on features extracted from the context, which remains the same
even though we exchange the arguments. Thus SRL still labels the
argument as positive.
Comparing with SP, our algorithm considers the coverage of
verb arguments, the parsing error and overlap of concepts to
give an optimal solution with different values of $k$.
Consequently, our algorithm generates a set of concepts
which cover more precise and diverse verb arguments instances.
The accuracy decreases when we use WordNet as the taxonomy
because WordNet covers 84.82\% arguments in the test data
while Probase covers 91.69\%. Since arguments that are not
covered by the taxonomy will be labeled as incorrect
by both methods, the advantage of our algorithm is reduced.
%From this observation, we suggest to use Probase as
%the taxonomy for our task.

