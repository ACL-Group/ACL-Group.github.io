Thank you for your comments and suggestions.

Review #1
- The score in our ranking model of Task 2 indicates the *likelihood* of 
a pair being physically close. In general KB, the relation between 
entities is factual and determination of 
such relations can be treated as a classification problem.
Instead, we are concerned with the *typicality* of the co-location 
relation, because we are interested in *commonsense* knowledge.

- Yes, task 1 object co-location classification problem is indeed independent
from task 2, which infers the commonsense LocatedNear knowledge.   
That's why we were careful to call these two tasks different names: co-location
classification problem and LocatedNear relation extraction. Two
objects that are co-located in one sentence may not be located near typically.
Yes, the result of task 1 can used as part of the solution for task 2.
We will rephrase line 140-144 accordingly.

- In this paper, a physical scene or real-world scene refers to a scene
in which objects are visible in the same sight or view by a person. Examples:
two buildings co-located in a street block when viewed from a distance 
(large scene); fork and knife co-located in a view of a dining 
table (small scene). Our annotators are educated this way. 

- We will clarify that this is a binary classification problem, 
so P/R/F1 all refers to the LocatedNear class. 

- We will include the following to Table 5.

Model                Acc    Pre     Rec     F1
Random               0.500  0.551   0.500   0.524
Majority (positive)  0.551  0.551   1.000   0.710

- ConceptNet cannot be compared to our results against our dataset due to 
lack of LocatedNear pairs in ConceptNet (only 49 pairs).

Review #2
- Our topic is specific, but no more specific
than the extraction of other important relations such as part_of or causality,
which were extensively studied. More importantly, the locatedNear relation is
very useful in AI, such as image object detection, audio event detection,
robotics (bullet shells on the floor implies weapons in the vicinity).

- This is exactly how we split the data. We will fix this writing issue in 
the camera-ready.

- We added two more features (BoW and Bag_Path_Word) using sentence patterns 
according to your suggestion and the results are as follows:
Model      Acc     Pre     Rec     F1
SVM(all) 0.591 0.611 0.708 0.664
SVM(w/o. BW) 0.577 0.579 0.675 0.623
SVM(w/o. BPW) 0.556 0.567 0.681 0.619
SVM(w/o BW BPW) 0.584 0.606 0.702 0.650

More discussion about this result will be included in the camera-ready.

- The commonsense baseline (f0) will be added to Table 6:

f MAP P@50 P@100 P@200 P@300
f0 0.42 0.40 0.44 0.42 0.38
... ...

Review #3
- The experiments address two research questions (tasks): sentence-level 
co-location classification and extraction of commonsense locatedNear relation.
Please refer to point 1 of #1 for more explanation. 

- Our intuition is that some words are more important to this task than others. 
For example, words with spatial connotations are 
preserved while other less important words are abstracted.
Table 5 shows that such complex model works better 
than simple word based models. Similar approaches were used in other 
relation classification work (Xu et al., 2016). 

