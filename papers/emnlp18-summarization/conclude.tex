\section{Conclusion}
\label{sec:conclude}
We presented a simple approach to modify existing CNN seq2seq model with
a summary length input and were able to train a model that produces 
summaries of desired length that are fluent and coherent. 
This is a better solution than the current practice of summary truncation. 
Compared with the existing
summarization methods, we show that our model has the ability to control the
output length on its own using its internal state without lossing semantic 
information or sacrificing the ROUGE score. 
%We find that the basic CNN seq2seq model 
%still has some problems, such as generating repeated word sequence. 
%We also argue that ROUGE is not a perfect evaluation metric for the abstractive 
%summarization. Our future work will focus on these two aspects.
