\section{Introduction}
\label{sec:intro}

%Summarization is the task of creating a short, accurate,
%informative and fluent summary for a longer text document.
%There are two fundamental approaches to text summarization:
%extractive and abstractive.
%The former generates summaries by using the words or sentences
%that appeared in the source document.
%The latter attempts to reproduce the semantics and topics of original text
%by paraphrasing. Thus, abstractive summarization
%can introduce fresh words and phrases into the summaries and is considered
%more favorable.

Great progress~\cite{RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17}
has been made recently on abstractive summarization.
Many use sequence-to-sequence model based on RNN
and attention mechanism \cite{RushCW15}, which was originally used
for machine translation \cite{SutskeverVL14,BahdanauCB14}.
Recently, \citet{gehring2017convs2s} proposed a convolutional 
sequence to sequence model equipped with
Gated Linear Units \cite{DauphinFAG17}, residual connections \cite{HeZRS16}
and attention mechanism. 
Such a convolutional model achieves state-of-the-art accuracies 
in abstractive summarization on single sentence summarization,
and it is much faster than the previous recurrent models as
it can be easily parallelized.
%Particularly, it achieves better ROUGE-2 score on DUC-2004 and Gigaword
%datasets. Moreover, the convolutional model is much faster
%than previous recurrent models because it can be easily parallelized.
Furthermore, unlike recurrent models, the convoluational model has more stable
gradients because of its backpropagation path. 
%Bai\shortcite{bai2018empirical} proved that the convolutional
%network is a more powerful toolkit for sequence modeling than recurrent network.

Constraining summary length, while largely neglected in the past,
is actually an important aspect of abstractive summarization.
For example, given the same input document,
if the summary is to be displayed on mobile devices, or within
a fixed area of advertisement slot on a website,
we may want to produce a much shorter summary.
Unfortunately, most existing abstractive summarization models are not
trained to react to summary length constraints.
%When such constraint, e.g., no more than $N$ words,
When the constraint is given at test time, the current
practice is \textbf{i)} to truncate the generated summary after $N$ tokens are generated when you want 
the summaries of length no more than $N$,
and \textbf{ii)} ignore EOS (end of summary) token until the first $M$ tokens are generated when you want
the summaries of length at least $M$.
Such a crude way of controlling
summary length makes the output summary incomplete or incoherent.
%such as getting information in a limited time or save the information
%in a limited device.

Previous research on controlling length of abstractive summary
has been scarce.
\citet{abs-1711-05217}, who applies convolutional sequence to sequence model on multi-sentence summarization,
converts length range as some special markers which are predefined and fixed.
These markers are included in the training vocabulary.
At training time, the model prepends the input of the summarizer with marker indicating 
the length of input sequence.
At test time, it controls the length of the generated summary also by
prepending length marker indicating the desired length.
%It is the state of art model for the abstractive summarization with
%length controlling in convolutional sequence-to-sequence model.
Unfortunately, this approach can not generate summaries of
arbitrary lengths. It only generates summaries in predefined
ranges of length, thus only meets the length constraints approximately.
This is shown in \tabref{tab:existing-example}.
The above truncation practice can be used in conjunction with
any of the length control methods but the excessive parts (red) will be
truncated leaving incomplete sentences.
%As the design of this model,
%the first bucket denote the range from 0 to 33 tokens. We use special marker
%of first bucket when generating the summary and set length as 30. The red part
%is the summary generated under 100 tokens.

\begin{table}[th!]
\begin{center}
\caption{\label{tab:existing-example} Example summaries generated by
different models with a desired length of 10 (red parts exceed the 10 token
limit).}
\small
\begin{tabular}{lclclclc}%{|p{7cm}|rl|}
\hline \bf Reference summary (53 tokens) \\
\hline david de gea and victor valdes enjoyed an afternoon \\
       off at a theme park . spanish duo donned shades as \\
	   they made the most of the rare sunshine . it has \\
	   certainly been a rollercoaster season for manchester \\
	   united . united are third in the premier league after \\
	   an impressive recent run .\\
\hline \bf Basic CNN summary (35 tokens)\\
\hline david de gea and victor valdes made the most of \\
       \color{red}{the rare english sun with a trip to a theme park .}\\
	   \color{red}{david de gea and victor valdes enjoyed some fun in}\\
	   \color{red}{the sun .}\\
\hline \bf \cite{abs-1711-05217} summary (30 tokens) \\
\hline david de gea and victor valdes enjoyed a trip to a \\
       \color{red}{theme park . the pair enjoyed a relaxing time just}\\
	   \color{red}{days after united 's win against manchester city .}\\
\hline \bf  Our Length Control summary (LC) (10 tokens)\\
\hline david de gea and victor valdes enjoy some fun .\\
%\hline \bf LRC summary \\
%\hline $<$unk$>$ rashid , 19 , is charged with terror offenses after he arrived on a flight from istanbul .
%he is due to appear in westminster magistrates ' court on \\
%\hline \bf LRCP summary \\
%\hline $<$unk$>$ rashid , 19 , is charged with terror offenses after he arrived on a flight from istanbul .
%he is due to appear in westminster magistrates ' court on \\
\hline
\end{tabular}
\end{center}
\end{table}

In our work, we extend the convolutional sequence to sequence
model \cite{gehring2017convs2s} by
controlling the length of summarization. Our approach seeks to generate summaries
of any desired number of tokens (also shown in \tabref{tab:existing-example}).
%\KZ{The summaries generated are {\em natural} and {\em complete}.}
%We propose three methods to implement length control
%for abstractive summarization: length control (LC), length range control without
%penalty (LRC) and length range control with penalty (LRCP).
%Length control method aims to generate summaries of exact length.
To do this, a length constraint is added to each convolutional block
of the initial layer of the model. This information is
propagated layer by layer during training. 
%Compared with other methods, our approach
%has a better performance on ROUGE score, length varianions and semantic similarity.
%The generated summaries from our model are natural and complete, especiall when the 
%desired length is short. 
Our contributions are as follows:
\begin{enumerate}
\item We propose a simple but effective method to generate summaries 
with arbitrary desired length (\secref{sec:lc}).
\item Our approach outperforms the state-of-art baseline methods 
substantially by all evaluation metrics, i.e., ROUGE scores, length
variation and semantic similarity (\secref{sec:eval}).
\item The generated summaries from our model are natural and complete,
especially when the desired length is short (\secref{sec:eval}).
\end{enumerate}

Next, we present the basic convolutional sequence to sequence  model 
and our extension, followed by the evaluation of our approach and a discussion
of related work. 
%\KZ{Itemize your contributions. Make sure these contributions are checkable
%in the experiments section.}
%This approach focuses on the information passing between layers.
%LRC and LRCP aim to generate summaries in desired range of lengths.
%In LRC, we embed the length range into
%a vector, then add the vector to each convolutional block in the
%initial layer of the model. LRWP has similar operation
%except it has a different loss function that penalizes gold summaries whose
%length doesn't fall into the desired length range.
%All of our models have the ability to manage the summary length by itself.
%Our methods can also generate summaries in any length or
%length range as we want.

%Next, we describe the convolutional model we use in \secref{sec:model}
%and present the details of
%the length controlling methods in \secref{sec:approach}. Then, we compare our model with other model \cite{abs-1711-05217}
%on the standard CNN/DailyMail benchmark and release comprehensive experimental
%results in \secref{sec:eval}. We discusse some related work in \secref{sec:related} and give a
%conclusion in \secref{sec:conclude}.
