\section{Related Work}
\label{sec:related}

In this section, we discuss some previous work on length control in
abstractive summarization and explain why we choose CNN
as our basic summarization model. 
%\KZ{you always need to keep our work
%in the context when discussing other works.}

\subsection{Length Control for Abstractive Summarization}
When summarizing a document, it is desirable to be able to control
the length of summary so as to cater to different users and scenarios. 
Most abstractive summarization systems are based on encoder-decoder models
and generate summaries whose length depends on the training summaries.
%How to control the summaries length is essential for summarization methods 
%based on neural encoder-decoder models.
Due to the variability of the sequence generation models, such 
as the different structures and functions, it is hard to design
a length constraint method on all summarization models.

Previous methods control summary length by 
generating EOS token at a particular time. 
\citet{RushCW15} used an ad-hoc method, 
in which the system is inhibited from generating the EOS tag by 
assigning a score of -$\infty$ to the tag and 
generats a fixed number of words. 
\citet{KikuchiNSTO16} proposed two
different methods for RNN seq2seq model which can 
control the summary length by taking length
embedding as an additional input for the LSTM and adding desired length into initial memory cell for the LSTM. 
In this model, they use the Gigawords as dataset and focus on 
the abstractive summarization in sentence level 
which generates one sentence as the summary.
%The RNN seq2seq model needs more time to training
%than convolutional model. 
For CNN seq2seq model, \citet{abs-1711-05217}
put some special markers into the vocabulary which denote different length ranges.
It prepends the input of the summarizer with the marker during training and testing. These special markers are predefined
and fixed. 
In this paper, we aim at generating complete summaries with arbitrary desired length 
naturally for CNN seq2seq model. We use multi-layers CNN seq2seq model on both encoder and decoder. 
We set the length constraint at the first layer of decoder to implement the length control of
the summarization. 
Compared with other methods, our approach can effectively 
control the length of generated summary in a natural manner. 
Meanwhile, it can generate summaries with length approximate
to the desired length without semantic lossing in less time. 
%\KZ{What is the advantage of our methods over
%the other three, intuitively?} 

\subsection{Encoder-Decoder for Abstractive Summarization}
%\KZ{I think the purpose of this subsection is to argue why we choose CNN as
%our basic model for abstractive summarization, compared to other methods,
%such as RNN.}
Automatic document summarization generates short summaries for original
documents. A summary should cover the key topics of 
the original document(s). A good summary should be coherent, 
non-redundant and readable\cite{YaoWX17}.
The research in abstractive summarization with 
encoder-decoder model
\cite{SutskeverVL14,RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17,abs-1711-05217} 
has made some progress.

Most of them use RNN with different attention mechanisms \cite{NallapatiZSGX16,SeeLM17,PaulusXS17}. 
\citet{RushCW15} used RNN with
soft-attention, 
%which denotes that many words in the target summary are retained from the source document.
while \citet{PaulusXS17} used the RNN with intra-attention.
%which is an attention mechanism relating different positions of a single sequence. 
Recently, research on CNN based summarization has gained momentum. 
\citet{gehring2017convs2s} proposed the CNN seq2seq model 
with multi-step attention,  which was extended in \cite{abs-1711-05217}.
\citet{bai2018empirical} showed that CNN is more powerful than 
RNN for sequence modeling.
What's more, CNN enables much faster training and more stable gradients 
than RNN. Therefore we select CNN seq2seq model as our basic model and 
do not compare our model with RNN seq2seq model. 

%In this paper, we use multi-layers CNN seq2seq model on 
%both encoder and decoder. We set the length constraint at 
%the first layer of decoder to implement the length control of
%the summarization. 


