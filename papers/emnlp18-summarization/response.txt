Review 1:
W1: "human-evaluation"
Refer to G1. 
The manual evaluation in Table 7 is a simplified human-evaluation 
of summarization, which just determines whether the sentences in summaries 
under length control are complete or not. If complete, the score is 1; 
if not, it is 0. It is easier to accomplish and more reliable
than other sophisticated human-evaluation.

W2: "Similarity"
Refer to G2 and G3. 
These show that the difference in semantic similarity among these three 
methods is indeed significant.

W3: "conclusion"
We will improve the conclusion in the camera ready.

Review 2:
W1: "RNN"
As the title of this paper suggests, the goal of the paper is to 
control the length of summaries generated by CNN based models. 
The length control methods designed for RNN models are not suitable for 
CNN models, hence they are not compared in this paper. 
We chose not to consider RNN models in this paper because,
as Gehring(2017) suggests, CNN seq2seq models are much 
faster and produce better summaries than RNN seq2seq models. 

W2&W3: "Similarity"
We are not claiming the Similarity metric as a contribution, but use it 
to complement the ROUGE score. The ROUGE score measures the lexical
and syntactic similarity between the generated summaries and the ground
truth. Referring to G2, Table 2 and Table 4 show that 
Similarity score certainly resembles semantic similarity.
Referring to G3, our significance test shows that the Similarity 
score is reliable. 

W4: "manual evaluation"
Refer to G1.

Review 3:
W1&Q3: "significance test"
Referring to G3, our significance test shows
that the difference in Similarity score among three methods is significant. 
We can add this analysis in the camera ready.

Q1: "bar chart"
We can add the numbers in the bar chart in the camera ready.

Q2: "PCC"
PCC measures the correlation between two variables, while CCC measures 
the agreement between two variables by the variation from the
45 degree line through the origin. 
Because of the different value ranges for ROUGE[0,1], Similarity[-1,1] and 
manual score[1,5], we focus on the degree of positive correlation between 
two variables, rather than whether these two variables are same.
Besides, the AESOP task in TAC uses PCC to evaluate different 
summarization evaluation methods. So the PCC is more suitable.

General:
Thank you all for the precious comments and suggestions.

G1: The reason for not using manual evaluation as the major
metric in this paper is that Lin(2002) showed 
that the manual evaluation is unstable and the inter-human agreement is 
low due to the variety in abstractive summaries. The ROUGE scores and 
Similarity scores can respectively measure the syntactic similarity 
and semantic similarity. They are complementary to each other
and give better quantitative assessment of the summarization quality. 

G2: The Similarity score is useful for evaluating semantic similarity.
Table 2 shows that such semantic similarity measure correlates better 
with human assessment on the TAC data. Table 4 shows that,
although the summaries generated by three methods and reference summaries
have large lexical overlap with each others, only our summaries are
semantically correct and receive higher Similarity score. 
Even though the numerical difference of Similarity score is 
little, it is consistent which is evident in significance
test in G3. So the Similarity metric is reliable. 

G3: We have done the significance test on Similarity score 
but it was excluded in the paper due to space constraint. 
Because the Similarity scores of generated summaries do not follow
normal distribution, we take Kruskal-Wallis test (Albert, 2017) as our 
significance test. All p-values are less than 0.05. 
For example, the p-value in experiment 1: 3.41e-32(Free), 
2.12e-45(Trunc), 0.01(Exact). We will add
the results of these significant test into the camera ready given
an extra page.

