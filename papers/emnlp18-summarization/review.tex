R1:

What is this paper about, and what contributions does it make?
   Authors propose an end-to-end trainable mechanism in a convolutional sequence to sequence architecture to control summary output length, that is, the text output by the decoder.

What strengths does this paper have?
   The proposed neural architecture for length control is clean and doesn’t introduce many parameters to train. The writing is very good, clear; it was easy to read and understand their paper. Length control is a learnable part of the model; model is still end to end. Uses Convolutional seq2seq instead of RNNs which is faster and has fewer parameters.

What weaknesses does this paper have?
   Since the work is arguing for better summary quality, I would really love to see a human-evaluation of a representative sample of summaries output by the system against at least 1 system that is the closest competitor. It looks like the authors did a preliminary version of this study as in Table 7.
   Evaluation: Table 3. Difference in ’Sim’ evaluation scores across models and setting doesn’t appear to be significant at all. They are so small that the authors have to report 4 decimal place to show any difference. Either the models aren’t producing very different summaries or ‘Sim’ is not a sufficient metric.
   Conclusion section is a bit terse and generic. It is also missing future work.

Overall Recommendation:	3.5

Presentation Improvements
  Labels in Figures 1 and 2 may be written more legibly and spaced better

R2:

What is this paper about, and what contributions does it make?
  The authors proposed an approach to constrain the summary length by extending a convolutional sequence to sequence model.

What strengths does this paper have?
  The authors proposed an approach to constrain the summary length by extending a convolutional sequence to sequence model.

What weaknesses does this paper have?
   If the authors really target at developing the good ability of controlling the summary length, the authors should compare the proposed method with the method in RNN based framework as well, because RNN based framework might suit well for the ability of length control.
   If the authors would like to claim similarity is a good new evaluation metric for summarization, the authors should validate it with stronger evidence.
   In the second experiment, the authors cannot obtain a good result for the proposed method with ROUGE score, and they yield the result only with their own weak evaluation metric, similarity.
   The authors should perform manual evaluation as well.

Overall Recommendation:	2.5

R3:

What is this paper about, and what contributions does it make?
   The authors use a CNN seq2seq model to constrain summary lengths. In the result, they observe improvements on the ROUGE score.

What strengths does this paper have?
   The CNN seq2seq approach is interesting and the topic is addressed too little elsewhere where focus is more on uncontrolled length summarization. The result presentation is extensive.

What weaknesses does this paper have?
  There is no proper significance analysis.

Overall Recommendation:	4

Questions for the Author(s)
   Can you please add numbers in the bar chart? Would CCC make more sense than Pearson's CC? Can you please add proper significance testing?

Missing References
   Well referenced, but a bit space for more.

Presentation Improvements
  See above - the bar chart is not too helpful.

Typos, Grammar, and Style
  All well.

