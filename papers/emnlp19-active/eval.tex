\section{Evaluation}
\label{sec:eval}
In this section, we present the dataset, baselines and experiment results.

\subsection{Datasets}
The data set we use is the E-commerce conversation data provided by a Chinese artificial intelligence company. The data set is aimed to train an intelligent customer service system. So the data is the questions asked by customers and the labels are the intent of the questions.Table   shows the data split between training and testing in different classes.

\begin{table}[th!]
\small
\centering
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{ccc}
\hline
%\textbf{Dataset}&Words&Concepts& Simplified words\\
%\hline
%Training   &10.03&3.79&5.17\\
%Validation&10.03&3.79&5.17\\
%Test         &9.57&3.59&4.90\\
Classes& Training & Test \\
\hline
3&13643&3410\\
4&15849&3962\\
5&17926&4481\\
10&22957&5739\\
\hline
\end{tabular}}
\caption{Data Split}
\label{tab:size}
\end{table}

\subsection{Baselines}
\subsubsection*{Random}
Random sampling is the most common sampling strategy. The samples chosen by this strategy will represent the distribution of original data. Every data in the unlabelled pool is unbiased and possesses the same probability to be selected.

\subsubsection*{Least Confidence}
This is a typical active learning sampling approach which selects data points according to the confidence given by the classifier. 

A probability classifier will generate a probability vector for each data point and assign the data point the class with maximum probability. So, the maximum probability in the probability vector can be regarded as the confidence of the model on the data point. Points with least confidence in the unlabelled data pool will provide best margin to the model. 

The pseudo code of selecting $b$ samples in an epoch is shown in \ref{alg:LC}.
\begin{algorithm}
\small
\caption{Least Confidence Sampling}
\label{alg:LC}
\begin{algorithmic}
\WHILE{sample data set $<$ batch size}
\STATE {Apply current classifier on unlabeled data and get the prediction matrix $P_{\theta}(Y|X)$} 
\STATE {$x = argmin_{x}max P_{\theta}(Y|X)$}
\STATE {append sample set with $x$}
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsubsection*{Weighted Uncertainty}
The intuition of this approach~\cite{zhao2017deep} lies in choosing both informative as well as representative data points. Informative can be quantified as the entropy of the data point, while the representative evaluation is done by comparing the distance between data point with the center of predicted class. So, the weighted uncertainty can be calculated in the following formula
$$WU(x) = Uncertainty(x)*Similarity(x),$$
where the uncertainty as well as similarity has been discussed in \ref{sec:approach}.

\subsection{Results}
We study all baselines above as well as our proposed methods on the E-commerce data set. All the approaches are assessed with the same model structure: LSTM-dense-reLu-dropout-dense-softmax, with dropout probability 0.5, dense layer with 256 units.

All models are trained on the E-commerce data set with a random and balanced initial training set of 300 data points. For each selection, we increase the labeled data by 100 samples. The test accuracy is reassessed after each sampling.
\label{sec:exp}

