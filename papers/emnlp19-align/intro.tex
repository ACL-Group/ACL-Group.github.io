\section{Introduction}
\label{sec:intro}

In natural language processing, there are many supervised learning models that require the existence of parallel corpus for training. However, manual annotation, which is used in constructing many reliable parallel corpora, is inefficient and time-consuming. Cross-lingual sentence alignment has been extensively studied for machine translation, but the construction of monolingual parallel corpus is also gaining its interest. For cross-lingual alignment, different words are relatively discriminative in data distribution, while in monolingual alignment, the ``style'' of a word might be much more subtle to distinguish. Monolingual parallel corpora can be used to learn text rewriting rules, which is involved in sentence simplification, natural language style transfer, and document summarization~\cite{hwang2015aligning}. It can also be used to learn the relation between monolingual texts such as semantic relatedness and similarity.

The monolingual sentence alignment model takes two comparable monolingual texts as an input, and outputs a $x$-to-$y$ alignment of these two corpora, where the sequential combination of $x$ sentences from the first corpus is aligned with $y$ sentences from the second corpus, and both combinations are ``similar'' in content. For instance, the following sentences, although different in their expressions, can be properly aligned to each other in terms of content.

\emph{Alyosha got up, walked over to the door, and reported that no one was listening to them.}

\emph{Alyosha went, opened the door a little, and reported that no one was eavesdropping.}

In previous research, mainly one-to-one and $n$-to-one matching schemes are studied. When only one-to-one sentence mapping is assumed, the similarity scores for each pair of sentences can be precomputed~\cite{kajiwara2016building}, which largely simplifies the problem. However, this condition does not hold for literature corpora, since one sentence might be rephrased as different number of sentences in another expression. In the work of Hwang et al.~\shortcite{hwang2015aligning}, many-to-one mappings are taken into consideration to align simple and standard Wikipedia, which is a safe assumption because a sentence in standard Wikipedia is very likely to be broken into several short sentences in simple Wikipedia. But this is not the case for general comparable texts. Our $x$-to-$y$ sentence mapping assumption is more challenging but practical.

The challenge of our problem is two-fold. First, similarity measure of sentences, which is crucial in alignment decision, is difficult to design for different corpora with different semantic features. Second, in each aligned pair, the number of successive sentences on each side is not known in advance but has to be discovered during the sequential alignment process. Moreover, some measurements that show higher accuracy in similarity evaluation work relatively slow when aligning large corpora. Therefore, the similarity measure for every possible pair and every possible sentence length cannot be assumed to be known, and thus the problem cannot be resolved as a simple matching problem. The second challenge is more significant in our $x$-to-$y$ matching scheme. These challenges are not present in previous problems, but are inevitable in our case because the monolingual documents differ in ``style'' in general and not as specific as ``simple/complex''.

The contributions of this work are the following.
\begin{enumerate}
	\item We extend the monolingual alignment problem to the $x$-to-$y$ matching solution of two documents in different styles, and devise an unsupervised alignment scheme that produces high-quality matching pairs. The styles of documents are not limited to simple/complex language (Section \ref{sec:method}).
	\item We experiment similarity measures based on Word2Vec model~\cite{mikolov2013distributed}, Universal Sentence Encoder~\cite{cer2018universal} and InferSent model~\cite{conneau2017supervised}. We evaluate the performance of these measures and compare with baseline models for paraphrase detection, and verify that our unsupervised methods have comparable performances with state-of-art (supervised and unsupervised) methods (Section \ref{sec:results}).
	\item Using a greedy algorithm and a filter mechanism for sequence alignment, we compare the performances of our model with baseline sentence alignment models on literature works with different linguistic characteristics. The results show that the filter mechanism can significantly improve the quantity of aligned sentences while preserving a reasonable quality (Section \ref{sec:method} and \ref{sec:results}).
	\item We construct a monolingual parallel corpus from various literature translations. This corpus can be used to learn general text rewriting models, and specifically, the ``style'' of a text (Section \ref{sec:data}).
\end{enumerate}


