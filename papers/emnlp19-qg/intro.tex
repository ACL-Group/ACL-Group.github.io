\section{Introduction}

Text question generation (Text-QG) aims to generate several meaningful questions from a given passage with a key phrase, as shown in Table ~\ref{table:example}. Text-QG has many valuable applications: (1) To generate questions for automated assessment in education purpose~\cite{HeilmanS10}; (2) To allow the dialogue agent to ask questions with dialogue context when more information is needed in a conversation; (3) To expand Q\&A dataset, which relies heavily on human-annotation.

Previous works on question generation can be divided into two parts. One is traditional approach which uses hand-crafted rules to convert an input passage into relevant questions ~\cite{HeilmanS10,chali2015towards}. And the other is deep learning approach which is data-driven. ~\citeauthor{du2017learning}~\shortcite{du2017learning} is the first to use seq-to-seq model with attention to do text-QG. ~\citeauthor{zhou2017neural}~\shortcite{zhou2017neural} proposed to encode 
key phrase information using binary indicators to generate 
key-aware questions so that the generated question is focused and they assume the answer to be key phrase. Considering answer is unavailable in reality before question is generated, ~\citeauthor{SubramanianWYT17}~\shortcite{SubramanianWYT17} applied a two-stage approach. First, key phrases (answers) are extracted by pointer network~\cite{ptrnet}. Second, key phrases are encoded in the same way as ~\citeauthor{zhou2017neural}~\shortcite{zhou2017neural}. With the intuition that questions could be asked in many ways, ~\citeauthor{Yao2018vae}~\shortcite{Yao2018vae} used conditional-VAE to increase the diversity of questions. More recently, models with auxiliary feature information~\cite{HarrisonW18} helped improve the question quality. 

\begin{table}[tbp]
\small
\begin{tabular}{@{}l@{}}
\toprule
{\begin{tabular}[c]{p{0.95\columnwidth}}\textbf{Sent:} \ul{Kenya 's inclusion among the beneficiaries of the US Government 's African Growth and Opportunity Act ( AGOA )} has given a boost to manufacturing in recent years . \\ \\ \textbf{Human:} What has given a boost to manufacturing in recent years ?\\ \textbf{Seq2seq:} what is the boost of the us government ?\\ \textbf{Our tree2seq:} what has given a boost to manufacturing in recent years ?\end{tabular}} \\ 
\bottomrule
\end{tabular}
\caption{Examples of text-QG, the key phrase is annotated with underline, and the generated question should be relevant to the key phrase. This example shows when key phrase is long, seq-to-seq model cannot output correct question but our tree2seq model can still work perfectly.}
\label{table:example}
\end{table}

However seq-to-seq models can only capture n-gram features in the sequence and can not use information directly from syntactic structure, which leads the dropout in performance. As shown in Table~\ref{table:example}, when the key phrase is long, the seq2seq model cannot treat it properly, but our novel tree-to-seq model which encodes both semantic and syntactic information has the knowledge that this key phrase is an object in the sentence and can generate a correct question.

In this paper, we propose novel models to incorporate syntactic information in question generator. The main idea is to encode the sentence parse tree to a structural feature recursively in a bottom-up fashion together with sequential information. Different from previous study in tree encoders~\cite{eriguchi2016tree,bowman2016fast,tai2015improved} which only work for binary trees, our tree encoder encodes trees of arbitrary fan-outs. While this work is inspired by \cite{liang2018automatic}, our model differs in that it considers both sequential and structure information and it adopts a stronger tree node cell to control the flow of child feature and sequential feature.

In summary, the technical contributions of this paper are:
 \begin{enumerate}
   \item We propose a new model that incorporates parse tree structure in
the encoding process and it is capable of encoding trees of arbitrary shape;
   \item Experiments on the SQuAD dataset shows substantial improvement over
the previous popular seq2seq method.
 \end{enumerate}
