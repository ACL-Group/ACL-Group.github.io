Review #1

Response to Weakness:
W1: The main purpose of our paper is to demonstrate the effectiveness of syntactic information in QG, and all experiments are designed to prove it. Therefore we don't think it is required to discuss the effect of the noise involved by parsing tools, just like the effect of tokenization is hardly discussed, which could also involve noise.

W2: We implemented two models to incorporate structural information and it has been proved improving the performance. From the experiments, the parallel model is better than stacking model, because both sequential encoder and tree encoder work independently in parallel setting which helps encoders extract features from different aspects.

Response to Reject:
R1: Our contribution is demonstrating the effectiveness of syntactic information in QG. We studied two ways to incorporate tree encoder. Besides, we compared our models with the advanced seq2seq model and the results show the effectiveness of our model.

R2: Although, tree encoder has been studied in other tasks as we mentioned in Related Works. The situation is quite different. In question generation task, we apply syntactic information to solve the problem that current seq2seq model cannot handle long key phrase in generated question correctly.

Response to Questions:
Q1: The decoder is the same in our experiments. In theory, a stronger encoder as ours which combines both semantic and syntactic information would improve the performance of the model, whatever decoder used.

Q2: See W2.

Q3: Thanks for your suggestion. It will be a good metric to evaluate the model in downstream tasks such as QA. However, current research on question generation is still in its infantry. We admit that the generated questions from either advanced seq2seq model or our tree2seq model are not good enough to improve the QA results substantially. It may be the reason why previous works on question generation didn't do this experiment to evaluate the performance.

Q4: We didn't do experiments to analyze the influence of tree structure on model performance since it's not our focus in this paper. Your suggestion is very valuable, as tree structure analysis would definitely make this paper stronger. 

Response to Typos:
Thanks for your correction, we will correct it in final version.


Review #2

Response to Weakness:
W1: Thanks for pointing out the mistake about "previous tree encoders".  We will correct it in final version. The reason why we use arbitrary trees is that it's more general and could be applied to other structural which cannot be converted to binary tree. And discussing which tree encoder is stronger is not our focus in this paper.

W2: The main purpose of our paper is to demonstrate the effectiveness of syntactic information in QG. That's why we compare advanced seq2seq model with our tree2seq model.

Response to Reject:
R1 & R2: See W1.

R3: The seq2seq baseline in table 3 is the seq2seq model with attention and copy mechanism, as mentioned in line 278-285. Therefore, the results prove the effectiveness of syntactic information involved by our model in QG.

Response to Questions:
We didn't do experiment to compare with other tree encoders, since choosing a stronger tree encoder is not the focus.


Review #3

Response to Weakness:
W1. As mentioned in line 41-42, our seq2seq baseline is the same as Zhou et al. From the results, our model outperforms the advanced seq2seq model.

Response to Reject:
See W1. 

Response to Questions:
Compared to these two papers, our model could generate more natural and diverse questions in end-to-end manner, instead of using rules to incorporate syntactic information and using templates to generate templated-based questions. 


General Response:
Thanks for your valuable suggestions.

