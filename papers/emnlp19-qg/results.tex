\section{Evaluation and Analysis}
\label{sec:eval}
We present the dataset used in the experiments, 
the end-to-end results as well as some ablations in this section.

\subsection{Dataset}
We evaluate our models on SQuAD 1.0 dataset~\cite{rajpurkar2016squad} and split dataset according to 
\citeauthor{SubramanianWYT17}~\shortcite{SubramanianWYT17}. 
There are 81,596 sentence-question-answer triples for training, 
10,438 for testing and 5125 for validation after removing 
those whose sentence that cannot be parsed correctly.

%\subsection{Evaluation Metrics}
%To evaluate models automatically, we conduct following metrics:
%BLEU 1, BLEU 2, BLEU 3, BLEU 4~\cite{papineni2002bleu}, METEOR~\cite{denkowski2014meteor} and ROUGE-L~\cite{lin2004rouge}.
%

\subsection{Results}
To show the importance of structural information from 
constituency parse tree and the effectiveness of our models. 
We conduct experiments on following models, 
where {\bf seq2seq} and {\bf codeRNN}~\cite{liang2018automatic} are 
our main baselines.

{\bf seq2seq:} This is the seq-to-seq model using Bi-GRU as encoder with attention and copy mechanism. We don't add extra advanced modules that could improve the performance of seq-to-seq model, such as maxout pointer and gated self-attention~\cite{zhao2018paragraph}, because the goal of this paper is to prove that structural information from constituency parse tree is important to question generation.

{\bf codeRNN:} This is the tree2seq model used to generate code comments from code tree. Its tree encoder uses fully-connected cell with summation aggregation, and its decoder takes code feature at each time step for better memory.

{\bf tree2seq+GRUCell/FCCell+stack/parallel:} Several variants of
our tree2seq model use tree encoder with modified-GRU cell or 
fully-connected cell in stack or parallel fashion.

All models are in the same setting. We set the GRU hidden unit size to 256 and only use 1 layer. The word embedding we used is ``glove.840B.300d'' pretrained embeddings~\cite{pennington2014glove} which is frozen when training. We use Adam optimization with initial learning rate of 1e-3 and the learning rate would be halved if there is no improvement after two epochs. The batch size is set to 32.

\subsubsection*{Different aggregation approaches}
%We do two experiments to prove the effectiveness of our models. In the first experiment, different aggregation approaches are compared. 
To simplify the process of this experiment and highlight the effectiveness of aggregation function, we remove the attention and copy mechanism from stacked tree2seq model with GRUCell and use unidirectional GRU as sequence encoder. The results in the first section in Table~\ref{table:results} shows that element-wise maximum over children node is a better aggregation approach than mean and summation. The reason is that element-wise maximum performs like a gate which filters out unnecessary and redundant information from each child node. 

% [th!]
\setlength{\tabcolsep}{1.5mm}
\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{BL-1} & \textbf{BL-2} & \textbf{BL-3} & \textbf{BL-4} & \textbf{M}  & \textbf{R-L} \\
\midrule
T2S+G[mean]+s[uG] &  27.13  &  12.49  &  6.52  &  3.47  &  9.64   &  29.63  \\
T2S+G[sum]+s[uG]&  28.33  &  12.96  &  6.71  &  3.54  &  9.67   &  29.92  \\
T2S+G[max]+s[uG]&  \textbf{29.56}  &  \textbf{13.79}  &  \textbf{7.32}  &  \textbf{4.03}  &  \textbf{10.46}   &  \textbf{31.11}  \\
\midrule
CodeRNN   &  17.66      &  7.71      &  3.94      &  2.09      &  6.90      &  23.13     \\
S2S+a+c &  42.37  &  26.46  &  18.48  &  13.42  &  17.87  &  42.28 \\
T2S+F+s+a+c&  42.86      &  26.84      &  18.79      &  13.55      &  17.94      &  42.21    \\
T2S+F+p+a+c&  43.05      &  26.82      &  18.55    &  13.31      &  17.85      &  41.68     \\
T2S+G+s+a+c&  \textbf{43.36}  &  27.22  &  19  &  13.75  &  18.06  &  42.06 \\
T2S+G+p+a+c&  43.13     &  \textbf{27.56}      &  \textbf{19.59}      &  \textbf{14.47}      &  \textbf{18.39}     &  \textbf{43.05}     \\
\midrule
S2S[uG]+a+c   &  41.79  &  26.11  &  18.24  &  13.26  &  17.45 &  41.69 \\
T2S+F+p[uG]+a+c &  42.38  &  26.55  &  18.66  &  13.65  &  17.73   &  41.87  \\
T2S+G+s[uG]+a+c  &  \textbf{42.77}  &  \textbf{26.97}  &  \textbf{18.88}  &  \textbf{13.72}  &  \textbf{17.92}  &  \textbf{42.13} \\
\bottomrule
\end{tabular}
\caption{Results for three experiments. BL=Bleu~\cite{papineni2002bleu}, M=Meteor~\cite{denkowski2014meteor}, R=Rouge~\cite{lin2004rouge},
T2S=tree2seq, S2S=seq2seq, G=GRU Cell, uG=uni-GRU (for sequence encoders),
F=FC Cell, s=stack, p=parallel, a=attn, c=copy.  ``+a'' and ``+c'' means
the models with attention and copy mechanism. ``sum/max/mean'' 
are three\textbf{} aggregation approaches from \secref{sec:treeEnc}.}
\label{table:results}
\end{table}

\subsubsection*{End-to-end results}
%In the second experiment, our models are compared to baseline models. 
From the results in the second section in Table~\ref{table:results}, 
the performances of our tree2seq models outperform seq2seq baseline, 
which implies the importance of structural information in question generation. 
Further, our tree2seq models also outperform the codeRNN, 
which shows that structural information itself is not enough to 
generate a good question and it should be encoded together 
with sequential information. The comparison between our own models 
showns that parallel model with modified-GRU cell is the best one achieving best scores in most metrics. 
The reason is that the modified-GRU cell has a better control of 
information flow from child nodes than FC cell and in the parallel fashion, 
both sequential encoder and tree encode work independently which helps 
encoders to extract features from different aspects. 
We also run experiments on unidirectional-GRU on early stage, 
and the results in the second and third section in 
Table~\ref{table:results} shows that the improvement gap between our 
tree2seq models and seq2seq baseline with uni-GRU sequential encoder 
is larger than that with bi-GRU. 
It implies that encoding the sequence from both directions enriches
the contextual features and makes up for the insufficient structural
information. 
Table~\ref{table:samples} shows the samples from test data. It is observed that when the key phrase is long, such as the second, the third and the last sample, the seq2seq cannot handle it correctly, but our tree2seq model with syntactic information could generate a reasonable question.

% [th!]
\begin{table}[t]
\scriptsize
\begin{tabular}{@{}l@{}}
\toprule
{\color[HTML]{000000} \begin{tabular}[c]{p{0.9\columnwidth}}\textbf{Sent:} 2013 Economics Nobel prize winner Robert J. Shiller said that \ul{rising inequality} in the United States and elsewhere is the most important problem .\\ \textbf{Human:} What is the most important problem in the United States and elsewhere ?\\ \textbf{Ours:} what was the most important problem in the us and elsewhere ? \\ \textbf{Base:} what was j. shiller 's shiller ?\end{tabular}} \\
\midrule
\begin{tabular}[c]{p{0.9\columnwidth}}\textbf{Sent:} There are direct contractual links between \ul{the architect 's client and the main contractor} . \\ \textbf{Human:} Who found that a culture had developed where few Commissioners had any sense of responsibility ?\\ \textbf{Ours:} what group found that a culture had developed where few commissioners ?\\ \textbf{Base:} what did the ecj 's relaxed approach take ?\end{tabular} \\
\midrule
\begin{tabular}[c]{p{0.9\columnwidth}}\textbf{Sent:} By contrast to the ECJ 's relaxed approach , \ul{a Committee of Independent Experts} found that a culture had developed where few Commissioners had ` even the slightest sense of responsibility ' . \\ \textbf{Human:} Who found that a culture had developed where few Commissioners had any sense of responsibility ?\\ \textbf{Ours:} what group found that a culture had developed where few commissioners ?\\ \textbf{Base:} what did the ecj 's relaxed approach take ?\end{tabular}\\
\midrule
\begin{tabular}[c]{p{0.9\columnwidth}}\textbf{Sent:} The \ul{Baltimore-Washington Conference of the UMC} has approved the appointment of an openly partnered lesbian to the provisional diaconate . \\ \textbf{Human:} What group has approved the appointment of an openly partnered lesbian to the provisional diaconate ?\\ \textbf{Ours:} what conference has approved the appointment of an openly partnered lesbian to the provisional diaconate ?\\ \textbf{Base:} what conference is the baltimore-washington conference ?\end{tabular} \\
\bottomrule
\end{tabular}
\caption{Selected questions generated by our best model ``T2S+G+p+a+c'' \& seq2seq baseline}
\label{table:samples}
\end{table}

