Review #1:
Q1: I don't understand...
The drop is due to random variance in experiments. Curve becomes smooth and slowly incremental when experimented using different splits. We will revise Figure 3 in the final version, but the conclusion in the paper stands.

Q2: Why "Acc w/ CE (%)"... 
This was an editing error. The number for SKBC (w/ CE) is 65.0%, and we will put it in the revision.

Q3: I think you can just use...
Our proposed simplification is applied as a preprocessing step to any sentence before encoding. This is done before both training the LM and making prediction on the short stories.

Q4: model design choices lack clear motivations…
The main intuition behind our approach (see Section 1), is that a few key concepts 
in a sentence alone are sufficient for understanding the whole sentence (illustrated in Fig. 1). This intuition follows Ostermann et al. (2018). This intuition plays out well in the evaluation. We will consider co-references as future work.

Q5: the specific application of NumberBatch…
See G1.

Review #2:
Q1:
The subgraph refers to the connections of the concepts from a single story within the bigger ConceptNet. The embeddings of the concepts were trained from the entire ConceptNet instead of the subgraph. We will add some explanation in the revision.

Q2:
Algorithm-1 is only simple pseudo code. In our implementation, we did construct a ‘look-up’ table for efficient search. The code will be released in the final version. 

Q3:
In Algorithm 1, we use “subsequence” rather than “subset” to show we respect the order of the words in sentences. We will emphasize this in the revision.

W1:
Our model is based on Roemmele et al., 2017. Our main contribution is how to selectively
represent commonsense information in sentences. The proposed method of simplifying 
a sentence into a sequence of key concepts has shown its effectiveness against the results of Roemmele. 
Different from Roemmele, our approach doesn't use the validation set from SCT at all due to its annotation bias.

W2:
See G1 and Review #1 (Q2).
In Table 7, we showed that SSE and CE each is better than nothing, SSE+CE is better than SSE or CE alone. We will add some more explanation in Sec 3.5 and 3.6.

W3:
In fact, we applied selective filtering (SSE) on some basic LMs, like DSSM, GenSim (Mostafazadeh et al., 2016a) and a LSTM LM. There’s consistently 1~2% improvement using SSE. This will be added to the revision. Also see G2 for SSE on GPT.

W4:
All previous works on the task, e.g., Srinivasan et al. (2018); Radford et al.(2018); 
Chen et al.,(2018), compare the end-to-end results with other methods trained using their original experimental setup and datasets. This paper merely follows that tradition and respects the original setup of those baselines.

Review #3:
See G2

General Comments:
G1: 
In Table 7, “No Events” means after simplification, a sentence is represented using only its concept embedding(CE), without the 2400-d sentence embedding. We will revise the wording in Table 7 to clarify.

G2:
The results for binary classification model with pretrained sentence embedding from BERT, GPT and Skip-thought in Table 6 show Skip-thought is more effective for this task. This is because GPT carries no information from context. BERT only encodes two consecutive sentences. Skip-thought considers 
the sentences before and after. Besides, BERT and GPT are trained with large scale of data and parameters which leads to excessive reliance on data quality and loss of generalization. If the styles for training and testing data are different, the result will be bad. Therefore, we make Skip-thought our primary LM.

