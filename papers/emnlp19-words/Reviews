Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This work addressed a word difficulty classification task where several linguistic and statistical features are used and combined within different supervised classification methods (such as SVM and MLPs).
An interesting (although not new) feature modeling is provides to represents words within the addressed classification task. The combination of such features is the core contribution of this work. Results are provided with respect to two languages to support the applicability of the proposed method.

Results are very promising if compared with the baselines, in particular the “Human Baseline” which also confirms the difficulty of the task.

Reasons to accept
The paper addresses an interesting task and results are promising.
The overall paper is clear and quite well written.

The experimental results suggest interesting considerations concerning the investigated features.

Reasons to reject
The paper does not provide real considerations about the results from a linguistic perspective. For example, why should word embeddings be helpful?
The applicability of the proposed methodology should have been evaluated on “distant languages” (e.g. Japanese or Italian) to prove the real applicability of this study.

Some (important) linguistic phenomena are neglected, such as word polysemy.

The contribution with respect to recent works, such as (Hiebert et al, 2019) seems quite limited if considering that previous works already considered the correlation between word difficulty and most of the features here proposed (even though this work provides an interesting combination of such feature modeling).

Some technical aspects are not completely clear (see below)

Questions for the Author(s)
Phrases and words that belong to multiple classes are removed from the reference word lists. How does it impact the analysis?
Embedding should work whenever words are not properly represented or statistics is not representative. Could you explain this? A deeper discussion (not just numerical) about the impact of the addressed features should be provided.

The adoption of MLP (with three layers) should define a non-linear classifier, while SVM is a linear one. Could you explain this choice?

It is not clear how features derived from words are combined in the learning process.

What about polysemy or features considered in previous works, such as (Hiebert et al, 2019)? A discussion should be useful.

****************************************************************************************************************************************

Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper discusses the task of quantifying the difficulty of words, and proposes a method based on a combination of morphological, syntactic and semantic features. The results indicate that the semantic features in particular are useful.
The main strength of the paper is the use of a wider range of features than has been used in previous works on measuring word difficulty, and the results support the usefulness of these additional features.

The main weakness of the paper is the lack of more powerful representation methods than standard embeddings.

Reasons to accept
The paper discusses a well-defined and concrete problem (quantifying word difficulty), and proposed a simple but effective measure that compares favorably to previous work.
Reasons to reject
The proposed method relies exclusively on well-known and standard techniques, and there is no discussion on how more powerful representations could be applied for this task.
Questions for the Author(s)
Since the embeddings seem to be useful for this task, one could imagine that more powerful representations, such as contextualized embeddings (i.e. embeddings produced with neural language models such as ELMo, BERT, GPT etc), would be even more useful. Also, since both the embeddings and syntactic features seem to be useful, how would dependency-based embeddings work in this setting? Would it be possible to include such representations in the experiments, or to at least discuss their potential usage?
How much does the embedding parameters (e.g. context size, sub-sampling rate) affect the results?

****************************************************************************************************************************************

Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
The paper describes an approach to automatically classifying words into learner difficulty levels (e.g. the CEFR scale). Similar work has been carried out previously, but then using quite simple features; here, a more extensive feature set is designed. A few different classifiers are tested, and the classifiers using the extensive feature set outperform simpler baselines (e.g. using the word frequency as the only feature).
Reasons to accept
This result seems quite decent, the simpler baselines are outperformed. This is probably worth a publication eventually.
Reasons to reject
However, the current version of the paper needs to be improved: some parts need to be discussed in more depth, other parts are standard well-known material and should be removed, and some proofreading should be done.
Significant parts of the feature descriptions are just textbook material describing how to compute language model probabilities using n-grams and how to encode features as one-hot vectors. This should just be cut.

The description of the phonetic feature representation is unclear. Are you counting syllables or just using some sort of bag of phonemes? This needs to be described more clearly. Why are words discarded if they have several possible PoS tags? That seems like an arbitrary choice and this aspect seems like it would be useful in terms of learner difficulty.

Typos, Grammar, Style, and Presentation Improvements
throughout: please enable English hyphenation; now, there are several words that have been oddly hyphenated

why are length and LM probability “morphological” features?

If you use a fairly fine-grained scale (e.g. the full six-level CEFR scale), it probably makes less sense to use accuracy in evaluations, and it’s probably better to use some distance-based score (e.g. mean absolute error).

196: what do you mean by “morphological language”? You mean that it has inflections?

440: “totally” -> “in total”

550: “It’s clear that … can“ -> should be “cannot”? There is no obvious clustering here.

596 : clarify what method was used to compute the p-value

Table 10 would probably be more interesting if gold-standard levels were also included

737: HANCKE -> Hancke

761: “We will also explore” – this seems hard without access to learners from those periods

I understand that you rely on a tradition in language teaching that assumes that each word can be assigned an inherent “difficulty”. But in reality this notion is quite ill-defined, as it depends on the learner’s L1, level of education, etc. Anyway it would be good if you could define and discuss this notion a bit, and discuss what principles were used when creating these gold standards (even though you didn’t build them this would be useful information).