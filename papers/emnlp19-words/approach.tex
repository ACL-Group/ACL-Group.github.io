\section{Features and Classification}
\label{sec:approach}

In this section, we describe the features that can be used to measure the word difficulty and these features cover several different aspects including frequency, morphology, syntax and semantics shown in Table \ref{tab:features}.
We also introduce the model to do the classification of word difficulty.
\begin{table}[ht]
	\centering
	\scriptsize
	\begin{tabular}{lll}
		\hline
		\textbf{Feature}  & \textbf{Description}   \\ \hline
		Frequency        & \tabincell{l}{The number of a words' occurrences in the corpus.}                  \\ \hline
		Length              & The number of characters in a word                                     \\ 
		Syllable Vector  & The bag-of-syllables representation of a word                                \\ 
		N-gram Vector            & N-gram transform of a word.                              \\ \hline
		N-gram Probability    & \tabincell{l}{A probabilistic language model based on \\n-gram at word level.} \\ \hline
		POS Tag              & Word's treebank-specific part-of-speech value.           \\
		%		& uPOS                      & Word's universal part-of-speech value.                   \\ \cline{2-3} 
		Type dependency& \tabincell{l}{Universal dependencies obtained by\\ constituency parsing. }   \\ \hline
		Word embedding       & Pre-trained word vectors by Skip-gram model                 \\ \hline
	\end{tabular}
	\vspace{-0.25cm}
	\caption{\label{tab:features} Multi-faceted features describing the word difficulty.}
\end{table}
\subsection{Features}
\label{sec:feature}
We denote the leveled word set as $\boldmath{W}=\{W_1,\dots,W_K\}$, where K is the number of levels. The vocabulary set extracted from the specific corpus is denoted as V.
\subsubsection{Frequency}
Frequency is a common measurement which is calculated on a huge corpus to assess vocabulary difficulty as mentioned above.
%Several researchers have applied words frequency counts as a means of estimating word difficulties. \JQ{delete "Several...difficulties" }
We obtain the word frequencies based on a large amount of texts. 
%choosing the frequency index (FI) calculated as $\log_{10}(F+1)$. \JQ{
	The frequency index (FI) is calculated as \begin{equation}
	FI=\log_{10}(F+1)
	\end{equation}  Here, the constant 1 is used to take out-of-corpus words in to computation.
%	prevent minus results for out-of-corpus words.
	
%If one word $w\in \boldmath{W}$ doesn't exist in vocabulary set $\boldmath{V}$ which is generated from a particular corpus, FI of word $w$ will be set as 0.\JQ{Following the equation above, it is zero. So, emmm???}
%In order to avoid confusion with words that appear only once in the corpus, we add one to each word's frequency in which their FI is $\log_{10}2$.\JQ{delete this paragraph}

\subsubsection{Morphological Features}
%English is a morphological language, whose sounds and spellings of words are corresponding to the morphemes units of English. 
English is a morphological language, where sounds and spellings of words are corresponding to the morpheme units.
In this part, we use a series of natural language processing (NLP) approaches to obtain the morphological characters of vocabulary. 
Intuitively, the morphology features of a word include its length, spelling rules and pronunciation rules as follows:

\textbf{Length and Pronunciation.} 
%\JQ{maybe need an example for syllables}
The length of a word can be obtained directly by counting the number of its characters.
%Pronunciation here is the way in which a word is spoken.\JQ{delete "Pronunciation...spoken"} 
In linguistic terminology, syllables are counted as units of phonemes and defined to describe represent the pronunciation. We use a standard pronunciation dictionary 
% CMU Pronunciation Dictionary (CMUdict) 
to generate the phonemes of a word~\cite{John2004CMU}.
For example, the phonemes of word ``cheese'' are ``CH'', ``IY'' and ``Z''.
A binary phonemes vector $\mathbf{p_w}=[p_1, p_2, \dots, p_P]$ is set to represent a word's pronunciation, where P is the number of all the phonemes. If the $i$-th phoneme $p_i$ exists in a word, $p_i=1$, otherwise $p_i=0$.
%We only consider the composition of phonemes, ignoring the accent of them when pronuncing a word.\JQ{delete the last sentence?}

\textbf{N-gram Probability.} 
Several researchers and linguists indicated that the spelling regulation of a word may influence its difficulty.
Therefore  we represent each word with its n-gram forms.
Different from traditional language models designed to predict the next word in the sequence of words,  we apply an n-gram language model at word level. 
Retaining those words only containing characters of an alphabet and transforming each word into its lower case, there are 26 single letters, from a to z, included in both bigram and trigram forms.
All the characters are normalized into lower cases.

The n-gram language model implemented in this part is to generate the character sequence of a word.
% is a sequence of $N$ characters. \JQ{I don't understand, What do u mean by "language model is characters"?}
For example, when the starting and the ending token of a word are also taken into consideration, the bigram combination of word ``a" is (``\$", ``a") and (``a", ``\$") and its trigram combination is (``\$", ``a", ``\$"). 

We make an assumption that if the bigrams appeared in a word are seen frequently in the whole corpus, this word is easy to write and remember.
Suppose the word $w$ is composed of the character sequence $c_1, c_2, \dots, c_n$, the word-level language model is implemented as follows:
\begin{equation}
\small
\label{equ:proba}
\begin{split}
p(w)&=p(c_1) p(c_2|c_1)\dots p(c_n|c_1,\dots,c_{n-1})\\
&=p(c_1) p(c_2) \dots  p(c_3)
\end{split}
\end{equation}
Here, the probability of a word is expanded based on the chain rule. Besides, we assume that the probability of each word is independent.



%Borrowing the traditional idea of a language model that implements word prediction, a probability here is assigned to the occurrence of an n-gram or a letter occurring next in a sequence of characters to make up the right word. 
%Therefore, the appearance  of the $n$-th character is related to the the first $n-1$ characters.
%% then the probability that the entire word appears is equal to the probability product of each letter. \JQ{delete "then the...each letter"}
%The probability of each character can be calculated statistically in the corpus. Suppose the word $w$ is composed of the character sequence $c_1, c_2, \dots, c_n$, and the n-gram language model implemented follows the chain rule:
%%The Laplace estimate for the probability distribution is applied to generate the n-gram frequency distribution.
%\begin{equation}
%\small
%\label{equ:proba}
%	\begin{split}
%		p(w)&=p(c_1) p(c_2) \dots  p(c_3)\\
%		&=p(c_1) p(c_2|c_1)\dots p(c_n|c_1,\dots,c_{n-1})
%	\end{split}
%\end{equation}
%\JQ{delete above}

The Laplace estimate for the probability distribution is applied to generate the n-gram frequency distribution~\cite{field1988laplacian}. 
It approximates the probability of an n-gram with count $c$ from a collection with $M$ outcomes and $B$ bins as $(c+1)/(M+B)$, which is equivalent to adding 1 to each bin and obtaining the maximum likelihood estimate of the frequency distribution result.
The probability of a given n-gram is in the range of $[0, 1]$.
%, then the entire word calculated by Formulation \ref{equ:proba} will be even small.\JQ{delete "then...small."}
%To get round this problem we compute the probability of each n-gram as log probability and the bigram and trigram language model implemented at word level is shown as Formulation \ref{equ:bi&tri}.\JQ{ 
	We compute the log probability of each n-gram to make the figures computable.
	The formulation is shown as :
\begin{equation}
\small
\label{equ:bi&tri} 
\begin{split}
\log\left(p_{Bi}\right)&=\log\left(p(l_1|``\$")\right)+\log\left(p(l_2|l_1)\right)\\
&+\dots+
%\log\left(p(l_n|l_{n-1})\right)+
\log\left(p(``\$"|l_n)\right)\\
\log\left(p_{Tri}\right)&=\log\left(p(l_2|l_1,``\$")\right)+\log\left(p(l_3|l_2,l_1)\right)\\
&+\dots+\log\left(p(``\$"|l_n,l_{n-1})\right)
\end{split}
\end{equation}

\textbf{N-gram Vector.} 
Different from the probability of entire word calculated by Formulation \ref{equ:bi&tri}, one word can also be represented by its n-gram components.
For the bigram representation of a word, it is a $729$-dimensional ($27\times 27$) vector because the starting and the ending token of word are included in the bigram forms.
%For example, the bigram combination of word ``a" is (``\$", ``a") and (``a",``\$") so that the position of these two bigrams in bigram representation vector can be set 1 and other positions can be set 0.\JQ{delete "a" example}
%For example, the bigrams in word ``the" are (``\$", ``t"), (``t", ``h"), (``h",``e") and (``e", ``\$"), thus the corresponding four positions of its bigram representation vector will be 1.
For one word, the corresponding positions of its n-gram representation vector will be 1.
%\JQ{I think this example can be deleted}
%Similarly the trigram representation of one word is a binary vector in larger size and each trigram segment is obtained by dividing the original word by length 3.
Figure \ref{fig:bitri} indicates the n-gram combinations and vectors in which 1 stands for an n-gram appears in the corresponding position and the dots in each vector means 0.
\begin{figure}[th]
	\centering
%	\epsfig{file=pic/bitri.eps, width=0.9\columnwidth}
	\includegraphics[width=1\linewidth]{pic/bitri.pdf}  
	%\caption{Overview of proposed model, which shows how Attention Filter Mechanism (ATTF) works when decoding $Y_i$.}
	\caption{Examples of Bigram and Trigram Vectors.}
	\label{fig:bitri}
\end{figure}
\vspace{-0.5cm}
\subsubsection{Syntactic Features}
The use of words can't be cut off from sentences. 
Intuitively, the difficulty of a word may be related to its role 
%in the sentence\JQ{delete "in the sentence"} 
and the relation with other words appearing in the sentence.
In this section, we apply the part-of-speech (POS) tagging~\cite{toutanova2003feature} and universal dependencies representation~\cite{schuster2016enhanced} obtained by Stanford CoreNLP to generate these features. 

\textbf{POS Tag.} A binary vector is selected to indicate the POS tag feature.
% and universal dependencies.
%For example, $T$-dimension POS vector $\mathbf{t_w}=[t_1, t_2,\dots, t_T]$ for word $w\in \boldmath{W}$ is the representation of  $T$ types of POS taggers marked in the documents. 
%The $i$-th element of vector $\mathbf{t_w}$ marked 1 indicates that the word $w$ has the $i$-th POS in the text. 
A $T$-dimension POS vector $\mathbf{t_w}=[t_1, t_2,\dots, t_T]$ is designed for each word, where $T$ represents the number of different POS tags. The $i$-th element of vector $\mathbf{t_w}$ marked 1 indicates that the word $w$ exists the $i$-th POS in the text.

\textbf{Universal Dependencies.}
%Vector of universal dependency follows the similar principle.\JQ{
	The vector of universal dependency follows the similar principle.
Figure \ref{fig:parser} shows an example of a parsing result obtained by Stanford CoreNLP\footnote{An online application of Stanford parser: \url{http://corenlp.run/}}.
\begin{figure}[th]
	\centering
	%	\epsfig{file=pic/bitri.eps, width=0.9\columnwidth}
	\includegraphics[width=1\linewidth]{pic/parsing.eps} 
	\caption{Parsing result for sentence ``He is a handsome man." obtained by Stanford CoreNLP parser.}
	\label{fig:parser}
\end{figure}

In order to represent the universal dependencies of each word into a vector, we considered the direction of each type.
%\JQ{lack of explanation for direcction}
Add ``\_in" to the word's relationship which the arrow points to and add ``\_out" to the relationship which the arrow points out.
For the example in Figure \ref{fig:parser}, 
word ``man" has the dependencies of ``amod\_out'', ``det\_out'', ``cop\_out'', ``nsubj\_out'' and ``punct\_out''.
While word ``He'' has the dependency of ``nsubj\_in".
Then we use the types of dependencies appeared in the documents to generate the bags as the words' dependencies representation whose binary vector form is similar to Figure \ref{fig:bitri}.

\subsubsection{Semantic Features}
The difficulty of a word is also related to its meanings.
However, the knowledge of the meaning of a word is not only rely on itself but also reflected by its context. 
%Rather, it's embedded in the networks of relationships with other words. \JQ{delete}
We utilize word embedding to represent each word appearing in the documents and capture the semantic and syntactic similarity.
%\JQ{.}, ralation with other words.\JQ{delete}
Word2Vec is an useful technique to learn word embedding and generates vector representation of a particular word. In order to obtain the best performance, dimension of embedding vector is selected by grid search.

\subsection{Classification}
Based on the extracted features, we implement support vector machine (SVM), logistic regression (LR) and multi-layer perceptron (MLP) to do the classification.

%With the intuition of data distribution and model function, we suspect that LR and MLP may perform better than other classifiers when the sample size is small and the feature dimension of the sample is large.\JQ{Isn't this a analysis of the results? suspect??}
During the training state of MLP, we choose a three-layer neural network and update the parameters by iteratively sampling one mini-batch from dataset.
Considering the lack of training samples and large feature dimensions, we choose linear kernel or optimization algorithm in SVM and LR.
