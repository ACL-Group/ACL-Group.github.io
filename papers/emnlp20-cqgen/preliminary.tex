\section{Preliminaries}

\subsection{Keyword-based CQGen}
Given a textual \textit{context} $x = (x_1, x_2, ..., x_{T_1})$, our aim is to generate a \textit{clarification question} $y = (y_1, y_2, ..., y_{T_2})$, so that $y$ ask for relevant but not repetitive information to $x$. In this work, we additionally consider \textit{keywords} $Z = (z_1, z_2, ..., z_k)$ that are expected to capture the main semantic of $y$. Here we define keywords as lemmatized, non-stopping nouns, verbs and adjectives appearing in questions. We extract a keyword dictionary $\mathcal{Z}$ of size $C$ from the training set using this definition. 

With keywords introduced, the marginal likelihood are decomposed as:

\begin{equation}
  \begin{split}
    p(y|x) &= \sum_{Z \subseteq \mathcal{Z}}p(y, Z|x) \\ 
    &= \sum_{Z \subseteq \mathcal{Z}}p(y|x,Z)p(Z|x)
  \end{split}
  \label{equ:decompose}
\end{equation}

\subsection{Specificity}
\label{sec:specific}
In this work, the specificity of a question is determined with the size of its applicable range. Question that can only be raised against one particular context is considered more specific than universal questions. Firstly, \textit{relevance} is the basic requirement of specificity. Traditional MLE training may generate generic but not relevant question for higher likelihood. 
% \KZ{Rephrase: Traditional MLE training may tend towards highly likely context, while sacrificing relevance to the context.} 
We believe that the additional task of keyword prediction will help focus on relevant aspects. Besides, \textit{specificity} of e-commerce questions are further promoted in 3 ways according to our observation : (1) Focusing on certain aspects, like the type, brand and attributes. (2) Mentioning components of a product, e.g. blade of a food grinder. (3) Describing a using experience specific to the product, such as cooking rice in a cooker. We hypothesize that many of them can be captured by keywords, with nouns and adjectives covering aspects and components, and verbs constituting the using experience.

\subsection{Diversity}

The writing assistant requires diverse questions to be generated about the same context, to cover various information needs as well as improve the robustness to problematic generations. This setting differs from some previous literatures \citep{wang2018learning, rao2019answer}, where we generate only one response at a time, and \textit{Diversity} is used to measure the expected variety of content and patterns among \textit{all generated response}. We call it \textit{global diversity}. Our setting is referred to as \textit{local diversity}, measuring the diversity \textit{within one usage}. This is also adopted by another line of literatures \citep{vijayakumar2018diverse, shen2019mixture}. If not specified, we mean \textit{local diversity} by using \textit{diversity}. Global diversity is also desired, as it increases the likelihood of the questions to be specific to various contexts. 

% KZ{I'm not sure if ppl will get the difference between these two types of diversity. To me seems to be just a matter of granularity. You need to distinguish better.}

To meet the diversity requirement as well as to promote specificity, we propose KPCNet below.

