# Response 1

Thanks for your comments. For your comments on the novelty of our work, we've provided more explanations in the general response.

# Response 2

Thanks for your constructive comments, and the encouragement of our research efforts on this under-explored problem.

Reply to questions:

- Use of standard sampling-based techniques
We supplement an experiment result for top-K, top-p sampling (with K=20, p=0.9) on the basis of KPCNet(threshold). We sample 6 outputs for each context and present the average automatic evaluation result (along with the result of KPCNet(threshold) for comparison): 
  - Dinstinct-3: 0.3618 (0.1530)
  - BLEU: 11.33 (17.77)
  - METEOR: 13.65 (16.18)
As is expected, the diversity is much better. However, it draws far behind in terms of quality (BLEU, METEOR) We randomly picked some examples and found them very likely to be ungrammatical and illogical. These quality problems may make it unreliable in practice. We also tried a more conservative setting (K=5, p=0.9), but still obtained 12.71 BLEU, and we thus concluded that sampling methods are not comparable with the proposed approach. 

- About Rao and Daum´e III (2019) as a baseline
Due to the instability of training, complexity of their proposed pipeline, under-specified details like the training time for each component and bugs in their released code, we are unable to reproduce a comparable result to what they claimed in their paper. Our best reproduced result so far was nearly identical to that of MLE. We think it's not the expected behavior of the model and thus don't include this result in our paper. 

- On dropout mitigating the noise introduced by keyword predictor: 
We admit that we haven't found any published work that can directly support our claim. However, we make this hypothesis not for the property of Dropout itself, but for the unique property of our model. The input for this Dropout layer is the keyword logits masked with keyword selection results, where there can be false negative problems. Because we train the model on the basis of context-question pair, and this pair is considered a positive sample only when keywords are present, which means keywords in other possible questions for the same context will be treated as negative samples, though they might be suitable there. We thus presume the random dropping behavior of Dropout can help simulate the condition when plausible keywords are not all present, and help the model be more robust to this noise.

- For "manually tune the hyperparameter"
We mean randomly choosing hyperparameter, and picking the one that yields the best BLEU on our dev set.

- For the different choices of word vectors
We use Glove for H&K because it is provided by Rao and Daum´e III (2019) along with the dataset, and we should use it for fair comparison, and we use word2vec for Office dataset because we compiled the dataset from scratch and thus there are no reference embedding, and we choose word2vec for its simplicity. 

# Response 3

Thanks for your detailed comments.

For your comments about our evaluation. 
We agree that a larger scale evaluation and online testing can make the result more convincing. However, unfortunately, we are unable to do this due to time and resource limit. We'll scale up the evaluation as we can and provide more details in the final version.

For your comments on the lack of baseline. 
We agree with you that more competitive baselines can further verify the strength of our model. 
We had considered several other works on QG, but we found most of the works are based on traditional QG datasets like SQuAD. They ask for information existing in the context, and their answers are usually available so that many works focus on the use of answer to improve generation. However, in our setting of clarification question, we are asking for missing information, which introduced the challenge of avoiding repetition. Moreover, we don't assume the access to answers either in training or testing. Consequently, many competitive models on QG are not directly applicable here. 
We also thought about other competitive methods for common generation tasks, like UniLM, BART, etc. We can compare with them, but their large parameters and pre-training would make the comparison unfair.  Furthermore, few of them claim explicitly the ability of diverse generation, except hMup which we had used as a baseline.
We may make more comparisons in the future when we find other suitable baselines.

For your comments on the novelty of out work.
We've provided more explanations on this in the general response.

# General Response

We'd like to provide more explanations about the novelty of our work:

1. About our task: Since we are tackling CQGen, we assume answers cannot be found in the context. As a result, recent advanced models in QG which heavily rely on answer information, either directly available or extracted from context [1], are not directly applicable on our task. Our setting is even more strict than Rao and Daum´e III (2019) on CQGen, as we don't assume access to answer at all.
  
2. About our discussion: we are the first to consider the possibility of applying CQs into e-commerce scenario with a concrete form of writing assistant. This scenario is first proposed by Rao and Daum´e III (2019) but with less consideration on practical issues, such as the requirement of diversity. 
   Moreover, we noticed that some researchers denied the value of CQs for e-commerce and thought there's no information need for product description [2]. Our detailed discussion about the value of CQs on Amazon may act as a defense for research efforts on this scenario, and provide valuable thoughts for other researchers.

3. About our approach: We concede most of the components are not new, but it is still not trivial to naturally integrate them to tackle the relatively novel challenge of specific and diverse CQGen. 
   First, we enhance the traditional seq2seq model with keywords as condition to improve specificity. 
   Next, the separation of keywords acts as explicit controlling factors, which is different from the more common variational models using latent factors. As a result, we can easily operate on them to control the generation to achieve desired effects like using keyword filtering to avoid repetition. 
   Last but not least, we explored application of the classical spectral clustering on a new basis, producing different and coherent keyword groups. Clustering alone won't have the ability to improve the generation quality without the separation of keywords as controlling factors.

[1] Subramanian, Sandeep and Wang, Tong and Yuan, Xingdi and Zhang, Saizheng and Bengio, Yoshua and Trischler, Adam, Neural Models for Key Phrase Detection and Question Generation ACL 2018 workshop

[2] Vaibhav Kumar, Alan W. black, ClarQ: A large-scale and diverse dataset for Clarification Question Generation, ACL 2020

We'll also release the code and processed dataset to facilitate reproducibility.
