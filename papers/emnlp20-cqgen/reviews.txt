# Review #1

## What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper describes a clarification question generation pipeline that first generates keywords, and then generates the question based on the keywords. They conduct experiments on two QG datasets and obtain improvements over some baselines, evaluated by both automatic metrics and human. This paper provides some tricks of keywords sampling to control the content of the generated questions, which differs from previous CQG works. However, most of the techniques introduced in the paper have been proposed in the literature (seq2seq model, topk sampling, clustering), which makes me less excited to see the model and not surprised by the results.

## Reasons to accept
The paper provides a practical pipeline for CQG; Code is available.

## Reasons to reject
Lack of novelty.

- Reproducibility:	4
- Overall Recommendation:	3


# Review #2
## What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
Summary: Paper proposes a writing assistant system design to generate clarifying questions conditioned on facts/attributes related to products. Instead of generating a single best question, the paper focuses on generating k-best questions. The paper shows that clarifying questions quality can be improved by conditioning generation on the keywords (product attributes, entities). Paper provides results on two datasets to show the effectiveness of their proposed method over a vanilla seq2seq baseline and mixture of expert based model.
Strength: Paper describes all models in details and provide motivation for all proposed components including keyword selection and conditional generation. Paper provides empirical evidence including ablation studies to show the effectiveness of the proposed model.

Weaknesses:

My main criticism of this paper is to not use a strong baseline method. **Paper heavily cites Rao and DaumÂ´e III (2019) but doesn't use it as a baseline model.** Given that this paper is solving the same task, I don't understand the reason for not using their method as a baseline. In past few years, a decent progress has been made in both answer aware and answer-unaware question generation methods. However, the paper doesn't cite most of the literature in this area including [1] which is doing a similar task of keyphrase detection and question generation. Since authors decouple the keyword generation and question generation process, my understanding is that they can also use any question generation method which relies on context, answer pair to generate questions. Once, you have generated/extracted keywords for a given question, you can simply use question, keywords pair to generate question which will provide a good baseline method. Multiple methods can be used for context, keyword pair to question generation task including pointer mechanism, copy mechanism [2, 3]

Authors heavily emphasize that diversity is an important metric for this task but **does not explore standard sampling-based techniques for diverse generations including unconstrained, top-k, and top-p/nucleus sampling.** We expect sampling-based methods to do much better in terms of diversity than beam search decoding.

## Reasons to accept
Question generation is an important task for NLP community. Diverse question generation is still an under explored area and including this paper at conference will bring community attention to this important problem.

## Reasons to reject
As mentioned above, there are two strong reasons to reject this paper
Paper doesn't consider strong baselines
While claiming that diversity in an important criteria for this task, paper doesn't explore various sampling based methods to improve generation diversity.

- Reproducibility:	3
- Overall Recommendation:	2.5

## Questions for the Author(s)
Some of the important details are missing from the paper/appendix:
Paper says that multiple hyperparams were manually tuned including \alpha, dropout. What does manually tuned mean here? Ideally, we should choose a dev set to select hyperparams.
Authors use Glove embedding for Home & Kitchen dataset, and word2vec embeddings for Office dataset. Why are you using two different set of embeddings ?
Other questions/suggestions: L274: we pass them through a dropout layer to mitigate the noise introduced by keyword predictor -> How can a dropout layer mitigate any kind of noise? Can you cite some paper to make this claim?

## Missing References
References:
[1] Subramanian, Sandeep and Wang, Tong and Yuan, Xingdi and Zhang, Saizheng and Bengio, Yoshua and Trischler, Adam, Neural Models for Key Phrase Detection and Question Generation ACL 2018 workshop

[2] Song, Linfeng and Wang, Zhiguo and Hamza, Wael and Zhang, Yue and Gildea, Daniel Leveraging context information for natural question generation NAACL 2018
[3] Sun, Xingwu and Liu, Jing and Lyu, Yajuan and He, Wei and Ma, Yanjun and Wang, Shi, Answer-focused and position-aware neural question generation EMNLP 2018

## Typos, Grammar, Style, and Presentation Improvements
L 191: We believe -> We hypothesize


# Review #3
## What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
The authors of this paper propose to address the clarification question generation task in E-Commerce to help supplement missing information during writing product descriptions. Specifically, a model, namely KPCNet is proposed, which decomposes the generation process into keyword prediction and conditioned generation, to generate both specific and local diverse questions. Through combing a bunch of keyword selection, keyword filtering, and Deduplication strategies, the proposed model achieves better results of automatic and human evaluations over two baselines on two E-Commerce datasets.
## Reasons to accept
In general, this paper is well-written and easy to follow. The design of the overall generation pipeline makes sense to me. The experiments conducted on two datasets, to some extent, are solid.
## Reasons to reject
1. The baseline models used in this paper are not strong enough to demonstrate the superiority of the proposed method. 
2. Since the authors have highlighted that this work is targeting at practical scenario, online testing and evaluations are required. 
3. Some important details are missed, e.g., how many evaluators are involved in the human evaluation process, what is the inter-rater rate between different annotators.
4. 100 cases are also not very convincing to lead to meaningful human evaluations. 
5. The proposed model is somewhat incremental.
- Reproducibility:	2
- Overall Recommendation:	2.5
## Typos, Grammar, Style, and Presentation Improvements
Part of the typos: Task-oriented writings like product description on Amazon often suffers from >>>> suffer may requires>>>> require with in >>>> within as condition >>>> as the condition