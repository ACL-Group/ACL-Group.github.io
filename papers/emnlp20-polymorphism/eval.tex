\section{Evaluation}
\label{sec:eval}

\subsection{Data Preparation}

We choose ConceptNet4 raw data for the experiment. The reason why we do not use the more recent ConceptNet5 is that: 
\begin{enumerate*}[label=(\alph*)]
\item ConceptNet5 is a multi-source knowledge base. We observe that most commonsense-related relations are entirely from Open Mind Common Sense (OMCS) project, which makes up ConceptNet4. 

\item ConceptNet4 has frame text for each triple (e.g. for triple (keys, CapableOf, open doors), the frame text is "\{1\} can \{2\}"). Given such context, we can lemmatize the arguments more accurately and conduct constituency parse to fetch the corresponding parse trees for the two arguments, which helps in our later data processing. 

\end{enumerate*}

After filtering data from blacklist contributors and activities (provided by the official ConceptNet group), we still find that the remaining data has some noise due to ambiguity of the frame text. For example, for frame text "\{1\} can \{2\}", which is supposed to express "someone/something is capable of doing something", we also find sentences like "Something you can make is peace". It makes no sense to separate it into (something you, CapableOf, make is peace). So we utilize the above parse trees and find the most dominant syntactic patterns for each relation. So for the above \textit{CapableOf} frame, we find the most dominant pattern that \{1\} is noun phrase (NP) and \{2\} is verb phrase (VP).

\subsection{Evaluation on approaches}

To measure our algorithms, we select 24 commonsense relations (\tabref{table:relation}) and randomly sample 250 triples at most per relation from the cleansed ConceptNet4. The distribution of commonsense argument length is shown in \
\tabref{table:lendistribution}. We use Stanford CoreNLP tools to parse the concepts and sentences. 
We will evaluate our algorithm from two perspectives: quantity and quality. 

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{l}
		\toprule
		AtLocation, CapableOf, Causes, CausesDesire,\\
		ConceptuallyRelatedTo, CreatedBy, DefinedAs, Desires,\\
		HasA, HasFirstSubevent, HasLastSubevent, \\
		HasPrerequisite, HasProperty, HasSubevent, IsA,\\
		LocatedNear, MadeOf, MotivatedBy, MotivatedByGoal,\\
		PartOf, ReceivesAction, SimilarSize, SymbolOf, UsedFor\\
		\toprule
	\end{tabular}
	\caption{Sampled commonsense-related relations from ConceptNet4.}
	\label{table:relation}
\end{table}

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{ccc}
		\toprule
		Length of Concept & Nums & Percentage \\
		1 & 2,586 & 30.80\% \\
		2 & 2,446 & 29.13\% \\
		3 & 1,582 & 18.84\% \\
		$\geq$ 4 & 1,782 & 21.23\% \\
		\midrule
		Total & 8,396 & 100.0\% \\
		\bottomrule
\end{tabular}
\caption{Length distribution of concept in sampled triples.}
\label{table:lendistribution}
\end{table}

\subsubsection{Exact string match with tolerance}
We remove some stopwords (i.e. ['a', 'an', 'the', 'that', 'it']) from the concepts because basically such deletion will not change the meaning of original concepts and may lead to higher recall. 

The experiments are done on NewYorkTimes and Gutenberg, the results are shown in \tabref{table:tolerentTable}

\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{cP{3cm}P{1.5cm}cc}
		\toprule
		corpus &sentence  & concept & percentage & accuracy \\ \hline 
		NYT & But Frye [works as hard] as any of the Knicks , often staying after [practice] to develop his [post] moves. &  work hard, practice, post & 23.49\% & 80.95\%  \\   \hline
		& Victor A. Bolden , general counsel for the NAACP Legal Defense and Educational Fund, said yesterday. & a general & & \\
		 \midrule
         GB&	Will the collectors, that have [taken their oaths] to make [the collection], dare to end it? &take an oath, a collection &x&\\  
         \bottomrule
	\end{tabular}
	\caption{ Sampled instances from two datasets: NewYorkTimes and Gutenberg. }
	\label{table:tolerentTable}
\end{table}

We randomly sample 50 sentences to annotate what percentage of concepts are retrieved with tolerence and if the matched concepts with tolerance in sentences maintains its original meaning by calculating the following metrics:

\begin{equation} \label{percent}
percentage = \frac{\#matched\ concepts\ with\ tolerance}{\#all\ matched\ concepts} \times 100\%
\end{equation}
\begin{equation} \label{acc}
accuracy = \frac{\#correct\ matched\ concepts\ with\ tolerance}{\#all matched\ concepts\ with\ tolerance} \times 100\%
\end{equation}

We found that the errors come from the informal text in ConceptNet4 and thus the wrong parsing output of Stanford CoreNLP tool. For example, a frame text from ConceptNet4 is ``The first thing you do when you {buy a beer} is {open it}''. Stanford CoreNLP mistakenly tags `open' as `JJ' because it follows `is'. As a result, the algorithm takes ``Margaret Thomas 's garden, one of the last [open] spaces in Fairfax County , is at risk because of its maker 's advancing age.'' as a matching sentence.


\subsubsection{Partial match with syntactic analysis}
\yuelan{waiting for results and analysis: here put a evaluation histogram, x-axis: top 5 configs found according to mwe's total freq; y-axis: sentence matching accuracy }

\subsubsection{Whole phrase semantic match} \yuelan{put accuracy for top phrases}

\subsubsection{Overall Evaluation}
Quantity

From the sampled triples, we evaluate number of unique commonsense knowledge triples we expand by using our algorithms.
\eve{compare the number of matching sentences obtained by 1.exact match 2.our algorithm}

Quality

\eve{human label the matching sentences. 1.whether the concept and the matching phrase are semantically similar 2.whether the sentence truly express the triple(optional)}



\subsection{Result Analysis}

1. Figure of generated sample results of each relation.

2. The overall results are in Table X.

3. Comparison of results with other KB in Table X.


% Ideas for data cleaning: Outlier detection is used to find outliers from some clusters, and is generally used on vector representation features. If we use some kind of word embeddings, it's hard to interpret why good triples will gather together in the feature space. So if we only use syntactic patterns, since uncommon patterns has a large decrease in frequency than common ones, it's straightforward to choose top-k patterns directly for each relation. And for each extracted triples, we verify whether they too are one of the top-k patterns.

% To provide high quality triples for training, we should clean the data. We choose ConceptNet4 raw data as our starting point. The reason why we do not use the more recent ConceptNet5, which has larger size, is that: (a) Most commonsense relation triples comes from ConceptNet4, and their size is negligibly increased in ConceptNet5. 15 commonsense relations have the only data source of ConceptNet4. 3 other relations have more than 95\% of data from ConceptNet4. (b) ConceptNet4 has frame text attached to each triple (e.g. for triple (keys, CapableOf, open doors), the frame text is "\{1\} can \{2\}"), which helps our later analysis. (c) After removing data of blacklist contributors and activities (which is provided by the ConceptNet Group) from ConceptNet4 raw data, it still contains twice the number of assertions (550K) than those moved to ConceptNet5 (225K).

% However, the accuracy of the raw data is not high, so we need to improve it. First, we put each triple in their frame text and restore to the raw sentence. After careful observation, we find the following causes of the inappropriate triples: (a) Some concepts are too vague, like "something", "it", "they". (b) Raw sentences do not make sense. (c) The raw sentence perfectly makes sense, but the head and tail part are not concepts but just segments. For example, for frame text "\{1\} can \{2\}", which is supposed to express "someone/something is capable of doing something", we also find sentences like "Something you can make is peace". It makes no sense to separate it into (something you, CapableOf, make is peace). (d) The frame texts for some relations are ambiguous. For example, for relation HasSubEvent, the dominant frame text "Something you might do while \{1\} is \{2\}" has multiple senses. It can describe sub-event relation, but it can also describe co-occur relation, like "Something you might do while singing is dancing".

% For problem (a), we can simply define some vague words and filter the data. For problem (b) and (c),  we use constituency parser on each raw sentence, extract the parse sub-trees for head and tail, and then manually select the appropriate constituency patterns of head and tail for each frame text. Here we make an assumption that restricting the syntax patterns can filter many of the semantic errors. The reason why we do not word on semantics directly is that it either requires other external knowledge bases or requires full manual check, which are neither feasible. Besides, finding and restricting the constituency patterns helps our later steps of finding variants and matching sentences in the corpus.
