\section{Introduction}
\label{sec:intro}

%\eve{define event (or give examples). add citations. 'These KBs can be 	generally classified into two categories:' needs citation. }

%\tit{introduce KB, KB can be divided into factual and CS}
In the past few decades, a variety of knowledge bases (KB) such as Freebase~\cite{bollacker2008freebase}, 
YAGO~\cite{suchanek2007yago}, Cyc~\cite{lenat1995cyc}, ConceptNet~\cite{speer2017conceptnet}, have been constructed to facilitate human-like
reasoning and inference. Such knowledge is usually presented in
triple format ($h$, $r$, $t$), where $r$ is the relation symbol, 
$h$ and $t$ are the two arguments. One example is 
\textit{(google, founders, larry page)}. There are mainly two categories of KB: \uline{factual} (or encyclopedic) and 
\uline{commonsense}. 

%\tit{Definition and property of fact}
A fact (or factual relation) is absolute truth where at least one of the
arguments is a named entity. But for simplicity, we call both arguments
of a fact \textit{entities}. 
The negation of a fact is \textbf{always} false. 
For example, given a fact \textit{(italy, administrative\_divisions, sicily)}, 
the negation ``Sicily is NOT a administrative division of Italy'' is considered
wrong.  


\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{ccccc}
		\toprule
		CSKB & Plausible & Objective&Named Entity&FN\\ \hline
		ConceptNet4 &88.0\% &4.5\%  &14.8\%  &77.3\%  \\
		Quasimodo &27.0\% &51.9\%  &81.2\%  &55.6\%  \\
		Webchild &48.0\% &85.4\% & 97.9\% &85.4\% \\
		ASER &37.0\% &0.0\% & 0.0\% & 19.0\% \\
		TransOMCS&14.0\% &14.3\%&21.3\%&14.3\% \\
		TupleKB &86.0\% &24.4\%  &48.8\%  &53.5\%  \\ \hline
		FactKB & Plausible & Objective& Named Entity&FN  \\ \hline
		
		 YAGO&100.0\% &100.0\%&100.0\%  &100.0\%  \\
		 Freebase & 100.0\% & 100.0\% &100.0\% &100.0\% \\
		
		 
		\bottomrule
	\end{tabular}
	\caption{ Characteristics of knowledge bases. Plausible: percentage of triples that are plausible; Objective: percentage of triples that are absolutely true. Named Entity: percentage of triples that at lease one of the arguments is a named entity. FN: percentage of triples that the negations are always false. These statics are evaluated from 100 sampled data per KB.}
	\label{table:properties}
\end{table}


%\tit{Definition and property of commonsense. Use Table1 to compare the characteristics of cs and fact kb}
A commonsense, on the other hand, describes a common belief or 
experience involving two generic objects. 
These objects can be general events, actions, concepts or even situations. 
It is important to note that commonsense arguments are usually \uline{generalized} 
from specific instances.
Commonsense arguments can refer to both noun and verbal phrase (a.k.a. event) 
while entities are typically nouns. For example, instead of mentioning a 
specific place, commonsense KB would be more likely to contain triples like 
\textit{(country, contains, city)} and  \textit{(eating too much, causes, 
gain weight)}. 
The negation of a commonsense triple is \textbf{not always} false. 
For instance, the negation of \textit{(bread, AtLocation, refrigerator)}, 
``bread is not in refrigerator'', is not always incorrect. 
For human beings, factual knowledge is often domain-specific and acquired
through learning, but commonsense knowledge is more universal and can be
gathered through experiences. \tabref{table:properties} compares the characteristics of commonsense knowledge and fact knowledge by sampling and extracting data from several well-known knowledge bases. \KZ{Need explain what each column 
means, either here or in the caption of the table.} This comparison shows that factual knowledge are objective and commonsense tends to be more subjective. All factual knowledge contains at least one named entity and the negation is always false, while commonsense knowledge is not the case.
%Artificial intelligence systems equipped with commonsense knowledge are able to think and infer like human. Thus, collecting such knowledge is an essential task. 

%\KZ{You need to spell out the def of factual knowledge and commonsense knowledge
%and their diff up front. You can show snippets of Freebase and ConceptNet
%to help with the explanation.}

%\eve{commonsense are concepts and factual are entities....   more examples of commonsense(np and vp).}


\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{ccccc}
		\toprule
		KB & \#concetps & \#relation & \#assertions  &sup/unsup\\ \hline
		%\multicolumn{4}{c}{Factual} \\ \midrule
		%Freebase & & & 1.9 billion \\
		%DBpedia & 4.58M & & \\
		%YAGO   & 10M & & 120M \\  \midrule
		%\multicolumn{4}{c}{Commonsense} \\ \midrule
		ConceptNet5(OMCS) & 167K & 26 & 226K & - \\
		%ConceptNet5 & 1.8M & 34 & 6M & - \\
		
		ATOMIC & 300K & 9 & 877k & - \\ \midrule
		NELL & 2.1M & 833 & 2.8M & unsup\\
		WebChild & 2M & 19 & 18M & sup \\
		TupleKB & 6.5K & 1.6K & 282K & unsup\\
		Quasimodo & 80K & 78K & 2.3M &unsup\\
		ASER(core) & 27M & 15 & 10M & sup\\
		\bottomrule
	\end{tabular}
	\caption{Statistics of several commonsense knowledge bases. The upper part is built via manual curation and the lower part is via automatic extraction. 'sup' stands for \textbf{supervised} and 'unsup' is \textbf{unsupervised}.}
	\label{table:cskb}
\end{table}

\tit{KB can be divided into manual curated and automatic extracted. Mention the shortcomings of manual curation}
There are mainly two ways to construct commonsense KBs: 
via manual curation or automatic extraction. The statistics of some commonsense KBs are listed in \tabref{table:cskb}. Much previous work (Cyc, ConceptNet and ATOMIC~\cite{sap2019atomic}) relied on crowd-sourcing human annotation. 
Such curated KBs have high precision but low or skewed coverage. 
For example, in ConceptNet, \textit{CreatedBy} has only 263 triples 
while \textit{Causes} contains 16801. Besides, such method does not scale well. 


\tit{limitations of applying supervised method on CSKB: need enrichment. Use Table2 to illustrate.}
As for automatic construction of commonsense KB, related works are limited. 

%Existing commonsense KBs are not exhaustive and still needs enrichment. 
%Though the scale of ConceptNet is quite large, the commonsense knowledge it contains mainly comes from OMCS project and is rather small compared with ATOMIC which is also human annotated. The scale of automatic generated KBs are basically larger than manual curated ones.

\tit{automatic extraction can be classified into unsupervised  and supervised. OpenIE is a representative of unsupervised method, indicated its shortcomings}
Automatic knowledge extraction from unstructured text can be modeled in two ways: unsupervised and supervised. TupleKB and Quasimodo are constructed in this way and they both utilized OpenIE, which generally relies on syntactic patterns of sentences to extract (\textit{Subject}, \textit{Predicate}, \textit{Object}) triples, and regard them as the knowledge. The downside of OpenIE is obvious: a) it can only extract the two arguments that are directly connected by a verb; b) it has a large number of relations in surface form, which are not properly grouped and abstracted. For example, \textit{can}, \textit{be able to}, \textit{have the ability of} all share the same meaning but cannot be easily recognized by their surface forms. As a result, the relations of TupleKB and Quasimodo are miscellaneous and not properly normalized. 

\tit{relation classification is a representative of supervised method, mention its pros compared to unsupervised method and why we use supervised method on CS KB}
On the other hand, WebChild~\cite{tandon2016commonsense} and ASER~\cite{zhang2019aser} employed supervised algorithms. Specifically, WebChild first use pattern to extract raw assertions from text and then design a family of algorithms to distill detailed fine-grained knowledge. However, according to  ~\citet{romero2019commonsense}, its quality is quite low.
As for ASER, we sample the assertion with highest weight from each relation and find out nearly none of them make sense. 

For factual knowledge extraction, it is typically modeled as a relation classification task, which is a supervised problem where argument pairs and a small set of relation types are given, and the task is to determine the relation between the argument pairs in text. The results are quite successful. For instance, ~\cite{xu2019connecting} achieves 86.1\% for P@10\% and 76.6\% for P@30\% on NYT dataset. Besides, it can make up for the shortcomings of unsupervised method: a) the two arguments doesn't have to be connected by verb; b) the relations are pre-defined and fixed and thus unnecessary to further group them. Thus we aim to automatically extract commonsense knowledge from text in a supervised way. 

%{Compare the quality of several CSKB and show ConceptNet has the best performance: Illustrate why we choose conceptnet}
%~\citet{romero2019commonsense} manually evaluated the quality of ConceptNet, WebChild~\cite{tandon2016commonsense}, TupleKB~\cite{mishra2017domain} and Quasimodo~\cite{romero2019commonsense} along three dimensions: 1) meaningfulness, 2) typicality, 3) saliency. 
%ConceptNet outperforms others in all metrics. TupleKB and Quasimodo are slightly worse but the relations are miscellaneous and not properly normalized. WebChild is far below the former three ones. 
%For ASER~\cite{zhang2019aser}, we sample the assertion with highest weight from each relation and find out nearly none of them make sense. 
%\KZ{Why we choose to use supervised approach?} 


%\eve{is TupleKB commonsense? add remarks on NELL and ATOMIC.}

%\eve{not mention ConceptNet, be as general as possible}
%\eve{why focus supervised but not unsupervised? supervised: nell , webchild unsupervised: others.}
%\eve{should be a table of comparison between factual and cs!}

%\eve{quality of diff kb}



%For factual knowledge, the arguments can be determined by the named entity recognition (NER) task. But for commonsense knowledge, we expect the arguments to take a more diverse form than named entities.


% Relation extraction, however, does not suffer from above problems. Factual relation extraction is always modeled as a problem where entity pairs and a small set of relation types are given (thus we will use 'relation extraction' and 'relation classification' mutually in the following part) while in commonsense relation extraction, the task definitions are quite different from each other. 
% Such diversity somehow disperses research efforts and therefore works in commonsense relation extraction are much less than factual relation extraction. 
% In ACL2019, there are 16 papers whose titles contain 'relation extraction' or 'relation classification' and all of them focus on factual relation. However, there is no paper about commonsense relation extraction. 

%\tit{experiments to show automatic CS extraction not successful}
%\KZ{Which parts shows that it's not successful?}
%In this paper, we conduct experiments of automatic commonsense knowledge extraction and show it is not successful. We further analyze why it is difficult to directly apply achievement of factual relation extraction to commonsense relation extraction. In particular, we investigate characteristic differences between these two types of knowledge and their potential presence in natural language text. 


\tit{reason of unsuccessful  automatic extraction on CSKB}
We applied a popular factual relation extraction model on commonsense knowledge. Experimental results showed that the model failed on commonsense relation extraction. 
We discovered one major reason is that the arguments of commonsense relations takes on a larger variety of forms (argument polymorphism) than factual relations. %\eve{!carefully check}
\eve{rewording. approach in detail}
To solve commonsense argument polymorphism problem, we find variants of a given commonsense argument by means of various resources.

\tit{contributions}
In summary, our contribution are as follows:
\begin{itemize}
\item We have showed characteristic differences between commonsense knowledge and
factual  knowledge (see \tabref{table:properties});
	\item we discovered commonsense argument polymorphism as a main challenge 
of automatic commonsense relation extraction (see \secref{sec:polymorphism});
	%\item we improve the precision of ConceptNet to offer higher quality triples for .... (\secref{sec:clean}); \eve{need to emphasize the diff goal between point 2 and 3}
	\item we develop a framework of distant supervision for commonsense: automatically and efficiently find semantically related commonsense arguments given the noun phrase and verb phrases in a sentence;
	\item using the training data created by our framework, we can extract more commonsense tripples with high accuracy and coverage. (\secref{sec:eval}); 
\eve{need to evaluate from acc and coverage}
%	\item after obtaining variants, we augment concept vocab. and the augmented concept vocabulary\eve{what is concept vocab? need to mention it before.} increases the number of sentences where head and tail concepts co-occur and thus improve the performance of relation extraction model (\secref{sec:eval}).
\end{itemize}

%Ideas for match arguments in sentences: find all NPs and VPs in a sentence. For each NP/VP and each argument in the training set, we find an alignment of words such that the average semantic similarity of each pair of aligned words is maximized (can use cosine similarity of word embeddings). If the average similarity is above a threshold, we think the two phrases express the same meaning.
