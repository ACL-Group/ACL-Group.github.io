\section{Approach}
\label{sec:approach}
We evaluate datasets with statistical features only. At first, we formulate  
these tasks in a general format. Then based on the count of unigram-related tokens
on each label, we try and design different metrics to 
measure the correlation between the unigram-related tokens
and labels. Finally, we aggregate the scores together to choose the predictive endings.


\subsection{Task Formulation}

%\textcolor{red}{Hongru: these are concrete examples, which i think should be given after the problem
%formulation} 
Given a data point $x$ of a Natural Language Understanding(NLU) task dataset $X$ we mentioned above, we formulate it as
\begin{equation}
    x = (p, h, l) \in X,
\end{equation}
\noindent
where $p$ is the context 
that we aim to understand which corresponding to ``premise'' in~\exref{exp:snli}.
$h$ is the understanding with respect to the context $p$. $l \in \mathcal{L}$ is the label that depicts the relation between the context $p$ and understanding $h$. 
The size of the relation set $\mathcal{L}$ varies from datasets. We argue that 
most of the discriminative NLU tasks can be formulated into this general form. 
%which will bring us much convenience to evaluate the cues of a dataset. 
For example, an NLI question consists of a \textit{premise}, a \textit{hypothesis} 
and a \textit{label} on relation between these two sentences. 
The formulation of this task will be $p =$ \textit{premise}, $q =$ \textit{hypothesis} and $l =$ \textit{label}. $|\mathcal{L}| = 3$ for three different 
relation \textit{entailment}, \textit{contradiction} and \textit{neutral}. 
We will talk about transformation into this form in \secref{sec:dynamic}. 
%The ROCStory~\cite{mostafazadeh2016corpus} dataset
%such as in~\exref{exp:roc}, 
%consists of a \textbf{context} and two possible story \textbf{endings}. 

%We will formulate this task by setting $p = $ \textit{context}, $h = $ \textit{ending1/ending2}. In this case, $|\mathcal{L}| = 2$. because $l$ is ``true'' or ``false'' indicating whether 
%the ending is a plausible ending of the story.

\subsection{Cue Metric}

For an NLU task dataset $X$, we collect a set of specific unigram-related tokens $\mathcal{N}$. These 
tokens can be unigram or cross-unigram that consists of a pair of
unigrams, one from the premise and other from the hypothesis.
%the token pair between $p$ and $h$.
The cue metric for a unigram
shows the degree to which the distribution of a single word on 
 different labels is unbalanced. 
The cross-unigrams, such as  ``swimmer-sea'' in~\exref{exp:snli}, 
represent the 
relational unigrams in a dataset. 
%The ``swimmer-sea'' cross-unigram can be identified as a cue if it always appear in the instances with 
%one label, like entailment.

Let $w_k \in \mathcal{N}$ be the $k$-th unigram-related tokens, we compute a scalar statistic metric $f_{\mathcal{F}}^{(k,l)}$ of $w_k$ with respect to label $l$ as
%\KZ{Consider changing $\mathcal{F}$ to $\mathcal{B}$ to avoid confusion with $f$?}

\begin{equation}
    f_{\mathcal{F}}^{(k,l)} = f_{\mathcal{F}}(w_k, l),    
\end{equation}
%We call $f_{\mathcal{F}}^{(k,l)}$ \textit{cue metric}, 
where $f_{\mathcal{F}}^{(k,l)}$ is a function which measures how much spurious information can be conveyed by token $w_k$ for a particular label $l$. 
$\mathcal{F}$ is a set of cue metrics that we used for computing the \textit{cue score}. We categorized the functions into two
genres: one is for only statistics and the other is in euclidean space.

\subsubsection{Statistics}
%\KZ{Since you show so many diff stats measures, you need to show some
%results in the eval how you pick the cond prob in the end.}
This category of functions measure the cues from the perspective of statistics.

\noindent\textbf{Frequency(Freq)}

The most simple but straight measurement is the co-occur of the 
unigram-related tokens and labels, where $\#()$ means naive counting.
\begin{equation}
    f_{Freq}^{(k,l)} = \#(w_k, l)
\end{equation}

\noindent\textbf{Conditional Probability(CP)}

The distribution of a dataset based on label may not always balance. 
So we also try conditional probability as a measurement of correlation.
\begin{equation}
    f_{CP}^{(k,l)} = \frac{\#(w_k, l)}{\#(w_k)}
\end{equation}

\noindent\textbf{Point-wise Mutual Information (PMI)}

PMI is a widely used method for association measurement in information theory and statistics.
We estimate the probability:
\begin{equation}
p(l) = \frac{\#(l)}{\#(\mathcal{L})}, p(l|w_k) = \frac{\#(w_k, l)}{\#(w_k)},
\end{equation}
where $\#(\mathcal{L}) = \sum_{l\in \mathcal{L}} \#(l)$.
The PMI score of token $w_k$ with respect to \textit{label} $l$ is
\begin{equation}
    f_{PMI}^{(k,l)} = \log \frac{p(l|w_k)}{p(l)}
\end{equation}

\noindent\textbf{Local Mutual Information (LMI)}

Considering the frequency of tokens can influence models with different weight and inspired 
by \citealp{schuster2019towards}'s work,
we estimate the probability by
\begin{equation}
    p(w_k, l) = \#(w_k, l) / \sum_{i=1}^{|\mathcal{N}|}\#(w_i).
\end{equation}

The LMI of token $w_k$ with respect to \textit{label} $l$ is 

\begin{equation}
    f_{LMI}^{(k,l)} = p(w_k, l)\log \frac{p(l|w_k)}{p(l)}.
\end{equation}

\subsubsection{Euclidean Space}

We can look at it from the perspective of angle in euclidean space.
Let $\mathcal{L'} = \mathcal{L} - \{l\}$ and define 
\begin{equation}
    \#(w_k, \mathcal{L'}) = \sum_{l' \in \mathcal{L'}} \#(w, l').
\end{equation}
Think of $v_w=[\#(w_k, l), \#(w_k, \mathcal{L'})]$ and $v_l = [(\#(l), \#(\mathcal{L'})]$ are two vectors on a 2D plane. 
Intuitively, if $v_w$ and $v_l$ are co-linear, token $w_k$ shall leak no spurious information. 
On the contrary, $w_k$ is suspected to be a spurious cue if it tend to appear with a specific
label $l$. The following functions are designed based on this intuition.

\noindent\textbf{Ratio Difference(RF)}

\begin{equation}
    f_{RF}^{(k,l)} = \left|\frac{\#(w_k, l)}{\#(w_k, \mathcal{L'})} -
    \frac{\#(l)}{\#(\mathcal{L'})}\right|
\end{equation}

\noindent\textbf{Angle Difference(AD)}

Angle Difference is similar to \textit{Ratio Difference} except that we take arc-tangent function.
\begin{equation}
    f_{AD}^{(k,l)} = \left| \arctan\frac{\#(w_k, l)}{\#(w_k, \mathcal{L'})} -
    \arctan \frac{\#(l)}{\#(\mathcal{L'})} \right|
\end{equation}

\noindent\textbf{Cosine(Cos)}
%We imply calculation of the cosine distance between the two vectors.
\begin{equation}
    f_{Cos}^{(k,l)} = \cos(v_w, v_l)
\end{equation}

\noindent\textbf{Weighted Power(WP)}

\begin{equation}
    f_{WP}^{(k,l)} = (1-f_{Cos}^{(k,l)})\#(w_k)^{f_{Cos}^{(k,l)}}
\end{equation}

All of the functions mentioned above are used to measure the correlation between a token $w_k$ and a specific label $l$. The \textit{cue score} of $f_{\mathcal{F}}^{(k,l)}$ is denoted as $s^{l}_k$.
%Once obtained the bias metric of each \textit{ngram-span} of $\mathcal{N}$, we can sort them in order and set a threshold. 
%If the bias metric of a \textit{ngram-span} $n_k$ surpass the threshold, we say it is a \textit{bias ngram-span}. 
%We denote the set of \textit{bias ngram-span} as $B \subseteq \mathcal{N}$. 
%If a model output a correct answer on a data point $x \in X$ that contains some \textit{bias ngram-spans} in domain $p$ and $h$, 
%the result is likely to be overestimated because the data point might have conveyed some spurious information.

\subsection{Aggregation methods}

The cue metrics represent each unigram-related token with the \textit{cue score} $s^{l}_k$. 
We use very simple methods $\mathcal{G}$  to aggregate these features for an instance to choose 
the correct answer.
%for convenience and effectiveness. 

A dataset consist of $m$ instances can be denoted as $E = (e_1, e_2, ...e_m)$. 
${\mathcal{L}}_E={l_1, l_2, ..., l_t}$ is the alternative label 
set for dataset $E$.
Given an instance $e_n$ with $d$ tokens and one gold label $l_n$, $e_{n}=({w}_{n_1}, {w}_{n_2},... w_{n_d}, l_n)$,  $l_n\in{\mathcal{L}_E}$, 
the easiest way to choose the tendentious label is getting the average or max \textit{cue score} in an instance. 
%Each span ${w}_{n_i}$ corresponding to t bias scores 
%We represent the aggregate score for $e_n$ with label $l\in{\mathcal{L}}$ as $\mathcal{G}_l$.

\begin{equation}
%    \mathcal{G}(average) = \frac{\sum_{i}^{d} f_{\mathcal{F}}^{(w_{n_i})}}{\left | d \right |}
     \mathcal{G}_{average} = \mathop{\arg\max}_{l}{\frac{\sum_{i}^{d}s^{l}_{n_i}}{\left | d \right |}},  l\in{{\mathcal{L}}_E}
 %\mathcal{G}(average) = \frac{\sum_{i}^{d}s^{l}_{n_i}}{\left | d \right |},  l\in{{\mathcal{L}}_E}
\end{equation}

\begin{equation}
 %   \mathcal{G}_l(max) = max(f_{\mathcal{F}}^{(w_{n_i})})  i\in{d}
 \mathcal{G}_{max} = \mathop{\arg\max}_{l}{\max(s^{l}_{n_i})},  i\in{d},l\in{{\mathcal{L}}_E}
 %\mathcal{G}(max) = \max(s^{l}_{n_i}),  i\in{d},l\in{{\mathcal{L}}_E}
\end{equation}

%For $e_n$ with labels $\mathcal{L}=(l_1,...l_v)$, we will get an aggregate score set $(\mathcal{G}_{l_1},..., \mathcal{G}_{l_v})$. 
%The chosen label can be get with:

%\begin{equation}
%    chosen\_label = \mathop{\arg\max}_{\theta}\mathcal{G}_{\theta}
%\end{equation}

For better take advantage of the \textit{cue score} to choose the right choice. We also use two linear model: SGDClassifier and 
logistic regression. The inputs of the models for instance $e_n$ is the concatenation of \textit{cue scores} for each label which can express as : 
\begin{equation}
\begin{aligned}
input(e_n) = &[ s^{l_1}_{n_1},..., s^{l_1}_{n_d},  s^{l_2}_{n_1},..., s^{l_2}_{n_d},\\&
..., s^{l_t}_{n_1},..., s^{l_t}_{n_d}].
\end{aligned}
\end{equation}

 %$input(e_n) = [ f_{\mathcal{F}}^{(w_{n_1}^{l_1})},..., f_{\mathcal{F}}^{(w_{n_d}^{l_1})},..., f_{\mathcal{F}}^{(w_{n_1}^{l_v})},..., f_{\mathcal{F}}^{(w_{n_d}^{l_v})}]$. 
 
 The loss for training the linear model is:
 \begin{equation}
  \hat{\phi}_{n} = \mathop{\arg\min}_{\phi_n}{loss({\mathcal{G}_{linear}(input(e_n);\phi_n)})}
\end{equation}
%The output for a certain instance is  The corresponding target is the correct label $l\_n$. 
% \begin{equation}
%output(e_n) = linear_classifier(input(e_n))
%\end{equation}
where $loss$ is between the
gold label $l_n$ and the predicted label $\mathcal{G}_{linear}(input(e_n);\phi_n)$.
 %is computed for each predicted
$\phi_n$ are the optimal
parameters in $\mathcal{G}_linear$
that minimize the loss
for label $l_n$. 

With the aggregator $\mathcal{G}$, we can choose the 
label l with strongest strongest tendency. 
Then this chosen label for $e_n$ will be compared with the gold label $l_n$ to
 determine whether the chosen answer is correct or not.

\subsection{Transformation of MCQs with dynamic choices}
\label{sec:dynamic}
So far the multiple-choice questions we targeted such as NLI problems
are actually classification problems with fixed set of choices. There is
another type of language reasoning tasks which are also in the form of
multiple-choice questions, but their choices are not fixed, as shown
below. 
\begin{example}\label{exp:roc}
An example question in ROCStory~\cite{mostafazadeh2016corpus}.

\noindent
\textbf{Context}: Rick grew up in a troubled household. 
He never found good support in family, and turned to gangs.           
It was n't long before Rick got shot in a robbery.             
The incident caused him to turn a new leaf.

\noindent
\textbf{Ending 1}: He joined a gang. 

\noindent
\textbf{Ending 2}:  He is happy now.

\noindent
\textbf{Answer}: 2
\end{example}

In this type of tasks,
we can separate the original story into two unified instances, 
$u_1=(context, ending1, false)$ and $u_2=(context, ending2, true)$.
We can predict the label with probability for each instance $\mathcal{G}(input(u_1);\phi)$ and 
 $\mathcal{G}(input(u_2);\phi)$.
Then we can choose the ending with higher probability to be true, 
and compare it with the gold label.
%For better take advantage of the bias score to choose the right choice. We also use two linear model: SGDClassifier and 
%logistic regression. The inputs of the models for instance $e_n$ is the concatenation of bias scores for each label which %can express as :  $input(e_n) = [ f_{\mathcal{F}}^{(w_{n_1}^{l_1})},..., f_{\mathcal{F}}^{(w_{n_d}^{l_1})},..., f_{\mathcal{F}}^{(w_{n_1}^{l_v})},..., f_{\mathcal{F}}^{(w_{n_d}^{l_v})}]$. The corresponding target is the correct label $l_{gold}\in{L}$.

 

