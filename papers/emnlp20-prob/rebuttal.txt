#Review 1
Thanks for your suggestions and we are sorry that our less-than-perfect expression may have prevented you from noticing the strength of our work. Our purpose is to design a general-purpose evaluation system (or a tool), just like the BLEU score, which can be widely used. The goal of our evaluation system is to evaluate the quality of datasets and models in the certain direction, artificial cues.
Q1: In addition, many of these findings aren’t novel / surprising...
A1: We are sorry that we haven't impressed you enough with our 
innovation.  You pointed out that our results did not surprise you. 
But in fact, the same performance on different datasets already shows 
the effectiveness of our evaluation system. 
Another big advantage is that our method is very light-weight. 
We only used the effective imbalance characteristics. 
Through experiments, we found the most effective cues 
evaluation metric from the previous score calculation 
methods and our own design calculation methods.
Even if some people choose LMI or PMI to give some examples 
of cues, like  Gururangan et. al. (NAACL 2018), 
they do not give the reason or comparison for using this score. 
In addition, our experiments show that the LMI algorithm which 
considers the cues' frequency is less effective, 
which shows that there is no significant relationship 
between the acquisition of deviation and the frequency. 
This finding has not been mentioned in any paper. 
The calculation of cue score is of 
great significance to the modification of data set.


Q2: Given that the filtering model is just a (simpler)...(to the end)
A2:  We are sorry that our expression might have confused you.
It is true that we still use linear model. 
However, the features we use are low-dimensional. 
The logistic regression model is almost overfitted, 
and we have to use regularization method 
to solve this problem. So this will not 
change even with the emergence of new models.

Q3: Section 3.1...
A3: Our first work is dedicated to measuring the quality of datasets. 
Some of the datasets we are exploring have been found 
to have problems or have been improved (SWAG). 
Our experimental results also verify that our evaluation 
results are highly consistent with previous studies, 
which shows the effectiveness of our evaluation method. 

Q4: Section 3.2... 
A4: The first work evaluate the whole dataset(train/test/dev).
Our second work is divide the test data for model evaluation. 
Since we have known which questions can be correctly chosen with only 
cue score, we can divide the test dataset into easy and hard part. 
Our partition method is based on cue features, which is different 
from the previous training with Bert or other models 
(Bras et al., 2020). They revise the dataset with models and 
then test on the same models (like Bert) which is not convincing. 
The features we use are more single and transparent(cues), 
and we can directly observe the features learned by the model.

Q5: Section 3.3... 
A5: The third work is the optimization of training set. 
As for the comparison with AFLite, 
their task is to separate out better training sets, 
rather than segment the test sets and evaluate 
the robustness of models. Their method can not 
be transform to separate the test dataset. 

Through our system, we can supplement the 
previous research. We can measure the data set, 
optimize the test set, and optimize the training set. 
Our work is more complete than the any previous research in the
same line. 


#Review 2
Thanks for your suggestions.

#Review 3
Thanks for your suggestions. It's a pity that you may have misunderstood the problem we described. 
Q1: Isn’t the definition of a cue an...
A1: Cue is defined as "leakage" because we 
mentioned in the introduction that many models 
are over evaluated. The reason for this is leakage. 
If the distribution of cues is inconsistent, 
the poor generalization ability of the model will be 
exposed and the there is no over-estimated 
problems of data sets (though there may be cues 
both in train and test datasets). 
Thus we only focus on the datasets that can lead to overestimation.

Q2: Cross-unigram seems to be just word alignments...
A2: In Sec 2.2 we mentioned that "The cue metric for a unigram
shows the degree to which the distribution of a single
word on different labels is unbalanced." 
Thus cross-unigram isn't word alignments 
but the word alignments based on labels.
For example, "trip" in premise and "happy" in hypothesis
is alway labeled with "contradiction" (which may be caused by the 
failure of dataset design , some models may only make 
decision with the imbalance information and make wrong selection
). Word alignment for this cross-unigram pair may be p("trip"|"happy"). 
However, we focus on p("trip","happy"| "contradiction"). 
Word alignments(("trip"|"happy") are useful information but 
its imbalanced frequency in the case 
of different labels(p("trip","happy"| "contradiction") 
is the source of information bias which make the questions too simple. 

Q3: “choices are not fixed”...
A3: "choices are not fixed" means this is a multiple choice question but not a classification problem. So ROCStories belongs to the second type.

Q4: " Why is random predictions used as..."
The options of the multiple choice problems we studied are all balanced, 
so “majority baselines“ are consistent with "random baselines". 
Thanks for your suggestions and we will use more accurate expressions 
"majority" in our revised version.

