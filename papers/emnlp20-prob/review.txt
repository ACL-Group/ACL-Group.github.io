
Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?

This paper proposes a dataset-agnostic strategy for identifying “easy” and “hard” instances in datasets (~instances that can be solved with shallow lexical cues, and instances that cannot). The authors apply their method to 12 existing datasets, and show that simple classifiers on a battery of simple lexical features can perform very well on most of them.
The authors use their strategy to filter out the easy examples from test sets and generate new, hard splits. They show that several existing models for NLI (BERT, ESIM, fastText) fail to generalize to these harder test sets.

Lastly, the authors explore the potential of filtering training data to train models that are more robust to these shallow lexical cues. In many cases, performance on the hard splits improves.

Strengths:

Interesting research question and thorough experiments.

Weaknesses:

The method isn’t actually model-independent (or, it is at least as model-independent as AFLite). AFLite is criticized for using a model to do the filtering, but a model is required to filter here as well---it’s just that the model is simpler (a linear classifier over manually defined lexical features).

In addition, many of these findings aren’t novel / surprising---the line of work in adversarial filtering has explored many of these research questions, and come to the same conclusions. It’s good to see a replication, but as it currently stands, I don’t think there is enough novelty for an EMNLP paper.

Section 2: Many of these statistics are explored in past work, and there are many papers that reveal the shortcomings of datasets. For instance: Suchin Gururangan et al., 2018 use PMI, and Tal Schuster et al, 2019 use LMI. Roy Schwartz et al., 2017 use simple features based on ngrams and length to win the ROCStories shared task. In general, many of these features are classical shallow features from pre-neural NLP, and the filtering model can be interpreted as a standard feature-based NLP model.

Given that the filtering model is just a (simpler) linear NLP model, it’s unfair to criticize Adversarial Filtering, AFLite, etc. for being model-dependent, since this method is also fundamentally model-dependent.

Section 3.1: This section seeks to show that for many datasets, a simple feature-based classifier is sufficient for modest (at least better-than-random) performance. These conclusions have already been shown in the literature (e.g., Roy Schwartz et al., 2017 for ROCStories, the SNLI paper baselines, etc). It don’t think it’s surprising that for many modern NLP datasets, you can extract signal with even the models of yore (i.e., linear models on simple features). A notable exception here are datasets that are adversarially generated / have filtering already built in, such as SWAG.

Section 3.2: This section seeks to show that models perform worse on the hard subset than the easy subset. This result is quite similar to the one in Gururangan et al. (2018), and also for the same task of NLI. The models are different, but the conclusion is one that is expected to be robust across models.

Section 3.3: This section argues that we can do better on hard data by training on hard data (rather than hard + easy data). This is similar to a lot of work in adversarial training / robust optimization. In addition, the AFLite paper (https://arxiv.org/abs/2002.04108) provides this same result with respect to filtered datasets possibly improving zero-shot generalization.

Reasons to accept

Thorough experiments and replication of results in the literature.
Reasons to reject

Lack of novelty compared to prior studies in the area.
Reproducibility:	3
Overall Recommendation:	1.5
Questions for the Author(s)

What does the bold in table 2 mean?
Typos, Grammar, Style, and Presentation Improvements

I noticed a few typos and grammar errors as I was reading, but I didn’t keep track of all of them. The one that jumped out at me was L744, BLUE -> BLEU.


Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?

What is the task?
light-weight framework to automatically identify possible biases from any NLU-related multiple-choice datasets and further evaluate the robustness of any models targeting the datasets.

What has been done before? Recent works have identified statistical spurious patterns like sentiment, word overlap and even shallow ngram tokens in benchmark datasets that are predictive of the correct answer.

Existing techniques to improve the quality of the datasets include data filtering, data augmentation and stress test generation. But these suffre from one or more of the following problems.

Model specific (do not generalize to arbitrary models) Expensive/manually intensive Handle certain kind of cues

What are the contributions (novelty) of the paper? A computationally efficient framework which can be used to evaluate a broader range of multiple choice datasets using following methods

A light-weight and effective method to evaluate the unigram related cues of multiple choice datasets By separating the test dataset two parts according to the extent of unbalance, one can measure the model’s robustness and real performance by differences of the results on the easy and hard part of the test data Splitting the training set using a heuristics that allows to train the model to better accuracy on the hard test data.

What are the key techniques to tackle this task?

Given the instances containing premise, hypothesis and label, calculate the correlation score between cues first.

Evaluate the “biasness” and “quality” of the dataset, by a very simple model using only the cues to try to solve the questions in the dataset and then separate the dataset into easy and hard parts based on whether the question can be correctly answered using the cue features only. This hard-easy split can be used to estimate the real performance of a model.

Splitting the training data into easy and hard parts, too and show that by filtering out the easy questions from training data, there is a potential to improve the robustness of the model by “forcing” it to steer its focus away from trivial cues.

What are the main results?

unigram and cross-unigram cues are general problems for multiple choice datasets. if a model is affected by cues in the dataset, it will have different performance on easy and hard parts it is possible to “split” the training data in the same way and use the hard part of the training data to train a better, more robust neural model

Reasons to accept

This work focused on the description of the extent to which datasets contain spurious cues, and improve these datasets in a way that can benefit the evaluation of all kinds of models.
The computationally efficient framework is dataset and model independent. Additionally, it has the potential of filtering given biased training data to help train a more robust model.

Reproducibility:	5
Overall Recommendation:	4
Typos, Grammar, Style, and Presentation Improvements

Line 497 it’s table 2
Typo in line 647



Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?

The paper uses metrics to evaluate whether there are “easy” shortcuts in a wide range of datasets. The paper uses these metrics to then filter out hard examples. Finally, the paper demonstrates how advanced models (BERT, ESIM, fastext) perform worse on the “hard” examples compared to the “easy” examples.
weaknesses: The paper relies on “cross-unigram cues” which seem to be just word alignment methods. The issue here is that it is not clear why a model relying on word-alignments is indicative that a dataset is “easy” or bad. strengths: The paper explores models robustness to spurious correlations in datasets.

Reasons to accept

The NLP community is interested in analyzing popular datasets and this paper continues that trend
Reasons to reject

It is not clear how much this paper adds to the very popular literature on model robustness and biases in datasets. 3 of the datastes explored here (SNLI, MNLI, ROC) have been explored in this way by many recent studies. The paper lacks qualitative examples that demonstrate and provide examples for the “leakage” for different datasets.
Reproducibility:	4
Overall Recommendation:	2
Questions for the Author(s)

Isn’t the definition of a cue an unbalanced pattern with “different labels in both training and test dataset , and the distributions are similar” the ideal situation for using ML to model a problem? Why is this version of “leakage” where “the same cue distribution exists in both training and testing data” such a bad thing Many of the papers that identify “spurious patterns” dont necessarily demonstrate that distributions for the patters are similar across train/dev/test splits. For example, Poliak et. al (StarSEM 2018)’s analyses are on the development sets and Gururangan et al. (NAACL 2018) compute PMI just using the training sets. Cross-unigram seems to be just word alignments. How is this different from word alignment methods that were used in RTE? Also, why would alignments between contexts and hypotheses be considered bad cues? The paper mentions that in the second type of datasets considered the “choices are not fixed”. This doesnt seem to fit the description of the ROCStories dataset (which is used in Example 2) where the task is to predict which sentence is the better ending of the story. What can be said about the “hard” examples discovered here compared to the “hard” examples identified by Gururangan et. al. (NAACL 2018) or other “hard”/”debiased” datasets. Why is random predictions used as the baseline as opposed to the “majority baseline”, which might be a stronger baseline for some of these datasets.
Missing References                          

The reference for Multi-NLI seems to be attributed to Wang et a. 2018 instead of Williams et. al. (2018 NAACL).
Typos, Grammar, Style, and Presentation Improvements

The writing can be cleaned up a bit. There are some phrases and sentences that are confusing The sentence “We separate the test dataset two parts according to the extent of unbalance” in line 155 is a bit confusing In 160 - 163, it seems like there is a missing word, “train the model to better accuracy …” probably should be “train the model to {achieve/attain} better accuracy”     Clear writing is better than overcomplicated writing. What exactly does it mean for a prediction to be a “tendentious choice”?
