We really appreciate the precious comments about our work and the following 
addresses some of the concerns raised by the reviewers.

Review #1:
1. Error propagation in terms of MaltSequence: Though we didn't analyze this 
issue in the paper, the performance did actually suffer from this problem 
by our observation. In fact, this is the main cause of unsatisfactory accuracy. 
Presently, we're trying to find a more reasonable order predictor instead 
to improve the preliminary attempt in the paper.

2. Against the easiest-first proposal: From extensive observation,
we have previously discovered that MALT tends to process the arcs in 
a buttom-up, left-to-right order (to construct the dependency tree). 
We also know that the modification words are easier to process than 
sentence backbones, i.e. easy-first turns out to be a buttom-up order for
outputting a dependency tree. Therefore MaltSequence supports the 
easy-first intuition. We mannually checked the MaltSequences of 100 
randomly selected  sentences whether they follows the easy-first intuition 
and computed the Spearman distance with breadth-first traversal as 
a measure of easy-first degree. MaltSequence gives the minimum Spearman 
distance, from which we can conclude why it outperforms other 
types of sequence. 

3. MaltParser is not needed: we agree with this. In fact, we didn't 
make use of MALT at first. As a human, we scans each word from 
left to right to determine whether to attach it or delay its processing, 
according to the context. It is a classfication problem leveraging
contextual features. While implementing this idea, we gradually discover 
this is just another explanation of the MALT parser! So we later mine 
this sequence directly from MALT to get a quick experiment result. 
As we can see from the result, better solution of sequence predictor is 
needed.

Review #2:
1. Early update: That's a very good suggestion! We also realized this problem 
from the comparison of accuracies with BeamParser(Yue Zhang, 2008). 
When a left-to-right sequence is fed to our decoder (in our experiment),
our decoder becomes a one-beam decoder. However, our results are worse 
than theirs, which can be attributed to noise introduced in the training method.

2. Bad performance of ScoreBased method: We did investigate this issue. 
In order to see the influence of different sequence types on the 
feature weight training (head mapper), we printed out the heat map of 
first order arc score of every word pairs. We found that better 
sequences are more capable of assigning higher scores to the important arcs 
and low variance with high average scores reflect the training noise. 
So the quality of feature weights have a big influence on the ScoreBased 
method. What's more, we find it's important to keep accordance of 
training sequence type and test sequence type from another interesting 
experiment (cross sequence types training-testing) and ScoreBased 
failed to meet this requirement. The reason may be exactly as spelled out
in the review and more work should be done. We currently just implement 
a prototype and want to show some promising preliminary results.

Review #3:
1. Specific high-order feature: We didn't list the features because of 
the limited space. We haven't try out all possible high-order features 
to enhance the preliminary performance, which is certainly part of
the future work.

2. Adjustment of MIRA: gold parse, current parse, feature vector and 
weight vector are all necessary input to each MIRA update. In order to get 
the final weight vector, we only have to compute the parse tree with 
highest score, given the weight vector from last update. 
MIRA only trains the weight vector of head mapper scorer and 
the sequence is always provided for this training process.

3. Other integrated sequence models: Yes, we are trying to 
raise more reasonable sequence models to get closer to the 
ultimate upper bound mentioned in the paper.
