Title:	Towards Sequence-Based Dependency Parser
Authors:	Wenjing Fang, Kenny Zhu, Yizhong Wang and Jia Tan
Instructions

The author response period has begun. The reviews for your submission are displayed on this page. If you want to respond to the points raised in the reviews, you may do so in the box provided below.

Please note: you are not obligated to respond to the reviews.

Review #1

Appropriateness:	5
Clarity:	3
Originality:	2
Soundness / Correctness:	2
Impact of Ideas / Results:	2
Meaningful Comparison:	3
Substance:	2
Replicability:	4
Recommendation:	2
Comments

An approach for easy-first dependency parsing is presented by this paper. Given an input sentence, a classifier called sequence predictor is first used to permute the words in the sentence, then another classifier called head-mapper is used to output a dependency tree. The sequence predictor, trained using oracle action sequences, determines the order of processing (the easiest is permuted to the front of the sentence). Three types of sequence predictors are proposed, among which the one based on Maltparser works the best, and the remaining two do not seem to work that well and the predictor based on MaltParser essentially uses the action sequence generated by MaltParser as part of its shift-reduce process.

This paper has some nice ideas, however using MaltParser to generate word sequences concerns me in terms of error propagation, and this seems to go against the easiest-first proposal of this paper, since the word sequences generated by MaltParser do not necessarily follow the easiest-first strategy if at all. It would have been much better if MaltParser is not needed.

Minor comments:

the reference format doesn't conform with the required style.

Review #2

Appropriateness:	5
Clarity:	4
Originality:	3
Soundness / Correctness:	2
Impact of Ideas / Results:	2
Meaningful Comparison:	3
Substance:	3
Replicability:	3
Recommendation:	2
Comments

This paper presents an approach to construct a non-projective dependency parse by bridging the graph-based parsing and transition-based parsing. At each parsing step, it first chooses a word to process and then folds it under some head word as a modifier. Experiments on non-projective treebank show that it can achieve better performances than the vanilla Malt parser which does not support non-projectivity.

The paper is clearly written and easy to follow. I have a few questions/concerned and I would like to see them addressed in the revised version.

In Section 3.1, the head mapper is trained with MIRA by rewarding the correct sequence and penalize the incorrect at the end of the decoding. The authors might also consider early update/max-violation update to better fit this structured prediction problem.

In Table 1, it is very interesting that MaltSeq achieves the best accuracy, which means that most attaching order can be decided locally with quite high accuracy. But it is weird to see that the performance ScoreBased is even worse than random, since ScoreBased is actually a joint model, which should optimize towards the objective better. Is there any explanation? (This might involve the first point I mentioned, that there are too many search error in the greedy search, so that early/max-violation update is necessary.)

Review #3

Appropriateness:	5
Clarity:	5
Originality:	4
Soundness / Correctness:	4
Impact of Ideas / Results:	3
Meaningful Comparison:	4
Substance:	4
Replicability:	4
Recommendation:	3
Comments

The paper makes an interesting decomposition of the dependency parsing problem into two stages: determining a processing order for the words in the sentence, and then scoring potential arcs (as in graph-based parsing) one complement at a time, based on that order.

The speed-up over MST is modest and by itself might not justify the two-fold approach (at the cost of greedy decoding), but the possibility of tractably introducing higher-order features is much more important. It would be worth including more detail about the second-order features used here. Ideally, higher-order features might be developed specifically for this approach.

It is disappointing that the one attempt at integrating the sequence determination with the arc scorer (feature score based predictor) resulted in poor performance, though perhaps unsurprising given the coarseness of the proxy for ease of parsing (distance between top two arc scores). It is particularly striking that this approach resulted in worse performance than random order. The authors make no mention of whether or how MIRA training was adjusted to accommodate this double-duty, so it seems likely that the order both during training and decoding was determined somewhat arbitrarily by this very rough proxy.

I do think it would be worth considering other integrated sequence models, such as directly training a greedy scorer to pick the next word at each step based on specifically chosen features that incorporate parser state.