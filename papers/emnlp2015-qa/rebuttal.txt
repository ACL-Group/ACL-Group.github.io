Many thanks to the previous comments that will help improve this work.
Below are our responses to some of the questions raised in the reviews.

Review #1: 
1. None of the existing work attempts to solve our problem. 
For example, (Nakashole et al., 2012) infers only one scheme but not a ranked
list of schemas from a binary relation pattern. (Ritter et al., 2010)
doesn't infer explicit schemas, but group entity pairs as different senses.
Due to the difference in the output, it's hard to make a comparison directly.
Thus we use a PMI-based method as the strong baseline to compare with 
our method, because it's a popular method widely used in the related
selectional preference task.

2. For the empirical values, we generated a validation set for 
parameter tuning, which contains 10 relation patterns that are not 
in the test set.  Due to space limits, we didn't explain this detail 
in the paper.


Review #2: 
1. Due to the formatting problem we encountered just before the 
submission deadline, the paper exceeds the paper limit by just 3 lines.
This can certainly be rectified.


Review #3:
1. For Eqn. 6, we follow the problem definition to find 
type pairs with largest coverage.
Due to the incompleteness of the knowledge base, the support size of 
different type pairs might not be exactly the same in all scenarios, 
therefore we use Eqn. 6 to relax the notion of "type pairs with the 
same coverage", to tolerate small differences in support size, which is
parameterized by lambda.

2. "P(1_or_2_hop_rel, w)" is a typo and should be "P(p->, w->)".

3. The result in Table shows that the PMI-based method tends to output 
excessively fine-grained type pairs first, even though the specific 
type pair is irrelevant to the semantic meaning of the relation pattern. 
Therefore, we think the difference betweene two methods are 
statistically significant.
