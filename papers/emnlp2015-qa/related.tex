\section{Related Work}

% Selectional Preference Technique...

Selectional preference is the technique aiming at abstracting subject and object arguments to the most appropriate types for a certain relation.
For example, in the relation \textit{X is the mayor of Y}, the type \textit{politician} is the suitable type to the first argument, and \textit{city} is to the second.

There are different approaches in selectional preference.
% class based, similarity based, generate based
\textit{Class based} approach is first proposed by Resnik \newcite{resnik1996selectional}.
By mapping each relation argument to entities in a structural taxonomy (e.g. WordNet), class based approach
abstracts relation arguments to human readable types in the taxonomy. Therefore, the coherence of the taxonomy
is crucial to the output quality.

% Add: Deterministic fact: coherence of taxonomy
Besides, non-class based approaches have been proposed in recent years.
%similarity based (2 sents)
One branch is \textit{similarity based} \cite{erk2007simple}, which judges whether one argument is plausible for a relation or not.
By collecting all the known arguments fitting a relation in advance, this approach judges whether one argument
is suitable for the relation or not, by measuring the distributional similarity between the current one and all seen arguments.
% talk about weakness ?

%generative based (2 sents)
Another branch is \textit{generative based}, where relation arguments are abstracted by generative models.
In the work of \newcite{ritter2010latent}, SP is modeled in the framework of Latent Dirichlet Allocation \cite{blei2003latent},
where relation types are represented as latent variables in the topic level, which are learned from data.

% Summary, we need human readable. (1 sent)
Our work belongs to class based approach, since the advantage of class based approach
is to produce human readable abstraction of relation arguments.
while similarity based approach couldn't produce an type abstraction over arguments,
and generative based approach couldn't convert latent argument types into human readable classes.


% the fact that need human readable


%TODO: Compare with other approaches


%Other Works for extracting patterns/semantic representations from Information Extraction Data
%PATTY
%and Patty's references?


The {\tt ReVerb} OpenIE system \cite{fader2011identifying} identified a large number of
binary relation textual patterns from the Web using syntactic and lexical constraints, which make sure the relations to be either verbs or verb phrases.
%, and then extracted pairs of arguments for each
%relation phrase. Then a logistic regression classifier is used to compute a confidence score for each tuple. The large pattern %collections {\tt ReVerb} provided benefited many information retrieval related problems like Question Answering (QA) systems
%\cite{berant2013semantic,cai2013large}.
{\tt NELL} \cite{carlson2010toward} is another similar system, where the relations are from a closed class, but the names of arguments are open. Comparing to {\tt ReVerb}'s constraints \cite{fader2011identifying}, {\tt PATTY} \cite{nakashole2012patty} can learn arbitrary patterns. Besides, {\tt PATTY} focuses more on relation taxonomy than on arguments conceptualization, so it forms a {\tt WordNet}-style taxonomy of binary relations. %However, neither of them exploited deep syntactic analysis or clustering techniques.
\newcite{moro2013integrating} presents a semantic network called {\tt WiSeNet} which clustered synonymous relational phrases extracted from Wikipedia into relation synsets, %and assigned semantic classes to the arguments of these synsets using deep syntactic and semantic techniques.
and then disambiguates each relation instance to different meanings. Although {\tt WiseNet} also presents semantic type signatures for each relation synsets, the method used is trivial. %\XS{describe their way is simple and that's not their main focus.}
However, our objective is mainly focused on conceptualizing relations through computing selectional preference on both sides of arguments simultaneously.
%
%Selectional preference learning is a broadly applicable NLP task, which usually focus on word-to-class relations. Previous work like \newcite{ritter2010latent} proposed a topic model method {\tt LDA-SP} for selectional preference to compute the probability of a binary relation taking both sides of specific arguments. \newcite{seaghdha2010latent} also proposed a series of LDA-style models using several grammatical relations. These works leans a distribution over topics for each relation while grouping related arguments into these topics in the mean time. Other works performed selectional preference in different corpus \cite{agirre2002integrating,judea2012concept} like {\tt Wikipedia} and {\tt WordNet} also inspired our work.
%
%In our distinctive work, we try to give a ranking list of preferred argument types which are linked in {\tt Freebase} \cite{bollacker2008freebase} for each relation in {\tt ReVerb}.

% \KQ{Check "Semantic Parsing on Freebase from Question-Answer Pairs", one part mentioned typed relations} 