\subsection{Mining Cross-cultural Differences of Named Entity}
\label{sec:mcdne}
This task is to discover and quantify cross-cultural differences of concerns towards name entities. 
We first explain how we obtain the ground truth from human annotators, then present several baseline methods to this problem and finally 
show our experiment results in detail.

\subsubsection{Ground Truth}
\label{sec:mcdne_truth}
Harris~\shortcite{harris1954distributional} states that the meaning of 
words is evidenced by the contexts they occur with. 
Likewise, in this work, we assume that the cultural properties of an entity 
can be captured by the terms they co-occur with in large text corpus. 
Thus, for each named entity, we present four human annotators\footnote{All four annotators are native Chinese speakers but bilingual. Two of them lived in the US extensively.} with two lists of 20 most co-occurred words with 
the named entity, from Twitter and Weibo respectively. 
We select 700 named entities for annotators to label, which
are the most frequently mentioned both in Twitter and Weibo. 
Annotators are instructed to rate the relatedness between the 
two word lists with one of following labels: ``very different'', 
``different'', ``hard to say'',  ``similar'' and 
``very similar''.\footnote{Our annotators are educated with many 
selected examples and thus have shared understanding of the five-level 
labels. Ranking based annotation method demands annotators to look 
at 40+40 words for the two terms in two languages before a decision can
be made, which is more expensive and harder to administer in our opinion.}

We then map the labels to numerical scores from 1 to 5
and use the average scores from the annotators as the ground truth 
for score ranking and binary classification.
For the binary classification problem, 
an entity is considered culturally similar 
if the score is larger than 3.0, and culturally different otherwise.
The inter-annotator agreement is 0.672 by Cohen's kappa coefficient, 
suggesting substantial correlation, according to the Wikipedia entry
of Cohen's kappa.
%\BL{ (0.531 without hanyuan)}

\subsubsection{Baselines and Our Method}

We propose five baseline methods. The first three
\emph{distribution}-based, while the next two 
are \emph{transformation}-based. 
Distribution-based methods compare the lists of surrounding
English and Chinese terms, denoted as $L_E$ and $L_C$, 
by computing the cross-lingual relatedness between the two lists, 
though different baselines differ by the
selection of words and the way similarity is computed.
Transformation-based methods compute the vector representation 
in English and Chinese corpus respectively, and
then trains a transformation.
% of words, known $L_E$ and $L_C$. The differences of these methods are the selecting method of terms and %the computation method of two word lists.  ii) the second type of baseline methods are first obtain the %comparable vectorial representation of the English title and Chinese title of the given entity, and then just %calculate the similarity between two comparable vectors.

\textbf{Bilingual Lexicon Jaccard Similarity (BL-JS)}
%	The $L_E$ and $L_C$ of both BL-JS and WN-WUP  are the same as the lists that annotators judge.
BL-JS uses the bilingual lexicon to translate $L_E$  to a Chinese word list 
$L_E^*$ as a medium and then calculates the Jaccard Similarity between 
$L_E^*$ and $L_C$ as $J_{EC}$. Similarly, we can compute $J_{CE}$. 
Finally, we compute $\frac{J_{EC}+J_{CE}}{2}$ as the cross-cultural similarity 
of this given name entity.
 
	\textbf {WordNet Wu-Palmer Similarity (WN-WUP)} Instead of using 
the bilingual lexicon and Jaccard Similarity, WN-WUP uses Open Multilingual 
Wordnet~\cite{wang2013building,bond2013linking} to calculate the average 
similarity of two lists of words from different languages.
	
	\textbf {Word Embedding based Jaccard Similarity (EM-JS)} EM-JS is 
very similar to BL-JS, except that its $L_E$ and $L_C$ are generated by 
ranking the similarities between the name of entities and all English words 
and Chinese words respectively. 

	\textbf {Linear Transformation (LTrans)}
	We follow the steps in Mikolov et al.~\shortcite{Mikolov:2013tp} 
to train a transformation matrix between \textit{EnVec} and \textit{CnVec}, 
using 3000 translation pairs with confidence of 1.0 in the bilingual lexicon. 
Given a named entity, this solution simply calculates cosine similarity 
between the vector of its English name and the \textit{transformed} vector 
of its Chinese name. 
	
	\textbf {Bilingual Lexicon Space (BLex)}
	This baseline is similar to \textit{SocVec} but it does not 
utilize socio-linguistic vocabularies and simply uses the bilingual lexicon
as the BSL.

\textbf{{Our SocVec-based method}} Given a named entity with its English and 
Chinese name, we simply compute the similarity between their 
\textit{SocVec}s as its cross-cultural difference score. 

\subsubsection{Experimental Results}

For qualitative evaluation, \tabref{tab:mcdne_res_4} shows some of 
the most culturally different entities obtained by our method. 
The hot and trending topics on Twitter and Weibo are 
manually summarized to help explain the cultural difference. 
All listed entities have large divergence on concerns, 
thus reflecting cross-cultural differences.
\begin{table*}[th!]
	\footnotesize
	\centering
	\caption{{Selected culturally different named entities, with Twitter and Weibo's trending topics manually summarized}}
	\begin{tabular}{|L{1.5cm}|L{5cm}|L{8cm}|}
		\hline
		\textbf{Entity} & \textbf{Twitter topics} & \textbf{Weibo topics}
		\\ \hline
		Maldives & coup, president Nasheed quit, political crisis & holiday, travel, honeymoon, paradise, beach \\ \hline
		Nagoya & tour, concert, travel, attractive, Osaka & Mayor Takashi Kawamura, Nanjing Massacre, denial of history\\  \hline
%		Quebec & Conservative Party, Liberal Party, politicians, prime minister, power failure & travel, autumn, maples, study abroad, immigration, independence   \\ \hline
%		Philippines & gunman attack, police, quake, tsunami & South China Sea, sovereignty dispute, confrontation, protest  \\ \hline
		Yao Ming & NBA, Chinese, good player, Asian  & patriotism, collective values, Jeremy Lin, Liu Xiang, Chinese Law maker, gold medal superstar   \\ \hline
		University of Southern California & college football, baseball, Stanford, Alabama, win, lose & top study abroad destination, Chinese student murdered, scholars, economics, Sino American politics \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_4}
\end{table*}

\begin{table}[th]
	\small
	\centering
	\caption{{Comparison of Different Methods}}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Method} & \textbf{Spearman} & \textbf{Pearson}  & \textbf{MAP} \\ \hline\hline
		BL-JS& 0.276 & 0.265 & 0.644   \\ \hline
		WN-WUP  & 0.335 & 0.349 & 0.677 \\ \hline
		EM-JS & 0.221 & 0.210  & 0.571\\ \hline
		LTrans& 0.366 & 0.385  & 0.644  \\ \hline
		BLex& 0.596 & 0.595  & 0.765 \\ \hline\hline
		SocVec:opn& 0.668 & 0.662   & \textbf{0.834} \\ \hline
		SocVec:all& \textbf{0.676} & \textbf{0.671}  & \textbf{0.834}\\ \hline
	\end{tabular}
	\label{tab:mcdne_res_1}
\end{table}
\begin{table}[th]
	\centering
	\small
	\caption{{Evaluation of Different Similarity Functions}}
	\label{tab:mcdne_res_2}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Similarity} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline\hline
		PCorr. & 0.631 & 0.625 & 0.806\\ \hline
		L1 + M & 0.666 & 0.656 & 0.824 \\  \hline
		Cos & \textbf{0.676} & 0.669 & \textbf{0.834} \\ \hline
		L2 + E & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
\end{table}

\begin{table}[th]
	\centering
	\small
	\caption{{Evaluation of Different Pseudo-word Generators}}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Generator} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline \hline
		Max. & 0.413 & 0.401 & 0.726\\ \hline
		Avg. & 0.667 & 0.625 & 0.831\\ \hline
		W.Avg. & 0.671 & 0.660 & 0.832 \\  \hline
		Top & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_3}
\end{table}

In~\tabref{tab:mcdne_res_1}, we evaluate the baseline methods and 
our approach with three metrics: Spearman and 
Pearson correlation on the ranking problem, and Mean Average Precision (MAP)
on the classification problem (see \secref{sec:mcdne_truth}). The \textit{BSL} of \textit{SocVec:opn} uses only OpinionFinder as English socio-linguistic vocabulary, while \textit{SocVec:all} uses the union of Emapth and OpinionFinder vocabularies.\footnote{
Having tuned the  parameters, we use the best parameters for the \textit{SocVec:opn} method and \textit{SocVec:all} method: 
monolingual word vectors are trained with 5-word context window and 150 dimensions;
choosing cosine similarity as the \textit{sim} function to compute the similarity within the \textit{\socvec}~space;
using ``\textit{Top}'' pseudo-word generator
(see \secref{sec:model}).
} Results show that \textit{SocVec} models perform the best and 
using the union of vocabularies is better.

We also evaluate the effect of four different similarity options in 
\textit{\socvec}, namely, Pearson Correlation Coefficient 
(\textit{PCorr}.), L1-normalized Manhattan distance (\textit{L1+M}), 
Cosine Similarity (\textit{Cos}) and  L2-normalized Euclidean distance (\textit{L2+E}).
%It is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking.
From~\tabref{tab:mcdne_res_2}, we conclude that among these four options, \textit{Cos} and \textit{L2+E} perform the best. 
%Although it is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking, we can find that 
\tabref{tab:mcdne_res_3} shows effect of using four different 
pseudo-word generator functions (see~\secref{sec:pg}), from which we can infer that ``\textit{Top}'' generator function performs best for 
it reduces the noise brought by the less possible translation pairs. 

