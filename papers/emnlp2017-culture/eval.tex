\section{Evaluation}
\label{sec:eval}
In this section, we evaluate our framework on two tasks requiring cross-cultural differences/similarities computation: i) mining cross-cultural differences in named
entities, ii) bilingual lexicon induction for slang terms. Next subsections first discuss some
preliminary setup, then present our experiments for the two tasks.

\subsection{Experiment Setup}
\label{sec:prelim}
Prior to evaluating the two tasks, we need to first preprocess the Twitter and 
Weibo corpus, perform entity linking, then train word embedding for all terms including
entity names, and finally construct the bilingual lexicon for common words and
socio-linguistic vocabularies, which contain opinion and sentiment related words. 

\subsubsection{Microblog Corpora}
%As the name of our model suggests, microblog corpora are extensively used for both our model and the two tasks (MCDNE and BLEIS).
%One highlight about this dataset is that it contains not only normal microblogs, but also those deleted or censored by force of authority. It enables our model to better look into the true and thorough opinions from netizens in China unhindered by censorship.
The English Twitter corpus is from Archive Team's Twitter stream 
grab\footnote{{\url{https://archive.org/details/twitterstream}}}.
The Sina Weibo corpus comes from Open Weiboscope 
Data Access\footnote{{\url{http://weiboscope.jmsc.hku.hk/datazip/}}}~\cite{fu2013assessing}.
Both corpora cover the whole year of 2012. 
We then downsample each corpus to 100 million messages, where each message contains at least 10 characters, normalize the tweets~\cite{han2012automatically} to reduce the noise, lemmatize the text~\cite{manning2014stanford} and use LTP~\cite{che2010ltp} to do Chinese word segmentation.

\subsubsection{Entity Linking and Monolingual Word Vectors}
After preprocessing the corpora, we first do entity linking.
%link all named entities to the corresponding concepts in Wikipedia. 
For the Twitter corpus, we use Wikifier~\cite{cheng2013relational,ratinov2011local}, a widely used
entity linker. 
Because no suitable Chinese entity linking tool is available, 
we implement our own tool that is optimized for high precision. 
This tool prefers to link to an entity with a surface form that appears
more frequently in our corpus. 
%We utilize context information for selecting entity candidates. 
%That is, only when a surface form is an exact match, or the candidate has been 
%linked in the surrounding text before, or satisfies the occurrence 
%frequency criterion, will it be linked by our tool. Also, we only focus on entities that have both English and Chinese 
%Wikipedia pages.
%We argue that such precision oriented approach is sufficiently good for our tasks, 
%because even if an entity is not recognized, it can still be captured as a normal
%word and contribute to the semantics of other terms.
%Due to the lack of existing state-of-the-art tools for Chinese entity linking and the fact that other tools pursue the balance of precision and recall and tend to generate too many links for our task requiring high precision. Thus we devised our own entity linking method suitable for this particular task.
%
%Basically, we obtain the set of possible surface forms for an entity by collecting all anchor texts of the entity in the
%Wikipedia corpus. In addition, we leverage a redirect system
%of Wikipedia and merge all entities that redirect to each other
%as one entity.
%We can also compute anchor-entity linking frequency
%from the Wikipedia corpus.
%Our entity linking algorithm starts by looking for potential
%anchors in the plain text corpus. 
%We adopt a longest match strategy here that prefers longer anchors. This is because we assume longer anchors are more reliable and bring about higher precision. 
%In Entity linking, we aim for high precision rather than recall because even if an entity is not recognized in the text, its constituent words will still be captured later in the ordinary word vector space and contribute to the semantics of other entities or words.
%After this process we turned our original bilingual corpora into annotated ones with all the Named Entities linked to a shared common entity identifier, the Wikipedia title with both English and Chinese page for a given entity. This bridges an inter-language link for named entities that we would like to mine cultural differences on afterwards.
%
Next we use Word2Vec~\cite{Mikolov2013distributed} to train English and Chinese word embedding on Twitter and Weibo corpus respectively.

\subsubsection{Bilingual Lexicon}
\label{sec:blpre}
Our biligual lexicon is Microsoft Translator 
\footnote{{\url{http://www.bing.com/translator/api/Dictionary/Lookup?from=en&to=zh-CHS&text=<input_word>}}}, which translate an English word into multiple
Chinese words, where each English-Chinese translation pair is associated with
%which contains 22,082 English and 37,645 Chinese words.
a confidence score ranging from 0 to 1. All named entities and slang terms used
in the following experiments are excluded from this bilingual lexicon.


%between Twitter/Weibo frequent words and the  
%In order to bridge the two cross-lingual word vector spaces for using cooperatively, we require a bilingual lexicon that maps the words between two languages.
%Our bilingual lexicon is built in an one-way manner, i.e, from English words to translated Chinese words. 
%We first derive the intersection of the word set from our Twitter corpus with a publicly available word count list\footnote{http://norvig.com/ngrams/count\_1w100k.txt} with 100,000 most popular words, which is our English Tweets common word list with over 20,000 words. 
%Using Bing Translate API\footnote{http://www.bing.com/translator}, we translate those common words into Chinese. As each English word can be translated into multiple Chinese
%words, and each Chinese word can be translated into multiple English words, with translation confidence ranging from 0 - 1 and sums to 1, this phase generates a many-to-many translation mapping, or bilingual lexicon.

\subsubsection{Socio-linguistic Vocabulary}
%As stated in previous sections, in order to capture the sociolinguistic properties and features in social media texts, we need another set of lexicons which represents the meanings that are crucial to our goal and tasks.
\label{sec:sv}
Our socio-linguistic vocabularies come from 
Empath~\cite{fast2016empath} and OpinionFinder~\cite{choi2005identifying} 
for English, and TextMind~\cite{gao2013developing} for Chinese.
Empath is similar to LIWC~\cite{pennebaker2001linguistic},
but with more words and more categories. 
%but agrees with LIWC on many categories they share. 
%It's based on a combination of word embedding models, 
%knowledge bases and crowdsourcing, wherein 
We manually select 91 categories of words that are 
relevant to human perception and psychological processes. 
OpinionFinder consists of words relevant to opinion and sentiment. 
TextMind is a Chinese counterpart for Empath.
%, which is similar to LIWC for analysis the preferences and degrees of different categories in text with an emphasize on Chinese characteristics.
%All of these three vocabularies are used on a combinatorial basis 
%acting as a parameter to our \textit{SocVec} model.
%
In summary, we obtain 3343 words from the Empath vocabulary,  
3861 words from OpinionFinder vocabulary, 
whose union contains 5574 unique words. 

\input{MCDNE}

\input{BLEIS}
