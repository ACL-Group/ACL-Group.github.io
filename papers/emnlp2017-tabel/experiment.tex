\section{Experiments}
\label{sec:eval}



In this sec, eval the accuracy of our table linking sys on the cross-lingual task.
We evluate ours and other state-of-the-arts, and perform ablation study to investigate
the importance of each kind of feature in the whole task.

\subsection{Experimental Setup}

\textbf{Datasets.} 
Chinese web table from the the previous work by XXXX \shortcite{}.
Size: consists of xx cell mentions with labeled ch-wiki concept from xx tables.
By looking up inter-lang link from ch to En, we collect xx cells with labeled En concepts
(x.xx cells per table).
We split the dataset into training / validation / testing sets (xx : yy : zz).

\noindent
\textbf{Evaluation metric.}
We follow state-of-the-arts on table linking and use accuracy as our evaluation metric.
In the following experiments, we link each cell mention to the English concept with highest score, and the accuracy is defined as the fraction of cell mentions that correctly link to the labeled concept.

\noindent
\textbf{State-of-the-art comparisons.}
Two soats: TabEL and XXX.
Both works focus on table linking in the monolingual scenario (one is En, one is Ch).
For a fair comparison in our bilingual task, we bridge the language gap as follows.
For TabEL, we applied the translation procedure (sec xx), converting each cell mention into
the most likely English surface form, and then we run TabEL on these translated English
tables and produce the final linking results.
For XXX, we map each predict Chinese concept to their corresponding English concepts by looking up the inter-lang-link.
Since inter-lang-link is directly applied in the predicting step, 
while our system doesn't assume such parallel information,
we regrad this specification as an oracle baseline.






\subsection{Cand Gen Eval}
\label{sec:cand-gen-eval}

%In this part, we evaluate the quality of candidate entity generation.

In this part, we investigate the translated English mentions from Chinese table inputs.
As described in sec xxx, English mentions is derived from multiple resources.
Compare with different combination of resources, we evaluate the quality by
measuring the proportion of cells that the correct entity appears in the top-$n$ candidates
(Hits@$n$).

From the results in \tabref{tab:cand-gen-quality}, we observe that
ensembling multiple translation resources is able to discover more correct entities 
without bringing too much noisy candidates.
Besides, the English mentions generated by Pinyin is a strong complementary to
those generated from pure translation methods.

\begin{table}[ht]
    \label{tab:cand-gen-quality}
    \centering
    \caption{Hits@$n$ results on candidate entity generation}
    \begin{tabular} {c|ccc}
        Resources   &   n=1     &   n=5     &   n=10    \\
        \hline
        Google      &   0.xxx   &   0.yyy   &   0.zzz   \\
        Baidu       &   &   &   \\
        Tencent     &   &   &   \\
        All Trans.  &   &   &   \\
        \hline
        Pinyin      &   &   &   \\
        \hline
        Trans. + Pinyin    &   &   &   \\
    \end{tabular}
\end{table}


\subsection{Main Results}

We show the experimental results in \tabref{tab:main-result}.
Our model outperforms previous work and improves the acurrency by a relative gain of xx\%.
For TabEL, the linking accuracy is limited by the quality of translated English table.
By manually inspecting the translated English mentions, the translation accuracy is around xx\%,
which is close to our quality of English candidate generation in \secref{sec:cand-gen-eval}.


\begin{table}[ht]
    \label{tab:main-result}
    \centering
    \caption{Accuracy results on Chinese tables}
    \begin{tabular} {c|cc}
        Dataset     &   Macro Acc.  & Micro Acc.    \\
        \hline
        TabEL       &   0.xxx       & 0.yyy         \\
        XXX-cos     &   0.xxx       & 0.yyy         \\
        XXX-oracle  &   0.xxx       & 0.yyy         \\
        \hline
        Ours-PY     &   \textbf{0.xxx}       & \textbf{0.yyy}         \\
    \end{tabular}
\end{table}


\subsection{Ablation Study}

We performed an ablation study on evaluate the effectiveness of the surface,
context and coherence feature use in our system.
\tabref{tab:ablation} shows the result of different feature combinations.
All features in our model make a positive effect on the final accuracy.
The surface feature is most important in the model since it encodes
the most direct information between the cell mention and the target entity.
And it also shows that our coherence feature is able to extract the latent correlation
between the target concepts, even though no explicit type or category information
is attached to each candidate entity.

\begin{table}[ht]
    \label{tab:ablation}
    \centering
    \caption{Ablation study between different feature combinations.}
    \begin{tabular} {c|c|c}
        Feature Combination &   Micro Acc.  & Decrease (\%) \\
        \hline
        Surface Only        &   0.xxx       & 0.yyy \\
        Context Only        &       &       \\
        Coherence Only      &       &       \\
        All-Surface         &       &       \\
        All-Context         &       &       \\
        All-Coherence       &       &       \\
        \hline
        All                 &       &       \\
    \end{tabular}
\end{table}
