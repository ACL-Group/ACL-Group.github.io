\section{Related Work}

%Previous work  \cite{Aho:72}

Entity linking has been a popular topic in NLP for a long time as it is the basic step for machines to understand natural language and an important procedure of many complex NLP applications such as information retrieval and question answering. Entity linking requires a knowledge base to which entity mentions can be linked, the most popular ones including Freebase~\cite{}, YAGO~\cite{} and Wikipedia~\cite{}, where each Wikipedia article is considered an entity. Wikipedia was first explored by \cite{bunescu2006using}, where an SVM kernel is used to compare the lexical context of an entity mention to each candidate's Wikipedia page. Since each entity mention needs to train its own SVM model, the experiment was limited. Later, \cite{mihalcea2007wikfy} proposed a system called Wikify! for the Wikification task. Their system consists of two parts: keyword extraction, where candidates are detected according to a vocabulary and then ranked by some statistical measures; word sense disambiguation, where detected candidates are linked to a Wikipedia page by 1) compare the mention's lexical context to content of disambiguation page 2) train a Naive Bayes classifier for each ambiguous mention. Later approaches make use of the observation that entity disambiguation in the same document should be related. \cite{cucerzan2007largescale} maximizes the agreement between the context data stored for each candidate entity and the contextual information in the document, and also the agreement among the category tags of the candidate entities. \cite{milne2008learning} took a similar approach but relies on unambiguous terms in the context. \cite{ratinov11local} formalized this task into a bipartite graph matching problem and proposed a score function which considers both local similarity and global coherence. \cite{yang15smart} proposed a tree-based structured learning framework, S-MART, which is particular suitable for short texts such as tweets. Due to its fundamental role in many applications, the task of entity linking has attracted a lot of attention, and many shared tasks have been proposed to promote this study.~\cite{}

The interest in web tables was inspired by \cite{cafarella2008webtables}. They found that there are about 154 million tables on the Web that could be used as a source of high-quality relational data, and implemented several applications such as schema auto-complete and attribute synonym finding. \cite{syed2010exploiting} proposed approaches to infer a partial semantic model of Web tables automatically with Wikitology~\cite{}.  \cite{limaye2010annotating} proposed a framework to annotate table cells with entity, type and relation information simultaneously using a graphical model. \cite{bhagavatula2015tabel} focused solely on table entity linking. They argued that models which jointly address entity linking, column type identification and relation extraction rely on the correctness and completeness of KB, thus may be adverse for the performance of entity linking. They also exploited a graphical model, where cells in the same row or column are connected. They train a model to rank the candidates of a given cell by its context, i.e. cells connected to the given cell, and the predicted entity for each cell are iteratively updated until convergence. Their work is the previous state-of-the-art for table entity linking.
\cite{wu2016entity} exploited multiple knowledge bases to enhance the performance of table entity linking by using ``sameAs'' relationship between entities from different KBs. Their approach proved to be useful for languages without a comprehensive knowledge base.