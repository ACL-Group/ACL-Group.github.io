\subsubsection{Question Representation}
\label{sec:qw-repr}

%Before encoding the question,
%%As a preprocess step, 
%we replace all entity mentions in the current query by a dummy token $\langle E \rangle$,
%and all time mentions by $\langle Tm \rangle$.
%The running example will be transformed into ``What is the second longest river in $\langle E \rangle$ ?''
%given the candidate query in \figref{xxx}.
%Therefore, two questions share the same semantic representation,
%if they have the same structure, but only differ in focus entities or times.


%We aim at finding the association between the question and different semantic components.
%Now we encode the question into vector representation.
%Given a candidate query structure with multiple focus nodes,
%our goal is to find the association between the question and different semantic aspects.
We encode the question in both global and local level,
which captures the semantic information with respect to each component $p$.

The global information takes the token sequence as the input.
We use the same word embedding matrix $E_w$ to convert the token sequence into vectors
$\{\bi{q}_1^{(w)}, \dots, \bi{q}_n^{(w)}\}$.
Then we encode the token sequence by applying bidirectional GRU network~\cite{cho2014properties}.
%\textbf{Recurrent-Words}: We use GRU~\cite{xx} as the recurrent cell.
%The sequence of word vectors are fed into a bidirectional GRU layer,
The representation of the token sequence is the concatenation
of the last forward and backward hidden states through the BiGRU layer,
$\bi{q}^{(tok)} = [\overleftarrow{\bi{h}}_1^{(w)};\overrightarrow{\bi{h}}_n^{(w)}]$.

%Then we feed the word embeddings into the bidirectional GRU layer
%for capturing long-distance dependencies of the sentence.
%%TODO: positional embedding
%Similarly, we concatenate the last forward and backward state to produce the embedding representation
%at token level $\bi{q}_p^{(tok)}$.
%%at token level: $\bi{q}^{(tok)} = [\overrightarrow{\bi{h}}_n^{(w)};\overleftarrow{\bi{h}}_1^{(w)}]$

To encode the question at local level,
%we look for relevant semantic clues with respect to the particular semantic component $p$.
%That is, given a path, try to find salient evidence sequence, and remove irrelevant words.
%discovering   the answer 
%For example in \figref{fig:nn}, the semantic meaning of \textit{contained\_by}
%is aligned to the sub-question ``What is in $\langle E \rangle$''.
%To this end,
we leverage dependency parsing to represent
long-range dependencies between the answer and the focus node in $p$.
%For this goal, we use dependency parsing as the source to guide us find the meaningful words through syntactic evidences.
Since the answer is denoted by the wh- word in the question,
we extract the dependency path from the answer node to the focus mention in the question.
%The path is unique since the dependency parsing result is a tree.
Similar with \citet{xu2016question},
we treat the path as the concatenation of words and dependency labels with directions.
For example, the dependency path between ``what'' and ``United States'' is
\{what, $\overrightarrow{nsubj}$, is, $\overrightarrow{prep}$, in, $\overrightarrow{pobj}$, $\langle E \rangle$\}.
%Given the parsing tree of the question,
%we extract the dependency path from the answer node (wh- words in the question) to the focus mention of the semantic aspect.
%For the first aspect in \figref{xxx}, the dependency path between ``what'' and ``China''
%is \{what, \textit{xxx-1}, is , \textit{yyy}, in \textit{zzz}, $\langle E \rangle$\},
%where focus word ``China'' is also replaced by $\langle E \rangle$.
%The path is unique since the parsing result is a tree, and the path is a combination
%of both related tokens and dependency arcs.
%We use \textit{xxx-1} for representing the reverse direction of the arc \textit{xxx}.
We apply another bidirectional GRU layer to produce the vector representation at dependency level
$\bi{q}_p^{(dep)}$, capturing both syntactic features and local semantic features.
%TODO: draw a figure for showing the two GRUs, one for sentential and one for syntactical.
%Fianlly, as illustrated in \figref{xxx}
%With the guide of semantic component $p$,
Finally we combine global and local representation by element-wise addition,
returning the representation of the question with respect to the semantic component, 
$\bi{q}_p = \bi{q}^{(tok)} + \bi{q}_p^{(dep)}$.
%TODO: try FC here???????




%Before the step of cross-attention, we encode the input questions as the distributional
%representation of each word in it.
%Specifically, the input question $q$ is expressed as the word sequence $q=(w_1, w_2, \dots, w_n)$,
%where $w_i$ denotes the $i$-th word.
%%TODO: trick of E and Tm
%We first use a word embedding matrix $E_w \in \mathbb{R}^{|V_w| \times d}$ to convert 
%the original sequence into word embeddings $\bi{q}=(\bi{w}_1, \bi{w}_2, \dots, \bi{w}_n)$,
%where $|V_w|$ denotes the vocablary size of natural language words,
%and $d$ denotes the embedding dimenson.
%The word embedding matrix is initialized by publicly available pre-trained results, 
%such as Word2vec~\cite{mikolov2013} and GloVe~\cite{xxx}. 

%In the next step, we feed the word embeddings into a bidirectional Gated Recurrent Unit (GRU)~\cite{xxx} networks.
%As a brief introduction, GRU is capable of effectively maintaining the long-distance dependency in many NLP tasks.
%Given $\bi{x}_t$ as the input of time step $t$ of RNN, and $\bi{h}_{t-1}$ as the hidden state at time stamp $t-1$,
%GRU calculates the current hidden state $\bi{h}_t$ through gated units, described in \eqnref{eqn:gru}:
%
%\begin{equation}
%  \label{eqn:gru}
%  \begin{aligned}
%    & \bi{r}_t & = & \sigma(\bi{W}_r\bi{x}_t+\bi{U}_r\bi{h}_{t-1}), \\
%    & \bi{z}_t & = & \sigma(\bi{W}_z\bi{x}_t+\bi{U}_z\bi{h}_{t-1}), \\
%    & \tilde{\bi{h}}_t & = &\mbox{tanh}(\bi{W}_h\bi{x}_t+\bi{U}_h(\bi{r}_t\cdot\bi{h}_{t-1})), \\
%    & \bi{h}_t & = & (1-\bi{z}_t)\cdot\bi{h}_{t-1}+\bi{z}_t\cdot\tilde{\bi{h}}_t. \\
%  \end{aligned}
%\end{equation}
%
%\noindent
%In the case of GRU, the vector $\bi{r}_t$ is the output of the \textit{reset gate},
%determining how much information of the last state $\bi{h}_{t-1}$ is ignored
%in the computation of the candidate state $\tilde{\bi{h}}_t$.
%The vector $\bi{z}_t$ is the output of the \textit{update gate}, which controls the interpolation 
%between the last state $\bi{h}_{t-1}$ and the candidate state $\tilde{\bi{h}}_t$.
%
%For encoding the input question, we employ the bidirectional GRU network, which consists a
%forward network and a backward network encoding in the reverse order.
%Taking the word embedding sequence $(\bi{w}_1, \dots, \bi{w}_n)$ as input,
%we get the forward hidden sequence
%$(\overrightarrow{\bi{h}_1}, \overrightarrow{\bi{h}_2}, \dots, \overrightarrow{\bi{h}_n})$ 
%as well as the backward one
%$(\overleftarrow{\bi{h}_1}, \overleftarrow{\bi{h}_2}, \dots, \overleftarrow{\bi{h}_n})$.
%We concatnate the forward hidden state of each word with corresponding backward hidden state,
%resulting in the distributional representation $\bi{h}^{(w)}_i = [\overrightarrow{\bi{h}_i};\overleftarrow{\bi{h}_i}]$.
%Thus, we obtain the representation of each word in the question, and each hidden state
%encodes the information from both before and after the corresponding word.


