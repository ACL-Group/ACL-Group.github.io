Question-1:
    The answer node ("A" in Figure 1&2) is the variable vertex representing
    real answer entities in the graph. Other focus nodes constrain the answer node
    via the predicate sequences among them.
    
Question-2:
    Please refer to Yahya et al. 2012 article
    "Natural Language Questions for the Web of Data", mainly Section 3.
    
Question-3:
    If by "query component" you mean "semantic component", then please refer to Section 2.2,
    the semantic components is defined as the path starting from the answer node to one focus node. 
    Hence one graph could be split into multiple components.



Question-1:
    We assume by "entity id" you meant "Id repr." in Table 4.
    We clarify that the id here doesn't identify the entities but the predicates (line 609).
    The improvement is not that large, because predicate id features are largely covered by
    their name features. We observed that for each question, among all predicates which 
    occurred in some candidate graph, ~75% of them have unique names (aliases).
    Therefore, the additional id repr. feature doesn't bring much extra information.
    
Question-2:
    Refer to line 369, the path vector p equals to p_w (or p_id),
    if id (or word) representation is set to None.
    
Question-3.1:
    The pooling outcome may lead to worse end-to-end result when there are too many entities 
    in one graph, because the pooling layer takes too many vectors as input,
    different semantic features between similar query graphs become indistinguishable
    (large values on most dimensions of the pooling output), hence hard to find the correct graph.
    In our task, only <0.5% of candidate graphs have more than 3 semantic components, 
    so pooling is a reasonable way to aggregate difference semantic components.
    
Question-3.2:
    It's easier to answer questions with fewer candidate graphs, mainly due to fewer
    negative graphs. The Spearman coefficient on CompQ is -0.175 (fewer graphs, higher F1).



Candidate Generation Issues:
    we're aware that candidate generation (CG) is not the main focus of our work, but as a necessary 
    part of the approach, the detailed steps are needed to enable the understanding by readers.
    
  Weakness-2:
    The first 3 steps of CG is similar to Bao.
    The main differences come from the type and time constraint generation.
    Improvements on type constraints aim at efficiency, reducing ~70% meaningless SPARQL queries.
    Improvements on time constraints are more important,
    where it supports matching a time (from question) with time intervals (from Freebase),
    while Bao only supports the comparison between time points.
    We define a 5-step approach because of the information dependency between neighbor steps:
    Step 4 needs all outgoing predicates of the answer from Step 3;
    Step 5 needs explicit type constraints from Step 4.
    
  Weakness-1:
    Since our improvements on CG concerns only time-related questions, 
    the performance gain doesn't largely come from CG.
    Removing these improvements, the F1 results on CompQ slightly drops:
    41.56 to 41.22 on baseline, 42.84 to 42.37 on full model.
    
  Weakness-4:
    The semantic matching model is easy to reproduce. Due to space limit, we didn't discuss
    too much on CG improvements. We will include more implementation details, the above 
    ablation tests and the code URL in camera-ready.


Question-1:
    Please refer to line 143. The "light-weighted" advantage comes from the NN-based semantic 
    matching model. It has fewer parameters and a simpler structure compared to Yu et al. 
    and Qu et al., thus is easier to tune, but remains effective.
    
Reference-1:
    The intuitive solution (line 114) is our baseline approach used in Table 5.
    It's similar to Bao's method, but not exactly the same.
    We will consider adding a reference there.
    
Typo-2:
    Thanks for pointing it out. Yes it's agnostic and we will mention this flexibility in
    the revised version. 
    
