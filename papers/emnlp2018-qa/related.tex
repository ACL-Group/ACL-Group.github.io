\section{Related Work}

% \KQ{
% be aware of new papers: \citet{cui2017kbqa},  \citet{hu2018answering},  \citet{abujabal2017automated}. 
% }


% \KQ{
% Part 1: QA system in categories: SP,  IR. 
% For the ``other'' papers like \citet{jain2016question},  \cite{cui2017kbqa}. 
% Brief talk about their main techniques,  in one sentence. 
% Our paper belongs to SP branch. 
% }

Knowledge Base Question Answering(KBQA) has been a hot research top in recent years.  Generally speaking,  the most popular methods for KBQA can be mainly divided into two classes: information retrieval and semantic parsing. 
%IR

Information retrieval based system tries to obtain target answer directly from question information and KB knowledge without explicit considering interior query structure.
There are various methods \cite{yao2014information,bordes2015large,dong2015question,xu2016question} to select candidate answers and to rank results.  
 % treat a KB as a graph connecting different topics and extract the entities in the problem and obtain the knowledge base subgraph centered on those entities.  Each node or edge in the subgraph can be used as a candidate answer,  and the problem vector is extracted by observing the problem according to certain rules or templates.  The final answer can be selected from candidate answers with the help of problem vector.  IR method has the advantage of easy training and fast speed.  
 % However,  due to the lack of semantic encoding,  features in IR methods are hard to interpret and solve complex questions. 

%Semantic parsing
Semantic parsing based approach focuses on constructing a semantic parsing tree or equivalent query structure that represents the semantic meaning of the question.  In terms of logical representation of natural language questions,  many methods have been tried,  such as query graph~\cite{yih2014semantic,yih2015semantic} or RDF query language~\cite{unger2012template,cui2017kbqa,hu2018answering}.  

% For example,  (Yih et al. ,  2014~\cite{yih2014semantic}; Golub et al. ,  2016~\cite{golub2016character}; Yu et al. ,  2017~\cite{yu2017improved}) are based on Freebase. 

% \KQ{Cannot quite get the point of this paragraph. }
% One drawback of traditional semantic parsing method to sovle KBQA problem is that it needs to generate queries based on large amount of entities and predicates in the KB,  a large proportion of which are unseen during the training process.  Therefore,  some research has focused on solving this issue.  One solution is embedding based method,  which represents entities and relations in vector space and predict soundness of candidate triplets from these latent vectors,  such as  
% TransE(Bordes et al. ,  2013)~\cite{bordes2013translating},  TransH~\cite{wang2014knowledge},  TransR~\cite{lin2015learning} and other enhanced but similar models.  The general idea behind these models is training embedding vectors of entities and relations by the formula $h+r\simeq t$.  Besides,  recent work has also focus on character-level model to enhance the performance,  such as (Golub et al. ,  2016).  Golub uses character-level modeling to handle traditional out-of-vocabulary(OOV) problem.  Furthermore,  there are some research on combination of knowledge base and open vocabulary to improve semantic parsing performance.  For example,  (Gardner et al. ,  2017)~\cite{gardner2017open} try to leverage the information contained in both a formal KB and a large corpus to avoid the limit of the schema of the underlying KB.  \citet{qu2018question} goes even further.  It use RNN to capture semantic-level information in question and use an attention mechanism to keep track of the entities and relations in the same time.  However,  most of these approaches can only deal with single-relation questions,  having no ability to deal with ordinal questions and multi-relation questions,  such as ComplexQuestions.  

% \KQ{
% Part 2: NN techniques in QA system. 
% Could briefly talk about KB embedding based approaches,  check \citet{hao2017end} for more information. 
% Many works used NN techniques,  including Xu,  Yih,  Bao,  and lots of SimpQ papers. 
% Try talk about multiple papers in one or two sentences. 
% Bao is most similar to us,  could talk more,  but not so much. 
% Our main difference: encode multiple relations (paths) into a uniform query structure representation (semantic composition).  
% }

Recently, as the development of deep learning,
NN-based approaches have been combined into the KBQA task \cite{bordes2014open}, showing promising result.
These approaches tries to use neural network models to encode both questions and answers (or query structures) into the vector space.
Subsequently, similarity functions are used to select the most appropriate query structure to generate the final answer.
%In NN-based approaches,  different methods for the crucial step, learning representations of questions and answers(or query structures), have been tried,  especially for simple questions.
For example,
\citet{bordes2014open}  focuses on embedding the subgraph of the candidate answer;
\citet{yin2016simple}  uses character-level CNN and word-level CNN to match different information;
\citet{yu2017improved} introduces the method of hierarchical residual RNN to compare questions and relation names; 
\citet{qu2018question} proposes the AR-SMCNN model, which uses RNN to capture semantic-level correlation and employs CNN to extract literal-level words interaction. 

 %  \citet{dong2015question}) takes the context and the type of the answer into account

Belonging to NN-based semantic parsing category, 
our approach employs a novel encoding structure method to solve complex questions.
Previous works such as \citet{yih2015semantic} and \citet{bao2016constraint}
require a recognition of a main relation and regard other constraints as variables added to this main relation.
Unlike their approaches, our method encodes multiple relations (paths) into a uniform query structure representation (semantic composition),  which allows more flexible query structures. 
% To handle multi-relation questions,  recent research has focused on NN-based semantic parsing method.  This approach tries to use RNN or CNN network models to encode both questions and query structures into vector space.  Subsequently,  similarity functions,  such as cosine similarity or dot product,  can be used to select the most appropriate query structure to generate final answer.  For example,  the MulCG (Bao et al. ,  2016)~\cite{bao2016constraint} requires a recognition of a core chain and regards other constraints as variables added to this main relation.  After,  the MulCG uses Siamese CNN to calculate the similarity of question embedding and schema embedding to rank candidate query structures.  The latent problem is (1) six patterns are not adequate to support all kinds of multi-relational questions,  (2) answering all sub-questions separately can be redundant and (3) they can't solve ordinal questions until the final answer refinement step.  However,  this final step adds an additional resource wikipedia to select one answer from possible candidate answers,  which is not only slow but easy to mismatch.  Compared to the MCCNN,  our model avoids redundant work to answer sub-questions successively by fully representing the multi-relational question in a uniform way.  Besides,  our schema treats the ordinal constraints and other constraints equally thus don't need extra work like the refinement step. 

% \KQ{
% Part 3: Other related papers from solving complex questions,  mainly non-NN based. 
% Checkout these papers: \citet{reddy2016transforming},  \citet{hu2018answering},  \citet{abujabal2017automated}. 
% All of them used dependency information,  which is similar with ous. 
% our difference: dependency as feature,  not decide the shape of query structures, 
% which allows more flexible query structures. 
% }

There are also some works can't be simply classified in to IR based methods or SP based methods.   \citet{jain2016question}  introduces Factual Memory Network,  which tries to encode KB and questions in same word vector space,  extract a subset of initial candidate facts,  then try to employ multi-hop reasoning and refinement to find a path to answer entity.   \citet{reddy2016transforming},  \citet{abujabal2017automated}, and \citet{cui2017kbqa} try to interpret question intention by templates,  which learned from KB or QA corpora.   \citet{talmor2018web}  attempts to answering complex questions by decomposing them into a sequence of simple questions. 

% However,  due to the limit of data and the complexity of knowledge graph,  the results of these methods are not very competitive.  
%  

 % However,  it seems inflexible in the MulCG that this model manually classifies constraints into 6 types and needs to determine a main path. 

 % requires a recognition of a core chain (Yih et al. ,  2015~\cite{yih2015semantic}; Bao et al. ,  2016~\cite{bao2016constraint}; Yu et al. ,  2017) and  they regard other constraints as variables added to this main relation.  The MulCG (Bao et al. ,  2016) handles multiple constraints systematically and expands non-entity constraints which develops the staged graph (Yih et al. ,  2015).  However,  they all treats constraint a variable added to a node of main relation,  while our model doesn't have to detect the main relation and treats constraints and relations parallel.  Moreover,  our model is word-based,  in other words,  we don't need KB information to generate our schema.  Our model represents every path a word sequence ended with the final answer entity and merges them together to form an overall schema.  The word sequence path only takes word features into account thus concise and uniform thus prevent an overfit problem. 

% In terms of schema generation,  the MulCG (Bao et al. ,  2016) is not flexible enough by mapping explicitly the exact word proof to detect and match constraints,  while our model applies the attention mechanism which can collect other textual evidences to help detecting constraints. 

% For the model and the ranking step,  the MulCG (Bao et al. ,  2016) embedss question and schema together in feature-based then uses learning to rank,  which requires feature engineering thus not flexible.  And our model embeds respectively question(word-based RNN) and schema then trains with neural networks to get a score. 

% The MCCNN (Dong et al. ,  2015~\cite{dong2015question};Xu et al. ,  2016~\cite{xu2016question}) proposed a dependency tree based method to handle multi-relational questions.  They decompose the original question into simple sub-questions using six dependency patterns,  and the intersection of answer sets of all the sub-questions will be selected as the final answer.   

%Attention
% The attention mechanism~\cite{vaswani2017attention} is widely used for NLP tasks in recent days. (Yin et al. ,  2017~\cite{yin2017type}).  

% % IARNN (Wang et al. ,  2016)~\cite{wang2016inner} is an inner attention RNN that the attention was imposed directly to the input therefore can avoid biased attention problem caused by outer attention RNN.  Nevertheless,  it is unidirectional  and it doesn't consider the mutual influnce of the question and the answer. 
 
% % The ABCNN model proposed by (Yin et al. ,  2015)~\cite{yin2015abcnn} integrates attention into CNNs in order to solve sentence pair modeling problem.  This ABCNN model sets up mutual influence between the sentence pairs into CNNs,  therefore the output embedding of each sentence contains the information of its counterpart.  This ABCNN model gives an intuition of cross attention which takes mutual influence into account. 
 
% Hao~\cite{hao2017end} proposes another cross attention model focus on KBQA task.  This cross-attention mechanism consists of two parts: one answer-towards-question attention part and one question-towards-answer attention part.  It focuses on different answer aspects of each question word and calculates the similarity score of the question and the four different candidate answer aspects.  The final score will be composed of the weighted sum of these four similarity score.  This model well develops the attention mechanism,  however,  the mutual interaction is still not fully established.  The reason lies below:  The four similarity vectors between question and answer aspect are independent to each other thus the mutual influence can't be fully expressed.  
 
% Our cross attention mechanism aims to represent both the question and the schema in a dynamic way.  We take each question word's embedding and each skeleton embedding as input and we import an attention matrix.  For the attention matrix,  each item $e_{ij}$ of this matrix denotes the weight of the \emph{i}-th skeleton and the \emph{j}-th question word.  Thus the \emph{i}-th row represents the contribution of each question word to the \emph{i}-th skeleton,  while the \emph{j}-th row represents the contribution of the \emph{j}-th question word to each skeleton.  This cross-attention mechanism fully takes the mutual influence of the question and the schema into account,  thus generates a more precise and meaningful representation of the schema.  

% In conclusion,  our model has a uniform framework,  and it is concise and flexible, also puissant to answer ordinal questions. 
