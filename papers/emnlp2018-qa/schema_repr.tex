\subsubsection{Semantic Component Representation}
\label{sec:schema-encoding}

%TODO: Put this part into problem definition.
%After candidate generation, we obtain a set of candidate complex query graphs for each question $q$. A complex query graph $g$ can be regarded as a set of simple skeletons $(sk_1, sk_2, \dots, sk_m)$. We define a skeleton $sk$ is a predicate path starting from a fixed node to the target answer. The fixed starting node can be an entity, a type, a number, datetime or an ordinal value. Together with predicate, we call them ``items''. Thus we denote $sk = (it_1, it_2, \dots, it_l)$ (Note that this sequence has an order from starting point to target answer). In the case of \figref{fig:overview}, the candidate query graph in the right side can be splited into three skeletons, which start from ``entity:China'', ``type:River'' and ``ordinal$\_$value:$2$'' respectively. 

%In the part of relation matching, we need to encode the query graph.
%To encode the query graph, we first decompose the graph into semantic aspects.
%The semantic aspect is defined as one path in the query graph,
%which starts from the answer node and ends with a leaf node (candidate entities, types, times, ordinals).
%As the example shown in \figref{xxx},
%the candidate graph is decomposed into 3 semantic aspects which provide the following parallel clues:
%``the answer is in China'',
%``the answer is a river'', 
%``the answer ranks second by descending order of length''.
%The whole query graph represents the combination of clues, representing a complex semantics.



To encode a semantic component $p$, we take the sequence of both predicate ids
and predicate names into consideration.
As the example shown in \figref{fig:nn}, the id sequence of the first semantic component
is \{\textit{contained\_by}\}, 
and the predicate word sequence is the concatenation of canonical names for each predicate,
that is \{``contained'', ``by''\}.
%We use both id and labels in the knowlege base to encode each semantic aspect.
%Each aspect repred by a sequence of KB predicates and types along the path.
%Leave entities, times and ordinal numbers out. (don't care the detail time, number or entities)
%Sequence at different granularities.
%word sequence of the path: $p^{(w)} = \{p_1^{(w)}, \dots, p_n^{(w)}\}$.
%id sequence of the path: $p^{(id)} = \{p_1^{(id)}, \dots, p_m^{(id)}\}$.
%(from type.object.name relation.)

%Different methods to encode the path representation given the sequences.

Given the word sequence $\{p_1^{(w)}, \dots, p_n^{(w)}\}$, %we use word embedding matrix to transform words into vectors.
we first use a word embedding matrix $E_w \in \mathbb{R}^{|V_w| \times d}$ to convert 
the original sequence into word embeddings $\{\bi{p}_1^{(w)}, \dots, \bi{p}_n^{(w)}\}$,
where $|V_w|$ denotes the vocabulary size of natural language words,
and $d$ denotes the embedding dimension.
%TODO: talk later
%The word embedding matrix is initialized by publicly available pre-trained results, 
%such as Word2vec~\cite{mikolov2013} and GloVe~\cite{xxx}. 
Then we represent the word sequence using word averaging:
%Then we encode the whole sequence in continuous bag-of-words 
%\textbf{Bag-of-Words}:
$\bi{p}^{(w)} = \frac{1}{n} \sum_{i}{\bi{p}_i^{(w)}}$.

%\textbf{Recurrent-Words}: We use GRU~\cite{xx} as the recurrent cell.
%The sequence of word vectors are fed into a bidirectional GRU layer,
%and the repr is the concatenation between the last forward and backward hidden states,
%$\bi{p}^{(w)} = [\overrightarrow{\bi{h}}_n^{(w)};\overleftarrow{\bi{h}}_1^{(w)}]$

For the id sequence $\{p_1^{(id)}, \dots, p_m^{(id)}\}$,
%similarly, we have \textbf{Bag-of-Ids} and \textbf{Recurrent-Ids}
%for encoding the id sequence via another id embedding matrix
%$E_{id} \in \mathbb{R}^{|V_{id} \times d|}$.
we simply take it as a whole unit, and directly translate it into vector representation
using the embedding matrix $E_p \in \mathbb{R}^{|V_p \times d|}$ at path level,
where $|V_p|$ is the vocabulary size of predicate sequences.
There are two reasons for using such path embedding:
1) the length of id sequence is not larger than two, based on our generation method;
2) the number of distinct predicate sequences is roughly the same as the number of distinct predicates.
%Considering that the length of id sequence is restricted (mostly no longer than 2 hops),
%we propose the third method \textbf{Whole-Path}, 
%as it takes the id sequence as a whole unit, and directly transforms it into vector representation,
%by using the embedding matrix at path level,
%$E_p \in \mathbb{R}^{|V_p \times d|}$.
We get the final vector of the semantic component by element-wise addition:
$\bi{p} = \bi{p}^{(w)} + \bi{p}^{(id)}$.




%Now we describe in detail how to obtain the skeleton representation $\bi{sk}$ and further represent a complex query graph $g$ into $\bi{g} = (\bi{sk}_1, \bi{sk}_2, \dots, \bi{sk}_m)$. 
%
%%1. item encoding
%Firstly, we represent each component (item) of a skeleton into hidden vectors. For entity, type and predicate, we use their names in KB as text infomation and thus employ a bi-GRU network to encode the items just as we do in question encoding (\secref{sec:q-encoding}). For example, the name of entity ``China'' in \figref{fig:overview} is \textit{``People's Republic of China''} and we regard it as a word sequence [``people'', ``republic'', ``of'', ``China'']. After embedding lookup, the sequence of word embeddings are fed into bi-GRU, resulting a sequence of hidden vectors $[\overrightarrow{\bi{h}_i};\overleftarrow{\bi{h}_i}]$. Then we average them to obtain the item representation $\bi{it}$.
%
%\begin{equation}
%\label{eqn:item}
%\begin{aligned}
%& \bi{it} & = & \frac{1}{N}\sum_{i}[\overrightarrow{\bi{h}_i};\overleftarrow{\bi{h}_i}]\\
%\end{aligned}
%\end{equation}
%\noindent
%where $N$ is the number of words in that name.
%For number, datetime and ordinal value, we add three special placeholders in word embedding vocabulary and initialize them randomly. Thus, these three items are represented directly through embedding lookup. 
%
%%2. skeleton representation
%
%Then, we represent a skeleton $\bi{sk}$ as a dynamic combination of its item representations $\bi{it}_i$ shown in \figref{fig:overview} (right side). Since the items of a skeleton also form into a sequence in order, we use another bi-GRU to encode item sequence.
%
%Up to now, we have obtained the representation of query graph $\bi{g}$ consisting of a set of skeletion representations $(\bi{sk}_1, \bi{sk}_2, \dots, \bi{sk}_m)$.

