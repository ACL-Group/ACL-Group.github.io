\section{Introduction}
%The commonsense causalities are causal dependencies between common events or actions.
Commonsense causal reasoning is the task of inferring commonsense causalities, which are causal relations
between common events or actions. 
It is crucial to many natural language processing applications, such as text understanding and question answering , etc.
In this work we focus on sentences with causal relationships. For example, ``Amanda feels hot'' can be a reasonable cause for the effect sentence ``Amanda turns on the fan''. 

Gordon et al.~\cite{} formulate commonsense causal reasoning as a selection problem.
Given a cause (or effect) as the premise, 
it requires the model to select its reasonable effect (or cause) 
from a manually labeled effect (or cause) candidates for the premise.
Previous works~\cite{} propose various causal strength metrics to rank those candidates and reason the more plausible alternative for the premise.
However, those ranking models reason about commonsense causality rely on human labeled candidates, which is not feasible for many generation scenarios in NLP, such as question answering and dialog completion.
Thus, we reformulate the commonsense causal reasoning task as a generation problem, which given a cause (or effect) sentence as the premise, requires the model to generate its reasonable effects (or causes) sentence called targets. We call the cause-to-effect inference process ``forward reasoning''. For example, given the cause sentence ``Amanda feels hot'', possible effect sentence can be ``Amanda turns on the fan'' or ``Amanda takes off her coat'', etc. Backward reasoning, in the contrary, treat the input sentence as an effect and try to infer the cause. An ideal cause output for the effect ``Amanda feels hot'' can be ``The air-conditioner stops working'' .

Previous methods~\cite{} for selection-based causal reasoning can easily adapt to the generation-based causal reasoning.
We first automatically generate a candidates set for the premise, then perform the selection-based methods on those candidates to reason about causalities.
% talk about the limitations or drawbacks
There are two limitations of the selection-based methods: 1) It requires an extra on-line step to build a proper candidates set for each premise, which is very cost. 2) The model can only select the targets from the candidates, which is not flexible enough for causal reasoning task.
The convolutional sequence to sequence model~\cite{} equipped with an attention mechanism can power the generation-based causal reasoning task.
In the encoder-decoder sequence to sequence framework, the attention mechanism suppose to summary the semantic dependency between the current decoding state and the encoding state. 
Such semantic dependency are interpreted as
the semantic alignments in machine translation,
are interpreted as \ZY{XXX} in summarization, 
and are interpreted as the causal dependency in causal reasoning.

Causalities embedded in texts are sparse and ambiguous. 
%To facilitate the training process for seq2seq model, 
We propose syntactic causal patterns to automatically harvest the causalities from text corpus. 
To ameliorate the sparsity of causalities, we propose a novel attention fusion mechanism which integrates  the global causal dependency guidance from an external knowledge source called CausalNet~\cite{} with the local causal dependency from the soft attention mechanism in seq2seq decoder. CausalNet is a large network with words or terms as nodes. It contains more than 62 million causal evidences (represented by directed edges between causal words) and their frequency of coexisting with causal relationships in a large web corpus. For example, the word ``rain'' is a strong cause of ``umbrella'' in CausalNet with frequency 123, and even stronger of ``wet'' with frequency 399. In this work, we compute the global word-level causal strengths based on te causal relationship and frequencies in CausalNet and use the strength scores as the global guidance for the causality generation.
The extensive results show that our causality fused attention mechanism outperforms all the other competitive baselines for the causality generation task.

In sum, this paper makes the following contributions:
\begin{itemize}
	\item We define a new problem, commonsense causality generation, which benefits many NLP applications such as question answering.
	\item We propose a method to automatically create a cause-effect pairs corpus which facilitates the training process for commonsense causality generation.
	\item We propose a novel attention fusion mechanism which introduces the global causal dependencies observed from external knowledge source. Extensive experiments show that our approach outperforms multiple strong baselines by a substantial margin.
\end{itemize}

%Luo et al.~\cite{} extracts the cause-effect pairs from text corpus to harvest the large scale causal knowledge, such as CausalNet~\cite{}.


% sequence to sequence model
