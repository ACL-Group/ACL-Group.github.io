\section{Dataset Construction}
\label{sec:data}
Previous QA datasets are in the form of independent QA pairs~\cite{yang2015wikiqa} and do not
provide surrounding dialogues as context.
Dialogue datasets do contain QAs~\cite{lowe2015ubuntu} but these QAs almost exclusively
appear next to each other in the dialogue session. In other words, long-range
QAs are very rare in these datasets. 
Although He et al.\shortcite{he2019learning} solved a similar problem as ours, their customer
service dataset is not open to public due to privacy 
concerns. Wei et al.~\shortcite{wei2018task} 
published their dialogue dataset collected from an online forum. 
However, their work focuses on the dialogue policy learning and 
the data doesn't preserve the original utterances.

Hence, we create a new dataset suitable for this task. 
Nearly 160,000 distinct dialogues\footnote{An dialogue example online: 
\url{https://www.120ask.com/shilu/0cjdemaflhj6oyw9.html}} are collected from 
an online health forum\footnote{\url{https://www.120ask.com/}}. 
All the personal information was removed in advance by the website. 
After some basic data cleaning methods such as deleting the irrelavant 
sentences like ``Please pay ** coins to continue consultation'', 
we labelled 1000 randomly selected two-party multi-turn dialogues with 
Q (question), A (answer) and O (others) labels. 
A small amount of turns (0.24\%) are considered by the annotators 
to be both a question and an answer, and these are treated as questions
uniformly. The Fleiss' Kappa of our annotation between three annotators 
was 0.75, indicating substantial agreement.

On average, each dialogue has 19.68 doctor turns and 17.32 patient turns. 
Most turns are made up of a sentence fragment, so the number of words 
for each turn is on average less than 10 
words~\footnote{There are on average 9.80 questions with 8.89 words, 
10.78 answers with 6.62 words, 16.41 casual chit chats with 6.99 words 
in each session according to annotated dialogues.}. 
$21.9\%$ of the questions have no answers, $22.7\%$ of the questions 
have more than one answer and the remaining questions have the only answer. 
For questions that do have answers, each of them is matched to 1.41 answers 
on average.


%The distribution of questions and answers is almost balanced.
The annotated dialogues are split into training/development/test sets by 7:1:2. 
The distribution of the QA pairs by distance is shown in Table \ref{tab:dataInfo}.

\begin{table}[th]
        \small
    \centering
    \begin{tabular}{cccccc}
    \toprule[1.2pt]
    \diagbox{Dataset}{Distance} & 1 & 2 & 3 & 4 & $\geq5$ \\
    \midrule[1pt]
    Training  & 3439 & 2068 & 1029 & 450 & 554\\
    %\hline
    Development & 454   &   331   &    167  &  76   &  99  \\
    %\hline
    Test  & 947 & 592 & 274 & 136 & 168 \\
    \bottomrule[1.2pt]
    \end{tabular}
    \caption{The distribution of QA pairs by Q-A distances.}
    \label{tab:dataInfo}
\end{table}

%To meet the need for pairwise models which score the probability of each Q-NQ pair being a QA pair, we reconstructed the labeled dialogues into Q-NQ pairs with distance, history and binary golden label.
We reconstructed the labeled dialogues into Q-NQ pairs with distance, history and binary golden label used for pairwise models. A NQ from a party is paired with every earlier Q from the other party. If the pair is a QA pair, it is labeled as True(T). Otherwise, it is labeled as False(F). The distribution of positive and negative data in three datasets is shown in Table \ref{tab:pairdata}.

\begin{table}[th]
        \small
        \centering
        \begin{tabular}{cccc}
                \toprule[1.1pt]
                \diagbox{Label}{Dataset} &Training& Development& Test\\
                \midrule[0.8pt]
                True &7540 & 1226  & 2116\\
                %\hline
                False & 80631 & 14889 & 23893  \\
                \bottomrule[1.1pt]
        \end{tabular}
        \caption{The distribution of positive and negative Q-NQ pairs on three datasets.}
        \label{tab:pairdata}
\end{table}

