%
% File emnlp2019.tex
%
%% Based on the style files for ACL 2019, which were
%% Based on the style files for EMNLP 2018, which were
%% Based on the style files for ACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}


\usepackage{array}
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{color}
%\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
%\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}

\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}
\renewcommand\appendix{\setcounter{secnumdepth}{-2}}


\newcommand{\KZ}[1]{\textcolor{red}{Kenny: #1}}
\newcommand{\mx}[1]{\textcolor{green}{Mengxue: #1}}
% the following package is optional:
%\usepackage{latexsym} 
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{hhline}
\usepackage{booktabs}
%\usepackage{ctex}
\usepackage{makecell}
\usepackage{amssymb}



%\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\title{Matching Questions and Answers in Dialogues from Online Forums}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  % Question motivated multi-turn dialogues, such as those between doctors and patients in online discussion forums, are rich in question-answer pairs. These QA pairs are valuable for researches on proactive questioning and dialogue comprehension. However, these real-world online dialogues are often noisy and disordered, and mixed up with questions, answers and chit chats, making it difficult to extract high quality QA pairs. In this work, the proposed pairwise matching models combine the dialogue history in an interleaving way with two parallel attention mechanisms. Experiments show that our models achieve the best results, and are significantly better than previous methods on identifying long-distance QA pairs.
  
 
%Two-party question-motivated dialogues from online medical forums are rich 
%sources of question-answer pairs. 
Matching QA relations between two turns of conversation is not only 
the first step in analyzing dialogue structures, 
but also valuable for training dialogue systems. 
This paper presents a pairwise matching model with consideration 
of distance information and dialogue history by two simultaneous attention 
mechanisms called mutual attention. 
  %\KZ{What do you mean by ``upgraded''? Too much details about the approaches below. Simplify to just 1 sentence.} 
  %Two parallel attention mechanisms are used to capture the information flow between two parties, and an one-hot vector with fixed dimension is used to encode the distance before the output layer.
Given scores computed by the trained model between each non-question 
turn with its candidate questions, a greedy matching strategy 
is used for final predictions. We create a dataset with 1,000 
annotated dialogues and demonstrate that our proposed model outperforms 
the state-of-the-art and controlled baselines, which is statistically significantly 
better in matching long-distance QA pairs. 
  
\end{abstract}


\input{intro}

\input{problem}

\input{method}

\input{dataset}

\input{eval}

\input{results}
\input{relatedwork}

\input{conclusion}



\bibliography{emnlp-ijcnlp-2019}
\bibliographystyle{acl_natbib}

\appendix

%\input{appendix}


\end{document}
