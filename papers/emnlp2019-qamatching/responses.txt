Response#1
Thank you very much for your comments.

The dataset: This will be explained in the General Response.

Evaluation: We did separate our test set by variable QA distances. According to the results listed in Table 3 and 4, we showed that our full model HDM not only works well on overall performances, but also on long distance QAs while competitive on short distance QAs. As explained in Sec 6.2, distance is an important feature, but getting a good trade-off between distance and history information is more powerful. As for creating an evaluation set contains only long distance QAs. This is hard to implement because, as shown in Sec 1, incremental QAs which is the main source of long distance QAs, is naturally mixed with short distance QAs. 

Related work: Due to the space constraints, some related works are not included the current version. We will adjust the structure of paper and include them in the final version. 

The method: We did mention the intuition behind the model design especially in the last paragraph of Sec 3.1, the first two paragraphs in Sec 3.2 and the second paragraph in Sec 3.4.


Response#2
Thank you for the precious comments.

1. Although we only talked about a specific task in this paper, the idea of capturing the interactions between interlocutors of the model can be used in many other scenarios such as analysis and summarization of dialogues. Our future work will also focus on analyzing the discourse relations in multi-turn dialogues, which can be regarded as a more general problem of this paper.

2. The two-party Ubuntu dataset is extracted from multi-party dialogues with some rules and the dialogues are not well separated. While the Ubuntu dataset may be a good dialogue dataset, it is designed for next utterance classification and dialogue generation as a large noisy unannotated dataset, but is not particularly suitable for the QA matching task. Due to the space limitation, here are some results on Ubuntu dialogue corpus:
|    |  GD1  |  GDN  | GD1+J | GDN+J |Distance|  HTY  |  HDM  |
| F1 | 70.52%| 81.31%| 70.90%| 61.64%| 70.93% | 75.36%| 81.77%|

We find that in the Ubuntu dataset 51.8% of questions have no answers and 64.4% of the QA pairs are consecutive in the dialogue, which can be also reflected by the high performance of GDN. However, it still shows the effectiveness of our full model HDM, which perfectly takes use of the distance and history information. The QAs in Ubuntu corpus mainly focus on step-by-step operations and the dialogues are lack of incremental QAs and long distance QAs, while the doctor-patient corpus better meets our needs. So, we didn't show these results in the paper.


Response#3
Thank you very much!

Although Attention has been widely used in neural network models, our model has a particular design that captures the interactions between two participants in dialogues while incorporating the history information. Our evaluation has shown that the novel design in this problem works reasonably well.

We compared our method with previous works as baselines as explained in Sec 5.1. Specifically, GDs are commonly used rule-based models, mLSTM comes from Wang et al, and RPN is proposed by He et al.


General Response:

Dataset: As we have discussed in the first paragraph in Sec 4, there is no published qualified dataset for the QA matching task by our definition. The IRC dataset and Reddit datasets are both multi-party dialogues instead of two. Other human-human dialogue datasets not suitable because: Twitter Triple Corpus and Sina Weibo are not multi-turn dialogues which only have two or three turns. MultiWOZ 2.0, CamRest67, and Standford Dialog Dataset are multi-turn dialogues with dialouge act annotations but these QA pairs appear next to each other. If we shuffle the well-ordered dataset randomly or by some rules, it's unnatural and incorrect. Moreover, the model would probably learn the rules we used to shuffle the pairs. 