#Reviewer 1
We tried to provide support in related fields when making claims, for example when discussing movie dialogues compared to real-world ones and relationship taxonomy, but maybe not enough details are included on prospective applications. We will add more with better organization of the space.



#Reviewer 2
I guess there is misunderstanding regarding ground-truth annotation and human performance tests. These are two separate steps!

1)  "The human annotators were given specific instructions, while the classifier did not have access to this info"
        Groundtruth annotators are given extra information to help with their decision in the hope of more accurate labels and a clean dataset, while human baseline participants are not. They are two separate experiments! When we measure human performance on our task, the dialogue texts provided to humans and to machine are identical. For binary classification, the inter-annotator agreement of groundtruth annotations are 96.6% while that of human performance it's 77.5%, which indicates that more information does help. 

2) Only annotating clear, relatively stable and typical relationships will affect performance measurements.
        Here we are talking about groundtruth annotation where we hope to gain clean, accurate labels. We provide the "N/A" option because sometimes there is not enough information online/in the dialogues to determine relationship and we don't want to use annotator's random guess as groundtruth label. This aims to produce a smaller but cleaner dataset. When we measure human performance, we ask participants to make choice even if they find no clue at all.
        Besides, "it will consider only clear-cut instances" is not true. The groundtruth annotator assign labels to each character pair after online searching and reading all their dialogues(multiple sessions). This label will be used on each session. The content of those sessions can still be very ambiguous. So our settings only aim to build a clean dataset, not to reduce the difficulty of this task.

(3) Weakness 2
        I don't quite understand where the confusion is, but to clarify: Of all the five features we used in this paper, four are defined and extracted by us and the last one (LIWC) was proposed by others. Selected combinations of our features, if fed into Logistic Regression classifier, can achieve close-to-human performance. 






#Reviewer 3
(1) The binary classification task does sound vacuous, even to us at first. However, in experiments we find it not really that trivial due to the limited length of dialogues and lack of context. Humans achieve only 81.69% on this task with an average self-reported confidence score 2.76 (out of 0-4). The indicators are usually implicit.

(2) Your concern of learning genre/style instead of relationship is reasonable, but I think it's not the case here. In our experiments the model doesn't tend to make the same prediction on dialogues from the same movie, and it often makes different predictions even on dialogue sessions between the same pair of characters. Note that we did remove names and address terms to avoid learning from unrelated information.

(3) We tried LDA for topic modeling, but it groups words in a way that doesn't make sense to humans. Actually it's often hard even for human to summarize what the dialogues are about since they are short and sometimes can't be contextualized.






#General:
Thank you for all the precious comments!
How we define the taxonomy: We referred to Reis and Sprecherâ€™s book "Encyclopedia of Human Relationships" when defining the 13 fine-grained classes. The family/workplace categories are obtained by merging several classes. For example, "workplace" relationships include coworkers(partners/opponents), leader and subordinate, and professional contact(e.g. doctor and patient). Groundtruth annotators are asked to choose from 13 classes, and the binary labels are obtained by the same merging process.
