R1:

Thank you very much. We will fix the suggestions in the revised version.

- Yes. Our method also suits other language scenarios: all we need is a dataset to pre-train a dialogue dependency parser and a pre-trained language model on dialogue datasets in the same language. We haven’t done experiments in other languages so far since there is no available pre-trained language model on dialogue datasets in other languages and our model utilize such pre-trained models as a basic component.

===================================================
(KZ: say why the method/model is language independent)


R2:

Thanks a lot for your reviews.

- For the dependency detection (parsing) datasets, the number of dialogues for test set in our new dataset and STAC corpus are 480 and 111 respectively. 
The answers for the new dataset are the dependency relations named "reply-to" provided in the original dialogue disentanglement dataset (Kummerfeld et al., 2019). We reconstruct this dataset by clustering the turns if there exists a "reply-to" edge and use the new dataset to train a dependency parser for dialogues (Sec 3.3). Since it’s not practical to annotate large amount of dependency relations in the response selection dataset by human, we utilize the dependency parser pre-trained with the in-domain smaller-sized labeled data (i.e. the new dataset). The accuracy on the test sets of our new dataset and STAC corpus are shown in Table 2.

- The statistics on the complexity of multi-thread including the mean, standard deviation and distribution of threads on the three response selection datasets are shown in Table 6.  We also have ablations on using different number of threads as shown in Sec 4.2.2. We also have had the ablation without extraction (Full-hty in Sec 4.2.1) and the results show that our thread-based method outperforms these ablations (Table 5) indicating the performance improvements gained by dependency information. 

- We tested the distance-based extraction setting in Sec 4.2.1 instead of the random one, since this is a more reasonable way for cutting a dialogue into pieces. Our thread-based method outperforms distance-based ablations (Dist-seg) as shown in Table 5.

===================================================
(KZ: Have you explained the complexity of the dependency parses (# of threads on
average) in our test data? He was worried that the accuracy gain was due to
to other reasons, not the dependency itself.)


R3:

Thank you for the precious comments and suggestions.

- Our work mainly focuses on the effectiveness of exploiting dependency information for dialogue context modeling, and we followed the data preprocessing steps in two-party dialogues (UbuntuV2 and DSTC7) which did not have the special designs for speaker IDs. This should be the main reason for the big gap. In DSTC8 papers, many heuristic rules based on the speaker ID are used for data preprocessing which greatly helps to filter out unrelated utterances but also loses some useful utterances. These hard rules will hurt the completeness of the meaning in each thread and are not suitable for us. We will take advantage of the speaker information into both extraction and dialogue understanding models as our future work.

- The dependency parsing model is borrowed from Shi and Huang (2019) and we further designed Algorithm 1 based on the parsing results as mentioned in Sec 2.1. Due to space limitation, we didn’t show more details about the explanation and experiments. We will add it to the revised version.

- We will carefully reorganize the Sec 4&5 in the revised version:
	* (Sec 4) Show the extraction results first, then the response selection results, and finally the ablations.
	* (Sec 4) Make our findings and conclusions more clearly in each paragraph.
	* (Sec 5) Point out the improvements/differences of our method over the previous work.


	
=================================================

(KZ: Are these preprocessing steps available to us?
If so why don't we make use the speaker IDs in the same way as the those papers to achieve similar results as they reported?)
(KZ: Give some details about how you will reorganize, so R3 knows that you know what exactly to do right.)


