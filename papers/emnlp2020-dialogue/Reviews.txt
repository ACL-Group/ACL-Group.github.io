
Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper targets on dependencies between utterances and turn within dialogues with emphasis on dialogue history. The authors propose a Transformer based model (Thread-Encoder) that tackles the task of response selection. They also present an algorithm to transform a dialogue history into threads based on dependency relations. They evaluated the proposed approach on three tasks/datasets and compared with several state-of-the-art baseline approaches and reached interesting results.
Reasons to accept
Good exploration of the dependencies within dialogues resulting in excellent results in response selection task in all three datasets. Interesting method. Well written paper.
Reasons to reject
The authors focused only on English datasets.
Reproducibility:	4
Overall Recommendation:	4
Questions for the Author(s)
Do you plan any related research with your model in multilingual scenarios?
Typos, Grammar, Style, and Presentation Improvements
The absolute minimum of any typos or grammatical errors. I would move the related work section into beginning and provide a better connection with the introduction.

Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper attempts to address an issue in modeling dialog response selection for multi-thread dialog. The Ubuntu dialog data corpora contains a complicated multi-thread structure generated by multiple users and it is essential to apply dependency between utterances in the multi-thread dialog to clearly understand the dialog discourse and generate/select an appropriate response. The paper proposed an approach to extract each closed thread and reconstruct the training data to be a sequence of closed thread and trained a Thread-encoder using the restructured data. The approach to extract thread is borrowed from other papers. The statistics to show the complexity of the multi-thread structure is not clearly reported. The tests on various conditions of the thread extraction were performed. The evaluation results on sentence selection show the improvement.
Strength: The Thread-encoder trained with a sequence of a closed thread extracted from multi-threads in dialogues is promising to characterize a consistent dialog discourse.

Weaknesses: It is not clearly pictured an impact of the performance of the dependency structure on the sentence selection. 

Reasons to accept
The idea to apply the thread extraction from multiple thread data sounds reasonable and the transformer encoder to embed threads shows promising results for dialog response selection. The approach could work for data augmentation by adding more reasonable patterns and excluding noisy ones. The idea is reasonable and the proposed methods are compared with the conventional approaches using the multiple common testbeds to confirm it's efficacy.
Reasons to reject
It is not clearly pictured an impact of the performance of the dependency structure for the sentence selection. The statistics on the complexity of multi-thread is not analyzed for the target data in detail. If there are not so many multiple-threads, there could be another reason to improve performance. To analyze the impact of the dependency detection accuracy, it is better to test the random dependency structure with various lengths. The upper bound given by the oracle is also useful. Additionally, it is better to analyze the correlation between the complexity of the multi-thread in the data and the sentence selection performance with the variation of the hyperparameters. In order to know the accuracy of the dependency detection, the answers for the dependency detection made by humans or the human judgment on the automatic detection results using a reasonable amount of the data needs to be reported. 
Reproducibility:	4
Overall Recommendation:	4
Questions for the Author(s)
Table 2 shows the dependecy parser accuracy but there is no descriptions of the number of test sets and how to make the answers for the new data set. It is very important to know the statistics for the complexity level based on the number of threads in the traing/test/dev for all target data sets. The mean and standard deviation is good to know the distribution of the multiple threads.

Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
The paper tackles the problem of structuring the history of conversation which it's often overlooked in current models. The authors use the discourse dependency parsing model that allows to divide the history of conversations to separate threads in multi-party dialogues. This results in MRR improvements on DSTC7 and DSTC8 challenges on the response retrieval task.
The paper proposes an intuitive improvement to modelling multi-party dialogues and the results backs-ups the heuristics.

I have doubts regarding the results on DSTC8 against the SOTA and I'm willing to change my score once the authors will have a chance to respond (see my questions). However, the paper requires substantial rewriting of the last two sections should it be accepted.

Reasons to accept
The paper proposes a very intuitive idea that helps in multi-party conversations where several threads exist at the same time. That's often the case in forum-based conversations. The paper reads well (the first 3 sections) and it's easy to follow.
Reasons to reject
The results on the DSTC8 challenge is substantial compared to SOTA and the authors do not address this - based on the answer to this question I'm willing to substantially change my score.
The main algorithm to extract threads is not explained at all. Sections 4 and 5 should be rewritten - it feels like it was rushed for submission. They are of really low quality.

Reproducibility:	5
Overall Recommendation:	3
Questions for the Author(s)
The results for subtask 2 in DSTC8 well above 0.6. Can you comment on the big gap between the results reported in your paper and the results from the challenge?
Typos, Grammar, Style, and Presentation Improvements
line 140 - style line 142 and others - what "*" means here? line 295 - style line 376 - double dot line 442 - truncated line 444 - I don't udnerstand this sentence 467 - style 472 - datasets 473 - style 491 - style The sections 4 and 5 should be rewritten - it feels like it was rushed for submission. 766 - style Discourse dependency parsing model - you should explain a high-level intuition behind the parser.