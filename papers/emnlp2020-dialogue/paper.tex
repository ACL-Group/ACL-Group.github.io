%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{times}
\renewcommand*\ttdefault{txtt}
\usepackage{soul}
\usepackage{url}
%\usepackage[hidelinks]{hyperref}
%\usepackage[utf8]{inputenc}
%\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\urlstyle{same}

\usepackage{footnote}
\usepackage{array}
\usepackage{soul}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{multirow}
%\usepackage{algorithm}
%\usepackage{algorithmicx}
%\usepackage{algpseudocode}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}

\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{215} %  Enter the acl Paper ID here

\setlength\titlebox{6cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Multi-turn Response Selection using Dialogue Dependency Relations}

\author{Qi Jia$^1$ \hspace*{1cm}
	Yizhu Liu$^2$ \hspace*{1cm}
  	Siyu Ren$^3$ \hspace*{1cm} Kenny Q. Zhu$^4$\thanks{\hspace{2mm}The corresponding author.}\\
  Shanghai Jiao Tong University \\
  Shanghai, China \\
  \texttt{\{$^1$Jia\_qi,$^2$liuyizhu,$^3$roy0702\}@sjtu.edu.cn} \hfill
  \texttt{$^4$kzhu@cs.sjtu.edu.cn} \\ 
  \AND
%  Kenny Q. Zhu \\
%  Shanghai Jiao Tong University \\
%  Shanghai, China \\
%  \texttt{kzhu@cs.sjtu.edu.cn} \\\And
  Haifeng Tang \\
 China Merchants Bank Credit Card Center\\
  Shanghai, China \\
  %Affiliation / Address line 3 \\
  \texttt{thfeng@cmbchina.com} \\
}

\date{}	

\begin{document}
\maketitle
\begin{abstract}
Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with
pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper,  we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact
representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.
\end{abstract}
\input{intro}





\input{method}


\input{eval}

\input{results}

\input{relatedwork}
\input{conclusion}

\section*{Acknowledgement}
This research was supported by the SJTU-CMBCC Joint Research Scheme, 
SJTU Medicine-Engineering Cross-disciplinary Research Scheme, and NSFC
grant 91646205.

\bibliographystyle{acl_natbib}
\bibliography{emnlp2020}

\end{document}
