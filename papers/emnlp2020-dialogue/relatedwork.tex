\section{Related Work}
\label{sec:relatedwork}
Related work contains dialogue dependency parsing and multi-turn response selection.

\subsection{Dialogue dependency parsing}


Discourse parsing has been researched by scientists especially in linguistics for decades. 
Asher and Lascarides~\shortcite{0031949} proposed the SDRT theory with the STAC Corpus~\cite{AsherHMBA16} which made a great contribution to the discourse parsing on multi-party dialogues. Shi and Huang~\shortcite{ShiH19} proposed a sequential neural network and achieved the state-of-the-art results on this dataset. Another similar task is dialogue disentanglement~\cite{DuPX17}. This task isn't focusing on developing discourse theories but trying to segment the long dialogues according to topics. It takes each turn in the dialogue as a unit, and only care about whether there is a relation between two turns, which is called ``reply-to'' relation. Due to the scarcity of annotated dialogues across domains under SDRT theory, the predicted dependency relations had never been used for down-streaming tasks, such as response selection and dialogue summarization. In this paper, we take advantage of both the simplicity of the ``reply-to'' relation and the sequential parsing methods~\cite{ShiH19} to do dialogue dependency parsing. Developing general discourse parsing with relations types and take relation types into consideration may be future work.



\subsection{Multi-turn response selection}
\label{sec:mtrs}
Multi-turn response selection task was proposed by Lowe et al.~\shortcite{LowePSP15} and the solutions for this task can be classified into two categories: the sequential models and the hierarchical models. To begin with, the sequential models~\cite{LowePSP15} were directly copied from the single-turn response selection task since we can regard the multiple history turns as a long single turn.  Considering the multi-turn characteristic, Wu et al.~\shortcite{WuWXZL17} proposed the sequential matching network (SMN), a new architecture to capture the relationship among turns and important contextual information. SMN beats the previous sequential models and raises a popularity of such hierarchical models, including DUA~\cite{ZhangLZZL18}, DAM~\cite{WuLCZDYZL18}, IOI~\cite{TaoWXHZY19}, etc. The ESIM~\cite{abs-1802-02614}, which is mainly based on the self and cross attention mechanisms and incorporates different kinds of pre-trained word embedding. It changed the inferior position of the sequential model, making it hard to say which kind of architecture is better.


Due to the popularity of the pre-trained language models such as BERT~\cite{DevlinCLT19} and GPT~\cite{radford2018improving}, the state-of-the-art performance on this task was refreshed~\cite{vig2019comparison}. Work such as \cite{abs-1908-04812} and \cite{humeau2019poly} further shows that the response selection performance can be enhanced by further pretraining the language models on open domain dialogues such as Reddit~\cite{abs-1904-06472}, instead of single text corpus such as BooksCorpus~\cite{ZhuKZSUTF15}. These models can be also regarded as the sequential models because they concatenate all the history turns as the input to the model while ignoring the dependency relations among the turns. Inspired by these works, we incorporate the dependency information in the dialogue history into the response selection model with the pre-trained language model on dialogue dataset.

In this work, we focus on the effectiveness of exploiting dependency information for dialogue context modeling and follow the data preprocessing steps in two-party dialogue datasets, including UbuntuV2 and DSTC7, which have no special designs for speaker IDs. In the papers for DSTC8 response selection track, such as \cite{abs-2004-01940}, many heuristic rules based on speaker IDs are used for data preprocessing, which greatly helps to filter out unrelated utterances. However, they also definitely lead to losing some useful utterances. These hard rules will hurt the completeness of the meaning in each thread and are not suitable for us. As a result, the results on the response selection task for DSTC8 dataset are not comparable.
We will take advantage of the speaker information into both extraction and dialogue understanding models as our future work.





