\section{Results and Analysis}
\label{sec:ra}
Here we show results on dialogue thread extraction and response selection of the three datasets, and give some discussions on our model design.
\subsection{Extraction Results}
\label{sec:extrresults}
\begin{table*}[th!]
	\centering
	\small
	\begin{tabular}{lc|ccc|cccc}
		\toprule[1pt]
		\multicolumn{2}{c}{Dataset} &  avg\#thd & avg\#turn & std\#turn & 1-thd(\%) & 2-thd(\%)& 3-thd(\%) & 4-thd(\%)\\
		\midrule[1pt]
		\multirow{3}*{UbuntuV2} & train & 1.42 & 3.24 & 0.29 & 68.92 &22.19& 6.65& 2.24\\
		~ & valid& 1.39 & 3.09 & 0.25  &70.78& 20.90& 6.47& 1.85\\
		~ & test & 1.39 & 3.13 & 0.25 & 70.69& 20.89& 6.18& 2.23\\
		\hline
		\multirow{3}*{DSTC7}  &train &1.40 &4.45  &0.38  &67.75& 25.37& 5.48& 1.40\\
		~ & valid& 1.39 & 4.42 & 0.37 & 67.76& 25.74& 5.44& 1.06\\
		~ & test& 1.45 & 4.42 & 0.42 &65.00& 26.10& 7.70& 1.20\\
		\hline
		\multirow{3}*{DSTC8*} &train & 3.82 & 24.70 & 5.39  & 2.96& 2.59& 3.85& 90.61\\
		~ & valid& 3.80 & 24.46 & 5.37 & 3.32& 3.09& 3.64& 89.95\\
		~ & test& 3.81 & 24.53 & 5.34 & 3.09& 2.76& 4.06& 90.09\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Statistics on extraction results. avg\#thd refers to the average number of threads per dialogue, avg\#turn refers to the average number of turns in each thread, and std\#turn refers to the average standard deviation of the number of turns in each thread per dialogue. 1-thd to 4-thd refers to the percentage of the number of dialogues with 1 to 4 threads in corresponding datasets.}
	\label{tab:parstatis}
\end{table*}
We first evaluate the extraction results on UbuntuV2, DSTC7 and DSTC8* with three metrics:
The average number of threads (\textbf{avg\#thd}) is to show how many dialogue threads are discovered in each dialogue, which ranges from 1 to 4. We didn't take all of the extracted threads into consideration, serving as a hard cut for the trade-off between information loss and memory usage of the model.
The average number of turns in each thread (\textbf{avg\#turn}) and the average standard deviation of the number of turns in each thread (\textbf{std\#turn}) are to measure the length of each thread. Dialogues context is not well separated if the length of each thread varies a lot~(i.e., the std\#turn is too high).


We apply the dialogue extraction algorithm in Section \ref{sec:DSA} on the three datasets. The statistics of extracted threads are in Table \ref{tab:parstatis}.
Firstly, we can find that the average number of threads is around $3.81$ for DSTC8* dataset while around $1.40$ for the other two datasets, which well aligns with the empirical observation that two-party dialogues tend to have more concentrated discussions with a smaller number of threads while multi-party dialogues usually contain more threads to accommodate conversation with high diversity. Also, as is listed in Table \ref{tab:dataset}, the number of turns for DSTC8* dataset is usually larger than UbuntuV2 and DSTC7 dataset, which naturally leads to more leaf nodes hence a larger number of threads. 
Secondly, the average length of threads is around $24.50$ for DSTC8* dataset while around $4.0$ for DSTC7 dataset and UbuntuV2 and the standard deviation for DSTC8* dataset is also larger. It shows that when the number of dialogue threads increases, the standard deviation of the length of each thread also tends to increase since some dialogue threads may catch more attentions while others may be ignored. 
In summary, DSTC8* is a more challenging multi-party dialogue dataset for dialogue context modeling than two-party dialogue datasets, including UbuntuV2 and DSTC7.


\subsection{Response Selection Results}
The response selection results of our Thread-Encoder models, including Thread-bi and Thread-poly, are shown in Table \ref{tab:V2D7} for UbuntuV2 and DSTC7 datasets, and in Table \ref{tab:discussion} for DSTC8*. 
\begin{table*}[ht!]
	\centering
	\small
	\begin{tabular}{l|cccc|cccc}
		\toprule[1pt]
		\multirow{2}{*}{} &
		\multicolumn{4}{c|}{UbuntuV2} &
		\multicolumn{4}{c}{DSCT7}\\ 
		Model &  hits@1 & hits@2 & hits@5 & MRR& hits@1 & hits@10 & hits@50 & MRR\\
		\midrule[1pt]
		DAM & - & -& -& - &  34.7 & 66.3 & - & 35.6\\
		ESIM-18 &  73.4 & 85.4 & 96.7 & 83.1 & 50.1 & 78.3 & 95.4 & 59.3 \\
		ESIM-19& 73.4 & 86.6& 97.4& 83.5 & 64.5 & 90.2 & \bf 99.4 & 73.5 \\
		IMN &  77.1 & 88.6 & 97.9 &- & -& -& -&- \\
		Bi-Enc & 83.6 & - &  98.8 & 90.1  &70.9 & 90.6 &  - & 78.1\\
		Poly-Enc & 83.9 & - & 98.8 & 90.3 &70.9 & 91.5 & - & 78.0\\
		Cross-Enc & \bf 86.5 & - & \bf 99.1 & \bf 91.9&71.7 & 92.4 & - & 79.0 \\
		\hline
		Thread-bi	& 83.8 &  92.4 & 98.5 & 90.0 & \bf 73.3$^\star$ &92.5 & 99.3 & 80.2$^\star$ \\
		Thread-poly & 83.6&\bf 92.5 &98.5 &90.0 &73.2$^\star$& \bf  93.6$^\star$&99.1& \bf80.4$^\star$\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Results on UbuntuV2 and DSTC7 dataset. Scores marked with $^\star$ are statistically significantly better than the state-of-the-art 
		with $p<0.05$ according to t-test.}
	\label{tab:V2D7}
\end{table*}

Since UbuntuV2 is too large, we only fine-tuned on this dataset for three epochs due to limited computing resources.
The performance of our model is similar to Bi-Enc and Poly-Enc on UbuntuV2.
Although the Cross-Enc rank top on UbuntuV2, it is too time-consuming and 
not practical~\cite{humeau2019poly}. It runs over 150 times slower than both Bi-Enc 
and Poly-Enc. Our model, Thread-bi, takes the top four threads~(see \secref{sec:number} for more details) 
into consideration with the inference time overhead similar to Bi-Enc and Poly-Enc. 
Besides, the reason why our model seems slightly worse than Poly-Enc is that UbuntuV2 is an easier dataset with fewer turns and threads according to Table \ref{tab:dataset} and Table \ref{tab:parstatis}. Consequently, our model degenerates towards Bi-Enc and Poly-Enc, and all four models~(Bi-Enc, Poly-Enc, Thread-bi, Thread-poly) actually yield similar results, with p-value greater than 0.05.




Due to the huge advancement of pre-trained models over other models shown on UbuntuV2 and DSTC7, we mainly compared the competitive state-of-the-art pre-trained models on DSTC8* dataset for through comparison as shown in Table \ref{tab:discussion}.
Our models achieve the new state-of-the-art results on both DSTC7 and DSTC8* dataset proving that threads based on dependency relation between turns are helpful for dialogue context modeling. 
We can see that using multiple vectors works much better than using only one representation. The gap between these two aggregation methods is not clear on UbuntuV2 and DSTC7, but much more significant on DSTC8* where the dialogues between multiple participants are much more complicated. This finding hasn't been shown in Humeau's work~\shortcite{humeau2019poly}. Besides, our model can enhance both kinds of pre-trained dialogue models on the multi-turn response selection task by comparing Thread-bi with Bi-enc and Thread-poly with Poly-enc.

It should be noted that the inherent properties of these three datasets are different according to \secref{sec:extrresults}. UbuntuV2 and DSTC7 datasets are dialogues between two parties, while DSTC8* dataset involves more complicated multi-party dialogue. This reveals that Thread-Encoder not only works under simple scenarios such as private chats between friends, but also acquires further enhancement under more interlaced scenarios such as chaos chat rooms. 


\begin{table}[th!]
	\centering
	\small
	\begin{tabular}{lccc}
		\toprule[1pt]
		Model & \#Para & Train(h) & Test(\#dialog/s)   \\
		\midrule[1pt]
		Bi-Enc & 256.08M& 10.22 & 6.79  \\		
		Poly-Enc & 256.13M & 12.34 & 4.78 \\
		\hline
		Thread-bi &  256.08M& 16.36 &  4.73  \\
		Thread-poly &256.13M & 17.09 & 4.77  \\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Total number of parameters, training time (h) and testing speed(\#dialogues per second) on DSTC8* main models.}
	\label{tab:dstc8}
\end{table}

The number of parameters, training time and testing speed are shown in Table \ref{tab:dstc8}. It takes more epochs for our model to convergence, while the testing speed is similar to Poly-Enc.




\subsection{Discussions on Model Design}

To further understand the design of our full model, we did several ablations on DSTC8*. All of the ablation results as listed in Table \ref{tab:discussion}. The descriptions and analysis are in following subsections.
\begin{table*}[th]
	\centering
	\small
	\begin{tabular}{lccccccccc}
		\toprule[1pt]
		ID &Method & Aggregation Type & Thread Type& \#Thread & hits@1 & hits@5 &hits@10 & hits@50 & MRR \\
		\midrule[1pt]
		1&\underline{Bi-Enc} & Average& Full-hty & 1& \underline{22.2} & \underline{43.0} & \underline{54.2} & \underline{88.7} & \underline{32.9} \\
		2&\underline{Poly-Enc} & Attention & Full-hty & 1 & \underline{32.5} & \underline{54.1} & \underline{64.4} & \underline{91.4} & \underline{43.1}  \\
		\midrule[0.5pt]
		3&Thread-bi& Average & Dep-extr & 1 & 20.2& 39.6& 51.1& 86.1& 30.5\\
		4&Thread-bi& Average & Dep-extr & 2 & 22.6& 42.5& 53.1&87.9 &32.9 \\
		5&\underline{Thread-bi}& Average & Dep-extr & 3 & \underline{23.4}& \underline{43.0}& \underline{54.9}& \underline{88.2}& \underline{33.8}\\
		6&Thread-bi& Average & Dep-extr & 4 &  22.9& 43.3& 55.1& 88.5& 33.5\\
		7&Thread-bi& Average & Dist-seg & 3 & 21.7 &43.2 & 55.2&88.8 &32.8 \\
		\midrule[0.5pt]
		8&Thread-poly& Attention & Dep-extr & 1 &29.4 &49.1 & 59.4& 88.7&39.5 \\
		9&Thread-poly& Attention & Dep-extr & 2 &32.0 &53.2 &63.2 &91.1 &42.5 \\
		10&Thread-poly& Attention & Dep-extr & 3 & 33.1 & 54.1& 64.2& 92.0 & 43.5 \\
		11&\underline{Thread-poly}& Attention & Dep-extr & 4 &  \underline{\bf 33.5$^\star$}& \underline{\bf 54.5$^\star$}& \underline{\bf 64.5}& \underline{91.7}& \underline{\bf 44.0$^\star$}\\
		12&Thread-poly& Attention & Dist-seg & 4 &33.2 &53.5 & 63.6&\bf 92.3$^\star$& 43.4\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{Main results of DSTC8* (underlined) and ablation tests on DSTC8*. Scores marked with $^\star$ are statistically significantly better than Poly-Enc 
		with $p<0.05$ according to t-test.}
	\label{tab:discussion}
\end{table*}





\subsubsection{Different ways to generate threads}
\label{sec:ways}
We evaluate some reasonable alternative methods to extract dialogue threads
from the history, i.e.``Thread Type'' in Table \ref{tab:discussion}. 
\begin{itemize}
	\item \textbf{Full-hty} concatenate the full dialogue history in one thread. 
Our model degrades to Bi-Enc and Poly-Enc.
	\item \textbf{Dist-seg} segments the turns based on their distance to the next response. This idea is based on the intuition that the adjacent turns are possible to have strong connections. For example, if we use 4 threads, the dialogue in Figure \ref{fig:algorithm} will be segmented into $\langle\langle t_6, t_7\rangle, \langle t_4, t_5\rangle, \langle t_2, t_3\rangle, \langle t_1\rangle\rangle$.
	\item \textbf{Dep-extr} refers to the threads extraction procedure as explained in \algoref{alg:DSA}. %based on the predicted dependencies 
\end{itemize}

Comparing in group ID-$\{1, 5, 7\}$ and ID-$\{2, 11, 12\}$, we get the following observations:
(1) Our extraction operations help with the response selection as both ID-$5$ and ID-$11$ have significant improvement despite the distance-based extraction method is a strong baseline. The dependency relations capture salient information in dialogue more accurately and yields better performance.
(2) Segmenting dialogues simply based on distance may hurt the storyline for each sub dialogue as ID-$7$ is worse than ID-$\{1, 5\}$, which hurts the representation ability of language models. 
(3) The information loss caused by Dist-seg can be partially made up by ``poly'' settings as ID-$12$ lies between ID-$2$ and ID-$11$. Generating multiple representations by aggregators may help to get multiple focuses in each thread. Thus interleaved sub-dialogues can be captured more or less. The gap between Dist-seg and Dep-extr will definitely be widened by improving the performance of sub-dialogue extraction. 


\subsubsection{The number of threads to use}
\label{sec:number}

After deciding the way for extraction, the number of threads (i.e., \#Thread in Table \ref{tab:discussion}) to use is another key hyper-parameter for this model design. 

We tested our model using the number of threads ranging from $1$ to $4$. The results are shown in ID-$\{3\sim6\}$ and ID-$\{8\sim11\}$ from Table \ref{tab:discussion},  we draw following conclusions.
First, by comparing the results with only $1$ thread, we can see ID-$3$ and ID-$8$ are worse than Bi-enc and Poly-enc respectively. It shows that there does exist many cases that correct candidates that do not respond to the nearest dialogue threads. Considering only the nearest sub-dialogue is not enough.
Second, with the increasing number of threads from $1$ to $4$, the results go up and down for Thread-bi. The peak value is achieved when \#Thread equals $3$. Although more than $90\%$ of dialogues can be extracted into $4$ threads according to Table \ref{tab:parstatis}, the results doesn't go up with one more thread.
Some redundant dialogue threads far from the current utterances may bring noises for response selection.
Also, the negative effects of redundant dialogue threads for Thread-poly reflect on the limited improvements and even decreases on hits@50 between ID-$10$ and ID-$11$. 
Designing a metric to filter the extracted dialogue threads automatically is our future work.











