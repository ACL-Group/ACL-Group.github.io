\section{Experiments}
\label{sec:eval}
This section is about the details about the experiment.
We will talk about the automatic evaluation metric
% and introduce the details of human evaluation which is chosen to 
%to investigate the effectiveness of the automatic evaluation.
and
% we will 
the experimental setup.
%We then discuss the result of each module to select the best combination for the pipeline method and the filtering method.
%Finally, we will discuss the results of pipeline methods and filtering methods, and the performance of the aspect.

\subsection{Evaluation Metric}
\label{sec:metric}
In two-step framework, we use the well-known metrics to evaluate the performance of each individual module.
%For paragraph retrieval, we choose the average Precision@$K$ and MRR@$K$ to score the list of paragraphs ranked with the relevance to the aspect, where $K$ is the number of the ground truth paragraphs.
For answer span extraction, we evaluate the performance of answer spans as Subramanian et al.~\shortcite{subramanian2017neural} did. 
We calculate the token-level F1 score matrix of elements $f_{i,j}$ between the two answers $a_i$ and $a_j$ and obtain the mean precision, recall, F1 among the paragraphs.
For question generation, we choose the widely used generation metrics, such as BLEU, METEOR, and ROUGE-L which are implemented by Sarma's work~\cite{sharma2017relevance}.

For the entirety of QA pairs, we design an end-to-end metric to score each pair of generated QA.
%We use the aspect as a unit to judge the QA pairs.
Each context is a evaluate unit to judge the quality of QA pairs.
Given a context and an aspect keyword, there is a set of ground truth QA pairs $(Q, A)$ and a set of generated QA pairs $(\hat{Q}, \hat{A})$.
%, where $A=\{a_1, a_2,...,a_{|A|}\}, Q=\{q_1, q_2,...,q_{|Q|}\}$.
We calculate the QA score matrix $M$ of elements $S_{i,j}$ between the ground truth QA pair $(q_i, a_i)$ and predicted QA pair $(\hat{a_j}, \hat{q_j})$.
The pairwise score is:
\begin{equation*}
\small
\begin{aligned}
\text{F-BLEU} &= \text{F}1(a_i, \hat{a_j})\times \text{BLEU}(q_i, \hat{q_j})\\
\text{F-METEOR} &=\text{F}1(a_i, \hat{a_j})\times \text{METEOR}(q_i, \hat{q_j})\\
\text{F-ROUGE} &= \text{F}1(a_i, \hat{a_j})\times \text{ROUGE}(q_i, \hat{q_j}),
\end{aligned}
\end{equation*}
where $a_i$ and $\hat{a_j}$ are the tokens of ground truth answer span and predicted answer span respectively, and $q_i$ and $\hat{q_j}$ are the strings of ground truth question and predicted question respectively.

We evaluate the QA pairs generation system with the standard evaluation metrics: Precision@K (P@K), Recall@K (R@K) and F1@K, where K is determined by 
the maximum value of the number of ground truth and the number of  generated QA pairs in each context.
%the number of ground truth in each context.
Max-pooling along the ground truth axis of score matrix $M$ assesses the precision of each QA pair generated for the aspect:
$p_j=max_i(S_{i,j}), j=1,...,K$.
Max-pooling along the predict axis of $M$ assesses the recall:  
%$mrr_i=max_j(S_{i,j})/argmax_j(S{i,j})$.
$ri = max_j (S_{i,j})$.
The multi-QA metric is:
\begin{equation*}
\small
\begin{split}
&Precision@K=mean(p)\\
&Recall@K=mean(r)\\
&F1@K = \frac{2\times Precision@K\times Recall@K}{Precision@K + Recall@K}
\end{split}
\end{equation*}

\subsection{Experimental Setup}
We use BERT tokenizer to do the tokenization for BERT related models and UNILM, and use Stanford CoreNLP~\cite{manning-EtAl:2014:P14-5} to do the tokenization in other models.
%The BERT retrieval model is fine tuned with the learning rate of 3e-6 and 3 epochs.
%For LSTM-CRF model, we use BiLSTM with hidden state size of 128.
%The Adam optimizer is adapted with the learning rate of 1e-2 and the model is trained for 300 epochs.
The classifiers involved in Chunking method and Filter method are fine tuned on $\text{BERT}_{\text{BASE}}$ with the max sequence length of 256, the learning rate of 3e-6 and 3 epochs.
The pointer network follows the parameters set as Subramanian et al's~\shortcite{subramanian2017neural}.
%The Seq2seq model for question generation follows the parameters set as Zhao et al~\shortcite{zhao2018paragraph}. We add additional 1-layer LSTM encoder and self-gated layer to encode aspect. The hidden size of LSTM and self-gated layer is the same as the hidden size which is used for encoding paragraph.
The UNILM model for question generation is fine tuned with the learning rate of 5e-5 and 8 epochs with half-precision training, then we choose the beam search with size of 1 to decode the output sequence.

%In this work, to prevent the filter from deleting all the QA pairs, those models using filtering method retain at least the best QA pair in each context.
%It should be noted that all of the ranking strategies are mentioned in Section \ref{sec:ranking}, the filter is only used to remove the irrelevant QA pairs but not participate in re-ranking.

