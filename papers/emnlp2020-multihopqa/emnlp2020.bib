@article{DBLP:journals/corr/RajpurkarZLL16,
  author    = {Pranav Rajpurkar and
               Jian Zhang and
               Konstantin Lopyrev and
               Percy Liang},
  title     = {SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  journal   = {CoRR},
  volume    = {abs/1606.05250},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.05250},
  archivePrefix = {arXiv},
  eprint    = {1606.05250},
  timestamp = {Mon, 13 Aug 2018 16:49:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RajpurkarZLL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1806-03822,
  author    = {Pranav Rajpurkar and
               Robin Jia and
               Percy Liang},
  title     = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  journal   = {CoRR},
  volume    = {abs/1806.03822},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.03822},
  archivePrefix = {arXiv},
  eprint    = {1806.03822},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-03822.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HermannKGEKSB15,
  author    = {Karl Moritz Hermann and
               Tom{\'{a}}s Kocisk{\'{y}} and
               Edward Grefenstette and
               Lasse Espeholt and
               Will Kay and
               Mustafa Suleyman and
               Phil Blunsom},
  title     = {Teaching Machines to Read and Comprehend},
  journal   = {CoRR},
  volume    = {abs/1506.03340},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.03340},
  archivePrefix = {arXiv},
  eprint    = {1506.03340},
  timestamp = {Mon, 13 Aug 2018 16:49:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HermannKGEKSB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Shen2017,
abstract = {Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to eeectively exploit and then reason over the relation among queries, documents, and answers. Diierent from previous approaches using a axed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.},
archivePrefix = {arXiv},
arxivId = {1609.05284v3},
author = {Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
doi = {10.1145/3097983.3098177},
eprint = {1609.05284v3},
file = {:Users/roy/Library/Application Support/Mendeley Desktop/Downloaded/Shen et al. - 2017 - ReasoNet Learning to Stop Reading in Machine Comprehension.pdf:pdf},
isbn = {9781450348874},
keywords = {Deep Reinforcement Learning,Machine Reading Comprehension,ReasoNet},
title = {{ReasoNet: Learning to Stop Reading in Machine Comprehension}},
url = {https://arxiv.org/abs/1609.05284},
year = {2017}
}

@article{DBLP:journals/corr/SeoKFH16,
  author    = {Min Joon Seo and
               Aniruddha Kembhavi and
               Ali Farhadi and
               Hannaneh Hajishirzi},
  title     = {Bidirectional Attention Flow for Machine Comprehension},
  journal   = {CoRR},
  volume    = {abs/1611.01603},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01603},
  archivePrefix = {arXiv},
  eprint    = {1611.01603},
  timestamp = {Mon, 13 Aug 2018 16:46:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SeoKFH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/XiongZS16,
  author    = {Caiming Xiong and
               Victor Zhong and
               Richard Socher},
  title     = {Dynamic Coattention Networks For Question Answering},
  journal   = {CoRR},
  volume    = {abs/1611.01604},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01604},
  archivePrefix = {arXiv},
  eprint    = {1611.01604},
  timestamp = {Mon, 13 Aug 2018 16:48:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/XiongZS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{weissenborn-etal-2017-making,
    title = "Making Neural {QA} as Simple as Possible but not Simpler",
    author = "Weissenborn, Dirk  and
      Wiese, Georg  and
      Seiffe, Laura",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K17-1028",
    doi = "10.18653/v1/K17-1028",
    pages = "271--280",
    abstract = "Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.",
}

@inproceedings{Chen2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.05210v1},
author = {Chen, Yen-chun and Bansal, Mohit},
eprint = {arXiv:1906.05210v1},
file = {:Users/roy/Desktop/1906.05210.pdf:pdf},
mendeley-groups = {MultiHop{\_}QA},
title = {{Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension}},
year = {2019}
}


@article{Welbl2018,
abstract = {Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence â€” effectively performing multihop, alias multi-step, inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 54.5{\%} on an annotated test set, compared to human performance at 85.0{\%}, leaving ample room for improvement.},
archivePrefix = {arXiv},
arxivId = {1710.06481},
author = {Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
doi = {10.1162/tacl_a_00021},
eprint = {1710.06481},
file = {:Users/roy/Desktop/1325-3836-1-PB.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
mendeley-groups = {MultiHop{\_}QA},
pages = {287--302},
title = {{Constructing Datasets for Multi-hop Reading Comprehension Across Documents}},
volume = {6},
year = {2018}
}

@article{DBLP:journals/corr/abs-1802-05365,
  author    = {Matthew E. Peters and
               Mark Neumann and
               Mohit Iyyer and
               Matt Gardner and
               Christopher Clark and
               Kenton Lee and
               Luke Zettlemoyer},
  title     = {Deep contextualized word representations},
  journal   = {CoRR},
  volume    = {abs/1802.05365},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05365},
  archivePrefix = {arXiv},
  eprint    = {1802.05365},
  timestamp = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-05922,
  author    = {Bhuwan Dhingra and
               Qiao Jin and
               Zhilin Yang and
               William W. Cohen and
               Ruslan Salakhutdinov},
  title     = {Neural Models for Reasoning over Multiple Mentions using Coreference},
  journal   = {CoRR},
  volume    = {abs/1804.05922},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.05922},
  archivePrefix = {arXiv},
  eprint    = {1804.05922},
  timestamp = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-05922.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1804-10490,
  author    = {Martin Raison and
               Pierre{-}Emmanuel Mazar{\'{e}} and
               Rajarshi Das and
               Antoine Bordes},
  title     = {Weaver: Deep Co-Encoding of Questions and Documents for Machine Reading},
  journal   = {CoRR},
  volume    = {abs/1804.10490},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.10490},
  archivePrefix = {arXiv},
  eprint    = {1804.10490},
  timestamp = {Mon, 13 Aug 2018 16:46:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-10490.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@techreport{Zhong2019,
abstract = {End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6{\%} on the blind test set, outperforming the previous best by 3{\%} accuracy despite not using pretrained contextual encoders.},
archivePrefix = {arXiv},
arxivId = {1901.00603},
author = {Zhong, Victor and Xiong, Caiming and Keskar, Nitish Shirish and Socher, Richard},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1901.00603},
file = {:Users/roy/Library/Application Support/Mendeley Desktop/Downloaded/Zhong et al. - Unknown - COARSE-GRAIN FINE-GRAIN COATTENTION NET-WORK FOR MULTI-EVIDENCE QUESTION ANSWERING.pdf:pdf},
title = {{Coarse-grain fine-grain coattention network for multi-evidence question answering}},
year = {2019}
}

@article{Cao2019,
annote = {apply R-GCN on the entity graph constructed by matching entities in the candidate answer set plus the query entity.

four types of edge relations
gated version of R-GCN},
author = {Cao, Nicola De and Titov, Ivan},
file = {:Users/roy/Desktop/N19-1240.pdf:pdf},
mendeley-groups = {MultiHop{\_}QA},
pages = {2306--2317},
title = {{Question Answering by Reasoning Across Documents with Graph Convolutional Networks}},
year = {2019}
}

@article{Tu2019,
annote = {no coreference resolution system to locate mentions of candidate entities that cannot be exactly matched.

Heterogenous Graph:
1) document nodes
2) surface candidate entitied nodes
3) contextual entity mention nodes

initial node representation are obtained by performing co-attention and self-attentive pooling},
author = {Tu, Ming and Wang, Guangtao and Huang, Jing and Tang, Yun and He, Xiaodong and Zhou, Bowen},
file = {:Users/roy/Desktop/P19-1260.pdf:pdf},
mendeley-groups = {GNN,MultiHop{\_}QA},
pages = {2704--2713},
title = {{Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs}},
year = {2019}
}

@article{BAG,
author = {Cao, Yu and Fang, Meng and Tao, Dacheng},
file = {:Users/roy/Desktop/N19-1032.pdf:pdf},
mendeley-groups = {MultiHop{\_}QA},
pages = {357--362},
title = {{BAG : Bi-directional Attention Entity Graph Convolutional Network for Multi-hop Reasoning Question Answering}},
year = {2019}
}

@article{Kundu2019,
abstract = {We propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. Although inspired by multi-hop reasoning over knowledge graphs, our proposed approach operates directly over unstructured text. It generates potential paths through passages and scores them without any direct path supervision. The proposed model, named PathNet, attempts to extract implicit relations from text through entity pair representations, and compose them to encode each path. To capture additional context, PathNet also composes the passage representations along each path to compute a passage-based representation. Unlike previous approaches, our model is then able to explain its reasoning via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1811.01127},
author = {Kundu, Souvik and Khot, Tushar and Sabharwal, Ashish and Clark, Peter},
doi = {10.18653/v1/p19-1263},
eprint = {1811.01127},
file = {:Users/roy/Desktop/P19-1263.pdf:pdf},
mendeley-groups = {MultiHop{\_}QA},
pages = {2737--2747},
title = {{Exploiting Explicit Paths for Multi-hop Reading Comprehension}},
year = {2019}
}

@article{Song2018,
abstract = {Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph neural networks, namely graph convolutional network (GCN) and graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our method performs better than all published results on these datasets.},
archivePrefix = {arXiv},
arxivId = {1809.02040},
author = {Song, Linfeng and Wang, Zhiguo and Yu, Mo and Zhang, Yue and Florian, Radu and Gildea, Daniel},
eprint = {1809.02040},
file = {:Users/roy/Documents/1809.02040.pdf:pdf},
title = {{Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks}},
url = {http://arxiv.org/abs/1809.02040},
year = {2018}
}

@article{Huang2019,
abstract = {Aspect level sentiment classification aims to identify the sentiment expressed towards an aspect given a context sentence. Previous neural network based methods largely ignore the syntax structure in one sentence. In this paper, we propose a novel target-dependent graph attention network (TD-GAT) for aspect level sentiment classification, which explicitly utilizes the dependency relationship among words. Using the dependency graph, it propagates sentiment features directly from the syntactic context of an aspect target. In our experiments, we show our method outperforms multiple baselines with GloVe embeddings. We also demonstrate that using BERT representations further substantially boosts the performance.},
annote = {homogenous, no edge label information},
archivePrefix = {arXiv},
arxivId = {1909.02606},
author = {Huang, Binxuan and Carley, Kathleen},
doi = {10.18653/v1/d19-1549},
eprint = {1909.02606},
file = {:Users/roy/Desktop/D19-1549.pdf:pdf},
mendeley-groups = {GNN},
pages = {5468--5476},
title = {{Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks}},
year = {2019}
}

@article{Zhang2019,
abstract = {Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a mechanism to account for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we propose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models, and further demonstrate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure.},
archivePrefix = {arXiv},
arxivId = {1909.03477},
author = {Zhang, Chen and Li, Qiuchi and Song, Dawei},
doi = {10.18653/v1/d19-1464},
eprint = {1909.03477},
file = {:Users/roy/Desktop/D19-1464.pdf:pdf},
mendeley-groups = {GNN},
number = {Limitation 2},
pages = {4567--4577},
title = {{Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks}},
year = {2019}
}

@article{Guo2019,
abstract = {Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.},
annote = {GCN with edge weight},
archivePrefix = {arXiv},
arxivId = {1906.07510},
author = {Guo, Zhijiang and Zhang, Yan and Lu, Wei},
doi = {10.18653/v1/p19-1024},
eprint = {1906.07510},
file = {:Users/roy/Desktop/P19-1024.pdf:pdf},
mendeley-groups = {GNN},
pages = {241--251},
title = {{Attention Guided Graph Convolutional Networks for Relation Extraction}},
year = {2019}
}

@article{Zhu2019,
author = {Zhu, Hao},
file = {:Users/roy/Desktop/P19-1128.pdf:pdf},
mendeley-groups = {GNN},
pages = {1331--1339},
title = {{Graph Neural Networks with Generated Parameters for Relation Extraction}},
year = {2019}
}

@article{DBLP:journals/corr/KipfW16,
  author    = {Thomas N. Kipf and
               Max Welling},
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1609.02907},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.02907},
  archivePrefix = {arXiv},
  eprint    = {1609.02907},
  timestamp = {Mon, 13 Aug 2018 16:48:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KipfW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{weston2015towards,
  abstract = {One long-term goal of machine learning research is to produce methods that
are applicable to reasoning and natural language, in particular building an
intelligent dialogue agent. To measure progress towards that goal, we argue for
the usefulness of a set of proxy tasks that evaluate reading comprehension via
question answering. Our tasks measure understanding in several ways: whether a
system is able to answer questions via chaining facts, simple induction,
deduction and many more. The tasks are designed to be prerequisites for any
system that aims to be capable of conversing with a human. We believe many
existing learning systems can currently not solve them, and hence our aim is to
classify these tasks into skill sets, so that researchers can identify (and
then rectify) the failings of their systems. We also extend and improve the
recently introduced Memory Networks model, and show it is able to solve some,
but not all, of the tasks.},
  added-at = {2019-03-21T13:38:40.000+0100},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van MerriÃ«nboer, Bart and Joulin, Armand and Mikolov, Tomas},
  biburl = {https://www.bibsonomy.org/bibtex/23cfceffe925eb4409fbdd49b221e2b83/kirk86},
  description = {[1502.05698] Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks},
  interhash = {2da251aba056c9141888cabdde7ed986},
  intrahash = {3cfceffe925eb4409fbdd49b221e2b83},
  keywords = {deep-learning memory},
  note = {cite arxiv:1502.05698},
  timestamp = {2019-03-21T13:38:40.000+0100},
  title = {Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks},
  url = {http://arxiv.org/abs/1502.05698},
  year = 2015
}

@inproceedings{khashabi-etal-2018-looking,
    title = "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
    author = "Khashabi, Daniel  and
      Chaturvedi, Snigdha  and
      Roth, Michael  and
      Upadhyay, Shyam  and
      Roth, Dan",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1023",
    doi = "10.18653/v1/N18-1023",
    pages = "252--262",
    abstract = "We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1{\%}. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.",
}

@article{DBLP:journals/corr/abs-1809-02789,
  author    = {Todor Mihaylov and
               Peter Clark and
               Tushar Khot and
               Ashish Sabharwal},
  title     = {Can a Suit of Armor Conduct Electricity? {A} New Dataset for Open
               Book Question Answering},
  journal   = {CoRR},
  volume    = {abs/1809.02789},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.02789},
  archivePrefix = {arXiv},
  eprint    = {1809.02789},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-02789.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Gardner2017AllenNLP,
  title={AllenNLP: A Deep Semantic Natural Language Processing Platform},
  author={Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord
    and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and
    Michael Schmitz and Luke S. Zettlemoyer},
  year={2017},
  Eprint = {arXiv:1803.07640},
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@article{DBLP:journals/corr/ChungGCB14,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@unpublished{spacy2,
    AUTHOR = {Honnibal, Matthew and Montani, Ines},
    TITLE  = {{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing},
    YEAR   = {2017},
    Note   = {To appear}
}

@inproceedings{clark2020electra,
  title = {{ELECTRA}: Pre-training Text Encoders as Discriminators Rather Than Generators},
  author = {Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
  booktitle = {ICLR},
  year = {2020},
  url = {https://openreview.net/pdf?id=r1xMH1BtvB}
}

@inproceedings{hashimoto-etal-2017-joint,
    title = "A Joint Many-Task Model: Growing a Neural Network for Multiple {NLP} Tasks",
    author = "Hashimoto, Kazuma  and
      Xiong, Caiming  and
      Tsuruoka, Yoshimasa  and
      Socher, Richard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1206",
    doi = "10.18653/v1/D17-1206",
    pages = "1923--1933",
    abstract = "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task{'}s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.",
}

@inproceedings{hewlett-etal-2016-wikireading,
    title = "{W}iki{R}eading: A Novel Large-scale Language Understanding Task over {W}ikipedia",
    author = "Hewlett, Daniel  and
      Lacoste, Alexandre  and
      Jones, Llion  and
      Polosukhin, Illia  and
      Fandrianto, Andrew  and
      Han, Jay  and
      Kelcey, Matthew  and
      Berthelot, David",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1145",
    doi = "10.18653/v1/P16-1145",
    pages = "1535--1545",
}
