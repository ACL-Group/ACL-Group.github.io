\section{Introduction}
The paraphrase of a sentence should retain the meaning but change the word expression of the original sentence. Paraphrase generation plays an important role in many downstream tasks, such as question answering, machine translation, and information retrieval \citep{hu2019improved}. 

% Domain adaptation is a common requirement in supervised paraphrase generation 
% since 
Most existing parallel datasets for paraphrase generation are 
domain-specific. Quora and WikiAnswers \citep{fader2013paraphrase} datasets 
only contain questions; sentences in MSCOCO \citep{lin2014microsoft} dataset 
are mostly descriptions for objects since they are from captions of images; 
and PPDB \citep{ganitkevitch2013ppdb} contains phrases rather than sentences. 
The performance of a supervised model declines seriously when it comes to 
another domain \citep{li2019decomposable}. 
% \KZ{But you just said
% domain adaptation is a common requirement. Does that fix the problem or not?
% You left me wondering..} 
Therefore, unsupervised methods 
are often used for paraphrase generation. 
% \KZ{you sound like unsupervised
% methods should achieve better than unsupervised method. But in your
% results, your results only matches supervised methods with DA in some of
% the datasets. This seems to suggest that unsupervised methods are not
% as strong as supervised. Seems contradicting.}

% \KZ{What are the common approaches in previous unsupervised methods? What 
% are the disadvantages? Did they not capture the human intuition when
% doing paraphrase?}
% 
% \KZ{The following is only one possible human paraphrase process.
% It is unconfirmed so say it with care. Instead of ``hidden meaning'',
% maybe say ``underlying semantics''.}

Existing unsupervised methods are mostly based on the variation of 
words and phrases and can hardly change the structure of the whole sentence. 
For example, \citet{liu2019unsupervised} proposed a method using simulated 
annealing for words and phrases, and \citet{miao2019cgmh} used Metropolis Hastings in the word space. 

In this paper, we propose a novel unsupervised paraphrase generation 
framework that can alter the expression at the sentence level. 
We extract the underlying semantics from the original sentence and 
extend it into a new sentence. Information loss may occur when 
extracting semantics. To retain more information of the original sentence, 
we extract in two different directions and combine the extracted 
information in a hybrid decoder 
(\secref{sec:joint}) to generate paraphrases.

The first expression of underlying semantics is a word set extracted 
from the original sentence. Bag of words are great carrier of information, 
as they harbor the central idea without syntactic constraints. 
People can generate different sentences of the same meaning from 
the same word set. Table~\ref{para-example} shows an example of such 
paraphrase sentences. We construct a word set from the 
original sentence and extend the word set into a complete sentence 
with a set-to-sequence (set2seq) model (\secref{sec:set2seq}), 
which is adapted from the well known sequence-to-sequence (seq2seq) model by ignoring the sequential information from the input sequence.

% \KZ{Don't have to say Chinese specifically here. Can be any language.}
The second carrier of semantics is the translation of the 
original sentence into another language. 
Semantics is preserved when the translation is translated back to the 
original language. This is known as back-translation~\citep{wieting2017paranmt}. 
We integrate the decoding part of the set2seq model and 
the back-translation model to generate paraphrases.

\begin{table}[th]
\small
\centering
\begin{tabular}{l}
\hline 
word set: \textbf{(man, sit, bike, bench)} \\
\hline
A \textit{\color{red}man} is \textit{\color{red}sitting} on a \textit{\color{red}bench} next to a \textit{\color{red}bike} \\
A \textit{\color{red}man} is \textit{\color{red}sitting} on a \textit{\color{red}bench} next to a \textit{\color{red}bicycle} \\
A \textit{\color{red}man sits} on a \textit{\color{red}bench} by a \textit{\color{red}bike} \\
\textit{\color{red}Man sitting} on a \textit{\color{red}bench} near a personal \textit{\color{red}bicycle} \\
A \textit{\color{red}man} is \textit{\color{red}sitting} on a \textit{\color{red}bench} with a \textit{\color{red}bike} \\
\hline
\end{tabular}
\caption{\label{para-example} An example of paraphrases formed
from the same set of words in red.} 
\end{table}

We evaluate our framework on four paraphrasing datasets, namely Quora, WikiAnswers, MSCOCO, and Twitter \citep{lan2017continuously}, and achieve the state-of-the-art accuracies compared to existing unsupervised models. 

% \KZ{This para sounds a bit repeat from paragraph 1. The connection is
% a bit strange. Consider restructuring it.}
Domain-adaptation is to train the model with parallel paraphrasing pairs 
in the source domain and fine-tune the model with non-parallel sentences 
in the target domain, which can also be considered unsupervised from 
the perspective of the target domain. Therefore, we also compare 
our method with domain-adaptation supervised methods with  in 
Quora and WikiAnswers. The comparison is not on all four datasets 
because results of the SOTA method~\citep{li2019decomposable} are only 
available on Quora and WikiAnswers.

% \KZ{Instead of cross-domain, call it ``common-domain'' or ``general-domain'',
% to go inline with the name of this model?}
We also train the set2seq model on a big common-domain dataset and test it on these four datasets, and still obtain decent results. We call the set2seq model trained from the big common-domain dataset ``set2seq-common'', it can be applied to any domain when there is no in-domain data to train an in-domain set2seq model.

% Both unsupervised methods and supervised methods with domain-adaptation require the non-parallel data from the target domain. However, there is not 
% always enough data for training. For example, the Twitter dataset only has 
% 110K non-parallel training data. To this end, we trained the set2seq model on a big cross-domain dataset, name it ``set2seq-common''. We use the set2seq-common instead of the domain-specific set2seq models on four datasets, and still receive decent results.

We propose an application of our paraphrase generator: 
to augment the training data of 
Neural Machine Translation (NMT) between low-resource languages and English. 
We paraphrase the English sentences in the parallel training pairs with set2seq-common and 
improve the BLEU score of X-to-English translation by 1.53 to 2.17, 
where X is a low-resource language.

In summary, the main contributions of our work are:
\begin{itemize}
\item We propose a novel framework for unsupervised paraphrasing at 
the sentence level and achieve state-of-the-art accuracies on 
four benchmark datasets compared with existing unsupervised methods.
\item We show that our framework outperforms most domain-adapted supervised methods including the current state-of-the-art method on two benchmark datasets.
\item We apply our method to augment the training data of low-resource
translation tasks and obtain significant improvement in translation quality.
\end{itemize}
