\section{Related Work}
% \KZ{The following two para can be significantly shorten.}
% \KZ{Add a preamble here to say why you include the two parts: paraphrase gen
% and backtranslation. Especially why you wanna talk about backtranslation.
% For each of the following sections, you need to put our approach in the context
% of other works and compare and contrast always. Don't just say what other ppl do
% without relating to us!}
We show the relevant work of paraphrase generation from the aspects of supervised and unsupervised methods.

For supervised methods, \citet{prakash2016neural} proposed ``stacked residual LSTM'' as the earliest deep-learning method in this topic, seq2seq models like transformer~\citep{vaswani2017attention} and MTL~\citep{domhan2017using} outperformed many methods due to the advantages of their model structures. We include these well-known methods in our baseline.  \citet{li2019decomposable} proposed the current state-of-the-art method DNPG and revealed the disadvantage of supervised methods when it comes to domain adaptation. \citet{fu2019paraphrase} also used BOW in their work, but they chose only a few important words to aid a seq2seq model, which is much different from ours. Other methods include VAE-SVG~\citep{gupta2018deep} and transformer-pb~\citep{wang2019task}, but these three methods perform worse than DNPG and have no discussion about domain adaptation, so we do not include them in our baselines.

For unsupervised methods, VAE\citep{kingma2013auto} can be used on this task directly, so it is often considered as one of the baselines. There are two mainstream approaches in recent works, one based on lexical expression and the other based on back-translation. For the former, \citet{miao2019cgmh} used Metropolis-Hastings Sampling to generate paraphrases, \citet{liu2019unsupervised} generated paraphrases with Simulated Annealing, both of them were the best at their times. Unsupervised methods usually need common word-level knowledge to help them deal with the relationship between words, for these two methods, they used GloVE \citep{pennington2014glove}, and for our method, we are using WordNet. We compare our framework with these two methods to show changes on the sentential level are more reliable than changes on the lexical level. For the latter, \citet{wieting2017paranmt} created a 50M parallel dataset for paraphrases with back-translation, \citet{hu2019parabank} used lexically-constrained to improve the diversity of generated paraphrase, and their work is proved to be useful for many downstream tasks like Natural Language Inference \citep{hu2019improved}. We compare our framework with these two methods since they are also using back-translation.

% This papers concerns a method that combines the decoding of set2seq and 
% backtranslation. The backtranslation is a common method in both NMT tasks 
% and paraphrase generation. This section thus discuss previous work on
% both paraphrase generation and back-translation. 


% \paragraph{Paraphrase generation. } 
% \KZ{Here say why we are comparing with some of the methods and not others.
% Say less on the supervised methods if they are not so related.}
% The earliest work on paraphrase generation can be traced back to 1983, 
% which was based on hard-code rules~\citep{mckeown1983paraphrasing}. 
% Later works were based on lexical\citep{kozlowski2003generation}, 
% grammar\citep{ellsworth2007mutaphrase} and 
% other statistical features~\citep{zhao2009application}. 
% After stacked residual LSTM\citep{prakash2016neural}, 
% deep neural networks have become the major approaches. 
% VAE\citep{kingma2013auto} and transformer\citep{vaswani2017attention} 
% are common baselines for unsupervised and supervised methods of 
% paraphrase generation. For supervised methods, 
% \citet{wang2019task} replaced the encoder of a transformer with three 
% different encoders that stand for roles, frames, and tokens. 
% \citet{li2019decomposable} divide the task into phrase-level and 
% sentence-level and \citet{gupta2018deep} combine the idea of VAE and 
% seq2seq model. All of these methods achieved the state-of-the-art 
% results of their times. 
% Other works attempted unsupervised domain adaptation of their 
% supervised model\citep{li2019decomposable, gulcehre2015using, domhan2017using}. 
% For unsupervised methods, 
% \citet{miao2019cgmh} used Metropolis-Hastings Sampling to generate paraphrases, \citet{liu2019unsupervised} generated paraphrases with Simulated Annealing, both of them were the best at their times. 

% \paragraph{Back translation.} 
% \KZ{You have to say why we picked this particular way of backtranslation.
% Why not some other methods?}
% We are using backtranslation as a way of retaining semantics. 
% It is widely used in data argumentation for machine translation 
% and unsupervised paraphrase generation. In 2004, Statistical Machine Translation(SMT) is used to generate paraphrases\citep{quirk2004monolingual}, \citet{zhao2008combining} improved the method by using multiple resources. In 2015, back translation was used to argument training data for NMT tasks\citep{sennrich2015improving}, Wieting and Gimpel created a 50M parallel dataset for paraphrases with back-translation\citep{wieting2017paranmt}, lexically-constrained is used for improving the diversity of generated paraphrase \citep{hu2019parabank} and their work is proved to be useful for many downstream tasks like Natural Language Inference and Question Answering \citep{hu2019improved}
