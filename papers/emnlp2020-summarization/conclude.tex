\section{Conclusion}
\label{sec:conclude}
%We find the inaccurate alignment problem in the
%state-of-the-art abstractive summarization.
To enhance the alignment between 
documents and summaries in ext-abs framework,
we propose a set-level matching heuristics to
extract pseudo summary as training set.
We introduce a new ext-abs framework that use RL to connect
keyword-based extractor and abstractor with pretrained models together.
%first
%and then summarizing the selected ones 
%by ext-abs framework.
%which 
%we respectively add HIBERT and BART 
%into our extractor and abstractor 
%because of their benefits to natural language understanding
%and generation.
Our model outperforms the SOTA methods on CNN/Daily Mail dataset
and performs better to DUC-2002 than BART.
%The summaries generated by our model are more readable.
Besides, our models are faster and occupy less memory than previous pretrained models 
during training and testing.
Keep improving the extractor and the relation between extractor and abstractor is our future work.
%In the future, we will improve the extractor and strengthen
%the relation between extractor and abstractor.
\cut{%%%%%%%%%%%
We find the inaccurate alignment problem in the
state-of-the-art abstractive summarization.
To tackle this problem, we adopt the Extractor-Abstractor framework
to deal with the abstractive summarization,
which extracting salient sentences (pseudo reference summary) from the source first
and then summarizing the selected ones.
The pseudo reference summaries with high quality can enhance the alignment of Enc-Dec model.
Thus, we present a novel topic-level matching heuristics
to obtain more aligned and accurate pseudo reference summaries from the source 
and compose the new training datasets for extractor and abstractor. 
We propose a keyword-based extractor with combinational loss, 
which reduce the redundant information of inputs of abstractor and select more relevant 
sentences from the source.
The extractor and abstractor are connected by reinforcement learning with topic-level rewards.
The alignment between inputs of encoder and decoder in abstractor are enhanced 
by our proposed extractor.
Moreover, we respectively add HiBERT and BART into our extractor and abstractor.
%because of their ability in natural language understanding.
We make the inputs of the model shorter by extract salient sentences
and run the extractor and abstractor with parallel decoding at the same time.
So our models are faster and occupy less memory than previous pretrained models 
during training and testing. 
Our model outperforms the state-of-the-art methods on CNN/Daily Mail dataset.
The summaries generated by our model are more readable.
}%%%%%%%%%%%%%%
