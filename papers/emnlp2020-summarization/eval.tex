\section{Evaluation}
\label{sec:eval}
%In this section, we introduce the experimental setup
%and analyze the performance of different models.
%All of the data and source code
%can be downloaded from http://202.120.38.146/sumrep.}.

\subsection{Datasets}
CNN/Daily Mail~\cite{HermannKGEKSB15}
is a popular summarization dataset, 
which contains 286,817 training pairs,
13,368 validation pairs and 11,487 test pairs.
We follow Nallapati~\shortcite{NallapatiZSGX16} in data preprocessing
and use the non-anonymized version as See et al~\shortcite{SeeLM17}.

The DUC-2002 dataset contains 567 document-summary pairs for single-document summarization.
We summarize the DUC documents by model trained on CNN/Daily Mail
for testing generalization of our models.

\subsection{Implementation Details}
We set batch size as $32$ for all training processes.
All models are optimized by Adam optimizer.
In extractor, we take a single-layer CNN model with 100 dimensions
as keywords encoder whose input are $128$-dimensional words embeddings randomly initialized.
For pointer network decoder, we employ LSTM
models with 256-dimensional hidden states.
We implement our document encoders, BiLSTM encoder and HIBERT encoder,
as described by Chen~\shortcite{FastAbs18} and Zhang ~\shortcite{HiBert19}.
We fine-tune HIBERT encoder with {\em learning rate} ({\em lr}) $5e-5$ and warmup steps $4,000$.
We set $\lambda_{c}=1.0$, 
$\lambda_{k} = 0.5$,
$\lambda_{s} = 0.5$ (Eq. \ref{func:loss}).
For abstractor, the {\em lr} of Base-Abs is $1e-03$.
We follow Lewise~\shortcite{BART19} in fine-tuning Pre-Abs (BART) 
on CNN/Daily Mail, $lr=3e-05$ and warmup $=500$.
We set {\em lr} of RL as $1e-04$. 
We set $\gamma_{sent}=0.5$, 
$\gamma_{set} = 1.0$,
$\gamma_{sum} = 1.0$ (Eq. \ref{func:reward}) with grid
search on validation set.
\footnote{The codes and data can be found in http://Anonymous}

%\subsection{Evaluation Metrics}
%In the experiments, 
%We use following evaluation metrics:

\subsubsection{Automatic Evaluation Metrics}
%\textbf{Automatic Metrics}
%We evaluate the performance of our method using {\em automatic metrics}
%and {\em human evaluaton}.
We compute two series of automatic metrics.
%\KZ{Make sure all the acronyms you defined are used later.}
%We compute the ROUGE recall between pseudo summaries and reference summaries.
%The more overlapped words, the higher AliAcc.
%\itemsep0em
%\begin{itemize}
%\item 

\textbf{ROUGE} scores (F1) include
%is the standard evaluation metric in summarization task,
ROUGE-1 (R-1), ROUGE-2 (R-2) and
ROUGE-L(R-L)~\cite{rouge}.

%\item 
\textbf{Automatic Alignment Accuracy} (autoAlign)
examines our set-level matching heuristics.
We train abstractor on pseudo summaries 
and reference summaries of training set.
%Then, we take the pseudo summaris of test set as input of abstractor
%to generate summaries and compute their ROUGE scores (autoAlign) to reference.
Then, we input pseudo summaris of test set to abstractor
to generate summaries and take the ROUGE scores of generated summaries with reference
as autoAlign.
The higher ROUGE scores, the more aligned training set.

%\end{itemize}

\subsubsection{Human Evaluation Metrics}
%\textbf{Human Evaluation}
We randomly select 50 samples
from CNN/Daily Mail dataset
and used the average score by two human annotators.
\footnote{
The Cohen's Kappa coefficient between annotators 
are $0.75$ (manAlign),
$0.64$ (KC)
and $0.81$ (Read), indicating agreement. 
}
%We record the average annotation score.
%Sentence-level method
%matches exactly one document sentence for each reference 
%sentence on the ROUGE-L recall.
%Summary-level method employs greedy search to obtain the best combination of document sentences 
%that maximizes ROUGE-2 F1 score with reference summary.
%\itemsep0em
%\begin{itemize}
%\item 

\textbf{Manual Alignment Accuracy} (manAlign).
We rank and score pseudo summaries with three matching scores
based on the informativeness and redundancy of pseudo summary with respect to reference,
i.e., better (2.0), equal (1.0) and worse (0.0). 

%\item 
\textbf{Keywords Coverage} (KC)
%is the percentage of generated keywords in reference keywords 
%for a generated-reference (gen-ref) summary pair.
reflects the accuracy of keywords in generated summary.
%The generated (reference) keywords are extracted from generated (reference) summary.
Given generated-reference summary pairs, we manually
extracts their keywords and sequence these keywords based on their locations in source. 
%In order to get keywords sequence, 
%we obtain all the words belonging to the extracted keywords and 
%put them in the order that they are in the summary.
The $KC$ is computed as the ROUGE-1 precision
between generated and reference keywords sequences.

%\item 
\textbf{Readability} (Read).
We rank summaries 
generated by our best model and that of
BART according to logical consistency with source document and informativeness,
The summary should be labelled as better, equally good (or bad).   
The Read includes the percentage of the number of summaries 
with different label to the total summaries.
%\end{itemize}

\subsection{Results}

\subsubsection{Alignment.}
%which extract the document sentence with the highest ROUGE score  
%for each reference sentence.
%However, the abstractive sentences are generated by summarizing multiple sentences of source.
%The sentence-level method cannot deal with the crossing information among multiple sentences.
%which is the feature of abstractive summarization.
%Summary-level methods
%select the best combination of document sentences
%that maximizes ROUGE score with reference summary. 
%As a result, the importance of each token in summary are regarded the same, causing information loss in pseudo summary.
As shown in \tabref{tab:align}, 
our set-level keyword-based matching heuristics outperforms 
sentence-level and summary-level methods, achieving the best manAlign score.
%Because our pseudo summaries contains more complete information of reference summaries.
As shown in \tabref{tab:align_exp}, 
the sentence-level and summary-level methods always
ignore crossing information among document sentences
and lose important information respectively. 
%Unlike sentence-level matching sentence one-to-one, 
The set-level method extracts the most aligned multi-sentence set 
for one or more reference sentences,
which can better align the reference sentences abstracted from multiple document sentences.
As set-level method is based on keywords, 
the pseudo summaries cover all keywords in reference summaries,
avoiding losing salient information.

\begin{table*}[ht!]
	\centering
	\scriptsize
	\begin{tabular}{|l|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{} &\multicolumn{2}{|c|}{R-1} &\multicolumn{2}{|c|}{R-2}& \multicolumn{2}{|c|}{R-L}&\\ 
		%\hline
		\cline{2-7}
		&  Base-Abs & Pre-Abs & Base-Abs & Pre-Abs & Base-Abs & Pre-Abs & manAlign\\
		\hline
		sentence-level & 49.70 & 50.64 & 26.63 & 27.60 & 46.94 & 47.59 & 1.64 \\
		\hline
		summary-level & 48.98 & 51.20& 26.85 &28.83&46.41&48.43& 1.92\\
		\hline
		set-level & \bf 50.2 & \bf 52.53 & \bf 27.64 & \bf 28.83 & \bf 29.88& \bf 49.12& \bf 2.46 \\
		\hline
	\end{tabular}
	\caption{The alignment accuracy of differet pseudo summaries. The ROUGE scores are autoAlign scores.}
	\label{tab:align}
\end{table*}

%To examine the automatic alignment accuracy of different pseudo summaries, we directly use the aligned pseudo summaries of training set and test set as the input to the abstractor. The results as listed in \tabref{tab:align}, showing the optimal scores for different alignment settings.
As we directly use the aligned pseudo summaries of training set and test set as the input to the abstractor, 
the results listed in \tabref{tab:align} show the optimal scores for different alignment 
settings.
%The results listed in \tabref{tab:align} show the optimal scores for different alignment settings.
%As for the automatic alignment accuracy,
%we generate summaries by abstractor models
%(Base-Abs and Pre-Abs) 
%training on competitive pseudo summaries and their corresponding reference summaries of training set.
%As shown in \tabref{tab:align},
%the ROUGE scores shows the optimal results of ext-abs framework based on different pseudo summaries
%because we use pseudo summaries of test set as input to generated abstractive summaries, which
%means that the extractor is perfect.
The best ROUGE scores between generated summaries and reference summaries
denote that the abstractor models 
can benefit from training on set-level pseudo summaries.
Thus, our proposed matching heuristics can produce more aligned training pairs
for generation and make abstractors better.
%Our propsed models are trained on pseudo summaries based on set-level keywords-enhanced
%matching heuristics in the following experiments.



\subsubsection{ROUGE scores}
We compare our proposed models with existing models on abstractive summarization.
%and show the effectiveness in terms of architectures.
The {\em Base-} and {\em Pre-} denote non-pretrained model
and pretrained model respectively.
Following previous work, we take reference summaries in CNN/Daily Mail as
the ground truth of extractive summaries and abstractive summaries. 

\textbf{Extractor.}
%The extractor used in our ext-abs framework
%consists of two encoders, document encoder and keywords encoder.
We train the extractor on source documents and pseudo summaries pairs.
%As our set-level pseudo summaries are obtained based on keywords,
%the keywords encoder (KE) is useful to guide 
%extractor to select more accurate sentences.
As shown in \tabref{tab:enc},
the basic models (Base-E and Pre-E) with only one document encoder 
have been improved on ROUGE scores by adding keyword encoder.
The ROUGE scores of extracted summaries generated by Pre-E
are higher
since the Pre-E is fine-tuned on a pretrained model which
can enhance the language modeling ability.
After adding combinational loss (CL), the ROUGE scores
become higher,
because the composition of CL is consistent with 
the extraction of pseudo summaries
and the structure of extractor.
Compared with Base-E, the Pre-E can capture
more information about the relationship between inputs of encoder and decoder,
including keywords information.
Therefore, the improvement of 
HIBERT document encoder (Pre-E)
is less than 
BiLSTM document encoder (Base-E).

\begin{table}[th!]
\scriptsize
\begin{center}
		\begin{tabular}{|l|c|c|c|}
		\hline
		Model &   R-1 & R-2 & R-L \\
		\hline
		Base-E &  37.04 & 16.57 & 33.81 \\
		Base-KE &  40.25 & 18.15 & 36.46 \\
		Base-KE$_{cl}$ & \bf 41.78 & \bf 18.95 & \bf 37.33 \\
		\hline
		Pre-E & 41.71 & 19.35 & 38.44 \\
		Pre-KE &  42.19 & 19.82 & 38.57 \\
		Pre-KE$_{cl}$ & \bf 43.16 & \bf 20.14 & \bf 39.66 \\
		\hline
		\end{tabular}
\caption{The ROUGE scores of extractive summaries.
}
\label{tab:enc}
\end{center}
\end{table}

\textbf{Abstractor.}
%We take existing Enc-Dec model
%as abstractor of our ext-abs framework.
We improve the abstractor by creating a new training set, pseudo summaries, 
which enhances the alignment between input of encoder and decoder.
\tabref{tab:align} shows that
the models training on set-level pseudo summaries
generate more accurate summaries.
The summaries generated by pretrained models achieve higher ROUGE scores
because of the better document representation.

\textbf{Combination of extractor and abstractor.}
%In order to generate the abstractive summaries from source document,
%we extract sentence by extractor (KE$_{CL}$) with keywords extractor and combinational loss
%followed by abstractor.
As shown in \tabref{tab:extabs}, we combine our extractor
and abstractor in different ways.
Compared with Pre-Abs,
the Base-Abs cannot abstract the extracted sentences effectively
and achieves worse ROUGE score than its connected extractors (as shown in \tabref{tab:enc}).
%As for extractor models, the Base-extractor makes 
%the summaries generated by
%its connected abstractors worse.
The ROUGE scores of summaries generated by Base-extractor with Pre-Abs are lower 
because the worse extractor brings noisy to its connected abstractor.
These results show that
the extractor is essential for ext-abs framework.

\begin{table}[ht!]
	\centering
	\scriptsize
	\begin{tabular}{|l|l|c|c|c|}
		\hline
		Ext. & Abs. & R-1 & R-2 & R-3\\
		\hline
		\multirow{2}{*}{Base-KE$_{cl}$} &Base-Abs & 38.09 & 16.61 & 35.64\\ 
		%\hline
		\cline{2-5}
		~  &  Pre-Abs & 38.70 & 19.27 & 36.23 \\
		\hline
		\multirow{2}{*}{Pre-KE$_{cl}$} &Base-Abs & 40.63 & 18.11 & 36.34 \\ 
		%\hline
		\cline{2-5}
		~  &  Pre-Abs & \bf 43.22 & \bf 20.93 & \bf 39.78 \\
		\hline
	\end{tabular}
	\caption{The ROUGE scores of summaries generated by combined extractor and abstractor.}
	\label{tab:extabs}
\end{table}

We use RL to connect extractor and abstractor,
which makes ext-abs framework an end-to-end trainable model.
We observe the changes of our weakest ext-abs model (Base-KE$_{cl}$-Abs)
and strongest model (Pre-KE$_{cl}$-Abs) after adding RL.

\begin{table}[th!]
\scriptsize
\begin{center}
\begin{tabular}{|l|l|l|l|}%{|p{7cm}|rl|}
\hline
\bf Model & R-1 & R-2 & R-L \\
\hline 
\multicolumn{4}{|c|}{\bf Extractive summarization }\\
\hline
lead-3~\cite{SeeLM17} & 40.34 & 17.70 & 36.57 \\
REFRESH~\cite{NarayanCL18} & 40.00 & 18.20 & 36.60 \\
HIBERT~\cite{HiBert19} & 42.37 & 19.95 & 38.83 \\
BERTSUM~\cite{LiuML19} & \bf 43.85 & \bf 20.34 & \bf 39.90 \\
Base-KE$_{cl}$ & 41.78 & 18.95 & 37.33 \\
Base-KE$_{cl}$-RL & 41.37 & 19.11 & 38.74 \\
Pre-KE$_{cl}$ &  43.16 & 20.14 & 39.66 \\
Pre-KE$_{cl}$-RL & 43.62 & 20.21 & 39.68 \\
\hline
\multicolumn{4}{|c|}{\bf Abstractive summarization }\\
\hline 
PointGen~\cite{SeeLM17} & 40.34 & 17.70 & 36.57 \\
FastAbs~\cite{FastAbs18} & 40.88 & 17.80 & 38.54 \\
Bottom-Up~\cite{GehrmannDR18} & 41.22 & 18.68 & 38.34 \\
BART~\cite{BART19}
\tablefootnote{Test BART on released model {\em bart.larg.cnn}
\url{https://github.com/pytorch/fairseq/tree/master/examples/bart}.}
& 43.25 & 21.09 & 39.63 \\
Pre-KE$_{cl}$-Abs & 38.70 & 19.27 & 36.23 \\
Base-KE$_{cl}$-Abs-RL & 39.66 & 20.08 & 36.61 \\
Pre-KE$_{cl}$-Abs & 43.22 & 20.93 & 39.78 \\
Pre-KE$_{cl}$-Abs-RL & \bf 43.30 & \bf 21.32 & \bf 40.25 \\
\hline
\end{tabular}
\caption{\label{tab:rouge} ROUGE scores on CNN/Daily Mail dataset.}
\end{center}
\end{table}

As shown in \tabref{tab:rouge},
RL can enhance extractor to select more accurate sentences.
The ROUGE scores of extractor extended by RL are improved.
Our best extractor (Pre-KE$_{cl}$-RL)
doesn't obtain highest ROUGE scores because the model
with highest score are pretrained on a larger corpus.
The RL bridges the backpropagation from abstractive summary
to source document. So the ROUGE-based combinational rewards
between generated summaries and reference summaries reflect the quality of generated summaries
and guide extractor to select correct sentences.
The higher ROUGE scores of ext-abs with RL also show
that the ext-abs model can benefit from a better extractor.
%the effectiveness of RL based on combinational rewards.
%that avoid abstractor training with irrelevant selected sentences
%and extractor with noisy reward.

As shown in \tabref{tab:rouge} and \tabref{tab:duc},
our strongest model with RL (Pre-KE$_{cl}$-Abs-RL) outperforms
the SOTA abstractive models on CNN/Daily Mail
and DUC-2002.
%which shows the better generalization of our model.
%The highest ROUGE scores of our model shows
This shows
that the ext-abs framework with our approaches are effective
and has a better generalization.
The abstractive models can be improved by locating the salience information.

\begin{table}[th!]
\scriptsize
\begin{center}
\begin{tabular}{|l|l|l|l|}%{|p{7cm}|rl|}
\hline
\bf Model & R-1 & R-2 & R-L \\
\hline 
BART & 43.47 & 19.84 & 35.58 \\
Pre-KE$_{cl}$-Abs-RL & \bf 44.46 & \bf 20.17 & \bf 36.46 \\
\hline
\end{tabular}
\caption{\label{tab:duc} ROUGE scores on DUC-2002 dataset.}
\end{center}
\end{table}


\textbf{Significance test.} 
%The ROUGE scores of our best model
%are better than the SOTA model (BART), 
%but they are close.  
We take t-test 
\citep{albert2017exploring} to
measure that the difference of ROUGE scores between our model 
and BART are significant or not. 
The p-values on ROUGE scores of BART and Pre-KE$_{cl}$-Abs-RL are:
2.25e-33 (R-1), 7.19e-48 (R-2) and 2.43e-12 (R-L).
All p-values are less than 0.05.
%The smaller p-value, the higher significant.
Thus, the improvement of our model on ROUGE scores is 
significant.% and reliable. 

\subsubsection{Readability.}
We compare the readability of our best model and the SOTA model.
As shown in \tabref{tab:read}, 
the summaries generated by our model and BART in label
{\em equal} make up a large proportion,
which denotes that both two models performs well on both benchmarks.
Our model generates
more summaries labelled as {\em Better}.
This means that our model improve BART by 
keyword-based extractor capturing salient and aligned information.
Accordingly, the KC score of our model is better.
Compared with summary of BART in \tabref{tab:example},
our generated summary is as follow:
\fbox{
\parbox{0.9\columnwidth}{
\scriptsize{
\textit{Set 1.} federal education minister smriti irani was visiting a fabindia store 
the tourist resort state of goa. she discovered a camera at the changing room.
authoroities discovered found it was able to take photos.
photos rom the store 's changing room. \\
\textit{Set 2.} the four store workers could spend three years in jail if convicted. 
}}}
\begin{table}[ht!]
	\centering
	\scriptsize
	\setlength{\tabcolsep}{1mm}{
	\begin{tabular}{|l|c|c|c|c|c|c|}
        \hline
		\multirow{2}{*}{} &\multicolumn{3}{|c|}{CNN/Dail Mal} & \multicolumn{3}{|c|}{DUC-2002} \\ 
        %\hline
        \cline{2-7}
		 &  Better & Equal & KC & Better & Equal & KC\\
        \hline
		BART & 0.17 & 0.56 & 0.39 & 0.12 & 0.60 & 0.32\\
        \hline
		Pre-KE$_{cl}$-Abs-RL & 0.25 & 0.56 & \bf 0.43 & 0.28 & 0.60 & \bf 0.38\\
        \hline
	\end{tabular}}
    \caption{The Read and KC of generated summaries.}
	\label{tab:read}
\end{table}


\subsubsection{Speed and Memory.}
As shown in \tabref{tab:eval_speed}, we evaluate our models on the speed and memory usage.
We compare our Base-KE$_{cl}$-Abs-RL and Pre-KE$_{cl}$-Abs-RL with
standard Enc-Dec model PointGen and SOTA model BART respectively
based on fune-tuning on the pretrained model or not, to be fair.
In \tabref{tab:eval_speed}(a), 
KE$_{cl}$-Abs-RL is almost $7$ times faster in total training time and occupies less memory than PointGen.
We cannot fine-tune BART on GPU RTX-2080ti due to out-of-memory.
We test BART based on the released pretrained model.
The Pre-KE$_{cl}$-Abs-RL performs much better than BART on speed and memory usage.
%To observe the decoding speed at test, we set batch size of testing as $1$
%and calculate the decoded summaries (words) per second.
In \tabref{tab:eval_speed}(b),
both of our proposed models can decode summaries (word) in faster speed
and occupy less memory.
%the decoding speed of Base-KE$_{cl}$-Abs-RL using less memory
%is $10$ times faster than PointGen.
%Pre-KE$_{CL}$-Abs-RL is also better than BART on speed and memory usage.
\begin{table}[th]
\centering
\scriptsize
\subtable[Training.]{
	    \begin{tabular}{|l|c|c|c|}
		\hline
        Model & T (h) & Epoch/h & M (G)\\
        \hline
        PointGen &  40.24 & 0.29 & 6.72 \\
        Base-KE$_{cl}$-Abs-RL & \bf 7.04 & \bf 1.57 & \bf 3.42\\
        \hline
        BART & - & - & OOM \\
        Pre-KE$_{cl}$-Abs-RL  & \bf 16.61 & \bf 0.30 & \bf 9.74 \\
		\hline
	    \end{tabular}
        }
\qquad
\subtable[Testing. (Batch size = 1)]{
	    \begin{tabular}{|l|c|c|c|c|}
		\hline
        Model & T (h) & summaries/s & tokens/s & M (G)\\
        \hline
        PointGen & 16.13 & 0.60 & 23.74 & 2.02\\
        Base-KE$_{cl}$-Abs-RL & \bf 1.60 & \bf 1.99 & \bf 69.82 & \bf 0.94 \\
        \hline
        BART & 31.28 & 0.1 & 14.28 & 10.08 \\
        Pre-KE$_{cl}$-Abs-RL  & \bf 4.63 & \bf 0.72 & \bf 68.93 & \bf 2.55\\
		\hline
	    \end{tabular}
        }
\caption{Total time (T), speed and memory usage (M) of models during training and testing
         on RTX-2080ti.} 
\label{tab:eval_speed}
\end{table}

%Abstractive models always suffer from low speed and large memory usage,
%since they have to encode long documents
%with the attention model looking at all encoded words
%at each time step of decoding.
Abstractive models have to encoder long documents with
attention model looking at all encoded tokens at each time step,
which causes low speed and large memmory usage.
As a point network, our extractor is faster than most abstractive models.
Our models first extract sentence sets from a source
and then input them to abstractor.
These inputs can be decoded in parallel, which speed up the model.
The average length of inputs is shortened from $780$ to $100$,
%The average length of inputs without extraction is $780$ and that of our extracted inputs is $100$. 
which reduces the memory usage.
%The parallel decoder speed up the models.

%Because the average length is shortened from $X$ (without extraction) to $X$ (with extraction)
%and the summaries can be generated in parallel. 



