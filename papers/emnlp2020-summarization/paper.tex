%
% File emnlp2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Improving Alignments in Neural Abstractive Summarization by Keyword-based Extractor-Abstractor Framework }

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{soul}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[ruled,vlined]{algorithm2e}
\urlstyle{same}

\usepackage{tablefootnote}
\usepackage{helvet}
\usepackage{courier}
\usepackage{color}
\usepackage{amsmath,amsfonts,amssymb,amsthm,amsopn}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{array}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{multirow}
\usepackage{subfigure}
\theoremstyle{definition}
\newtheorem{example}{Example}

\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\JQ}[1]{\textcolor{red}{JQ: #1}}
\newcommand{\YZ}[1]{\textcolor{red}{Yizhu: #1}}

\date{}

\begin{document}
\maketitle

\begin{abstract}
%\JQ{keyword-based or keyword-enhanced?}
%The encoder-decoder (Enc-Dec) generative models 
%are popular in abstractivate summarization.
%The attention mechanism in Enc-Dec models 
%utilize the alignment between the inputs of encoder and decoder 
%\KZ{Cut it down}
End-to-end neural abstractive summarization 
models have difficulty learning the latent alignment between 
source documents and summaries, because of the
vast disparity in length.
In this paper, we propose a keyword-based extractor
to select salient sentence sets from document 
and then use an abstractor to paraphrase these sentences, 
which are more aligned to 
the summary, to generate the final summary, in an extractor-abstractor framework.
%The extractor and abstractor are further connected by
%reinforcement learning.
The new extractor and abstractor are trained from pseudo-salient sentence sets
extracted from a specially designed heuristics.
%We present a noval approach to extracts salient sentences sets of 
%source as the groud truth of extractor's outputs and 
%abstractor's inputs for training.
% \JQ{In this paper, we use an extractor-abstractor framework. We propose a keyword-based extractor to select salient sentence groups from the source document first, which are better alighed to the summary. Then, these groups of sentences will be fed into abstract to generate the final abstractive summarizations.}
%by extracted sentences from source based on keywords coverage and the multi-sentence level (set-level)
%ROUGE score with reference summaries.
%Due to the operation from set-level to word-level, 
%our abstractor can paraphrase a pseudo reference summary in parallel
%that results in faster speed and occupies less memory than previous Enc-Dec models.
%In addition, we add pretrained models into our model.
Results show that the new model outperforms the state-of-the-art models
on CNN/Daily Maily and DUC-2002 using less training time and memory.


\cut{%%%%%%%%%%%%
The popular models used in abstractive summarization are 
encoder-decoder (Enc-Dec) models,
which utilize the alignment between the inputs of encoder and decoder 
to find the latent relationship between source documents and reference summaries.
As summarization is a task of creating a shorter text from a longer text,
optimizing the alignment between source and summary is a challenge.
In this paper, we adopt the Extractor-Abstractor framework
to deal with the abstractive summarization,
which extracting salient sentences (candidate summary) from the source first
and then summarizing the selected ones.
We present a novel topic-level matching criteria
to obtain candidate summaries that are more helpful for alignment.
%to obtain more aligned and accurate candidate summaries from the source during data preprocessing.
%and compose the new training datasets for extractor and abstractor. 
One candidate summary consists of multi-sentence sets on different topics.
%The candidate summaries with high quality can enhance the alignment of Enc-Dec model.
We propose a topic-extractor, 
which is connected to abstractor by reinforcement learning with topic-level rewards.
Due to the operation from topic-level to word-level, 
our abstractor can decode a summary in parallel that results in faster speed and occupies less memory than previous Enc-Dec models.
In addition, we respectively add HiBERT and BART into our extractor and abstractor.
The results show that our model outperforms previous state-of-the-art on CNN/Daily Mail dataset.
%Due to the extraction, 
%the input sequences of encoder and decoder in abstractor become similar, 
%and their alignment is optimized.
%The previous work on this framework train extractor by using sentence-level labels, which are created heuristically using rule-based methods.
%The previous works on this framework train extractor on 
%source documents and pseudo reference summaries
%extracted from the source via 
%single-sentence-level or summary-level ROUGE score between sentences in source and true reference summaries,
%causing inaccurate training data and weak alignment. 
%To tackle this problem, we present a novel semantic-level matching criteria
%It is important to obtain candidate summaries, 
%which are the outputs of extractor and the inputs of abstractor.
%The candidate summaries with high quality can optimize the alignment between encoder and decoder.
%Both extractor and abstractor are trained on new training datasets.
%because of their ability on natural language understanding and natural language generation.
}%%%%%%%%%%%%%
\end{abstract}

\input{intro}
\input{approach}
\input{eval}
\input{related}
\input{conclude}


\bibliographystyle{acl_natbib}
\bibliography{bibfile}
\end{document}
