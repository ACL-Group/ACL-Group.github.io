\section{Related Work}
\label{sec:related}

%\subsection{Extractor-Abstractor Framework}
Recently, the extractor-abstractor (ext-abs) framework
become popular methods for abstractive summarization,
Unlike the end-to-end models
\cite{NallapatiZSGX16,SeeLM17,LiuLZ18,BART19,ProNet20} 
in abstractive summarization,
the ext-abs framework trains two Enc-Dec models, extractor and abstractor.
%In this framework,
The extractor captures salient content (pseudo summary) of source document, 
where the pseudo summary can be either sentence-level~\cite{TanWX17, hsu18,FastAbs18}
or summary-level~\cite{summlevel19, SharmaHHW19},
and then abstractor paraphrases the salient content to generate a summary.

%Both extractor and abstractor are encoder-decoder models.
%The salient content (pseudo summaries) 
%as the ground truth of extractor's outputs and abstractor's inputs
%should be first identified, usually at sentence-level or summary-level.
%The approaches at sentence-level~\cite{TanWX17, hsu18, KeyGuid18,FastAbs18} 
%employ greedy search to select the sentence from source that maximaze the
%ROUGE scores with each sentence in reference symmary.
%As for summary-level methods~\cite{summlevel19, SharmaHHW19}, 
%it is to find an optimal
%combination of sentences, which achieves the best ROUGE scores with reference summary.

Extractive models adopt hierarchical neural network as encoder
and pointer network as decoder~\cite{Cheng16,NallapatiAAAI17}.
It is extended with variant models, 
such as reinforcement learning~\cite{NarayanCL18} and
joint scoring~\cite{score18}. 
As transformer preforms excellent on language model, 
Liu et al.~\shortcite{LiuML19} and Zhang et al.~\shortcite{HiBert19} 
applies pretrained transformers to extractive summarization.

Abstractive models are based on sequence-to-sequence learning~\cite{SutskeverVL14,BahdanauCB14}.
The pointer-generator networks~\cite{SeeLM17} consisting of
copy mechanism and coverage model are the most popular 
baseline in abstractive summarization. 
The pretrained transformer language models have success in 
natural language processing.
%natural language understanding (NLU) and natural language generation (NLG). 
Through fine-tuning the pretrained models on summarization task,
the quality of generated summaries are improved~\cite{HiBert19,ZhongLWQH19}.
Keywords play an important role in abstractive summarization.
Zhou~\cite{ZhouYWZ17} uses a selective gate network to retain 
key information.
Li et al.~\shortcite{KeyGuid18} 
utilize keywords
to guide the summarization generation.

The reinforcement learning (RL) is always used to connect the extractor and abstractor together,
which makes an end-to-end trainable model.
Chen~\shortcite{FastAbs18} and Bae et al.~\shortcite{summlevel19} encourage extractor
to select sentences with high ROUGE scores by RL. 
Sharma et al.~\shortcite{SharmaHHW19} propose an entity-driven encoder and
utilize RL with coherent rewards to make abstractor generate readable summaries. 

%\subsection{Pretrained Models for Summarization}
%The pretrained transformer language models have success in natural language understanding (NLU)
%and natural language generation (NLG)
%NLU models are pretrained on unidirectional and bidirectional prediction.
%GPT~\cite{GPT18} employs a unidirectional transformer~\cite{attn17} to predict the sequence. 
%ELMo~\cite{ELMo18} learns two unidirectional language models of forward and backward.
%BERT~\cite{Bert19} uses a bidirectional transformer encoder 
%to predict the masked words.
%NLG models pretrain on sequence-to-sequence (seq2seq) models. 
%UniLM~\cite{UniLM19} is a multi-layer transformer network,
%including unidirctional, bidirectional and seq2seq language model. 
%BART~\cite{BART19} takes combines bidirectional transformer encoder 
%and auto-regressive transformer decoder.
%ProphetNet~\cite{ProNet20} trains on the transformer seq2seq model and 
%takes future n-gram prediction as self-supervised.
%Through fine-tuning the pretrained models or representations on summarizatoin task,
%the quality of generated summaries are improved~\cite{HiBert19,ZhongLWQH19,MatchSum}.

%In order to enhancing the alignment bewteen source documents and
%summaries in neural abstractive summarization,


%In this paper, we present a set-level matching heuristics to construct the pseudo summaries,
%which are more aligned to reference summaries.
%%and cover all keywords of the reference summaries.
%%Based on Extractor-Abstractor framework, 
%We propose a keyword-based extractor. Both extractor and abstractor are
%%Moreover, we fine-tune the extractor and abstractor 
%fine-tuned on pretrained models accompanied with RL and a reward considering ROUGE and keywords.
 
