@inproceedings{bleu,
  author    = {Kishore Papineni and
               Salim Roukos and
               Todd Ward and
               Wei{-}Jing Zhu},
  title     = {Bleu: a Method for Automatic Evaluation of Machine Translation},
  booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational
               Linguistics, July 6-12, 2002, Philadelphia, PA, {USA}},
  pages     = {311--318},
  publisher = {{ACL}},
  year      = {2002},
  url       = {https://www.aclweb.org/anthology/P02-1040/},
  doi       = {10.3115/1073083.1073135},
  timestamp = {Mon, 18 May 2020 15:12:39 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/PapineniRWZ02.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0909",
    pages = "65--72",
}



@inproceedings{sedoc-ungar-2020-item,
    title = "Item Response Theory for Efficient Human Evaluation of Chatbots",
    author = "Sedoc, Jo{\~a}o  and
      Ungar, Lyle",
    booktitle = "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.eval4nlp-1.3",
    doi = "10.18653/v1/2020.eval4nlp-1.3",
    pages = "21--33",
    abstract = "Conversational agent quality is currently assessed using human evaluation, and often requires an exorbitant number of comparisons to achieve statistical significance. In this paper, we introduce Item Response Theory (IRT) for chatbot evaluation, using a paired comparison in which annotators judge which system responds better to the next turn of a conversation. IRT is widely used in educational testing for simultaneously assessing the ability of test takers and the quality of test questions. It is similarly well suited for chatbot evaluation since it allows the assessment of both models and the prompts used to evaluate them. We use IRT to efficiently assess chatbots, and show that different examples from the evaluation set are better suited for comparing high-quality (nearer to human performance) than low-quality systems. Finally, we use IRT to reduce the number of evaluation examples assessed by human annotators while retaining discriminative power.",
}

@inproceedings{huang-etal-2020-grade,
    title = "{GRADE}: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems",
    author = "Huang, Lishan  and
      Ye, Zheng  and
      Qin, Jinghui  and
      Lin, Liang  and
      Liang, Xiaodan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.742",
    doi = "10.18653/v1/2020.emnlp-main.742",
    pages = "9230--9240",
    abstract = "Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.",
}

@inproceedings{mesgar-etal-2020-dialogue,
    title = "Dialogue Coherence Assessment Without Explicit Dialogue Act Labels",
    author = {Mesgar, Mohsen  and
      B{\"u}cker, Sebastian  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.133",
    doi = "10.18653/v1/2020.acl-main.133",
    pages = "1439--1450",
    abstract = "Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment. We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code.",
}

@inproceedings{yuma-etal-2020-ubleu,
    title = "u{BLEU}: Uncertainty-Aware Automatic Evaluation Method for Open-Domain Dialogue Systems",
    author = "Yuma, Tsuta  and
      Yoshinaga, Naoki  and
      Toyoda, Masashi",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-srw.27",
    doi = "10.18653/v1/2020.acl-srw.27",
    pages = "199--206",
    abstract = "Because open-domain dialogues allow diverse responses, basic reference-based metrics such as BLEU do not work well unless we prepare a massive reference set of high-quality responses for input utterances. To reduce this burden, a human-aided, uncertainty-aware metric, 螖BLEU, has been proposed; it embeds human judgment on the quality of reference outputs into the computation of multiple-reference BLEU. In this study, we instead propose a fully automatic, uncertainty-aware evaluation method for open-domain dialogue systems, 蠀BLEU. This method first collects diverse reference responses from massive dialogue data and then annotates their quality judgments by using a neural network trained on automatically collected training data. Experimental results on massive Twitter data confirmed that 蠀BLEU is comparable to 螖BLEU in terms of its correlation with human judgment and that the state of the art automatic evaluation method, RUBER, is improved by integrating 蠀BLEU.",
}

@inproceedings{sato-etal-2020-evaluating,
    title = "Evaluating Dialogue Generation Systems via Response Selection",
    author = "Sato, Shiki  and
      Akama, Reina  and
      Ouchi, Hiroki  and
      Suzuki, Jun  and
      Inui, Kentaro",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.55",
    doi = "10.18653/v1/2020.acl-main.55",
    pages = "593--599",
    abstract = "Existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. We focus on evaluating response generation systems via response selection. To evaluate systems properly via response selection, we propose a method to construct response selection test sets with well-chosen false candidates. Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to the ground-truth response and (ii) those acceptable as appropriate responses. Through experiments, we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation, compared with widely used automatic evaluation metrics such as BLEU.",
}

@inproceedings{deriu-etal-2020-spot,
    title = "Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems",
    author = {Deriu, Jan  and
      Tuggener, Don  and
      von D{\"a}niken, Pius  and
      Campos, Jon Ander  and
      Rodrigo, Alvaro  and
      Belkacem, Thiziri  and
      Soroa, Aitor  and
      Agirre, Eneko  and
      Cieliebak, Mark},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.326",
    doi = "10.18653/v1/2020.emnlp-main.326",
    pages = "3971--3984",
    abstract = "The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot{'}s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work. The framework is released asa ready-to-use tool.",
}

@article{DBLP:journals/corr/abs-1906-09308,
  author    = {Asma Ghandeharioun and
               Judy Hanwen Shen and
               Natasha Jaques and
               Craig Ferguson and
               Noah Jones and
               {\`{A}}gata Lapedriza and
               Rosalind W. Picard},
  title     = {Approximating Interactive Human Evaluation with Self-Play for Open-Domain
               Dialog Systems},
  journal   = {CoRR},
  volume    = {abs/1906.09308},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.09308},
  archivePrefix = {arXiv},
  eprint    = {1906.09308},
  timestamp = {Thu, 27 Jun 2019 18:54:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-09308.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{deriu-cieliebak-2019-towards,
    title = "Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement",
    author = "Deriu, Jan Milan  and
      Cieliebak, Mark",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8654",
    doi = "10.18653/v1/W19-8654",
    pages = "432--437",
    abstract = "We present {``}AutoJudge{''}, an automated evaluation method for conversational dialogue systems. The method works by first generating dialogues based on self-talk, i.e. dialogue systems talking to itself. Then, it uses human ratings on these dialogues to train an automated judgement model. Our experiments show that AutoJudge correlates well with the human ratings and can be used to automatically evaluate dialogue systems, even in deployed systems. In a second part, we attempt to apply AutoJudge to improve existing systems. This works well for re-ranking a set of candidate utterances. However, our experiments show that AutoJudge cannot be applied as reward for reinforcement learning, although the metric can distinguish good from bad dialogues. We discuss potential reasons, but state here already that this is still an open question for further research.",
}

@inproceedings{pang-etal-2020-towards,
    title = "Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation",
    author = "Pang, Bo  and
      Nijkamp, Erik  and
      Han, Wenjuan  and
      Zhou, Linqi  and
      Liu, Yixian  and
      Tu, Kewei",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.333",
    doi = "10.18653/v1/2020.acl-main.333",
    pages = "3619--3629",
    abstract = "Open-domain dialogue generation has gained increasing attention in Natural Language Processing. Its evaluation requires a holistic means. Human ratings are deemed as the gold standard. As human evaluation is inefficient and costly, an automated substitute is highly desirable. In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues. Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) $n$-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency. The empirical validity of our metrics is demonstrated by strong correlations with human judgments. We open source the code and relevant materials.",
}

@article{gpt2,
  added-at = {2019-02-27T03:35:25.000+0100},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/2b30710316a8cfbae687672ea1f85c193/kirk86},
  description = {Language Models are Unsupervised Multitask Learners},
  interhash = {ce8168300081d74707849ed488e2a458},
  intrahash = {b30710316a8cfbae687672ea1f85c193},
  keywords = {learning multitask},
  timestamp = {2019-02-27T03:35:25.000+0100},
  title = {Language Models are Unsupervised Multitask Learners},
  url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
  year = 2018
}

@inproceedings{mehri-eskenazi-2020-usr,
    title = "{USR}: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation",
    author = "Mehri, Shikib  and
      Eskenazi, Maxine",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.64",
    doi = "10.18653/v1/2020.acl-main.64",
    pages = "681--707",
    abstract = "The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research. Standard language generation metrics have been shown to be ineffective for evaluating dialog models. To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog. USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog. USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0). USR additionally produces interpretable measures for several desirable properties of dialog.",
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhao-etal-2020-designing,
    title = "Designing Precise and Robust Dialogue Response Evaluators",
    author = "Zhao, Tianyu  and
      Lala, Divesh  and
      Kawahara, Tatsuya",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.4",
    doi = "10.18653/v1/2020.acl-main.4",
    pages = "26--33",
    abstract = "Automatic dialogue response evaluator has been proposed as an alternative to automated metrics and human evaluation. However, existing automatic evaluators achieve only moderate correlation with human judgement and they are not robust. In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training and pretrained (masked) language models. Experimental results demonstrate that the proposed evaluator achieves a strong correlation ({\textgreater} 0.6) with human judgement and generalizes robustly to diverse responses and corpora. We open-source the code and data in https://github.com/ZHAOTING/dialog-processing.",
}

@inproceedings{liu-etal-2016-evaluate,
    title = "How {NOT} To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
    author = "Liu, Chia-Wei  and
      Lowe, Ryan  and
      Serban, Iulian  and
      Noseworthy, Mike  and
      Charlin, Laurent  and
      Pineau, Joelle",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1230",
    doi = "10.18653/v1/D16-1230",
    pages = "2122--2132",
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
   publisher = {American Psychological Association},
   address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@article{DBLP:journals/corr/abs-1909-03087,
  author    = {Margaret Li and
               Jason Weston and
               Stephen Roller},
  title     = {{ACUTE-EVAL:} Improved Dialogue Evaluation with Optimized Questions
               and Multi-turn Comparisons},
  journal   = {CoRR},
  volume    = {abs/1909.03087},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.03087},
  archivePrefix = {arXiv},
  eprint    = {1909.03087},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-03087.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{roller2020recipes,
      title={Recipes for building an open-domain chatbot}, 
      author={Stephen Roller and Emily Dinan and Naman Goyal and Da Ju and Mary Williamson and Yinhan Liu and Jing Xu and Myle Ott and Kurt Shuster and Eric M. Smith and Y-Lan Boureau and Jason Weston},
      year={2020},
      eprint={2004.13637},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dinan2019second,
      title={The Second Conversational Intelligence Challenge (ConvAI2)}, 
      author={Emily Dinan and Varvara Logacheva and Valentin Malykh and Alexander Miller and Kurt Shuster and Jack Urbanek and Douwe Kiela and Arthur Szlam and Iulian Serban and Ryan Lowe and Shrimai Prabhumoye and Alan W Black and Alexander Rudnicky and Jason Williams and Joelle Pineau and Mikhail Burtsev and Jason Weston},
      year={2019},
      eprint={1902.00098},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{see2019makes,
      title={What makes a good conversation? How controllable attributes affect human judgments}, 
      author={Abigail See and Stephen Roller and Douwe Kiela and Jason Weston},
      year={2019},
      eprint={1902.08654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shuster2020dialogue,
      title={The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents}, 
      author={Kurt Shuster and Da Ju and Stephen Roller and Emily Dinan and Y-Lan Boureau and Jason Weston},
      year={2020},
      eprint={1911.03768},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{smith2020together,
      title={Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills}, 
      author={Eric Michael Smith and Mary Williamson and Kurt Shuster and Jason Weston and Y-Lan Boureau},
      year={2020},
      eprint={2004.08449},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2020dont,
      title={Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training}, 
      author={Margaret Li and Stephen Roller and Ilia Kulikov and Sean Welleck and Y-Lan Boureau and Kyunghyun Cho and Jason Weston},
      year={2020},
      eprint={1911.03860},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2020dialogpt,
      title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation}, 
      author={Yizhe Zhang and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},
      year={2020},
      eprint={1911.00536},
      archivePrefix={arXiv},
      primaryClass={cs.CL}

}

@article{finch2020towards,
  title={Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols},
  author={Finch, Sarah E and Choi, Jinho D},
  journal={arXiv preprint arXiv:2006.06110},
  year={2020}
}

@article{li2015diversity,
  title={A diversity-promoting objective function for neural conversation models},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1510.03055},
  year={2015}
}
@article{bao2020plato,
  title={Plato-2: Towards building an open-domain chatbot via curriculum learning},
  author={Bao, Siqi and He, Huang and Wang, Fan and Wu, Hua and Wang, Haifeng and Wu, Wenquan and Guo, Zhen and Liu, Zhibin and Xu, Xinchao},
  journal={arXiv preprint arXiv:2006.16779},
  year={2020}
} 
