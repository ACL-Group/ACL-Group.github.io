\section{Conclusion}
In this paper we studied a simple yet effective adapter-based visual grounding~(AVG) method for improving natural language understanding. We found existing VLP frameworks that learn entangled visiolinguistic representation in one shared parameter space are not guaranteed to have positive effects on pure-text tasks. Experimental results show that AVG not only exhibits superior efficiency in pretraining but can also adaptively incorporate visual knowledge when needed.
