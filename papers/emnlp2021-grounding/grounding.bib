@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{DBLP:journals/corr/abs-1907-11692,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  archivePrefix = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mpnet,
	title={MPNet: Masked and Permuted Pre-training for Language Understanding},
	author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
	journal={arXiv preprint arXiv:2004.09297},
	year={2020}
}

@article{voken,
  author    = {Hao Tan and
               Mohit Bansal},
  title     = {Vokenization: Improving Language Understanding with Contextualized,
               Visual-Grounded Supervision},
  journal   = {CoRR},
  volume    = {abs/2010.06775},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.06775},
  archivePrefix = {arXiv},
  eprint    = {2010.06775},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-06775.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{macd,
    title = "Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning",
    author = "Cui, Wanyun  and
      Zheng, Guangyu  and
      Wang, Wei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.444",
    doi = "10.18653/v1/2020.emnlp-main.444",
    pages = "5511--5520",
    abstract = "We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.",
}

@article{oscar,
  author    = {Xiujun Li and
               Xi Yin and
               Chunyuan Li and
               Pengchuan Zhang and
               Xiaowei Hu and
               Lei Zhang and
               Lijuan Wang and
               Houdong Hu and
               Li Dong and
               Furu Wei and
               Yejin Choi and
               Jianfeng Gao},
  title     = {Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks},
  journal   = {CoRR},
  volume    = {abs/2004.06165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.06165},
  archivePrefix = {arXiv},
  eprint    = {2004.06165},
  timestamp = {Thu, 23 Apr 2020 13:59:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-06165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lxmert,
    title = "{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers",
    author = "Tan, Hao  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1514",
    doi = "10.18653/v1/D19-1514",
    pages = "5100--5111",
    abstract = "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22{\%} absolute (54{\%} to 76{\%}). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert",
}

@article{uniter,
  author    = {Yen{-}Chun Chen and
               Linjie Li and
               Licheng Yu and
               Ahmed El Kholy and
               Faisal Ahmed and
               Zhe Gan and
               Yu Cheng and
               Jingjing Liu},
  title     = {{UNITER:} Learning UNiversal Image-TExt Representations},
  journal   = {CoRR},
  volume    = {abs/1909.11740},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.11740},
  archivePrefix = {arXiv},
  eprint    = {1909.11740},
  timestamp = {Sat, 23 Jan 2021 01:12:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11740.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kadapter,
  author    = {Ruize Wang and
               Duyu Tang and
               Nan Duan and
               Zhongyu Wei and
               Xuanjing Huang and
               Jianshu Ji and
               Guihong Cao and
               Daxin Jiang and
               Ming Zhou},
  title     = {K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters},
  journal   = {CoRR},
  volume    = {abs/2002.01808},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.01808},
  archivePrefix = {arXiv},
  eprint    = {2002.01808},
  timestamp = {Wed, 12 Feb 2020 13:04:16 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-01808.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vqa,
  author    = {Yash Goyal and
               Tejas Khot and
               Douglas Summers{-}Stay and
               Dhruv Batra and
               Devi Parikh},
  title     = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding
               in Visual Question Answering},
  journal   = {CoRR},
  volume    = {abs/1612.00837},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.00837},
  archivePrefix = {arXiv},
  eprint    = {1612.00837},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalKSBP16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vcr,
  author    = {Rowan Zellers and
               Yonatan Bisk and
               Ali Farhadi and
               Yejin Choi},
  title     = {From Recognition to Cognition: Visual Commonsense Reasoning},
  journal   = {CoRR},
  volume    = {abs/1811.10830},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.10830},
  archivePrefix = {arXiv},
  eprint    = {1811.10830},
  timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-10830.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{itr,
  author    = {Kuang{-}Huei Lee and
               Xi Chen and
               Gang Hua and
               Houdong Hu and
               Xiaodong He},
  title     = {Stacked Cross Attention for Image-Text Matching},
  journal   = {CoRR},
  volume    = {abs/1803.08024},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.08024},
  archivePrefix = {arXiv},
  eprint    = {1803.08024},
  timestamp = {Thu, 10 Jan 2019 15:03:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-08024.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{groundedL1,
  author    = {Karl Moritz Hermann and
               Felix Hill and
               Simon Green and
               Fumin Wang and
               Ryan Faulkner and
               Hubert Soyer and
               David Szepesvari and
               Wojciech Marian Czarnecki and
               Max Jaderberg and
               Denis Teplyashin and
               Marcus Wainwright and
               Chris Apps and
               Demis Hassabis and
               Phil Blunsom},
  title     = {Grounded Language Learning in a Simulated 3D World},
  journal   = {CoRR},
  volume    = {abs/1706.06551},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.06551},
  archivePrefix = {arXiv},
  eprint    = {1706.06551},
  timestamp = {Mon, 13 Aug 2018 16:47:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HermannHGWFSSCJ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{groundedL2,
author = {Roy, Deb K. and Pentland, Alex P.},
title = {Learning words from sights and sounds: a computational model},
journal = {Cognitive Science},
volume = {26},
number = {1},
pages = {113-146},
keywords = {Language acquisition, Cross-modal, Sensor grounded, Learning, Computational model},
doi = {https://doi.org/10.1207/s15516709cog2601\_4},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2601_4},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog2601_4},
abstract = {Abstract This paper presents an implemented computational model of word acquisition which learns directly from raw multimodal sensory input. Set in an information theoretic framework, the model acquires a lexicon by finding and statistically modeling consistent cross-modal structure. The model has been implemented in a system using novel speech processing, computer vision, and machine learning algorithms. In evaluations the model successfully performed speech segmentation, word discovery and visual categorization from spontaneous infant-directed speech paired with video images of single objects. These results demonstrate the possibility of using state-of-the-art techniques from sensory pattern recognition and machine learning to implement cognitive models which can process raw sensor data without the need for human transcription or labeling.},
year = {2002}
}

@inproceedings{attentionIsAllYouNeed,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{resnext,
author = {Xie, Saining and Girshick, Ross and Dollar, Piotr and Tu, Zhuowen and He, Kaiming},
title = {Aggregated Residual Transformations for Deep Neural Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@article{infoNCE,
  author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
  title     = {Representation Learning with Contrastive Predictive Coding},
  journal   = {CoRR},
  volume    = {abs/1807.03748},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.03748},
  archivePrefix = {arXiv},
  eprint    = {1807.03748},
  timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mscoco,
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in
object recognition by placing the question of object recognition in the context
of the broader question of scene understanding. This is achieved by gathering
images of complex everyday scenes containing common objects in their natural
context. Objects are labeled using per-instance segmentations to aid in precise
object localization. Our dataset contains photos of 91 objects types that would
be easily recognizable by a 4 year old. With a total of 2.5 million labeled
instances in 328k images, the creation of our dataset drew upon extensive crowd
worker involvement via novel user interfaces for category detection, instance
spotting and instance segmentation. We present a detailed statistical analysis
of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide
baseline performance analysis for bounding box and segmentation detection
results using a Deformable Parts Model.},
  added-at = {2020-06-07T20:25:18.000+0200},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll√°r, Piotr},
  biburl = {https://www.bibsonomy.org/bibtex/2f4ab9f41677ee189a8cbc5a92cc0dc74/jan.hofmann1},
  description = {Microsoft COCO: Common Objects in Context},
  interhash = {a3a26c6fe173264a6b812e3b7b4119bd},
  intrahash = {f4ab9f41677ee189a8cbc5a92cc0dc74},
  keywords = {thema:pyramid_scene_parsing},
  note = {cite arxiv:1405.0312Comment: 1) updated annotation pipeline description and figures; 2) added new  section describing datasets splits; 3) updated author list},
  timestamp = {2020-06-07T20:25:18.000+0200},
  title = {Microsoft COCO: Common Objects in Context},
  url = {http://arxiv.org/abs/1405.0312},
  year = 2014
}

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  archivePrefix = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{csqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.",
}

@article{socialiqa,
  author    = {Maarten Sap and
               Hannah Rashkin and
               Derek Chen and
               Ronan LeBras and
               Yejin Choi},
  title     = {SocialIQA: Commonsense Reasoning about Social Interactions},
  journal   = {CoRR},
  volume    = {abs/1904.09728},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.09728},
  archivePrefix = {arXiv},
  eprint    = {1904.09728},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-09728.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cosmosqa,
    title = "Cosmos {QA}: Machine Reading Comprehension with Contextual Commonsense Reasoning",
    author = "Huang, Lifu  and
      Le Bras, Ronan  and
      Bhagavatula, Chandra  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1243",
    doi = "10.18653/v1/D19-1243",
    pages = "2391--2401",
    abstract = "Understanding narratives requires reading between the lines, which in turn, requires interpreting the likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset of 35,600 problems that require commonsense-based reading comprehension, formulated as multiple-choice questions. In stark contrast to most existing reading comprehension datasets where the questions focus on factual and literal understanding of the context paragraph, our dataset focuses on reading between the lines over a diverse collection of people{'}s everyday narratives, asking such questions as {``}what might be the possible reason of ...?'', or {``}what would have happened if ...'' that require reasoning beyond the exact text spans in the context. To establish baseline performances on Cosmos QA, we experiment with several state-of-the-art neural architectures for reading comprehension, and also propose a new architecture that improves over the competitive baselines. Experimental results demonstrate a significant gap between machine (68.4{\%}) and human performance (94{\%}), pointing to avenues for future research on commonsense machine comprehension. Dataset, code and leaderboard is publicly available at https://wilburone.github.io/cosmos.",
}

@article{piqa,
  author    = {Yonatan Bisk and
               Rowan Zellers and
               Ronan Le Bras and
               Jianfeng Gao and
               Yejin Choi},
  title     = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  journal   = {CoRR},
  volume    = {abs/1911.11641},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.11641},
  archivePrefix = {arXiv},
  eprint    = {1911.11641},
  timestamp = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-11641.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{openbookqa,
  author    = {Todor Mihaylov and
               Peter Clark and
               Tushar Khot and
               Ashish Sabharwal},
  title     = {Can a Suit of Armor Conduct Electricity? {A} New Dataset for Open
               Book Question Answering},
  journal   = {CoRR},
  volume    = {abs/1809.02789},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.02789},
  archivePrefix = {arXiv},
  eprint    = {1809.02789},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-02789.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@article{bookcorpus,
  author    = {Yukun Zhu and
               Ryan Kiros and
               Richard S. Zemel and
               Ruslan Salakhutdinov and
               Raquel Urtasun and
               Antonio Torralba and
               Sanja Fidler},
  title     = {Aligning Books and Movies: Towards Story-like Visual Explanations
               by Watching Movies and Reading Books},
  journal   = {CoRR},
  volume    = {abs/1506.06724},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.06724},
  archivePrefix = {arXiv},
  eprint    = {1506.06724},
  timestamp = {Mon, 13 Aug 2018 16:47:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhuKZSUTF15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{ig,
      title={Axiomatic Attribution for Deep Networks}, 
      author={Mukund Sundararajan and Ankur Taly and Qiqi Yan},
      year={2017},
      eprint={1703.01365},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{flickr,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    url = "https://www.aclweb.org/anthology/Q14-1006",
    doi = "10.1162/tacl_a_00166",
    pages = "67--78",
    abstract = "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
}

@inproceedings{sbu,
 author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Im2Text: Describing Images Using 1 Million Captioned Photographs},
 url = {https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
 volume = {24},
 year = {2011}
}

@inproceedings{cc,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@article{gqa,
  author    = {Drew A. Hudson and
               Christopher D. Manning},
  title     = {{GQA:} a new dataset for compositional question answering over real-world
               images},
  journal   = {CoRR},
  volume    = {abs/1902.09506},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.09506},
  archivePrefix = {arXiv},
  eprint    = {1902.09506},
  timestamp = {Tue, 21 May 2019 18:03:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-09506.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}