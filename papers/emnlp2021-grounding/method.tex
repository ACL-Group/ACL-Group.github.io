
\section{Adapter-based Visual Grounding}
In this section, we introduce adapter-based visual grounding in detail. First, we elaborate on the network architecture of adapter. Then, we introduce the vision-language multimodal grounding procedure with a simple yet effective contrastive learning objective. The learned adapter will be used together with the text encoder in downstream tasks. The overall structure is shown in \figref{fig:overview}.

\subsection{Adapter}
\label{sec:adapter}
Adapter~\citep{kadapter} is a neural network consisting of $M$ stacks of adapter block. Each adapter block contains one down-projection layer, $N$ transformer layers, and one up-projection layer. A residual connection is applied across two projection layers to avert vanishing gradient.

The $M$ adapter blocks are placed among different transformer blocks of the text encoder, which is assumed to be a pretrained Transformer-based~\citep{attentionIsAllYouNeed} language model, e.g., BERT. The input to $i$-th adapter block is the summation of the output of $(i-1)$-th adapter block and the output of $Index(i)$-th transformer block:
\begin{align}
	\bm{h}_{AB}^{(i)} &= AB^{(i)}(\bm{h}_{AB}^{(i-1)}+\bm{h}_{TB}^{(Index(i))}) \\
	\bm{h}^{(0)} &= \bm{h}_{zero} 
\end{align}
where $Index(i)$ is the index of transformer block corresponding to $i$-th adapter block. We apply mean-pooling upon the output of last adapter block $\bm{h}_{AB}^{(M)}$ and obtain the refined representation $h_{text}$.
\subsection{Visual Grounding with Adapter}
In the visual grounding phase, we assume a multimodal training data $D_{train}=\{x_i, y_i\}_{i=1}^{N}$ is given. Each data point consists of a pair of sentence $x_i$ and image $y_i$ that depict the same semantic content. At the core of our adapter-based visual grounding is aligning the textual representation of $x_i$ to the visual representation of $y_i$.

To acquire the visual representation, we apply pretrained ResNeXt-101~\citep{resnext} as the backbone of the image encoder and fix it during training. We then stack another MLP layer with trainable parameters $\bm{\theta}_m$ on top of the backbone. The output of MLP is taken as the visual representation $h_{image_{i}}$ of image $y_i$.

To acquire the textual representation, as stated in \secref{sec:adapter}, we feed sentence $x_i$ into a pretrained language model and get its refined textual representation $h_{text_i}$ through adapter. Note that we do not fine-tune the language model and only update adapter with trainable parameters $\bm{\theta}_{a}$.

We maximize the mutual information $I(X, Y)$ that represents the correspondence between the sentence $X$ and image $Y$. Since the precise computation of $I(X, Y)$ is intractable, we optimize the tractable lower bound of $I(X, Y)$ by minimizing the following noise contrastive learning objective~\citep{infoNCE}:
\begin{align}
	\nonumber
	\mathcal{L}_{t2i}&=-\mathbb{E}_{x_i,y_i\sim P(x)P(y|x)}\{\log{\sigma(h_{image_i}, h_{text_i                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    })}-\\ 
	&\log{\underset{y_j\sim \hat{P} (y)}{\sum} \sigma(h_{image_j}, h_{text_i})}] \} \label{eq:t2i} \\
	\nonumber
	\mathcal{L}_{i2t}&=-\mathbb{E}_{y_i,x_i\sim P(y)P(x|y)}\{\log{\sigma(h_{image_i}, h_{text_i                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    })}-\\ 
	&\log{\underset{x_j\sim \hat{P} (x)}{\sum} \sigma(h_{image_i}, h_{text_j})}] \} \label{eq:i2t}
\end{align}
where $\sigma(h_{image}, h_{text})=\exp (h_{image}^T \bm{W}_{g} h_{text})$ is a bilinear matching layer to compute the relevance between image and sentence. $P(x)$ and $P(y)$ denote the empirical distribution of $x$ and $y$. $P(y|x)$ denotes the distribution of $y$ for given $x$, and vice versa. $\hat{P}(x)$ and $\hat{P}(y)$ denote the noise distribution of $x$ and $y$. In practice we adopt in-batch negative sampling for modeling $\hat{P}(x)$ and $\hat{P}(y)$.

Combining \eqnref{eq:t2i} and \eqnref{eq:i2t} we derive the final learning objective w.r.t. 
all trainable parameters $\{\bm{\theta}_m,\bm{\theta}_a,\bm{W}_g\}$:
\begin{align}
	\bm{\theta}_{m}^\star,\bm{\theta}_{a}^\star,\bm{W}_g^\star = \mathop{\arg\min}_{\bm{\theta}_m,\bm{\theta}_a,\bm{W}_g} \mathcal{L}_{t2i}+\mathcal{L}_{i2t}.
\end{align}
In downstream tasks, the output of the text encoder and adapter are concatenated and fed into task-specific layers for finetuning.
