Response to Reviewer #1:
Q1: "I do not understand the difference between the BERT-AVG and BERT-AVGr ..."
R1: BERT-AVGr means when finetuned on downstream tasks, the adapter is randomly initialized. BERT-AVG, instead, use the adapter pretrained after visual grounding phase. This
setup is included to illustrate the benefit of visual grounding in addition to the additional parameters and structural inductive bias introduced by adapter.

Q2: "Can you explain the different improvements in visual heavy tasks and NLP heavy tasks?"
R2: In visual headvy tasks, the data more frequently contain entities/objects that have visual manifestation in the external physical world. For those tasks, our
visually-enhanced adapter can provide richer information for inference. For NLP heavy tasks, the is a higher chance that over-parametrized language models taking advantage of spurious cues in the dataset. In such case, adapter brings less significant improvement.

Response to Reviewer #2:
Q1: "the BERT baseline is not trained on the visual grounding data ..."
R1: This setup is covered by the MACD baseline we compared to. MACD adopts BERT as backbone text encoder and trains it on the same visual grounding data, i.e., MS COCO. It additionally incorporates knowledge distilation loss to better retain language modeling capability. Therefore, we didn't include the vanilla BERT trained on visual grounding data.

Q2: "The case study in sec 3.3 is not informative. The presented example seems too simple ..."
R2: The more 'simple' the question seems, the more it relies on the 'right' knowledge to resolve. For the example we show, it is important for models to know an axe can smash a tree. BERT-AVG is able to 
leverage the powerful language modeling capability of BERT backbone as well as the additional visual information provided by the adapter.

Response to Reviewer #3:
Q1: "the BERT baseline is not trained on the visual grounding data ..."
R1: 
