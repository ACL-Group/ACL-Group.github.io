\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[review,nohyperref]{emnlp2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage{microtype}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{courier}
\usepackage{color}
\usepackage{amsmath,amsfonts,amssymb,amsthm,amsopn}
\usepackage{epsfig}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{subcaption}

\newcommand{\ft}[1]{\textsc{#1}}
\newtheorem{example}{Example}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}
%\newcommand{\citealp}[1]{\citeauthor{#1}~\shortcite{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\sgn}{\text{sgn}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\title{Statistically Profiling Biases in Natural Language Reasoning Datasets}
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}

\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi

\begin{document}
\maketitle
\begin{abstract}
Recent work has suggested that many natural language 
reasoning datasets contain statistical cues that
may be taken advantaged of by NLP models whose
capability may thus be grossly overestimated. 
%However, none of the work has been able to
%easily pinpoint what these cues are. 
%Inspired by black-box test in software engineering,
No existing work has been able to pinpoint what these cues are
and quantify their potential to be exploited by
models. 
%To discover the potential weakness in the models, some human-designed 
%stress tests have been proposed but they are expensive to create 
%and do not generalize to arbitrary models. 
We propose a light-weight and general statistical profiling framework, 
ICQ (I-See-Cue), which automatically identifies possible cues
in any multiple-choice NLR datasets without 
the need to create any additional test cases. Preliminary
studies show that some of the cues identified in several well-known
datasets are indeed picked up by prominent models such as BERT and
RoBERTa. 
\end{abstract}

\input{intro}
\input{formulation}
\input{approach}
\input{experiment}
\input{related}
\input{conclusion}

\bibliographystyle{named}
\bibliography{aaai21}

\end{document}
