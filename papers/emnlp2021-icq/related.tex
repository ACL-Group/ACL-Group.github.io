\section{Related Work}
\label{sec:related}

Our work is related to and, to some extent, comprises of 
elements in three research directions: spurious features analysis, 
bias calculation and dataset filtering. 
 
\textbf{Spurious features analysis} has been increasingly studied recently. 
Much work~\cite{sharma2018tackling,srinivasan2018simple,zellers2018swag} 
has observed that some NLP models can surprisingly 
get good results on natural language understanding questions in MCQ form without 
even looking at the stems of the questions. Such tests are called
``hypothesis-only'' tests in some works.
Further, some research~\cite{sanchez2018behavior} discovered that these models 
suffer from an insensitivity to certain small but semantically significant alterations
in the hypotheses, lending to speculations that the hypothesis-only performance
is due to simple statistical correlations between words in the hypothesis and 
and the labels. 
Spurious features can be classified into
lexicalized and unlexicalized~\cite{bowman2015large}:
lexicalized features mainly contain indicators of n-gram tokens, cross-ngram tokens, 
while unlexicalized features involve word overlap, sentence length and BLUE score between 
the premise and the hypothesis. ~\citealp{naik2018stress} refined the 
lexicalized classification to Negation, Numerical Reasoning, 
Spelling Error. ~\citealp{mccoy2019right} refined the word overlap 
features to Lexical overlap, Subsequence and Constituent 
which also considers the syntactical structure overlap. ~\citealp{sanchez2018behavior} 
provided an extra lexicalized feature, unseen tokens. 
%Most of these research works for finding the specific cues or 
%provide accuracy baselines rather than evaluate the datasets and

\textbf{Bias calculation} is concerned with methods to quantify the severity of the cues. 
Some work~\cite{clark2019don,he2019unlearn,yaghoobzadeh2019robust} 
attempted to encode the cue feature implicitly by 
hypothesis-only training or by extracting features associated with a certain label 
from the embeddings. 
%They learn a bias model given source of bias as
%input, and de-bias through logit re-weighting or logit ensembling. 
Other methods compute the bias by statistical metrics. 
For example, \citealp{yu2020reclor} used the probability of seeing a word 
conditioned on a specific label to rank the words by their biasness. 
LMI~\cite{schuster2019towards} was also used to evaluate cues and 
re-weight in some models. 
However, these works did not give the reason to use these metrics, one way or 
the other.
Separately, \citealp{Marco2020acl} gave a test data augmentation method, 
without assessing the degree of bias in those datasets.

\KZ{Distinguish between this dataset filtering and our own
data filtering..} 
\textbf{Dataset filtering} is one way of achieving
higher quality in datasets by reducing artifacts. 
In fact, datasets such as SWAG and RECLOR evaluated in this paper
were produced using variants of this filter approach which 
iteratively perturb the data instances until a target 
model can no longer fit the resulting dataset well. 
Some methods~\cite{yaghoobzadeh2019robust}, instead of preprocessing
the data by removing biases, leave out samples with biases 
in the middle of training according to decision made between 
epoch to epoch. \citealp{bras2020adversarial} investigated 
model-based reduction of dataset cues and designed an algorithm 
using iterative training. Any model can be used in 
this framework. Although such an approach is more 
general and more efficient than human annotating, 
it heavily depends on the models. Unfortunately, different models
may catch different cues. Thus, such methods may not be complete.

%\KZ{Is it really more general? And why are we comparing with
%human annotation?}

%or different statistical methods and then 
% reweighting the samples with these scores.



%Large scale datasets are fraught with give-away
%phrases (McCoy et al., 2019; Niven and Kao,
%2019). Crowd workers tend to adopt heuristics
%when creating examples, introducing bias in the
%dataset. In SNLI (Stanford Natural Language Inference)
%(Bowman et al., 2015), entailment based
%solely on the hypothesis forms a very strong baseline
%(Poliak et al., 2018; Gururangan et al., 2018).
%Similarly, as shown by Kaushik and Lipton
%(2018), reading comprehension models that rely
%only on the question (or only on the passage referred
%to by the question) perform exceedingly
%well on several popular datasets (Weston et al.,
%2016; Onishi et al., 2016; Hill et al., 2016). To
%address deficiencies in the SQuAD dataset (Jia
%For example, a model might assign a label of contradiction
%to any input containing the word ``not", since
%``not" often appears in the examples of contradiction
%in standard NLI training sets. The adversarial stress test balance the word ``not'' with 
%each label to test whether the model is just sensitive to this word or not. 
%These methods provide corresponding adversarial test data for each feature to 
%test the robustness of models for the known concrete spurious features, 
%like ``word overlap'', ``negation'', ``length mismatch'', ``antonyms'', 
%``spelling error'' and ``numerical reasoning'' in MNLI, 
%  However, it's also not generalized. For other different genres 
%of statistical cues, like word ``a'' exploited in COPA~\cite{roemmele2011choice}, 
%design other logic rules to generate stress test or 
%balance that by human~\cite{kavumbabalanced-copa}.
