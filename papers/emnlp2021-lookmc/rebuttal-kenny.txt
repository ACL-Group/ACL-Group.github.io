Review 1:
Q1: The solution is neither convincing nor elegant and lacks...
A1: Thank you for acknowledging the importance of the short circuit problem 
we studied. 
First of all, we hereby emphasize that short-circuit we defined in this paper
is only one kind of model fragility. Other kinds include sensitivity to
POS tags, typos, etc. We mentioned a range of stress tests in 
this paper.  All of which can be used to test model fragility, 
but only six (which we call proxy tests) can be used to test short circuit 
problems. In other words, short-circuit tests are part of the stress tests. 
The reasons are explained in lines 192-204. 
But in this paper, we focus on short circuits in MCQs which was never
previously studied.

Most existing proxy tests are fine-grained tests on specific features, 
and it's hard to enumerate all the possible features, not to mention
combinations of them. Therefore, we proposed crossover test, which is 
not only simple, but also not limited to fine-grained specific features. 
It can effectively test whether the model is only interested in choices 
rather than reasoning from a broad perspective. 
Table 4, by comparing the distances between each proxy test and
the center of all proxy tests in flip test, we can see that
crossover is the best method to test short-circuit problem, even better
than human tests.  

Furthermore, crossover can also be used as a method of
data augmentation to strengthen the connection between premise and choices. 
In Table 5, it shows that crossover can not only improve the performance of 
original model on short-circuit tests, but also on many stress tests (we mentioned in Table 3). 
The specific stress test results are shown in the Appendix. 
While such data augmentation method can be seen as a type of 
adversarial training, we proposed a brand new operator which is shown to be
effective not only in improving model fragility against short-circuit (a 
brand new problem for MCQs) but also other types of fragilities, with
more details in the Appendix. 

Review 2:
Q1: the swapping strategy may lead some sample error.
A1: Thanks for asking about this situation. We did some experiments following
your advice. By randomly selecting 100 stress test questions created from 
the crossover operator and asking three volunteers to check their correctness. 
The results shows that all test questions are correct, i.e., the there is 
exactly one correct choice and one incorrect choice in each MCQ. This
indicates that the probability of introducing errors by swapping is very small.

Review 3:
Q1: It is a bit unclear how these stress operations…
A1: According to Table 5, models after data augmentation (using +C or +M)
improves the accuracy substantially not only on short circuit tests
but also on all stress tests (which tests for other types of model
fragilities), e.g., 6% for RB+C and 7% for XL+C. 
Meanwhile, these models are doing equality well on
the original test sets, e.g., BT/XL/RB+C did almost identically well compared to
the original models (difference is under 1%). 
This, to us, best demonstrates that data augmentation
using our proposed operators improves the model's robustness.
We also want to stress that robustness here refers to the model instances 
and not model architecture. For example, the same Bert model architecture 
trained with different hyper-parameters or training data are considered 
different model instances in this paper. All the tests proposed in this paper 
are black-box tests which are model agnostic.
In table 5, the numbers are comparable between different models in the 
same dataset. For example, the numbers in (a) suggests that by
the severity of short circuit problem specifically: XL > BT > RB, 
which means among the three, XL is the most susceptible to learning spurious 
features only from the choices instead of really reasoning between the premise
and the choices.

Q2: Could the authors explain more about how human evaluation
A2: The attention maps are obtained using an offthe-shelf tool (Vig, 2019) 
as shown in Figure 1. Human annotators are then
asked to determine whether there exists strong visual correlation
between the correct choice and the premise. We consider the model not
short-circuiting only if more than half of the human annotators
confirm a strong correlation. We will add this description to our 
revised version. However, we stress here that human evaluation is not a
gold standard and only acts as one of the proxy tests.
The reason is explained in line 369-375.

Q3: Could it help the original task…
A3: Please refer A1 for Reviewer 3.
