\section{Evaluation}
\label{sec:eval}
In this section, we introduce the dataset and experimental setup.
%We compare our proposed model
We analyze the experimental results
%with existing opinion summarizaiton models 
and demonstrate the advantages of our approach
~\footnote{ Data and source code:\url{http://anonymized.for.blind.review}.}.

\subsection{Datasets}
In this experiment, we use two datasets.


\textbf{Yelp} 
%\footnote{https://www.yelp.com/dataset}
contains a large number of reviews on business.
%in the area of business. 
%For summarization purpose, 
\citet{MeanSum19} also released the development and test set with 8 reviews for each human-written summary under the same business.

\textbf{Amazon} 
%\footnote{https://cseweb.ucsd.edu/~jmcauley/datasets.html}
is a similar dataset~\cite{HeM16} made up of product reviews from four different categories. %Similary, a set of summaries are created by Amazon Mechanical Turk (AMT) workers in~\cite{Copycat20} . 
The development and test set~\cite{Copycat20} accompany with 8 reviews and 3 human-written summaries under the same product.

For training, we construct the synthetic semi-structured training set for these two datasets.
Human-annotated multi-review and summary pairs
are used as development set and testing set.  
%Details are shown in Table~\ref{tab:datasets}.

\begin{table}[th]
	\small
	\centering
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{} & \textbf{Training} &\textbf{Development} & \textbf{Testing}\\
		\hline
		Yelp & 100k & 100 & 100 \\
		Amazon & 90k & $28\times3$ & $32\times3$ \\
		\hline
	\end{tabular}
	\caption{%Data statistics show 
		The number of  train/dev/test pairs. 
		Training set is our synthetic semi-structured data. 
		$\times3$ means three human-written  summaries per multi-review.}
	\label{tab:datasets}
\end{table}
%\KZ{What are the units of these numbers in the table? Are they reviews or multi-reviews?
%what about the lengths of the those reviews (number of words) etc. More detailed
%stats which are related to our method can be released here. Cos people might be thinking
%is it appropriate to use your methods under the characteristics of these two 
%datasets or are you taking advantage of some peculiar features in the dataset?}

\subsection{Implementation Details}
In this experiment,
we apply aspect-guided models on transformer seq2seq model~\cite{Transformer17}
and pretrained BART~\cite{BART20}
\footnote{https://github.com/pytorch/fairseq}.

For creating synthetic data, 
we set $N_r$ (\secref{sec:data}) as $8$ 
since every summary in the human-annotated development and test set has $8$ reviews.
For transformer seq2seq model, we use SGD as optimizer and set the initial learning rate as $0.1$.
A momentum $\beta=0.1$ and a decay of  $\gamma=0.1$.
For BART,
we follow~\citet{BART20} in fine-tuning BART with
$lr=3e$-$05$ and warmup $=500$.
At decoding, we set beam size equaling 5.
We train our models on one RTX 2080Ti GPU with 11G RAM. 
The average training time of our approachs is about $10$ hours.
%Previous methods using different versions of ROUGE,
%the ROUGE scores shown in papers cannot be compared.
In our experiment, we use the general ROUGE evaluation tool {\em file2rouge} in summarization tasks~\cite{BART20,DialogMV2020}.


\subsection{Models under Comparison}

We evaluate different methods trained on their corresponding synthetic training data.
The brief description are shown in \tabref{tab:baselines}.
%\begin{itemize}
%	\item \textbf{MeanSum}~\cite{MeanSum19}. An unsupervised auto-encoder model decodes the summary based on the mean representation of input reviews.
%	\item \textbf{CopyCat}~\cite{Copycat20}. A hierarchical variational autoencoder model with controlling of novelty between inputs and the generated new review.
%	\item \textbf{OpinionDigest}~\cite{SuharaWAT20}. A framework consists of opinion extraction, opinion selection and summary generation.
%	\item \textbf{Denoise}~\cite{Denoise20}. A denoising summarization model with linguistically motivated noising datasets.
%	\item \textbf{FewSum}~\cite{Fewshot20}. A conditional transformer model with specially design on consideration of review properties, including content coverage, writing style and length deviations.
%	\item \textbf{PlanSum}~\cite{abs-2012-07808}. A method explicitly take the form of aspect and sentiment distributions for synthetic dataset construction and summary generation.		
%\end{itemize}

\begin{table}[th]
   \small
	\centering
	\begin{tabular}{|m{1.2cm}<{\raggedleft}|p{5.6cm}|}
		\hline
		\textbf{Abbrev.} & \textbf{Description} \\ 
	    \hline
         MeanSum &Mean representations~\cite{MeanSum19}  \\
        \hline
		Copycat & Copycat-Review~\cite{Copycat20}\\
		\hline
		OpiDig & OpinionDigest~\cite{OpiDig20}\\
		\hline
		Denoise\cut{\tablefootnote{Without released data or code of Denoise, we implement their approach.} } & \tabincell{l}{Nosing \& denoising \\ ~\cite{Denoise20}} \\
		\hline
		FewSum & Few-shot learning~\cite{Fewshot20}\\
		\hline
		PlanSum & Content Planning~\cite{Plansum20}\\
		\hline
		\hline
	    BAG & \tabincell{l}{Single encoder with only noisy OAs as input.} \\
		\hline
		BAI & \tabincell{l}{Single encoder with the concatenation of \\ noisy OAs and ISs as input.} \\
		\hline
		MAI & \tabincell{l}{Dual encoder. OA encoder with noisy OAs as \\ input and IS encoder with noisy ISs as input.} \\
		\hline
		MB & \tabincell{l}{Training MAI based on pretrained BAG\\ (\secref{sec:training})} \\
		\hline
		MB-T & \tabincell{l}{Applying MAI and BAG on Transformer\\ \cite{Transformer17} } \\
		\hline
		MB-B & \tabincell{l}{Applying MAI and BAG on BART \\ \cite{BART20} \\} \\
		\hline
	\end{tabular}
	\caption{The abbreviation and description of methods.}
	\label{tab:baselines}
\end{table}

\cut{%%%%
To evaluate the effectiveness of our semi-structured synthetic data,
we convert the previous synthetic datasets into semi-structured version.
%For structured synthetic data, we sample the noisy OAs and ISs for each output.
%For textual synthetic data, we 
The details are listed in \tabref{tab:semi}.
\begin{table}[th]
	\small
	\centering
	\begin{tabular}{|r|p{5.6cm}|}
		\hline
		\textbf{Converted Data} & \textbf{Description} \\ 
		\hline
		Semi-OpiDig & \tabincell{l}{Sample the noisy OAs and ISs for each \\output in OpiDig synthetic data} \\
		\hline
		Semi-Denoise & \multirow{3}{*}{\tabincell{l}{Nosing \& denoising \\ ~\cite{Denoise20}}} \\
	   \cline{1-1}
		Semi-FewSum & \\
		\cline{1-1}
		Semi-PlanSum & \\
		\hline
		BAG & \tabincell{l}{Single encoder with only noisy OAs as input.} \\
		\hline
	\end{tabular}
	\caption{The abbreviation and description of methods.}
	\label{tab:semi}
\end{table}
}%%

%\KZ{
%I think u also need to compare different ways of synthesizing data, and that can be included here
%as well?}  

\subsection{Evaluation Metrics}
We evaluate the performance of our methods by {\em automatic metrics}
and {\em human evaluation}.

%\textbf{Automatic Metrics.}

%\begin{itemize} 
%\item 
\textbf{ROUGE} scores (F1) include
%is the standard evaluation metric in summarization task,
ROUGE-1 (R-1), ROUGE-2 (R-2) and
ROUGE-L(R-L)~\cite{rouge}.


\textbf{Diversity} (Div). It uses self-BLEU~\cite{SelfBleu18}
which measures BLEU scores of each generated sentence by considering others as reference. 
The lower value means more diversity.

\textbf{Aspect Coverage} (AC).
We extract aspects from summaries 
by a rule-based method~\cite{aspect14},
which is different from MIN-MINER in our approach
\footnote{This is to eliminate the bias caused by using the same aspect extraction tool in the approach and evaluation.}.
We take aspects of gold summary as reference,
and compute R-1 recall between reference  
and aspects of generated summaries.
AC is the average of these R-1 recall scores.
%\end{itemize}

\textbf{Human Evaluation.}
We randomly select 32 samples (the same as Amazon) from Yelp and all samples from Amazon.
We ask three human annotators
%\footnote{The Cohen's Kappa coefficient among annotators are $0.6$, indicating substantial agreement.}
who are 
native or proficient English speakers to evaluate generated summaries.
They score gold summary and summaries 
generated by our best model and 
PlanSum according to the consistency with multi-review and informativeness,
i.e., best (1.0), average (2.0) and worst (3.0).
%For a generated summary, we average the scores by
For a generated summary, we average the scores by
three annotators.
The human evaluation score of a model is the average scores of its all generated summaries.


\subsection{Results}
\label{sec:results}
In this section, we analyze our proposed synthetic training data  and 
aspect guided models.


\textbf{Synthetic training data.}
\cut{Existing synthetic training datasets
either takes textual review as input or only takes opinion-aspect pair as input.}
%The synthetic data of the state-of-the-art model, PlanSum,
%consists of multi-review and summary pairs.
To evaluate the effectiveness of training on semi-structure data,
we convert the previous synthetic data into semi-structured version,
and compare the original synthetic data trained on its own model and
semi-structured verison of that data with model MB.
The results are listed in \tabref{tab:traindata}.

\begin{table*}[th]
	\begin{center}
		\small
		\begin{tabular}{|r|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\multicolumn{2}{|c|}{\bf Approach} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\
			\hline
			\textbf{Synthetic data} & \textbf{Model} & R-1 & R-2 & R-L & AC & Div & R-1 & R-2 & R-L & AC & Div \\
			\hline
			%&\multirow{2}{*}{OpiDigest} & OpiDigest-B & 28.49 & 5.64 & 26.48 & 0.39 & 0.25 & 29.71 & 5.58 & 26.14 & 0.25 & 0.28\\ \cline{2-12}
			OpiDig & OpiDig & 28.68 &5.00 & 25.33& 0.39 & 0.33 & 29.52 & 5.26 & 26.65 & 0.23 & 0.27 \\
			%\cline{2-12}	 
			semi-OpiDig& MB-T & 28.94 & 5.41 & 26.11 & 0.41 & 0.21 &30.42 & 5.43& 27.05 & 0.26 & 0.28\\
			\hline
			FewSum & FewSum & 29.64 & 5.80 & 27.01 & 0.38 & 0.28 & 31.02 & 6.06 & 27.94 & 0.20 & 0.30 \\
			%\cline{2-12}	 
			semi-FewSum & MB-T & 30.04 & 6.14 & 28.67 & 0.39 & 0.24 &31.42 & 6.09& 28.27 & 0.28 & 0.27\\
			\cline{1-12}	 
			Denoise & Denoise& 29.75 & 5.00 & 26.88 & 0.39 & 0.27 &27.96 & 4.01 & 24.20& 0.16 & 0.42  \\
			%\cline{2-12}	 
			semi-Denoise & MB-T & 29.94 & 5.84& 27.11 & 0.41 & 0.24 & 28.03& 4.54& 24.96 & 0.20 & 0.38 \\
			\cline{1-12}	 
						OURs & MB-T & 30.43 & 6.37 & 28.09 & 0.41 & 0.22 & 31.79 & 6.18 & 28.52 & 0.33 & 0.26 \\ 
						\hline
			PlanSum & PlanSum & 31.19&6.83 &29.02 &0.38 & 0.27 & 31.89 &6.13 & 28.53 & 0.23 & 0.32\\  %\cline{2-13}
			semi-PlanSum& MB-B & 32.03& 6.92 & 28.98 & 0.41 & 0.23 & 31.29 & 6.23 &  27.95& 0.28 & 0.28 \\
			\hline
			OURs& MB-B & \bf 32.20 & \bf 7.06 & \bf 29.15 & \bf 0.44 & \bf 0.20 & \bf 32.65 & \bf 6.78 & \bf 29.14 & \bf 0.34 & \bf 0.25 \\ 
			\hline
		\end{tabular}
	\end{center}
	\caption{The different synthetic datasets in semi-structured version
		are trained on our model MB-T and MB-B because of the characteristics of semi-structured data. ``semi-'' means the semi-structured version. OURs is our created synthetic training data.}
	\label{tab:traindata}  
\end{table*}

For the datasets with textual multi-review as input (FewSum, Denoise and PlanSum), 
we convert them into semi-structured version by extracting opinion-aspect pairs (OAs) and implicit sentences (ISs)
from their multi-reviews as input.
For the synthetic data with strucutured input (OpiDig),
it takes a review as output and the OAs of this review as input.
%During generation, OpiDig clusters the OAs from multi-review and
%takes centroids of clusters as input.
We construct the semi-structured version of such data
by generating the noisy OAs and noisy ISs for each output in original dataset as input.
To be fair, as shown in \tabref{tab:traindata}, we train MB-B (pretrained) on semi-PlanSum and MB-T (non-pretrained model) on the other semi-structured dataset, because the model of PlanSum is pretrained and the models of the other approaches is non-pretrained.
%, which should use MB for training.
%that synthetic datasets are specific and 
%the models designed for datasets can make full use of data.
%As shown in \tabref{tab:traindata}, 
We apply different models on different original synthetic datasets because
the synthetic dataset is compatible with its model.
The semi-structured version of previous textual synthetic dataset training on MB
is better than the original version in terms of ROUGE, AC and Div scores,
showing that semi-structured data
is helpful in highlighting the aspects and opinions.
Compared with the structured datasets OpiDig, the semi-OpiDig get better ROUGE scores and Div scores denotes that the structured data may lose some information and the ISs in semi-structured data help model capture implicit information.
Our synthetic dataset with semi-structured data
achieves the best scores. This shows that
%usefulness of representing review by semi-structured OAs and ISs. 
%And 
the way of getting noisy OAs and ISs according to sampled summary 
is optimal.

%The ROUGE scores of
 %BAG model in \tabref{tab:abla} (b) training on BART with only OAs as input is better than OpiDig
 %shows that our approach avoids the noise caused by the clustering in OpiDig
 %and make model have the ability of denoising.
 
 
%\KZ{Is there alternative ways of creating the semi-structured
%synthetic data? If so, i think this should be compared with our
%approach.}
\cut{%%%
\begin{table*}[th]
	\begin{center}
		\small
		\begin{tabular}{|c|r|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\multicolumn{3}{|c|}{\bf Dataset} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\
			\hline
			\textbf{Type}&\textbf{Synthetic data} & \textbf{Model} & R-1 & R-2 & R-L & AC & Div & R-1 & R-2 & R-L & AC & Div \\
			\hline
			%&\multirow{2}{*}{OpiDigest} & OpiDigest-B & 28.49 & 5.64 & 26.48 & 0.39 & 0.25 & 29.71 & 5.58 & 26.14 & 0.25 & 0.28\\ \cline{2-12}
		\multirow{2}{*}{Structured}&OpiDig & OpiDig & 28.68 &5.00 & 25.33& 0.39 & 0.33 & 29.52 & 5.26 & 26.65 & 0.23 & 0.27 \\
			%\cline{2-12}	 
			&semi-OpiDig& MB-T & 28.94 & 5.41 & 26.11 & 0.41 & 0.21 &30.42 & 5.43& 27.05 & 0.26 & 0.28\\
			\hline
			\multirow{6}{*}{Textual}&FewSum & FewSum & 29.64 & 5.80 & 27.01 & 0.38 & 0.28 & 31.02 & 6.06 & 27.94 & 0.20 & 0.30 \\
			%\cline{2-12}	 
			&semi-FewSum & MB-T & 30.04 & 6.14 & 28.67 & 0.39 & 0.24 &31.42 & 6.09& 28.27 & 0.28 & 0.27\\
			\cline{2-13}	 
		&Denoise & Denoise& 29.75 & 5.00 & 26.88 & 0.39 & 0.27 & - & - & - & - & - \\
			%\cline{2-12}	 
			&semi-Denoise & MB-T & 29.94 & 5.84& 27.11 & 0.41 & 0.24 &-& -& - & - & - \\
			 \cline{2-13}	 
			&PlanSum & PlanSum & 31.19&6.83 &29.02 &0.38 & 0.27 & 31.89 &6.13 & 28.53 & 0.23 & 0.32\\  %\cline{2-13}
			&semi-PlanSum& MB-B & 32.03& 6.92 & 28.98 & 0.41 & 0.23 & 31.29 & 6.23 &  27.95& 0.28 & 0.28 \\
			\hline
			Semi- & OURs & MB-T & 30.43 & 6.37 & 28.09 & 0.41 & 0.22 & 31.79 & 6.18 & 28.52 & 0.33 & 0.26 \\ 
			structured& OURs& MB-B & \bf 32.20 & \bf 7.06 & \bf 29.15 & \bf 0.44 & \bf 0.20 & \bf 32.65 & \bf 6.78 & \bf 29.14 & \bf 0.34 & \bf 0.25 \\ 
			\hline
		\end{tabular}
	\end{center}
	\caption{The different synthetic datasets in semi-structured version
		are trained on our model MB-T and MB-B because of the characteristics of semi-structured data.}
	\label{tab:traindata}  
\end{table*}
}%%%%
\begin{table*}[th]
	\centering
	\small
	\begin{tabular}{|r|l|c|c|c|c|c|c|c|c|c|c|}
		\hline
		&\multirow{2}{*}{\bf Dataset} & \multicolumn{5}{c|}{\bf Yelp} &  \multicolumn{5}{c|}{\bf Amazon} \\ \cline{3-12}
		&& R-1 & R-2 & R-L& AC &Div & R-1 & R-2 & R-L& AC & Div\\
		\hline
		\multirow{6}{*}{Non-Pretrained}&Meansum & 26.84 & 3.57 & 23.62 & 0.34 & 0.38 &  28.20 &4.67 & 24.15& 0.17& 0.40\\
		&Copycat & 28.05& 4.90& 25.32& 0.38 & 0.34 & 29.71 & 5.78 & 26.42  & 0.18 & 0.43 \\
		&OpiDig & 28.68 &5.00 & 25.33& 0.39 & 0.33 & 29.52 & 5.26 & 26.65 & 0.23 & 0.27 \\
		&Denoise & 29.75 & 5.00 & 26.88 & 0.39 & 0.27 & 27.96 & 4.01 & 24.20& 0.16 & 0.42 \\
		&FewSum \tablefootnote{In our experiments, the summary is generated by FewSum without finetuned on human-annotated data, to be fair.} 
		& 29.64 & 5.80 & 27.01 & 0.38 & 0.28 & 31.02 & 6.06 & 27.94 & 0.20 & 0.30 \\
		&MB-T & 30.43 & 6.37  & 28.09 &0.41 & 0.22 &31.79 &6.08 & 28.52 & 0.28 & 0.26 \\
		\hline
		\multirow{2}{*}{Pretrained}&PlanSum & 31.19&6.83 &29.02 & 0.38 & 0.27 & 31.89 &6.13 & 28.53 & 0.23 & 0.32\\
		&MB-B & \underline{\bf 32.20} & \underline{\bf 7.06} & \underline{\bf 29.15} & \bf 0.44 & \bf 0.20 & \underline{\bf 32.65} & \underline{\bf 6.78} & \underline{\bf 29.14} & \bf 0.34 & \bf 0.25 \\
		\hline
	\end{tabular}
	\caption{Automatic evaluation. The scores underlined are statistically significantly better than PlanSum  with p$<$0.05 according to t-test.
	}\label{tab:all}  
\end{table*}





\textbf{Generation.}
As shown in \tabref{tab:all}, we compare our best model MB with previous models.
The MB-T is non-pretrained model and performs best among all non non-pretrained models.
Compared with PlanSum, the ROUGE scores of MB-T is lower
because PlanSum use a pretrained model, BERT~\cite{BERT19}.
Similar to PlanSum, we convert MB-T models to MB-B on pretrained BART.
MB-B achieves the best scores in terms of ROUGE, AC and Div.
This shows that our MB-B model 
fully utilize our created synthetic training data.
MB trained on synthetic training data with semi-structured input
learns how to expand important OAs from input to generate some general
sentences and abstract OSs to describe implicit information 
in specific sentences.
The higher AC score of MB-B means
that the model benefits from directly taking OAs as input.
The lower Div scores show the model capture the information of
ISs.
The improvement of our best model on Yelp is less than Amazon
because each sample in Amazon has 3 reference summaries and each sample in Yelp only has one reference summary.
For one reference summary,
the evaluation on ROUGE is strict.
This causes that even if the generated summary covers
more important information of multi-review, 
the ROUGE scores may not be changed much.
%The multiple reference summaries are friendly to various abstractive summaries.
The improved generated summaries are more likely to match tokens
in multiple reference summaries. 


As the abstractive summary of multi-reviews is various,
we use human evaluation
to help automatic evaluation.
The lower score on human evaluation means that generated summaries are better.
As shown in  \tabref{tab:human}, 
the human evaluation score of Gold is the smallest
since the Gold summaries are written by human.
%The score of our model lower than PlanSum
%denotes that 
%human-annotators thinks that
the summaries generated by our model 
are more consistent with multi-reviews than PlanSum with the lower score.


\begin{table}[th]
	\centering
	\small
	\begin{tabular}{|l|c|c|c|}
		\hline 
		& Gold & PlanSum & MB-B\\
		\hline
	   Yelp & \bf{1.3} &  2.4 & 2.2\\
	   Amazon &\bf{1.8} & 2.4 & 2.0 \\
	   \hline
	\end{tabular}
	\caption{Human evaluation. 
	The Kappa coefficient is $0.6$, indicating substantial agreement.
	}\label{tab:human}  
\end{table}

\textbf{Ablations.}
As shown in \tabref{tab:abla} and \tabref{tab:acdv},
we compare the different models designed for semi-structured training data.
%The performance of
%BAG is worst because
%of only using OAs as input.
%The generated summary of BAG in \tabref{tab:exp} 
%shows that BAG with only OAs as input always generate 
%sentences in general and cannot capture
%the implicit information in multi-review.
BAG with only the noisy OAs as input performs worst,
because BAG training on noisy OAs and textual summary only
learns to make sentence. At test, the BAG can only generate summaries
from the OAs in multi-review and cannot capture the information of sentence without OAs.
As shown in \tabref{tab:exp}, the sentences in the summary of BAG 
are in general and have no specific information.
BAI 
%uses the same model structure as BAG but 
takes the concatenation of noisy OAs and ISs as input.
BAI 
%introduce implicit information to model by ISs and 
performs better than BAG on ROUGE and Div score 
because of introducing ISs.
With noisy OAs and ISs as input, the model should learn to expand important OAs as sentences and summarize ISs in a better way.
However, BAI equally deals with OAs and ISs,
which interferes with the model's understanding of OAs and ISs.
In \tabref{tab:acdv} and \tabref{tab:exp},
BAI gets lower AC and fails to filter some noisy aspects, such as ``atmosphere'' and ``experience''.
MAI deals with OAs and by OA encoder and ISs by IS encoder.
So MAI gets better performance than BAI.
The AC scores of models in \tabref{tab:abla} are similar
because all of them directly use OAs as input.
To make full use of semi-structured data,
MB is the model first trained on BAG and then finetuned on MAI.
which first gets general information about OAs and
then gets specific information from ISs.  
MB obtains the best results on both datasets, which shows
that the way of training the semi-structured data on 
the model with dual encoder is effective. % for opinion summarization.



\begin{table}[th]
	\centering
	\small
	\subtable[Transformer]{
		\begin{tabular}{|m{0.7cm}<{\raggedright}|m{0.6cm}<{\centering}|c|c|c|c|m{0.6cm}<{\centering}|}
			\hline
			\multirow{2}{*}{\bf Model} & \multicolumn{3}{c|}{\bf Yelp} &  \multicolumn{3}{c|}{\bf Amazon} \\ \cline{2-7}
			& R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
			\hline
			BAG & 28.04 & 5.37 & 25.60 & 29.20 & 5.42 & 26.47 \\
			BAI & 28.99 & 5.55 & 26.40 & 30.27 & 5.73& 27.39 \\
			MAI & 29.49 & 5.71 & 26.86 & 29.94 & 5.88& 26.69\\
			MB & \bf 30.43 & \bf 6.37  & \bf 28.09 & \bf 31.79 &\bf 6.08 & \bf 28.52 \\
			\hline
		\end{tabular}
	}
	\qquad
	\subtable[BART]{
		\begin{tabular}{|m{0.7cm}<{\raggedright}|m{0.6cm}<{\centering}|c|c|c|c|m{0.6cm}<{\centering}|}
			\hline
			\multirow{2}{*}{\bf Model} & \multicolumn{3}{c|}{\bf Yelp} &  \multicolumn{3}{c|}{\bf Amazon} \\ \cline{2-7}
			& R-1 & R-2 & R-L & R-1 & R-2 & R-L\\
			\hline
			BAG & 29.79 & 5.83 & 27.07 & 30.01 & 5.77 & 26.54  \\
			BAI & 30.27 & 5.94  & 26.90 & 30.18 & 5.90 &27.14 \\
			MAI & 30.93 & 6.40  & 28.11 & 31.47& 6.27& 28.03\\
			MB & \bf 32.20 & \bf 7.06 & \bf 29.15 & \bf 32.65 & \bf 6.78 & \bf 29.14  \\
			\hline
		\end{tabular}
	}
	\caption{The ROUGE scores of summaries generated by models training on our created synthetic data.
	}\label{tab:abla}  
\end{table}

\begin{table}[th]
	\begin{center}
		\small
		\begin{tabular}{|l|m{6.2cm}|}	
			%\hline \bf{BAG} \\
			\hline
			Gold & 
			the \textbf{servers} are \textbf{kind} and \textbf{knowledgeable} . \textit{they will} 
			\textit{patiently answer your questions .} \textit{they offer patio  seating .} 
			%\textit{seating if you ' d prefer to sit outside .} 
			the \textbf{free chips} and \textbf{salsa} are always a plus , and the \textbf{margaritas} are \textbf{amazing} too . the menu is \textbf{full tasty authentic mexican food .} \\
			\hline
			BAG & the \textbf{servers} are always \textbf{friendly} and the \textbf{food} is \textbf{great} . it is a \textbf{good mexican restaurant .} \\
			%\hline \bf{BAI} \\
			\hline
			BAI & \textbf{great food} , \textbf{great service} , \textbf{great atmosphere} , and \textbf{great prices} . \color{gray}{i have been there a few times and have \textbf{never} had a \textbf{bad experience} . }\\
			%\hline \bf{MAI} \\
			\hline
			MAI &i love this place. the \textbf{food} is \textbf{good} and the \textbf{service} is \textbf{great} . the \textbf{atmosphere} is \textbf{great} too. \color{gray}{the only thing is that it 's a little pricey for what you get .}\\
			% i will definitely be coming back.}  \\
			%\hline \bf{MB} \\
			\hline
			MB & it 's one of the \textbf{authentic mexican restaurant} in the area. the \textbf{food} is \textbf{great} . the \textbf{servers} are \textbf{very friendly} and \textbf{knowledgeable} . \textit{they took the order patiently . } the \textbf{chips} and \textbf{salsa} are \textbf{good} too . \textbf{it is huge and has patio seating .}  
			%\color{gray}{you can get a lot of food for the price you pay . }
			\\
			\hline
		\end{tabular}
	\end{center}
	\caption{Summaris generated by BART-based models.
	}\label{tab:exp}  
\end{table}


\begin{table}[th]
	\centering
	\small
	\begin{tabular}{|l|l|c|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{\multirow{2}{*}{\bf Model}} & \multicolumn{2}{c|}{\bf Yelp} &  \multicolumn{2}{c|}{\bf Amazon} \\ \cline{3-6}
		\multicolumn{2}{|c|}{} & AC & Div & AC & Div \\
		\hline
		\multirow{4}{*}{Trans} & BAG & 0.35 & 0.26  & 0.26& 0.28 \\
		& BAI & 0.33 & 0.24 &0.24 & 0.27 \\
		& MAI & 0.39&  0.23 & 0.30 & 0.26 \\
		& MB & \bf 0.41 & \bf 0.22 & \bf 0.33 & \bf 0.26\\
		\hline
		\multirow{4}{*}{BART} & BAG &0.40 & 0.25 &0.28 & 0.27 \\
		& BAI & 0.38 & 0.22 & 0.28 & 0.26 \\
		& MAI & 0.43 & 0.22 & 0.32 & 0.26 \\
		& MB & \bf 0.44 & \bf 0.20 &\bf 0.34 & \bf 0.25 \\
		\hline
	\end{tabular}
	\caption{The aspect coverage and diversity scores of summaries generated by transformer (Trans) and BART training on our created synthetic training data.
	}\label{tab:acdv}  
\end{table}







