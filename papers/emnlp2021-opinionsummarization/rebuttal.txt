G:
We noticed that the previous papers adopted different test sets
and ROUGE implementations, which causes big differences in ROUGE scores.
In the paper, we uniformly use files2rouge (Section 3.2), a popular ROUGE tool 
(Lewis et al., 2020;Chen and Yang, 2020),
which makes the ROUGE scores in our paper different from previous papers.
In files2rouge, R-L uses longest common subsequence, which is more
appropriate and faithful to original Rouge (Lin 2004),
while previous papers uses longest common substring, which means the R-L
smaller.
Nevertheless, we recomputed the ROUGE scores using previous versions (R-1/R-2/R-L):
ROUGE-FewSum: 
   FewSum: 33.56/7.16/21.49 (Amazon), 37.29/9.92/22.76 (Yelp)
   PlanSum: 35.87/8.43/22.08 (Amazon), 38.25/10.01/22.87 (Yelp)
   MB-B: 36.12/9.77/22.84 (Amazon), 39.03/10.32/23.10 (Yelp)
ROUGE-PlanSum:
   FewSum: 30.67/5.21/17.76 (Amazon), 32.32/6.89/18.59 (Yelp)
   PlanSum: 32.61/5.68/18.97 (Amazon), 34.94/7.12/19.82 (Yelp)
   MB-B: 33.14/6.35/19.03 (Amazon), 35.54/7.87/20.34 (Yelp)
ROUGE-FewSum uses the same test set as FewSum, which is different from other papers, and uses GoogleROUGE for ROUGE.
ROUGE-PlanSum follows PlanSum to use rouge==1.0.0rc2 for ROUGE.
The best performance of our approach (MB-B) on different ROUGE calculations shows its effectiveness.
We will add this explanation in the revised version.


R1:
1. Encode the word sequences of OAs and ISs by a single encoder.

We have taken such method as baseline in Section 2.2.2.
BAI takes the concatenation of x^p (OAs) and x^s (ISs) as the input for a single encoder 
and summary as output for a single decoder.
As shown in Table 7, 8 and 9, the MAI outperforms BAI in terms of all evaluation metrics,
which shows the effectiveness of our proposed model. 

2. Significant test.

As shown in Table 5, we take t-test as the significant test
between the existing state-of-the-art model (PlanSum) and our best model (MB-B)
``p<0.05'' shows the difference between PlanSum and MB-B is significant.
For ROUGE metric, 
The 95% confidence interval (95%-conf.int.) of PlanSum are 0.29070-0.32167(R-1)/0.06003-0.07644(R-2)/0.27609-0.30252(R-L)
and MB-B are 0.30772-0.33569(R-1)/0.06196-0.07927(R-2)/0.27888-0.30479(R-L).
The higher 95%-conf.int. of MB-B on ROUGE scores shows its advantage.
We will add above results into the revision.

3. The size of Yelp and Amazon is too small. 

Yelp and Amazon are standard opinion summarization datasets, which are used in many published papers 
(Chu and Liu, 2019; Brazinskas et al., 2020a; Amplayo et al., 2021). 
It is too expensive and difficult to obtain human-annotated summaries,
so the size of the existing test sets is small.

4. C_t in equation (8).
C_t is the context vector containing OA encoder and IS encoder states.
Following Vaswani et al.(2017) (Line-276),
the C_t should be modified by feed-forward network layer and become G_t. 
The equation (2) becomes p(y_t|y_0,...,y_{t−1},x^p,x^s)=softmax(WG_{t−1}+b).


R2:
1. Missing references. 

[1] needs control tokens as prefix for decoding to guide generation.
Instead, we create synthetic data by simulating the distribution of the OAs and ISs in real multi-reviews. 
Our approach can generate summaries from only reviews without external labels (control tokens).
[2] proposed a framework for single-document summarization taking one document as input, 
and cannot be simply applied to opinion summarization. We will cite these papers and include
the discussion in the related work.

2. ``leave-one-out'' sampling.

``leave-one-out'' sampling has two problems:
1) some of the sampled summaries cannot be
summarized from their corresponding input reviews,
when some aspects only appear in the sampled summaries.
2) The input multi-reviews of different sampled summaries of an entity are very similar.
For our method, 
we ensure all aspects of sampled summary are in other reivews (Section 2.1), 
and construct noisy OAs and ISs from the OAs and ISs in sampled summaries,
which avoids above problems and makes synthetic data better.
Besides, the FewSum is more similar to ``leave-one-out'',
which sample a summary and randomly samples N reviews from other reviews as input.
FewSum is the optimized ``leave-one-out''.
We compare our methods and FewSum in Table 4 and 5.

3. “consistent” perturbation.

OAs are extracted from sentences and ISs are sentences that cannot extract OAs.
For example,
``the food here is fantastic and the atmosphere is warm. i highly recommend trying out their prime rib. ...''
is a human-written summary.
In this summary, OAs are fantastic food and warm atmosphere, 
and IS is ``i highly recommend trying out their prime rib.''
The OAs and ISs are independent. 
So the noisy OAs and ISs constructed from the OAs and ISs in sampled summary are also independent.

4. MB outperforms MAI.

As shown in Table 3, compared with MAI, MB is pretrained on OAs by BAG model, 
which enhances the ability of model to select important OAs (Section 2.2.3). 
Because OAs are more important for opinion summaries, 
the enhancement of OAs selection is effective.  
As shown in Table 7,8 and 9, the MB fine-tuning on pretrained BAG are better than MAI.

5. Best-worst scaling.

We convert our human evaluation scores into Best-Worst scaling as follows:
Gold: 0.32, PlanSum: -0.48, MB-B: 0.16.


R3:
1. Quantitive analysis for ``using OAs to represent a review alone ignores some specific information''.

We calculate the distribution of the OA-sentences that can extract OAs and ISs that cannot extract OAs in multi-reviews and reference summaries in human-annotated test set.
-Yelp Multi-reviews: 54%(OA-sentences)/46%(ISs), Reference Summaries: 72%(OA-sentences)/28%(ISs).
-Amazon Multi-reviews: 55%(OA-sentences)/45%(ISs), Reference Summaries: 68% (OA-sentences)/32%(ISs).
The large proportion of ISs supports the assumption in the question.

2. ROUGE scores.
In PlanSum, the number of the matched unigrams includes repeated unigrams in the generated summaries, 
which make the ROUGE-1 score higher.
