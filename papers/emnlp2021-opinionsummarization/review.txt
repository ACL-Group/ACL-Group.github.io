R1:
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes a synthetic data construction for abstractive opinion summarization of multiple reviews. The dataset that was created by the proposed data construction method contains both opinion-aspect pairs and sentences, both of which were generated by the corresponding noising techniques. The novelty of the dataset is that the dataset was designed to exhaustively cover the aspects of an input in the corresponding summary. The proposed summarization method that was trained using the proposed synthetic dataset outperformed the existing methods in the evaluations using Yelp and Amazon datasets. This paper also confirmed that the proposed two noising techniques contributed to performance improvement via their ablation test.
In the proposed MAI architecture, opinion-aspect pairs (OAs) and implicit sentences (ISs) are separately encoded by different encoders, but is it helpful for performance improvement? A simple way to use both OAs and ISs is to encoder them by a single transformer encoder (i.e., the word sequences of OAs ans ISs are simply concatenated to a single one) and generate the output by a single decoder. If the authors add the results of such a simple method to the revised version of this paper, the authors can more clearly show the effectiveness of the proposed architecture.

Reasons to accept
A high-quality synthetic data construction of abstractive (opinion) summarization is essential for accurate summarization because the discrepancy between an input and its summary directly causes performance degradation. This work focused on the synthetic data construction issue. Although the proposed data construction method led to a slight performance improvement in this work, I think that the data construction method itself is beneficial to researchers focusing on automatic summarization.
Reasons to reject
The performance of the proposed method (the pretrained MB-B in Table 5) consistently is consistently higher than that of the existing method (the pretrained PlanSum), but there is not a big margin between two methods with regard to ROUGE-{1,2,L} F-scores. For the ROUGE metric, 95% confidence interval that was provided by the original ROUGE tool has been typically used as a significance test, but in this paper the authors did not adopt it as a significance test. In addition, the size of the evaluation datasets (Yelp and Amazon) is too small. I think that the data size may be not enough for investigating the performance difference between the proposed method and other competitors.
Questions for the Author(s)
How did the authors use C_t in equation (8)? (I guess that it was used in equation (2) instead of z_{t-1}, but there is no explanation about it.)
Typos, Grammar, Style, and Presentation Improvements
Figure 2. should be referred to within the body text of this paper.

Reproducibility:	3
Overall Recommendation - Long Paper:	3.5

R2:
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper develops a weakly-supervised opinion summarization model that takes two inputs: (1) textual data (i.e., input review) and (2) opinion-aspect pairs. The paper also addresses the inconsistency between input reviews and summary review that are used for training by developing a new data creation method that collects consistent input-output pairs. The proposed method that has different encoders shows strong performance on the Yelp and Amazon benchmarks.
Strengths:

The paper considers using two types of information for opinion summarization: textual data and opinion-aspect information.

The proposed data creation method addresses the input-output inconsistency issue that has been overlooked by the line of work.

Weaknesses:

Important references/comparisons are missing. The idea of using both textual information and opinion-aspect information itself is not novel.

The proposed framework is not fully justified. The major benefit seems to be obtained from the pre-training step, which does not provide additional information compared to the vanilla proposed model.

Automatic evaluation results seem to have some issues: (1) ROUGE scores by the baseline methods look significantly different from the original papers, and (2) ROUGE-L scores are similar to ROUGE-1 scores, which should be very unlikely to happen. This makes evaluation results less reliable.

(1) Important references are missing. The paper should clarify the novelty against [1] and [2] regarding the novelty of two types of inputs. (although the proceedings versions of the papers were published within 3 months. The first arXiv versions have been there for a while. The author(s) should at least cite them and further clarify the similarity/differences.)
[1] takes sentiment keyword and input summaries as input, and is very close to the paper. [2] is not for opinion summarization, but it offers a general framework that takes textual input and “guidance” information. In this case, aspect-opinion paris can be considered guidance information.

[1] Hady Elsahar, Maximin Coavoux, Matthias Gallé, Jos Rozen, Self-Supervised and Controlled Multi-Document Opinion Summarization, EACL 2021 (The arXiv version 4/30/2020) https://arxiv.org/abs/2004.14754

[2] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig, GSum: A General Framework for Guided Neural Abstractive Summarization, NAACL 2021 (The arXiv version 10/15/2020) https://arxiv.org/abs/2010.08014

(2) The paper takes the “collect-clean-data-then-add-noise” approach, which totally makes sense. However, the original leave-one-out sampling strategy (originally proposed by [Brazinskas et al. 2020b] and recent papers use this approach), which simply collects a randomly sampled review as a pseudo summary and the other reviews as pseudo input reviews, may happen to have a similar effect as the method proposed in the paper. If this were true, this would challenge the most important assumption of the paper “the pseudo training data should be consistent.” This point is not discussed or clarified. Thus, it is difficult to judge if the proposed data creation + adding noise strategy is practically superior to the original version. Of course, the proposed approach should have better controllability on the data creation and thus I do see (potential) benefits.

Following the description in the paper, OA pairs noising and IS noising are conducted independently, and thus it is not ensured that “consistent” perturbation is made for both OA pairs and IS.

Also, I’m wondering why MB significantly outperforms MAI. Both of the models should converge into the same model after careful fine-tuning as they have the same model architecture and the same information is provided. Without a further investigation, it is difficult to judge the robustness of the proposed framework.

(3) ROUGE-L scores on the Yelp and Amazon datasets in Tables 4, 5, 7 look significantly different from the values in the previous papers that use the same benchmarks. ROUGE-1 and 2 look reasonably close. By seeing ROUGE-L scores be close to ROUGE-1, I feel like something went wrong with ROUGE-L score calculation.

Minor comments

I would suggest the author(s) use Best-Worst scaling, which is commonly used for human evaluation for summarization research instead of simply averaging multi-graded scales for Table 6.

Reasons to accept
The paper takes two types of information: textual data and opinion-aspect information, which is a natural extension of existing methods. The proposed data creation method addresses the input-output inconsistency issue that has been overlooked by the line of work.
Reasons to reject
Lack of comparisons with missing references makes it difficult to judge the novelty/technical contribution of the paper. Some experimental results seem to be not aligned with the previous papers, which makes the paper less reliable.
Questions for the Author(s)
Please clarify the points raised in the in-depth review.
Missing References
Please see the in-depth review.
Typos, Grammar, Style, and Presentation Improvements
L445: get better -> gets better

Reproducibility:	3
Overall Recommendation - Long Paper:	2.5

R3:

What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes a data-creation method for multi-review summarization. The proposed method generate semi-structured data, including both textual and structured inputs, to provide training signals for the model. The authors also propose a dual-encoder pretrain-finetune framework that is suited on the semi-structure data.
Strengths:

propose a novel way to combine opinion-aspect pairs and textual sentences for data creation
propose a model framework for the synthetic data
Weaknesses

In the introduction, the authors argue that OAs represent explicit information while ISs represent implicit and specific information, but the support for this argument is an example (Table 1). There is an absence of strong evidence to support the authors' argument.
The reported results of previous work in this paper are different from the results in those original papers. For example, in PlanSum paper, it was reported that ROUGE-1 was 34.79 on Yelp, but this paper reports 31.19 for PlanSum.
Reasons to accept
propose a novel way to combine opinion-aspect pairs and textual sentences for data creation
propose a model framework for the synthetic data
Reasons to reject
In the introduction, the authors argue that OAs represent explicit information while ISs represent implicit and specific information, but the support for this argument is an example (Table 1). There is an absence of strong evidence to support the authors' argument.
The reported results of previous work in this paper are different from the results in those original papers. For example, in PlanSum paper, it was reported that ROUGE-1 was 34.79 on Yelp, but this paper reports 31.19 for PlanSum.
Questions for the Author(s)
In line 84, The authors argue that "using OAs to represent a review alone ignores some specific information". Could you provide quantitive analysis to support this argument?

Why the reported results of previous work in your paper are different from the results in their original papers? Am I missing some important descriptions about the comparison?

Reproducibility:	4
Overall Recommendation - Long Paper:	3
