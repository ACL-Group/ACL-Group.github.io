\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[review]{emnlp2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}


\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{bm}
\usepackage[linesnumbered, boxed, ruled]{algorithm2e}


\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\renewcommand\arraystretch{1.2}
\setlength\parskip{0.1\baselineskip}
\setlength{\textfloatsep}{0.5cm}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}

\usepackage{tikz}
%\usepackage{geometry}
%\usetikzlibrary{automata,positioning}
%\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}


\title{xxxx}

\author{Siyu Ren, Kenny Q. Zhu \\
	Advanced Data and Programming Technologies Lab \\
	Shanghai Jiao Tong University \\
	\texttt{\{roy0702@sjtu.edu.cn, kzhu@cs.sjtu\}.edu.cn} \\
}
\date{}


\begin{document}

\appendix
\section{Derivation for Stochastic Pruning}
\begin{table*}[tb!]
	\centering
	\scriptsize
	\begin{tabular}{l|ccccccc}
		\toprule
		\textbf{Task} & \textbf{RTE}&\textbf{COPA} &\textbf{CSQA} &\textbf{SWAG} &\textbf{HellaSWAG} &\textbf{aNLI} &\textbf{CosmosQA} \\
		\midrule
		\textsc{BERT} &$0\cup 6\cup 14$ &$5\cup 8\cup 14$ &$3\cup 4\cup 8\cup 12\cup 14$ &$1\cup 6\cup 10\cup 11$  &$0\cup 3\cup 5\cup 8\cup 14$ &$0\cup 3\cup 5\cup 8\cup 14$ &$0\cup 3\cup 5\cup 8\cup 14$ \\
		\bottomrule
	\end{tabular}
	\caption{Optimal fine-tuning knowledge type combination for \textsc{BERT-base} on commonsense reasoning tasks.}
	\label{table:finetuning}
\end{table*}
\label{ap:derivation}
To re-parametrize the discrete binary Bernoulli variable $m_{i,j}^l\sim B(\sigma(g_{i,j}^l))$, denote the approaximate differentiable variable as $\tilde{m}_{i,j}^l=\sigma(\frac{g_{i,j}^l+\log{U}-\log{(1-U)}}{\tau})$ where $\tau$ is a real-valued temperature value, we have the following derivation holds for arbitrary $\epsilon \in (0, 0.5)$:
\begin{align}
	P(m_{i,j}^l=1) - P(\tilde{m}_{i,j}^l\geq 1-\epsilon) \leq (\frac{\tau}{4})\log{\frac{1}{\epsilon}}
\end{align}
Specifically, when temperature $\tau$ approaches $0$, $\tilde{m}_{i,j}^l = m_{i,j}^l$.

\noindent
\textit{\textbf{Lemma 1:}} $\sigma^{-1}(x)=\log{\frac{x}{1-x}}$.\\
\noindent
\textit{\textbf{Lemma 2:}} $\frac{\sigma(x)-\sigma(y)}{x-y} \leq \frac{1}{4}$.

\noindent
\textit{\textbf{Proof:}}
\begin{align}
	&P(\tilde{m}_{i,j}^l\geq 1-\epsilon) \\
	=&P(\sigma(\frac{g_{i,j}^l+\log{U}-\log{(1-U)}}{\tau}) \geq 1-\epsilon)\\
	=&P(\frac{g_{i,j}^l+\log{U}-\log{(1-U)}}{\tau} \geq \log{(\frac{1}{\epsilon}-1)}) \\
	=&P(g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)}\geq \log{(\frac{1}{U}-1)})\\
	=&P(e^{g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)}} \geq \frac{1}{U}-1)\\
	=&P(U\geq \frac{1}{1+e^{g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)}}})\\
	=&\sigma(g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)})
\end{align}
Then:
\begin{align}
	&P(m_{i,j}^l=1) - P(\tilde{m}_{i,j}^l\geq 1-\epsilon)\\
	=&\sigma(g_{i,j}^l)-\sigma(g_{i,j}^l-\tau \log{\frac{1}{\epsilon}-1})\\
	\leq&\frac{\tau}{4} \log{(\frac{1}{\epsilon}-1)} \\
	\leq&\frac{\tau}{4} \log{\frac{1}{\epsilon}}
\end{align}
The process for deriving $P(m_{i,j}^l=0) - P(\tilde{m}_{i,j}^l\leq \epsilon) \leq (\frac{\tau}{4})\log{\frac{1}{\epsilon}}$ can be analogously obtained.
$\square$

\section{Knowledge Combination for Many-shot Learning, Zero-shot Learning and Triple Classification on Commonsense Reasoning}
\subsection{Notation for Knowledge Type}
\textit{HasSubevent}: 0\\
\textit{MadeOf}: 1\\
\textit{HasPrerequisite}: 2\\
\textit{MotivatedByGoal}: 3\\
\textit{AtLocation}: 4\\
\textit{CausesDesire}: 5\\
\textit{IsA}: 6\\
\textit{NotDesires}: 7\\
\textit{Desires}: 8\\
\textit{CapableOf}: 9\\
\textit{PartOf}: 10\\
\textit{HasA}: 11\\
\textit{UsedFor}: 12\\
\textit{ReceivesAction}: 13\\
\textit{Causes}: 14\\
\textit{HasProperty}: 15\\
In the remainder of this section, we use $\cup$ to indicate mask union operation upon multiple commonsense knowledge types.
\subsection{Many-shot Learning}
For fine-tuning on commonsense reasonging tasks, we only experiments with \textsc{BERT-base} due and perform hyper-parameter search only in terms of batch size in the range of $\{8, 16, 32\}$ and learning rate in the range of $\{3e^{-5}, 4e^{-5}, 5e^{-5}\}$ due to computational budget. We also adopt early stopping based on accuracy on the devlopment set. The combination achieving highest accuracy is shown in \tabref{table:finetuning}.

\subsection{Zero-shot Learning}
In constrast with fine-tuning, zero-shot evaluation is deterministic as long as the model does not involve any stochastic module, thereby averting extensive hyperparameter tuning.
Instead we perform exaustive search over knowledge combinations for each pretrained language model with number of knowledge types in $\{3,4,5\}$. The ConceptNet-grounded knowledge type combination achieving highest accuracy is listed in Table \ref{table:zero}.
\begin{table*}[tb!]
	\centering
	\scriptsize
	\begin{tabular}{l|ccccccc}
		\toprule
		\textbf{Task} &\textbf{COPA~(Dev.)} &\textbf{CSQA} &\textbf{CA} &\textbf{WSC} &\textbf{SM} &\textbf{ARCT1} &\textbf{ARCT2}\\
		\midrule
		\textsc{DistilBERT}  &$1\cup 6\cup 14$ &$2\cup 3\cup 13$ &$0\cup 1\cup 7\cup 9$  &$6\cup 7\cup 10$ &$2\cup 8\cup 13$ &$2\cup 3\cup 14$ &$1\cup 2\cup 7$\\
		\textsc{BERT}  &$4\cup 11\cup 15$ &$1\cup 2\cup 15$ &$6\cup 8\cup 12$  &$2\cup 9\cup 14$ &$6\cup 12\cup 15$ &$1\cup 9\cup 10$ &$1\cup 5\cup 8$\\
		\textsc{RoBERTa} &$2\cup 3\cup 8$  & $0\cup 2\cup 5$&$0\cup 1\cup 8$  &$1\cup 2\cup 4\cup 5\cup 11$ &$8\cup 11\cup 12$ &$2\cup 5\cup 11\cup 13$&$0\cup 8\cup 11\cup 13$\\
		\textsc{MPNet}  &$1\cup 6\cup 8\cup 10$ &$6\cup 12\cup 13$ &$2\cup 3\cup 10$  &$1\cup 3\cup 4\cup 9$ &$6\cup 10\cup 13\cup 15$ &$2\cup 5\cup 6\cup 11$ &$5\cup 6\cup 7\cup 11$ \\
		\bottomrule
	\end{tabular}
	\caption{Optimal zero-shot knowledge type combination for each PLM on each commonsense reasoning tasks.}
	\label{table:zero}
\end{table*}
\begin{table*}[tb!]
	\centering
	\footnotesize
	\begin{tabular}{l|ccc|c|c|c}
		\toprule
		\textbf{Model} & \textbf{P@1} & \textbf{P@2} & \textbf{P@3} & \textbf{Sparsity}  & $\bm{l_b-l_t}$ & \textbf{\# Param.}\\
		\midrule
		\textsc{BERT-large} w/o pruning &15.1  &20.9   &24.6  & 0\% & - &336M  \\
		\textsc{BERT-large} w/ stochastic pruning &22.1  &30.1   &35.4   & $\sim$30\% & 17-24 &336M \\
		\textsc{BERT-large} w/ deterministic pruning &69.2  &74.1   &76.3   & $\sim$50\% & 17-24 & 284M\\
		\bottomrule
	\end{tabular}
	\caption{Macro-averaged precision metrics of \textsc{BERT-large} on the ConceptNet subset of LAMA.}
	\label{table:rank}
\end{table*}
\subsection{Triple Classification}
In analogy with zero-shot evaluation, here we show the optimal knowledge type combination of each PLM for triple classification task on ConceptNet-100K~\footnote{\url{https://ttic.uchicago.edu/~kgimpel/commonsense.html}}.

\noindent
\textsc{DistilBERT-base}: $3\cup 4\cup 12$\\
\textsc{BERT-base}: $9\cup 13\cup 14$\\
\textsc{RoBERTa-base}: $0\cup 4\cup 9\cup 13$\\
\textsc{MPNet-base}: $1\cup 4\cup 9$
\section{Additional Pruning Results}
\subsection{\textsc{BERT-large}}
We also apply our pruning procedure upon \textsc{BERT-large}, the rank-based metrics on LAMA~\footnote{\url{https://github.com/facebookresearch/LAMA}} is shown in Table \ref{table:rank}.

\section{Extracted Commonsense Triples}
Applying the pruned \textsc{DistilBERT-base} model to predict missing objects for triples in ConceptNet-100K test set, we obtain commonsense triples deemed to be novel by three human annotators with Flessi's Kappa score $\kappa$ of $0.65$. We further filtered out triples that are included in the training or development set of ConceptNet-100K. Here we show some representative cases categorized by their relations:

\noindent
\textbf{\textit{CapableOf:}}\\
$(computer, crash)$, $(computer, communicate)$\\
\textbf{\textit{IsA:}}\\
$(sex, relationship)$, $(submarine, weapon)$, $(submarine, vessel)$\\
\textbf{\textit{AtLocation:}}\\
$(knife, war)$, $(knife, dinner)$, $(crab, dinner)$\\
\textbf{\textit{UsedFor:}}\\
$(stage, fun)$, $(stage, performance)$, $(literature, education)$, $(literature, research)$\\
\textbf{\textit{HasA:}}\\
$(book, index), (book, information)$\\
\textbf{\textit{HasProperty:}}\\
$(music, loud)$\\
Future work involves using seed triples beyond ConceptNet-100K dataset, e.g., the whole ConceptNet knowledge graph , and mining more novel and plausible commonsense knowledge.
\end{document}
