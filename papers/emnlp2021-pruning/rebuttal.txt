Thanks for your valuable comment!
Q1: "Can you elaborate how you obtained the tSNE ..."
R1: We use both original PLM and pruned subnetworks to compute the [CLS] representation of prompts in the test set and then apply t-SNE to visualize the projected 2D representations. Prompts of relation r is represented by pruned subnetwork for relation r.

Q2: "How much space do you need to store ..."
R2: A pruned subnetwork is identified by its pruning mask, so the space used for storing mask for each subnetwork is 1-bit * num_of_params, e.g., ~22MB for BERT-base.

Q3: "The motivation of the approach isn't so clear ..."
R3: The universal representations produced by original PLMs have proven to be useful. But such representations mainly memorize abundant statistical linguistic dependencies(e.g. conditional MI between tokens)[1], which might not be desirable in structured tasks where robust modeling of high-level knowledge is expected. With your permission we will explicate our motivation more clearly in the revised version.
References:
[1]. On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies. NAACL 2021.


Q4: "Other baselines: Besides the vanilla version of the models, ..."
R4: As suggested by the title, in this paper we primarily focus on widely adopted PLMs and explore their prior relational knowledge representation. So we didn't include results of further fine-tuning PLMs on tuples for different 
relations in this paper, which changes the model parameters. In case reviewers are interested, the C-LAMA test set P@1/2/3 results of fine-tuned relation-only BERT-base-uncased on are 29.2/37.4/41.3 (original: 12.9/18.4/21.8, pruned: 57.6/63.8/67.2) and 27.9/36.3/41.4 for Distilbert-base-uncased(original: 11.4/16.6/19.9, pruned: 44.1/52.9/57.6).



Thanks for your valuable comment!
Q1: "Is the impact of the pruning on the performance is only due to disentanglement ..."
R1: Randomly pruning the network will almost definitely results in poor performance on downstream tasks(finetuned and zero-shot), as validated by both
previous literature and our work: Chen et al.[1] showed drastic performance degradation on all GLUE tasks(22.3 points on average) by fine-tuning randomly pruned pretrained BERT.
In Figure 2 we showed the poor knowledge probing results of randomly pruned models. We also observe similar trend in our preliminary experiments that randomly pruned BERT with comparable sparsity achieved a far lower MRR score of 1.1 on CKBC link prediction.
The impact of pruning on performance arises from the fact that pruning potentially reduces certain undesirable/spurious patterns ([2],[3]) modeled by original PLMs that are 
harmful for end-tasks, and the preserved relational knowledge in subnetworks can more reliably address such tasks.
References:
[1]. The Lottery Ticket Hypothesis for Pretrained BERT Networks. NeurIPS 2020
[2]. Stress Test Evaluation for Natural Language Inference. COLING 2019.
[3]. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. ACL 2019.

Q2: "In the zero-shot setting, the author only reports the best results ..."
R2: Contrary to finetuning, in zero-shot setting there is no randomness of performance, so no variance or significance is reported.

Q3: "How well such a network can capture knowledge about a combination ..."
R3: Taking two relations 'IsA' and 'HasProperty' as example. We can derive a combination of two triples <s1, IsA, o1> and <s2, HasProperty, o2>, where o1==s2, into a
new triple <s1, HasProperty, o2>. We test original and pruned BERT on these 5151 combined triples(absent in C-LAMA): BERT achieves 11.9/21.8/29.1 P@1/3/5 and our HasProperty subnetwork achieves 42.9/58.4/62.2 P@1/3/5.

Q4: "an analysis to see for which relation ..."
R4: We've included some preliminary analysis in Appendix. Table 2 shows the best combination of relation
masks for different PLMs and tasks. When all relations are used, the results on the downstream commonsense tasks deteriorate. This is because most tasks use only a few relations. 


Thanks for your valuable comment!
Q1: "The experiments were thorough, but also felt unfocused ... "
R1: To summarize, our contributions are two-fold:
First, our primary goal/motivation is to investigate can we find disentangled subspaces of the original general-purpose/entangled language representation space of PLMs such that different subspaces characterizes different types of relational knowledge contained in the input. Based on our proposed pruning procedure, we provide results and analysis in section 3.1 that corroborate the existence of such subspaces/subnetworks.
Second, we explore the possibility of using pruned subnetworks in commonsense-intensive tasks and indeed observe substantial improvements on CKBC and CSR(section 3.2 and 3.3).
With your permission we will explicate it more clearly in the revised version.

Q2: "it might have been more helpful to also include a comparison to the original network fine-tuned ..."
R2: It's straightforward to compare with original PLMs fine-tuned on relation-specific data, however, we didn't do that because the main goal of this paper is to investigate how relational knowledge 'already present'(or to say learned from pretraining) in PLMs is represented. Due to space constraint, please refer to response to Reviewer #1's Q4 for more detail.

Q3: "In 3.1, you say ..."
R3: It's not intended to be a specific validation setup, just a normal selection procedure as in many other ML researches.

Q4: "How much did you experiment with the pruning rate?"
R4: We experimented pruning rates in {40%, 50%, 60%, 70%, 80%} and observed that a pruning rate within 50%-70% would optimally extract subnetworks representing
different relation types while remaining transferable to end-tasks. Pruning rate higher than 70% starts to lose the capacity for representing relational knowledge hence perform worse in end-tasks. Since our goal is not network compression, pursuing an extremely high pruning rate is not necessary.
