Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes an approach to prune pretrained language models fine-tuned on relational knowledge to cater to specific relations. Through network pruning, this paper attempts to find a network subspace that would be highly predictive on a specific relation. Specifically, they present two forms of network pruning: stochastic and deterministic, that are applied to different layers of the transformer weights. Through experiments, they show that models finetuned on the ConceptNet subset of LAMA, when pruned, achieve higher precision@k values compared to the baseline models. Further, the authors claim that the pruned networks are also helpful in downstream tasks such as commonsense knowledge base completion and for many/zero-shot transfer to other commonsense reasoning benchmarks.
The positives of the paper include that it is experimentally strong at showing the benefits of pruning and clearly written. On the other hand, the paper has some conceptual weaknesses. Firstly, I believe the pruning approach might not be scalable when there are significantly many more relations in the knowledge base. The paper also doesn't compare with too many other baselines besides the vanilla version of the model to which pruning is applied. Finally, the motivation of the paper isn't clearly expressed in the introduction. It is only much later that the authors mention concretely what characteristics they desire from the subnetworks.

Reasons to accept
The experiments in section 3.1 are helpful in understanding the effectiveness of the pruning technique. It seems like the masks are indeed tuned well for the target relation.

The paper is written well and provides a clear description of the approach.

In their experiments, the authors go beyond the LAMA retrieval setup to show the effectiveness of their approach towards other commonsense reasoning benchmarks.

Reasons to reject
The motivation of the approach isn't so clear to me. The authors mention that not knowing how relational knowledge is represented in PLMs hinders their utility in structured tasks. I'm not sure why this would obviously be true. This does not not seem to be an empirically backed statement. Since this is one of the main motivations of the paper, I would request the authors to elaborate on this.

Other baselines: Besides the vanilla version of the models, did the authors consider other baselines for their experiments in Table 2? Perhaps, one could compare the pruning approach to relation-only models (models trained only on tuples for a relation) of a similar capacity? Although this might not be comparable in terms of space, it could still be useful to see that comparison. Other than that, I wonder if the authors considered other baselines.

CKBC experiments: 1) There are some strong baselines (which could get high MRR scores) on ConceptNet-100K (for eg, see "DualTKB: A Dual Learning Bridge between Text and Knowledge Base"). Have the authors compared with this work on the same splits as the ones used in this paper? 2) Evaluation sets for CKBC are too small (349, 446 instances) for reliable results.

Some of the many-shot learning experiment results don't appear to be significant.

Questions for the Author(s)
Can you elaborate how you obtained the tSNE visualization in Figure 4? Which network is used for representing each of the examples?

What is the guarantee that the proposed approach provides "maximally predictive" subnetworks, as mentioned in Line 172?

Do the zero-shot improvements still hold if the authors apply their pruning approach to models trained on other commonsense reasoning datasets such as WinoGrande?

How much space do you need to store a pruned network?

Typos, Grammar, Style, and Presentation Improvements
Line 95: "experiments", Line 118: "prompts", Line 373: "ConceptNet".
Reproducibility:	3
Overall Recommendation - Long Paper:	3

Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
The author[s] proposed to apply network pruning techniques to explore the latent relational knowledge hidden in pertained language models. In this study, the author[s] focused on the task of knowledge probing with cloze prompts. The author[s] proposed to restrict subspaces to have correspondences with subnetworks of LM parameters. The main idea is that a subnetwork of a relation is obtained by setting some dimensions of LM_parameters to zero. To do that they focus on estimating the representation space for relation_r, that is maximally predictive of prompts of relation_r. They use two pruning methods: (a) Stochastic (b) Deterministic pruning.
The author[s] performs their experiment on several tasks: Link prediction, triple classification, relational knowledge probing, and commonsense reasoning tasks. The results on relational knowledge probing are very impressive. In the conclusion, line 583, the author[s] claim that their model is robust zero-shot reasoner, is a bit strong.

Question: Is the impact of the pruning on the performance is only due to disentanglement between relational subnetworks or is it also because pruning acts as regularization to the model? Randomly pruning the network could have been a good baseline.

Reasons to accept

[1] The paper proposes a novel and interesting approach to explore the latent relational knowledge hidden in large pretrained LMs.

[2] The results on link prediction: their unsupervised pruned modelâ€™s result is comparable to supervised models and also unsupervised PLMs.
[3] The results on relational knowledge probing tasks are impressive. The deterministic pruning works very well.
Reasons to reject
For Commonsense Reasoning tasks, the fine-tuning results are not significant. In the zero-shot setting, the author[s] only reports the best results, so it would be important to know the variance and significance. To better understand the latent relational knowledge hidden in PLMs relational knowledge, it would have been interesting to see how their pruned model performs on CSR tasks where different relational types are explicitly required as knowledge. And an analysis to see for which relation types the model performs best and worse.
Questions for the Author(s)
How well such a network can capture knowledge about a combination of relational knowledge?
Reproducibility:	4
Overall Recommendation - Long Paper:	3.5

Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper explores the question of how commonsense knowledge can be discovered in particular subnetworks of a pretrained masked language model. Two variations of a pruning method are presented, both of which have the objective of pruning a pre-trained network such that it performs well on cloze-style tasks with a particular type of commonsense relation. The authors present evidence that the resulting pruned networks are better-suited to the task than the full, unpruned network, and argue that these subnetworks a largely disentangled from one another. They also evaluate the resulting pruned subnetworks on a variety of downstream tasks in commonsense reasoning and knowledge base completion. The applicability of the work is not obvious, but I would recommend it be accepted to the proceedings since the results are nonetheless compelling and the work adds to the field's understanding of how pre-trained language models represent structured information.
Reasons to accept
The experiments are thorough and paint a compelling narrative about the types of subnetworks that exist as the correspond to commonsense relation types.
The contribution is marginal but broad, adding to the literature about (a) pruning methods, (b) common sense in language models, and (c) the representation of information in language models.

The methodology is well explained and the authors cover the details of their experiments very well.

Reasons to reject
As mentioned above, it is not entirely obvious what exactly to take away from this paper. The experiments were thorough, but also felt unfocused, as if there was no clear question that was trying to be answered.
Many of the results compared the pruned network to the original network, but this feels somewhat unfair because the pruned network had task-grounded data to learn from. I think the comparison to the original network was helpful, but it might have been more helpful to also include a comparison to the original network fine-tuned on relation-specific data as well as the subnetwork pruned on this data. E.g., in Table 2 it would be helpful to have a fourth entry like "DistilBert w/ fine-tuning" as a baseline. Same thing for figure 4.

50% pruning is pretty low by pruning standards and undermines the argument for the subnetworks being disentangled (since they will have a lot of overlap in the actual parameters).

Questions for the Author(s)
In 3.1, you say "For a given LM, we save subnetworks that achieve highest micro-averaged P@1 on the validation set and report the micro-averaged P@K on the test set." What's the reason for this? It seems like a fairly specific validation setup.
How much did you experiment with the pruning rate? Did a higher rate of pruning (>90%) severely worsen your results?

Missing References
Your ref. to the Hugging Face Transformers paper is out of date. Use their 2020 EMNLP demo paper.
Reproducibility:	4
Overall Recommendation - Long Paper:	3.5

