\section{Packages used for Baselines}
\label{sec:pkgs}

The packages we adopted to re-implement the baseline mentioned in \secref{sec:implementation} are listed as follows:%in Table~\ref{tab:baseline}.

\textbf{Reading Comprehension}
\begin{itemize}
	\item Dataset: \url{https://github.com/nlpdata/dream/tree/master/data}
	\item Baseline Code: \url{https://github.com/huggingface/transformers}
	\item Evaluation Metric: \url{https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py}
\end{itemize}

\textbf{Dialogue Summarization}
\begin{itemize}
	\item Dataset: \url{https://arxiv.org/src/1911.12237v2/anc/corpus.7z}
	\item Baseline Code: \url{https://github.com/huggingface/transformers}
	\item Evaluation Metric: \url{https://github.com/pltrdy/files2rouge}; \url{https://github.com/Yale-LILY/SummEval}
\end{itemize}

\textbf{Style Transfer}
\begin{itemize}
	\item Dataset: \url{https://github.com/martiansideofthemoon/style-transfer-paraphrase}
	\item Baseline Code: \url{https://github.com/martiansideofthemoon/style-transfer-paraphrase}
	\item Evaluation Metric: \url{https://github.com/martiansideofthemoon/style-transfer-paraphrase}
\end{itemize}

\textbf{Question Generation}
\begin{itemize}
	\item Dataset: \url{https://github.com/microsoft/unilm/tree/master/unilm-v1}
	\item Baseline Code: \url{https://github.com/microsoft/unilm/tree/master/unilm-v1}
	\item Evaluation Metric: \url{https://github.com/microsoft/unilm/tree/master/unilm-v1}
\end{itemize}

\textbf{News Summarization}
\begin{itemize}
	\item Dataset: \url{https://drive.google.com/file/d/0BzQ6rtO2VN95a0c3TlZCWkl3aU0/view?resourcekey=0-toctC3TNM1vffPCZ7XT0JA}
	\item Baseline Code: \url{https://github.com/huggingface/transformers}
	\item Evaluation Metric: \url{https://github.com/pltrdy/files2rouge}; \url{https://github.com/Yale-LILY/SummEval}
\end{itemize}

\section{Preliminary Studies on TCL}
\label{sec:preliminary}

Preliminary studies on dialogue summarization for TCL under different settings are shown in Table~\ref{tab:tclpre}.
We can see that the ``soft'' setting does help the TCL with sub-sequence generation curricula, which is consistent with the results in \citet{liang-etal-2021-token-wise}.
Results are opposite for TCL with our proposed sequence completion curricula. The ``soft'' setting considering the loss from prefix tokens actually hurts the intuition that ``the shorter the target is, the easier the tasks is'' as explained in \secref{sec:icl}.  As a result, SC-hard performs better than SC-soft.


\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{cccccc}
		\hline
		{} & {R1} & {R2} & {RL} & {Met} & {BertS} \\
		\hline
		w/o CL& 51.88 & 27.30 & 42.77 & 24.75 & 71.38 \\
		\hline
		SG-hard &50.70 & 27.31 & 43.00 & 23.47 & 70.85\\
		SG-soft & 52.43 & 27.65 & 43.56 & 25.17 & 71.86 \\
		\hline
		SC-hard &  52.69 & 28.28 & 43.89 & 25.08 & 71.95 \\
		SC-soft & 51.39 & 27.53 & 43.06 & 23.84 & 71.35 \\
		\hline
	\end{tabular}
	\caption{Ablations on TCL learning algorithm with different settings.}
	\label{tab:tclpre}
\end{table} 
\begin{table}[th]
	\scriptsize
	\centering
	\begin{tabular}{cccccc}
		\hline
		{Curriculum Step} & {R1} & {R2} & {RL} & {Met} & {BertS} \\
		\hline
		w/o CL & 51.88 & 27.30 & 42.77 & 24.75 & 71.38 \\
		\hline
		1 epoch & 52.48 & 27.86 & 43.47 & 25.50 &71.83 \\
		1.58 epoch(70\%) & 51.89 & 27.64 & 43.51 & 24.37 &71.55 \\
		2 epoch & 51.93 & 27.75 & 43.37 & 24.73 & 71.57\\
		3 epoch & 52.43 & 27.65 & 43.56 & 25.17 & 71.85\\
		\hline
	\end{tabular}
	\caption{Performances on TCL-SG with different curriculum steps.}
	\label{tab:tclpre2}
\end{table}
Experiments on the sensitivity of curriculum step in TCL-SG~\cite{liang-etal-2021-token-wise} are in Table~\ref{tab:tclpre2}. 
It consistently has improvements on dialogue summarization compared with the baseline. However, the performances also vary a lot with different curriculum steps, especially on R1, Meteor and BertScore. The estimation rule proposed in~\citet{liang-etal-2021-token-wise} of computing the number of steps it takes to reach approximately 70\% of final scores doesn't perform well for dialogue summarization. So, we choose to set curriculum steps to 3 epochs for dialogue summarization and news summarization, and 2 epochs for reading comprehension and style transfer, which not only achieve better results, but also are fairer for comparisons. For news summarization, we still adopted their estimation rule and trained with 5200 curriculum steps.





%\begin{table}
%	\scriptsize
%	\centering
%	\begin{tabular}{p{1.8cm}p{5.0cm}}
%		\hline
%	\multicolumn{2}{l}{\textbf{Reading Comprehension}} \\
%	Baseline Code& \url{https://github.com/huggingface/transformers} \\
%	Evaluation Metric& \url{https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py} \\
%	\hline
%	\hline
%	\multicolumn{2}{l}{\textbf{Dialogue Summarization}} \\
%	Baseline Code &\url{https://github.com/huggingface/transformers} \\
%	Evaluation Metric& \makecell[l]{\url{https://github.com/pltrdy/}\\\url{files2rouge} \\ \url{https://github.com/Yale-LILY/}\\\url{SummEval} }\\
%	\hline
%	\hline
%	\multicolumn{2}{l}{\textbf{Style Transfer}} \\
%	Baseline Code &\url{https://github.com/martiansideofthemoon/style-transfer-paraphrase} \\
%	Evaluation Metric & \url{https://github.com/martiansideofthemoon/style-transfer-paraphrase}\\
%	\hline
%	\hline
%	\multicolumn{2}{l}{\textbf{Question Generation}} \\
%	Baseline Code& \url{https://github.com/microsoft/unilm/tree/master/unilm-v1} \\
%	Evaluation Metric& \url{https://github.com/microsoft/unilm/tree/master/unilm-v1}\\
%	\hline
%	\hline
%	\multicolumn{2}{l}{\textbf{News Summarization}} \\
%	Baseline Code& \url{https://github.com/huggingface/transformers} \\
%	Evaluation Metric& \makecell[l]{\url{https://github.com/pltrdy/}\\\url{files2rouge} \\ \url{https://github.com/Yale-LILY/}\\\url{SummEval} }\\
%	\hline
%	\end{tabular}
%	\caption{Details of Baselines}
%	\label{tab:baselines}
%\end{table}