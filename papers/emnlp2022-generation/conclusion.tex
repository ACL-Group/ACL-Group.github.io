\section{Conclusion}

This paper defines a new kind of curriculum learning strategy for NLG tasks called in-sample curriculum learning (ICL) by manipulating the difficulty of training within a training sample instead of ranking among samples. We propose the ICL algorithm with the sequence completion curriculum which boosts the performance of strong baselines on a wide range of tasks, showing the effectiveness and strong generalization ability of our approach. More training strategies under ICL digging the inherent difficulties of generating a language sequence are expected in the future.


%Previous state-of-the-art models for different NLG tasks shows further significant improvements when armed with the propotional ICL, including dialogue summarization, style transfer, reading comprehension and question generation. Comprehensive ablations and analysis show that ICL not only generalizes well to different tasks, but also alleviates the exposure bias to some extent.

\section{Limitations}

\begin{table} [h]
	\scriptsize
	\centering
	\begin{tabular}{p{2.5cm}p{1cm}p{1cm}p{1cm}}
		\hline
		{Tasks} & {w/o CL} & {TCL-SG} & {ICL-SC} \\
		\hline
		Reading Comprehension  &6.67 ep & 13.00 ep & 7.67 ep \\
		Dialog Summarization &6.00 ep &15.67 ep & 11.67 ep\\
		Style Transfer &6.50k st & 14.78k st & 9.67k st  \\
		Question Generation & 17.67k st& 37.73k st  & 21.00k st  \\
		News Summarization &21.00k st  & 47.20k st &36.00k st\\

		\hline
	\end{tabular}
	\caption{Average number of training steps for different approaches. ``ep'' and ``st'' are short for ``epochs'' and ``steps'' respectively.}
	\label{tab:humaneval}
\end{table}

One limitation of our approach is that in-sample curriculum learning methods (both TCL-SG and ICL-SC) always incur extra overhead during training compared with the vanilla model. Nevertheless, the inference time of different approaches is the same as the vanilla model.
In a word, it's worthwhile because (1) ICL-SC can perform significantly better than both baselines without additional computational requirements during inference in real applications; (2) ICL-SC doesn't rely on task-specific expertise and has strong generalization ability.

Due to the limited computational resources, we were unable to do experiments on machine translation.
According to the implementation details in \citet{liang-etal-2021-token-wise}, all of their machine translation experiments were done on 32G NVIDIA V100 GPUs which are much more powerful than a single RTX 3090.
Even for the low resource setting with around 133K to 612K training samples, they used dynamic batching with 4096 maximum tokens and trained for 60 epochs.
This will either lead to an out-of-memory error or take us several weeks or even months to get the results of a single run on our machine.
Instead, we tried our best to cover a range of representative natural language generation tasks and corresponding datasets with different characteristics, such as sizes and output lengths (\tabref{tab:taskdata}).