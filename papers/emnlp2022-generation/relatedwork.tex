\section{Related Work}
\textbf{Natural language generation} has received great attention with deep neural networks, especially pre-trained language models. It refers to the task where expected outputs for different purposes are in natural language~\cite{dong2021survey}. The inherent characteristic of having more than one correct output given the same input is the core challenge of solving this kind of task, especially for evaluation~\cite{singh2018does}.
%Previous work~\cite{singh2018does} has shown that that curriculum learning helps for data-to-text generations tasks, where the inputs are meaning representations.
% text-to-text也有 但是泛化较差 难以迁移



\textbf{Curriculum learning}~\cite{bengio2009curriculum} boost models' performances in a range of machine learning areas by reordering the training samples.
It meets great obstacles when applying to NLG tasks as it's hard to evaluate the difficulties of training samples. 
Different rules are developed for different tasks~\cite{platanios2019competence,chang2021does}. For example, \cite{liu2018curriculum} measures the complexity of question-answering pairs from the view of frequency and grammar simply for answers. \cite{kocmi2017curriculum} focuses more on POS features and the length of translation pairs.
Other works utilize additional models or targeting models in the previous training step~\cite{zhang-etal-2019-curriculum,zhang2018empirical}. \cite{shen2020cdl} reorder samples by the accuracy from an independent emotion classifier for response generation. However, such salient features do not always exist or can be well classified. There is also work~\cite{zhou2020uncertainty} on machine translation using either the reference perplexity or generations evaluated by corresponding metrics for ranking during training, while these scores are not ideal due to the one-to-many characteristic of NLG. 
Thus, designing a CL strategy generalizing well for NLG is difficult.
%\cite{xu2020curriculum} proposed a unified framework by utilizing different golden metrics to fit corresponding natural language understanding tasks(NLU). These classification tasks are well-defined with specific ground truths, which doesn't hold true for NLG tasks. 

Instead of figuring out the oracle scoring function for training samples, 
we propose to measure the language generation difficulty within a sample. 
\citet{liang-etal-2021-token-wise} did something similar though their approach
amounts to data augmentation by doing sub-sequence generation, which is
not exactly curriculum learning. We, on the other hand,  train on the original sample with a decreasing prefix length and thus learn from easy to hard.
