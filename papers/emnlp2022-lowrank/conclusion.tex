\section{Conclusion}
We first conduct a series of exploratory experiments on common weights 
pruning and low-rank factorization for language model compression. 
We identify a unique low-rank sparsity phenomenon in models produced by first-order pruning, 
which in turn motivates us to propose LPAF, a generic framework for language model compression. 
Augmented with sparsity-aware SVD and mixed-rank fine-tuning, our framework exhibits 
substantially better compression-performance trade-off on diverse natural language processing tasks.

