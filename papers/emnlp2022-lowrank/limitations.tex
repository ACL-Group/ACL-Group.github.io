\section*{Limitations}
\begin{table}[th]
	\centering
	\scriptsize
	\begin{tabular}{l|lllll}
		\toprule
		\multicolumn{1}{l|}{} & \multicolumn{5}{c}{Training time~(Minutes)}                  \\
		Task & SST-2   & QNLI     & MNLI     & QQP      & SQuAD    \\
		\midrule
		Fine-tuning           & 48  & 85   & 330  & 294  & 420  \\
		SMvP                  & 60  & 102  & 390  & 354  & 480  \\
		\bottomrule
	\end{tabular}
	\caption{Training time comparison.}
	\label{table:timecost}
\end{table}
\paragraph{Extra overhead.} In LPAF, a first-order pruning algorithm~(e.g., SMvP) is adopted to obtain a low-rank sparse model $T_{sparse}$, which serves as the starting point for further compression. Similarly, existing compression methods like distillation also needs to  fine-tune a PLM as the teacher model. \tabref{table:timecost} compares the training time of fine-tuning and SMvP. 
Compared to fine-tuning, SMvP has a small price to pay~(e.g., 12 extra minutes on SST-2 and 
1 extra hour on SQuAD) to complete training. Afterwards, sparsity-aware SVD is used to obtain $T_{factorized}$, which can be efficiently executed on GPU with negligible overhead. For  mixed-rank fine-tuning, LPAF takes approximately $1.4\times$ memory/$1.3\times$ time vs. vanilla fine-tuning, which is worthwhile since we only need to do it once and the compressed model can be deployed anywhere needed.
