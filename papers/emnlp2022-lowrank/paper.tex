% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
%\usepackage[review]{emnlp2022}
\usepackage[review]{emnlp2022}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{url,color}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{multirow,array}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{amsfonts,amssymb} 
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\minitab}[2][l]{\begin{tabular}{#1}#2\end{tabular}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Towards Low-Rank Sparsity for Language Model Compression}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
%Compression of large pre-trained language models~(PLMs) has attracted considerable attention recently. 
In this paper, we first investigate two lines of methods for compressing large pre-trained language
models (PLMs): weights pruning and low-rank factorization. We discover an exclusive low-rank sparsity pattern in models produced by a family of first-order weights pruning algorithms, which motivates us to unite the two approaches and
achieve more effective model compression. 
We further propose two techniques: sparsity-aware SVD and mixed-rank 
fine-tuning, that improves the initialization and training of the 
compression procedure respectively.  
Experiments on natural language understanding and question answering tasks 
show that the proposed method achieves superior compression-performance 
trade-off compared to existing approaches.
\end{abstract}

% Entries for the entire Anthology, followed by custom entries
% \bibliography{anthology,custom}


\input{intro}
\input{method}
\input{experiment}
\input{conclusion}
\input{limitations}
 \bibliography{emnlp2022}
 \clearpage
 \input{appendix}
\end{document}
