\section{Conclusion}
\label{sec:conclude}
In this work,
we proposed a new method to generate
mix-structured synthetic training data for 
opinion summarization.
%which is known for lacking training data.
We designed a transformer-based seq2seq model with a dual encoder
to deal with OAs and ISs separately.
The results showed that
our approach can make full use of mix-structured data
and generate better opinion summaries.
%As the  approach is more effective on reviews with explicit opinion information, in the future,
%we will enhance our approach to better handle various review summarization.


\section{Limitations}
The limitation of our proposed approach is that it is more sensitive to explicit opinion information and more effective in reviews with explicit opinions.

We observe that 
the performance of the models on different datasets.
\tabref{tab:abla} shows that
OURS trained on RT improves most because the movie reviews in RT contain more ISs, such as character and plot descriptions after using ISs.
OURS performs better than OURS$_{basic}$ because of the pretraining on single encoder with only sampled OAs as input.
The difference between OURS and OURS$_{basic}$ trained on Yelp and Amazon is greater than that trained on RT, 
because Yelp and Amazon contain more OAs than RT.
Thus, our approach is more effective on the reviews with more explicit opinions, like Yelp and Amazon.
 
\tabref{tab:all} shows that although our model achieves the best among all for RT, the margin here becomes even less.
This is because the summaries in RT are shorter with fewer explicit OAs.
%The average number of tokens in reviews of Yelp and Amazon is about $65$, whereas that of RT is about $25$.
Movie reviews include the discussion on plots, such as italicized sentences of Gold in \tabref{tab:exprt},
which makes the proportion of OAs in movie reviews less than that of Yelp and Amazon.
As shown in \tabref{tab:exprt}, even though the summary generated by OURS represents more information of gold summary than other baselines,
it is not much closer to gold summary. 
Therefore, the gain of our proposed OURS on RT is less than the other two datasets.

\begin{table}[th]
	\begin{center}
		\scriptsize
		\begin{tabular}{|l|m{5.8cm}|}	
			%\hline \bf{BAG} \\
			\hline
			Gold & 
			\textit{\textbf{movie} begins with promise .}
			but it suffers from a flimsy \textbf{narrative} and poor \textbf{execution} . \textit{with alien-sized plot holes} \\
			\hline
			OpiDig & \color{red}{great \textbf{concept} . a strange but cool \textbf{comedy} .}
			\\
			\hline
			TranSum& \textbf{hancock} is a \textbf{movie} that's a lot of fun, \color{red}{but it'll be a bit of the same time as the \textbf{movie}.}
			%\vspace{0.25em}
			\\
			\hline
			%\hline
			%BM & \textcolor{red}{great \textbf{concept}} , shaky \textbf{narrative} .  \textcolor{red}{\textbf{hancock} is a strange and fun \textbf{film} .} \\
			%\hline
			%Basic & \textbf{hancock} has a promising premise ,
			%but the \textbf{narrative} slips into a confusing \textbf{backstory} . \\
			\hline
			OURS & \textbf{hancock} has a promising premise ,
			but the \textbf{narrative} slips into a confusing \textbf{backstory} . 
			\\
			\hline
		\end{tabular}
	\end{center}
	
	\caption{Summaries generated by different models and their gold summary (Gold) from RT. Bolded words are aspects. The sentences in red don't match gold summary. The italicized sentences are ISs. %BM and OM are based on BART.
	}\label{tab:exprt}
\end{table}


%In summary, our proposed approaches are more sensitive to explicit opinion information. 
