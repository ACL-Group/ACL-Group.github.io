Review1:
We agree that the size of the testing sets is small. This is because the cost of writing opinion summaries for multi-reviews is very high. The improvement of our proposed approach on all three different testing sets shows that the results are reliable. The summaries in RT are more abstractive and generally better than Yelp and Amazon. The quality of Yelp and Amazon are similar to each other. 

----In the final version, we will polish Fig. 3 by using one label to represent a pair of blue and orange bars and distinguishing them by the legends, which can reduce the labels on the x-axis.

Review2:
R1&Q1:
Adding other evaluation models is a good suggestion. The reason for choosing BART as our basic model is that BART is effective on summarization and easy to use. We will apply our approach on T5 in the final version. As our proposed approach is orthogonal to the seq2seq model, we expect our approach to benefit opinion summarization methods regardless of the seq2seq model architecture.

Q2:
We tried to use Snippext [1] for aspect mining. As the aspect mining results of Snippext and MIN-MINER are similar, their corresponding final results are similar. However, most aspect mining tools with good performance are supervised, which is not suitable for synthesizing datasets for cross-domain datasets in this paper. As aspect mining is not our main contribution, we just chose unsupervised MIN-MINER for aspect mining (Please see the first footnote on Page 3).

[1] Zhengjie Miao, Yuliang Li, Xiaolan Wang, and WangChiew Tan. Snippext: Semi-supervised opinion mining with augmented data. WWW 2020.

Q3: 
1) During inference, we extract all opinion-aspect pairs from real multi-review as the combination of popular and unpopular pairs. 
2) GloVe: Refer to G1.

Q4:
Similar to obtaining popular and unpopular pairs during inference, we extract all ISs from real multi-review as input.

Review3:
R1:
We will modify the presentation of section 2 in the final version.
R2:
We uploaded the codes of the model and the main script to the anonymous github. As our approach is based on BART, the pre-processing and supporting scripts for execution can be found with the source codes of BART (https://github.com/facebookresearch/fairseq). We will release the complete data and codes after the acceptance of our paper.

Q1:
As shown in section 2.1.2, the target outputs for training are sampled from all reviews, because we don’t have training pairs with multi-review as inputs and summary as outputs. The test sets consist of multi-reviews as input and human-annotated summaries as outputs. Thus, during inference, we extract all OAs and ISs from multi-reviews as mix-structured inputs. We will make this clearer in the final version.

Q2:
Given all reviews of an entity, we first randomly sample a review. Then, we extract aspects (A) from the sampled review and extract aspects (A’) from other reviews. If A is the subset of A’, the sampled review can be seen as a summary.

Q3:
1) The meaning of this sentence is that the information in the summary comes exclusively from its input. Synonyms and rephrasing don't change semantic information. 
2) GloVe: Refer to G1.

Q4:
1） The proportion of unpopular pairs in each sampled OAs is about 26.7%. 
2） The number N is a parameter of the problem (Line 225), which depends on the number of input reviews in each test pair. Each test pair in Yelp or Amazon has 8 reviews as input. Each testing pair in RT has about 100 (on average) reviews as input. 
3） The annotators are the students who are proficient in English. We showed them some examples similar to the reviews in Table 1 and told them that the words, like food, service and experience, are aspects. 
4） The annotators are the students in the lab, but not the authors. We show annotators each multi-review and its corresponding summaries generated by different approaches. We shuffled the summaries and masked their generation methods.

Thank you very much for your comments and suggestions!
G1: ``GloVe’’
We use embeddings to measure the similarity between extracted opinion-aspect pairs, which don’t need the contextual information of the reviews. Thus, we select easy-to-use pretrained non-contextual embeddings, GloVe.
