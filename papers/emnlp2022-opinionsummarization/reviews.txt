Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes using mixed-structured data as weak supervision for opinion summarization. The mixed-structured data consists of both "opinion aspects" (OA, obtained from parsing) extracted from the data and "implicit sentences" (IS), namely the sentences where no OA is extracted.
Reasons to accept
The authors demonstrated that using both OA and IS performs better than using either with a fairly comprehensive ablation study (Sec 3.6)

The proposed method achieves state-of-the-art performance on 3 standard opinion/review summarization datasets.

The paper is nicely written and easy to follow

Reasons to reject
I don't see major risks regarding the proposed method. Perhaps there is one regarding the benchmarks themselves -- How reliable are the results given the small size of the testing sets of existing opinion summarization datasets? I am wondering which of the 3 datasets has the highest/lowest quality based on your human evaluation.
Questions for the Author(s)
N/A
Typos, Grammar, Style, and Presentation Improvements
Better polish Fig. 3. It is a bit too tight to fit/read.
Reproducibility:	3
Ethical Concerns:	No
Overall Recommendation - Long Paper:	3.5
Overall Recommendation - Short Paper:	3.5

Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
The paper presents a novel approach to opinion summarization, exactly to data generation for model training in opinion summarization task. The strengths of this paper is this complex approach description and the evaluation including the human assessment. As a weakness of this paper I see not well enough grounded choices of basic components (see below).
In closing, I like this paper and I recommend it for publication, although I expect the authors to add some justification of their decisions on the modules in their system.

Reasons to accept
New state of the art results in opinion summarization achieved due to novel data preparation method for model training.
Reasons to reject
Lack of architecture choices support in the paper body.
Questions for the Author(s)
The BART model is opensourced and well known, but it is not state of the art in summarization tasks for quite a time. I would suggest to add some evaluations of other models, e.g. T5 [1].
In 2.1.1 the authors follow Bhutani et al. in usage of MIN-MINER for aspect mining. They state that this choice is out of hand, so I think this is a natural question to ask: Have you tried other methods? It would be interesting to see the effects of different aspect mining methods on the final results.
In 2.1.3 the authors propose sampling of OAs. They use concept of "popularity" which requires a knowledge of golden summary. It is not clear to me, how this could be used in inference time, when we do not know the golden summary. 3a. The authors use GloVe embeddings to measure the aspect similarity, this choice is not explained in the paper. It would be nice to see some justificaiton.
In 2.1.4 the authors propose to use ROUGE-1 on IS to compare it with the golden summary. I am again not sure if it was possible to use it in inference mode.
Missing References
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W. and Liu, P.J., 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), pp.1-67.
Reproducibility:	4
Ethical Concerns:	No
Overall Recommendation - Long Paper:	4
Overall Recommendation - Short Paper:	4

Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes a novel solution for multi-review summarization. The authors propose to use semi-structured data such as opinion-aspect (OA) pairs and implicit sentences (IS) as the processed input to machine learning models and prove the effectiveness by empirical studies. The presented experimental results show better performance than the previous approaches. Despite providing clear motivation and a nice related work section, the approach description is challenging to follow. Also, the given code does not contain the data preparation part, so it's unclear how to reproduce the results. The trained model is unavailable to try out, the code for the training is messy, and the readme is insufficient to run the experiments successfully.
Reasons to accept
Simple yet effective approach with clear motivation and sufficient related work overview

Evaluation methodology sounds solid, the proposed methods compared with seven existing methods on three datasets, authors also provide an error analysis on generated summaries.

Reasons to reject
The presentation of the paper could be improved; the current description of the approach is often misleading;

The supportive code is minimal, such as not including pre-processing and supportive scripts for running, and needs a proper review. Trained models are absent.

Questions for the Author(s)
Line 156: This bit is unclear. It could be understood that we are sampling mix-structured input by similarity to our target output. This scheme would be odd as we would not have a target summary on the inference stage, and thus, it is unclear how to sample them. But I suppose that this is just my misunderstanding due to a misleading description. In any case, this part would benefit from rewriting.
Line 178: Could you describe how it was done (exact matching/similarity?) for reproducibility?

Line 180: "Because a summary should not discuss things outside of its input.‚Äù What about synonyms and rephrasing? Also, one might argue that a sophisticated enough system could draw logical conclusions from a set of summaries; thus, if many customers had arguments with staff, the target summary could contain a sentence like "unfriendly personnel or lack of good service, etc.". Line 197: Why Glove word vectors were chosen instead of other embeddings?

Line 201: on average, what's the proportion of unpopular pairs? Line 319: how this number was chosen? Line 347: how this aspect was described to annotators? Who were the annotators Line 357: how were assessors recruited? are they volunteers? are they authors? What was the task interface?

Reproducibility:	4
Ethical Concerns:	No
Overall Recommendation - Long Paper:	3
Overall Recommendation - Short Paper:	3

