\section{Experiment}
In this section, we will introduce our experiment setup and results.
%We will show the advantages of our two-step framework over the end-to-end pre-trained model and other strong baselines on three datasets.
%\textcolor{green}{Not only experiment setup?}

\begin{comment}
\KZ{The following descriptions of the 3 datasets are too verbose.
Simplify each into one or two sentences and put into \tabref{tab:statitics-datasets}.}
\begin{itemize}
%\item Question Rewriting in Conversational Context (\textbf{QReCC}) \citep{anantha-etal-2021-open}. The authors built it on questions from TREC CAsT \citep{DBLP:journals/corr/abs-2003-13624}, QuAC \citep{choi-etal-2018-quac} and Google Natural Questions \citep{kwiatkowski-etal-2019-natural}. Its task is to find answers to conversational questions within a collection of 10M web pages split into 54M passages.

\item Context Abstraction: Necessary Additional Rewritten Discourse (\textbf{CANARD}) \citep{elgohary-etal-2019-unpack}. The authors crowdsource context-independent paraphrases of QUAC \citep{choi-etal-2018-quac} questions and use the paraphrases to train and evaluate question-in-context rewriting. The first sentence of the multi-turn dialogue is usually a news title or the name of a person. Then two people will 
%ask and answer 
talk around the 
%content
context. 
%and 
The rewriting of the current turn of question will be given at the end.

\item Contextual Query Rewrite (\textbf{CQR}) \citep{DBLP:journals/corr/abs-1903-11783}. One task-oriented multi-turn dialogue dataset, the dialogues in which take place between human users and intelligent agents. Users will ask the intelligent agent to 
%meet their needs 
answer specific questions such as driving or shopping. The user's unclear requirements will be completed as the ground truth.

\item Multi-domain Coreference (\textbf{MuDoCo}) \citep{martin-etal-2020-mudoco}. The dataset contains authored dialogs between a ficional user and system, where the user asks the system to perform extensive tasks within multiple domains, as well as switch across a set of 6 task domains (calling, messaging, reminders, weather, news, and music).

%and CamRest676 dataset from the restaurant domain annotated with coreference and ellipsis (\textbf{CamRest}) \citep{quan-etal-2019-gecor}.



\end{itemize}

\end{comment}

\begin{comment}
\begin{table}[th]
\setlength\tabcolsep{3pt}
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrr}
\resizebox{\linewidth}{!}{\begin{tabular}{lp{1.8cm}p{1.8cm}p{1.8cm}}
%\hline
\toprule
 &    \textbf{CANARD} & \textbf{CQR} & \textbf{MuDoCo} \\ \midrule
Desc. &	\tabincell{l}{Teacher and \\student talking \\about a news title \\or a person.}	&	\tabincell{l}{Task-oriented \\dialogue between a \\user and an agent.}	&	\tabincell{l}{Daily conversation \\of six domains.}	\\ \midrule
Train & 16.88k  & 0.52k &  2.39k \\
Dev  & 1.79k & 0.06k & 0.29k  \\
Test &   2.96k & 0.06k & 0.30k  \\
Ave Len    & 429.77  & 143.70 & 73.43  \\
\% rewrite & 0.9291 & 0.9838 & 0.2668  \\
\bottomrule
\end{tabular}}
\caption{Descriptions of the datasets. ``Ave Len'' means the average length of context. ``\% rewrite'' denotes 
the percentage of samples whose current utterance is actually rewritten.}
%\KZ{This is not clear what it is: is percentage of dialogue instance that is rewritten or
%percentage of utterances that is rewritten? Give a formula if u can't say it clearly: ``Rew Per'' means the occurrence rate of rewriting.}
\label{tab:statitics-datasets}
\end{table}
\end{comment}

\subsection{Datasets}
%\KZ{Cite the three datasets in the text below.}
We tested the baseline and our framework on 3 public datasets in English.
The brief descriptions, statistics and samples are shown in \tabref{tab:datasets-samples}.

We can see that \textbf{CANARD} \citep{elgohary-etal-2019-unpack} has the largest size and the longest context length.
%and is the most difficult.
The sentence pattern in CANARD is complex, the understanding is difÔ¨Åcult, and the rewriting degree is high.
%the highest difficulty. 
%Our research is mainly conducted on CANARD. 
\textbf{MuDoCo} \citep{martin-etal-2020-mudoco} 
has a lower rewriting rate, which makes the rule-based method less accurate in predicting the locations to be rewritten. CANARD is a question and answer dialogue dataset between two people around a topic.
%The sentence pattern is complex, the understanding is difficult, and the rewriting degree is higher. 
\textbf{CQR} \citep{DBLP:journals/corr/abs-1903-11783} and MuDoCo contains imperative dialogues in life (between people or between people and intelligent agents). The sentence patterns are relatively simple, fixed and easy to understand. In the following part of this section, 
we can also tell from the results 
that our model has better performance on the latter two datasets.
As CANARD is the most challenging dataset among the three,
our research is mainly conducted on CANARD.

\begin{table}[th]
\setlength\tabcolsep{3pt}
\centering
\scriptsize
\resizebox{\linewidth}{!}{\begin{tabular}{lll}
%\hline
\toprule
\multicolumn{2}{c}{\textbf{Dataset}}  &\textbf{Description and Examples}
%&\textbf{Current Utterance}&\textbf{Ground Truth} 
\\ \midrule
\multicolumn{2}{c}{\textbf{CANARD}}  & \tabincell{l}{Teacher and student talking about news or a person.} \\ \midrule

& &\multirow{10}{*}{\tabincell{l}{
\textbf{Context} \\
\textbf{A:} anna politkovskaya\\ \textbf{B:} the murder remains unsolved , 2016 \\\\  \textbf{Current Utterance}
\\
 \textbf{A:} did they have any clues ? \\
\\ \textbf{Ground Truth} \\
\textbf{A:} did \textbf{investigators} have any clues \textbf{in the}\\ \textbf{unresolved murder of anna politkovskaya} ?}} \\
&&\\
Train     &     16.88k &\\
Dev     &     1.79k &\\
Test    &     2.96k &\\
Ave Len  &    429.77 &\\
\% rewrite  & 0.9291 &  \\
&&\\
&&\\
&&\\
\midrule

\multicolumn{2}{c}{\textbf{CQR}}  & \tabincell{l}{Task-oriented dialogue between a user and an agent.} \\ \midrule
 &  & \multirow{12}{*}{\tabincell{l}{\textbf{Context}\\
\textbf{A:} What gas stations are here ? \\ \textbf{B:} There is a Chevron . \\ \textbf{A:} That ' s good ! Please pick the quickest route\\to get there and avoid all heavy traffic ! \\ \textbf{B:} Taking you to Chevron . \\
\\  \textbf{Current Utterance} \\ 
\textbf{A:} What is the address ? \\
\\ \textbf{Ground Truth} \\
 \textbf{A:} What is the address \textbf{of the gas station Chevron} ?}} \\
 &&\\
 &&\\
 Train & 0.52k &\\
Dev     &     0.06k &\\
Test    &     0.06k &\\
Ave Len  &    143.70 &\\
\% rewrite  & 0.9838 &  \\
&&\\
&&\\
&&\\
&&\\
\midrule


\multicolumn{2}{c}{\textbf{MuDoCo}}  & \tabincell{l}{Daily conversation of six domains.} \\ \midrule
 &  & \multirow{11}{*}{\tabincell{l}{\textbf{Context}\\
\textbf{A:} put me on active now . \\ \textbf{B:} you are active now . \\ \textbf{A:} did i miss any calls or messages here ? \\ \textbf{B:} mila called yesterday at 1 am . \\
\\  \textbf{Current Utterance} \\
 \textbf{A:} is she on now ? \\
\\ \textbf{Ground Truth} \\ 
\textbf{A:} is \textbf{mila} on now ?}} \\
&&\\
&&\\
Train & 2.39k & \\
Dev     &     0.29k &\\
Test    &     0.30k &\\
Ave Len  &    73.43 &\\
\% rewrite  & 0.2668 &  \\
&&\\
&&\\
&&\\

\bottomrule
\end{tabular}}
\caption{Information and examples of three datasets.}
\label{tab:datasets-samples}
\end{table}

\subsection{Baselines}
We choose 4 strong baselines 
to compete with our framework.

\noindent
\textbf{T5-small model} and \textbf{T5-base model}~\footnote{A fine-tuned version of T5-base is used:\\ https://huggingface.co/castorini/t5-base-canard.}~\citep{2020t5}. 
We directly take the context and the current utterance as inputs, use the training set to fine-tune the T5 model, and test its end-to-end output on the test set as the result of rewriting the utterance.

\noindent
\textbf{Rewritten U-shaped Network (RUN)}~\citep{liu-etal-2020-incomplete}. 
In this work, the authors regard the incomplete utterance rewriting task as a dialogue editing task, and propose a new model using syntactic segmentation to solve this task.

\noindent
\textbf{Hierarchical Context Tagging (HCT)}\citep{hct}. A method based on sequence tagging is proposed to solve the robustness problem in rewriting task. They predict whether each position of the current uttrance should be deleted and which span in the context should be replaced. They also use a rule-based approach to improve the situation of adding additional prepositions and adding multiple spans to a single rewritten position.

\noindent
\textbf{Rule-based method (Ours-rule)}. This is a variant of ours which
uses the rule-based method in \secref{sec:ruolan-rule} to generate blanks in phase 1.

\subsection{Evaluation Metrics}
%First, 
We use the \textbf{BLEU}$_n$ score \citep{papineni-etal-2002-bleu} to measure the similarity between the generated rewritten utterance and the ground truth. Low order n-gram \textbf{BLEU}$_n$ score can measure precision, while high-order n-gram can measure the fluency of the sentence. 
We also use the \textbf{ROUGE}$_n$ score \citep{lin-2004-rouge} to measure recall of rewritten utterance. 
\textbf{Rewriting F-score}$_n$ \citep{pan-etal-2019-improving}
is used to examine the words newly added to the current sentence. 
We calculte Rewriting F-score by comparing words added by the rewriting model with added words in ground truth.
%These words and the words added by ground truth are used to calculate F-score,
It is a widely accepted metric that can better measure the quality of rewriting. In addition to the automatic evaluation method, we also asked human annotators to conduct comparative tests on the rewriting results.

\subsection{Implementation Details}

All of the models are running and evaluated on 2 Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz with 4 NVIDIA GeForce RTX 2080 GPU and a 128GB RAM. Due to the memory constraints of our experimental environment,
we adopt T5-small model in the second phase of our framework.

\subsection{Main Results}
\label{sec:main-results}

\begin{table}[ht!]
\setlength\tabcolsep{3pt}
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrrrrr}
\resizebox{\linewidth}{!}{\begin{tabular}{lccccccc}
%\hline
\toprule
\multicolumn{8}{c}{CANARD}\\ \midrule
\textbf{Method}&    \textbf{F}$_1$ & \textbf{F}$_2$ & \textbf{B}$_1$ & \textbf{B}$_2$ & \textbf{R}$_1$ & \textbf{R}$_2$ & \textbf{R}$_L$   \\ \midrule
HCT    & 0.3393 & 0.2841 & 0.6787 & 0.6171  & 0.8009 & 0.6648 & 0.7949\\
RUN    & 0.4384 & 0.3048 & 0.7012 & 0.6221  & 0.8053 & 0.6294 & 0.7902\\
T5-small & 0.5152 & 0.4035 & 0.7037 & 0.6410  & 0.8017 & 0.6661 & 0.7807\\
T5-base  & 0.5232 & 0.4120 & 0.6883 & 0.6259  & 0.7892 & 0.6546 & 0.7700\\
Ours-rule  & 0.4856 & 0.3404 & 0.7132 & 0.6200  & 0.7991 & 0.6118 & 0.7718\\
Ours-sup   & \textbf{0.5339} & \textbf{0.4136} & \textbf{0.7750} & \textbf{0.7011}  & \textbf{0.8277} & \textbf{0.6825} & \textbf{0.8113}\\ \midrule
Ours-gold  & 0.5820 & 0.4790 & 0.8013 & 0.7132  & 0.8622 & 0.7004 & 0.8314\\
\bottomrule
\end{tabular}}
\caption{Results on CANARD. ``Ours-sup'' (Our-supervised) is our framework. ``Ours-rule'' is our framework with unsupervised method introduced in \secref{sec:ruolan-rule}. ``Ours-gold'' is the result of directly inputting the sentence with the correct blanks into the T5-model in phase 2. ``\textbf{F}'' refers to Rewriting F-score. ``\textbf{B}'' refers to BLEU score. ``\textbf{R}'' refers to ROUGE score.}
\label{tab:canard-result}
\end{table}

\tabref{tab:canard-result} shows the results of our framework and baselines on CANARD. Among the three datasets we used, samples in CANARD is the most difficult and the most complex. 
%In all the experimental metrics, 
Our model is superior to other baseline methods
in all the experimental metrics. 
Especially in \textbf{BLEU} score, 
our method is significantly 
%higher
better 
than all baselines. 
%On the two metrics of 
As for \textbf{Rewriting F-score} and \textbf{ROUGE}, we found that the
performance of 
 end-to-end T5 model is close to 
%the results of 
our method. 
This is because the generative T5 model is very powerful and can generate fluent sentences. However, our 2-phase framework can better predict which positions in the current sentence should be rewritten, which
can not be achieved by the end-to-end model.
% the end-to-end model cannot do. 
In the following analysis, we will further analyze this point.




\begin{table}[ht!]
\setlength\tabcolsep{3pt}
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrrrrr}
\resizebox{\linewidth}{!}{\begin{tabular}{lccccccc}
%\hline
\toprule
\multicolumn{8}{c}{CQR}\\ \midrule
\textbf{Method}&    \textbf{F}$_1$ & \textbf{F}$_2$ & \textbf{B}$_1$ & \textbf{B}$_2$ & \textbf{R}$_1$ & \textbf{R}$_2$ & \textbf{R}$_L$   \\ \midrule
HCT    & 0.5858 & 0.3230 & 0.6419 & 0.5291  & 0.6780 & 0.4732 & 0.6558\\
RUN    & 0.5402 & 0.2948 & 0.6306 & 0.5185  & 0.6732 & 0.4506 & 0.6426\\
T5-small & 0.8084 & 0.7226 & 0.6227 & 0.5988 & 0.8432  & 0.7683 & 0.8304 \\
Ours-rule  & 0.6495 & 0.5773 & 0.6951 & 0.6518 & 0.7230  & 0.6031 & 0.6969 \\
Ours-sup   & \textbf{0.8747} & \textbf{0.8027} & \textbf{0.8864} & \textbf{0.8576} & \textbf{0.9119}  & \textbf{0.8389} & \textbf{0.8989}\\ \midrule
Ours-gold  & 0.8932 & 0.8291 & 0.9130 & 0.8899 & 0.9356  & 0.8803 & 0.9322 \\
\bottomrule
\end{tabular}}
\caption{Results on CQR.}
\label{tab:cqr-result}
\end{table}



\begin{table}[ht!]
\setlength\tabcolsep{3pt}
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrrrrr}
\resizebox{\linewidth}{!}{\begin{tabular}{lccccccc}
%\hline
\toprule
\multicolumn{8}{c}{MuDoCo}\\ \midrule
\textbf{Method}&    \textbf{F}$_1$ & \textbf{F}$_2$ & \textbf{B}$_1$ & \textbf{B}$_2$ & \textbf{R}$_1$ & \textbf{R}$_2$ & \textbf{R}$_L$   \\ \midrule
HCT    & 0.5610 & 0.4920 & 0.9302 & 0.9073  & 0.9493 & 0.8777 & 0.9485\\
RUN    & 0.4478 & 0.3199 & 0.9297 & 0.9019  & 0.9440 & 0.8543 & 0.9427\\
T5-small & 0.6244 & 0.5670 & 0.8747 & 0.7938 & 0.9495  & 0.8838 & 0.9487 \\
Ours-rule  & 0.6043 & 0.4742 & 0.8298 & 0.7827  & 0.9231 & 0.8058 & 0.9202\\
Ours-sup   & \textbf{0.6881} & \textbf{0.6250} & \textbf{0.9563} & \textbf{0.9405} & \textbf{0.9611}  & \textbf{0.8950} & \textbf{0.9605}\\ \midrule
Ours-gold  & 0.7590 & 0.6974 & 0.9741 & 0.9627 & 0.9783 & 0.9219 & 0.9778 \\
\bottomrule
\end{tabular}}
\caption{Results on MuDoCo.}
\label{tab:mudoco-result}
\end{table}

\tabref{tab:cqr-result} shows 
the results of our framework and baselines on CQR. And \tabref{tab:mudoco-result} shows the results on MuDoCo. Compared with CANARD, the two datasets are smaller in size and simpler in sentence structure. Our approach was significantly better than all baselines on all metrics. For \textbf{Rewriting F-score}, our method is 6.37 and 6.63 percentage points higher than the sub-optimal end-to-end T5-small model, respectively. This metric strongly shows that our method can introduce more new words provided in the ground truth (compared with the original sentence). Relatively larger advantages of our model compared with T5-small in \textbf{BLEU} and \textbf{ROUGE} show that 
our method based on blank prediction and filling can retain the structure of the original sentence to the greatest extent, so as to retain more correct same information when calculating these 2 metrics and comparing the two sequences.
However, end-to-end T5 model generate the whole rewriting utterance directly, which may lose some information from the original sentence.

An important reason why our framework is better than baseline on CQR and MuDoCo is that CQR mainly contains dialogues that users are asking agents for help. The positions and forms of words that can be added are relatively fixed, such as adding place adverbials. Samples in MuDoCo are basically imperative dialogues in daily life. It also has the same feature, which makes our model easier to learn.
The results in \secref{sec:results-step1} can also illustrate this point. The accuracy of the first phase of our framework is higher on CQR and MuDoCo.

\subsection{Human Evaluation}

\begin{table}[ht!]
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrr}
\begin{tabular}{lccc}
%\hline
\toprule
 &    \textbf{Win} & \textbf{Tie} & \textbf{Loss} \\ \midrule
Ours v.s. HCT    & 0.66 & 0.10 & 0.24 \\
Ours v.s. RUN & 0.50 & 0.16 & 0.34 \\
Ours v.s. Rule  & 0.70 & 0.10 & 0.20 \\
Ours v.s. T5-small   & 0.46 & 0.16 & 0.38\\
\bottomrule
\end{tabular}
\caption{Human evaluation on CANARD. ``Rule'' means our rule based method conacted with T5-small of 2nd phase, introduced in \secref{sec:ruolan-rule}.}
\label{tab:human-eval}
\end{table}

\tabref{tab:human-eval} shows the human evaluation results on CANARD. For each pair of competing models, 50 pairs of rewriting results were randomly sampled from the testset for comparative testing. A total of 200 questions were randomly assigned to 5 human volunteers on average. Each person needs to choose the better one from the prediction results of the two models. As can be seen from the table, our method is significantly stronger than RUN, HCT, and the rule-based method in \secref{sec:ruolan-rule}. When compared with the end-to-end T5-small model, our advantage is relatively small. After observing the feedback of human annotators, we find that the end-to-end model has the advantage of direct generation and can generate more complete and fluent sentences. Our method only generates the words needed in blank, which lacks a certain degree of sentence fluency. However, our 2-phase framework can accurately predict the position that needs to be rewritten in the current sentence, which is beyond the reach of the end-to-end model (see \secref{sec:case-study} for specific analysis).
%\textcolor{green}{can't get the logic here.}
 Taken together, our method should be even better.

\subsection{Ablation Tests}
\label{sec:ablation}

\begin{table}[ht!]
\setlength\tabcolsep{3pt}
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrr}
\resizebox{\linewidth}{!}{\begin{tabular}{lccccccc}
%\hline
\toprule
\textbf{Variant}&    \textbf{F}$_1$ & \textbf{F}$_2$ & \textbf{B}$_1$ & \textbf{B}$_2$ & \textbf{R}$_1$ & \textbf{R}$_2$ & \textbf{R}$_L$\\ \midrule
Ours-sup   & \textbf{0.5339} & \textbf{0.4136} & \textbf{0.7750} & \textbf{0.7011}  & \textbf{0.8277} & \textbf{0.6825} & \textbf{0.8113} \\ \midrule
w/o LCS    & 0.5268 & 0.4070 & 0.7620 & 0.6857 & 0.8248 & 0.6773 & \textbf{0.8113} \\
w/o split  & 0.5040 & 0.3919 & 0.7662 & 0.6745 & 0.8246 & 0.6522 & 0.7883 \\
w/o hint & 0.5210 & 0.4018 & 0.7674 & 0.6935 & 0.8244 & 0.6771 & 0.8084 \\
\bottomrule
\end{tabular}}
\caption{
%\KZ{No good to use resize to change the fonts arbitrarily. The fonts are too small now.
%Maybe keep only 3 decimal places, or adjust the spaces between columns?}
Ablation test on CANARD. ``w/o LCS'' means replace LCS algorithm with a greedy algorithm. ``w/o split'' and ``w/o hint'' respectively represent removing the 2 kinds of optimizations in \secref{blanks-filling}.}
\label{tab:ablation}
\end{table}

\tabref{tab:ablation} shows the results of ablation test on CANARD. We can see that by replacing LCS algorithm with greedy algorithm, the experimental results have decreased to a certain extent, which shows the effectiveness of LCS algorithm. On the other hand, due to the diversity of experimental data, the matching algorithm can only approach the correct results, and can not guarantee the complete correctness. Greedy algorithm is also a substitute. Our greedy algorithm is described as follows.

%Use 
We use 2 pointers to traverse the current utterance and ground truth utterance. The pointers point to the current word in each of the two utterances. If they cannot be matched, the pointer of the ground truth will advance to the next matching position and stop, and the scanned span will be marked as an ``ellipsis''. If no match can be made until the end, the pointer of the current occurrence moves forward one bit and adds the previous position to the span of ``coreference''.

If we remove the two optimizations of splitting sentences according to the number of blanks and adding hint from our framework, the decline will be more obvious. The reason is that splitting sentences can keep more syntactic information in sentences, and multiple blanks will make sentences look ``full of loopholes''. Adding hint will prompt the original words in the language model in phase 2, so as to provide more information. For example, if our hint is ``he'', the model will not tend to fill in a female name or other things here.
\subsection{Time Cost Evaluation}

\begin{table}[th]
\setlength\tabcolsep{3pt}
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrrr}
\resizebox{\linewidth}{!}{\begin{tabular}{l|cc|cc|c}
%\hline
\toprule
\multirow{2}{*}{\textbf{Model}}&    \multicolumn{2}{c|}{\textbf{T5}} & \multicolumn{2}{c|}{\textbf{Bert-CRF}} & \multirow{2}{*}{\textbf{Total}}\\
 &    Fine-tune & Predict & Train & Predict & \\ \midrule
Ours-sup  & 4h31m20s  & 18m10s & 9m17s & 24s &  4h59m11s \\ 
T5-small & 4h2m54s  & 24m32s & - & - &  4h27m26s \\
\bottomrule
\end{tabular}}
\caption{Time cost of our method and end-to-end T5-small model on CANARD.}
\label{tab:time-analysis}
\end{table}

\tabref{tab:time-analysis} shows the results of training and predicting time on CANARD. In \secref{sec:main-results}, we found that our model has the least advantage over the end-to-end T5-small model. Therefore, in this section, we compare their time consumption. Under the same configuration, we found that our method would take more time to fine-tune. This is understandable because although there are only 5571 samples in the testset of canard dataset, we will segment sentences according to the number of blanks. Even if there are sentences with zero blanks, this optimization also leads to an increase in the number of samples to 6569. Interestingly, in the subsequent prediction process, our model takes less time. This may be because our model does not need to generate a whole sentence, but only needs to fill in the blank, which makes the total length of the generated sentences shorter. Due to the short time of Bert-CRF, our method only takes 11.9\% more time than the end-to-end T5 model, and the overall size of the model is almost the same as other training requirements. Therefore, we believe that even a small increase in results can illustrate the effectiveness of our method.


\subsection{Case Study}
\label{sec:case-study}

\begin{table}[th]
\centering
\scriptsize
\resizebox{\linewidth}{!}{\begin{tabular}{cl}
%\hline
\toprule
\textbf{Context}  & \tabincell{l}{\textbf{A:} betsy devos \\ \textbf{B:} school vouchers \\ \textbf{A:} what are the school vouchers ? \\ \textbf{B:} would allow students to attend private schools \\with public funding .} \\ \midrule
\textbf{Current}  & 
\tabincell{l}{\textbf{A:} how do people get them ?} \\
\textbf{Gold}  & 
\tabincell{l}{\textbf{A:} how do people get the school vouchers ?} \\
\textbf{Ours-sup}  & 
\tabincell{l}{\textbf{A:} how do people get \textcolor{red}{school vouchers} ?} \\
\textbf{HCT}  & 
\tabincell{l}{\textbf{A:} how do people get \textcolor{red}{private} ?} \\
\midrule

\textbf{Context}  & \tabincell{l}{\textbf{A:} anna ella carroll \\ \textbf{B:} 1850s political career \\ \textbf{A:} what made anna get into politics ? \\ \textbf{B:} carroll joined the american party ( the know \\nothing party ) following the demise of the whigs .} \\ \midrule
\textbf{Current}  & 
\tabincell{l}{\textbf{A:} where was she when she started \\the american party ?} \\
\textbf{Gold}  & 
\tabincell{l}{\textbf{A:} where was anna ella carroll \\when she started the american party ?} \\
\textbf{Ours-sup}  & 
\tabincell{l}{\textbf{A:} where was \textcolor{red}{anna ella carroll} \\when she started the american party ?} \\
\textbf{RUN}  & 
\tabincell{l}{\textbf{A:} where was \textcolor{red}{anna ella anna ella carroll} \\when she started the american party ?} \\
\midrule

\textbf{Context}  & \tabincell{l}{\textbf{A:} yogi berra \\ \textbf{B:}major leagues \\ \textbf{A:}what team signed him ? \\ \textbf{B:}berra was called up to the yankees \\and played his first game on september 22 , 1946 ;} \\ \midrule
\textbf{Current}  & 
\tabincell{l}{\textbf{A:} how long was he there ?} \\
\textbf{Gold}  & 
\tabincell{l}{\textbf{A:} how long was yogi berra with the yankees ?} \\
\textbf{Ours-sup}  & 
\tabincell{l}{\textbf{A:} how long was yogi berra \textcolor{red}{at the yankees} ?} \\
\textbf{T5-small}  & 
\tabincell{l}{\textbf{A:} how long was yogi berra \textcolor{red}{there} ?} \\
\midrule

\textbf{Context}  & \tabincell{l}{\textbf{A:} real love ( beatles song ) \\ \textbf{B:} early origins} \\ \midrule
\textbf{Current}  & 
\tabincell{l}{\textbf{A:} who originally wrote real love ?} \\
\textbf{Gold}  & 
\tabincell{l}{\textbf{A:} who originally wrote beatles song real love ?} \\
\textbf{Ours-sup}  & 
\tabincell{l}{\textbf{A:} who originally wrote real love ?} \\
\textbf{T5-small}  & 
\tabincell{l}{\textbf{A:} who originally wrote \textcolor{red}{the beatles song} real love ?} \\

\bottomrule
\end{tabular}}
\caption{Typical examples extracted from the prediction results on CANARD.}
\label{tab:case-study}
\end{table}

%\tabref{tab:case-study} shows the result of case study. 
\tabref{tab:case-study} shows some specific examples
of rewriting using our model and other baselines.
The examples of predicting results of our model, HCT, RUN and T5-small on CANARD dataset are shown from top to bottom. 
%\KZ{Why don't we show some more results for the other 2 datasets in the appendix?} 
HCT tends to copy the predicted span directly from the context. From the first example, we can find that HCT predicts the correct position of coreference in the current sentence, but finds the wrong span. From the second example, we can see that RUN's edit based model duplicates the span from the context.  Our model uses T5 to find the corresponding span from the context, which is significantly stronger than RUN and HCT.

In the third example, our model is compared with T5 model for end-to-end prediction. It can be observed that the word ``there'' is not considered to be replaced by the end-to-end model, which is due to the fact that the position to be rewritten is not obviously predicted. Our two-phase framework can make up for this. The sequence annotation model indicates that ``there'' is a part that needs to be replaced, so the T5 in the second phase can be predicted correctly. This is our advantage over the end-to-end model.

The fourth example shows the shortcomings of our model. Compared with the end-to-end T5-small model, the first step of our framework failed to predict the need to insert words between ``write'' and ``real'', so the second step could not fill in the correct answer. This shows the inherent defect of the 2-step framework, that is, the result of the second step depends on that of the first step, and there is a certain gap.


\subsection{Results of Predicting Positions}
\label{sec:results-step1}


\begin{table}[ht!]
\centering
\scriptsize
%\small
%\begin{tabular}{lrrrr}
\resizebox{\linewidth}{!}{\begin{tabular}{clcccc}
%\hline
\toprule
& \textbf{Methods}&    \textbf{Acc} & \textbf{Precision} & \textbf{Recall} & \textbf{F}$_1$\\ \midrule
\multirow{2}{*}{\textbf{CANARD}} & Ours-sup   & \textbf{0.9103} & \textbf{0.7387} & \textbf{0.6525} & \textbf{0.6930} \\ 
 & Ours-greedy    & 0.8956 & 0.6914 & 0.6209 & 0.6543   \\    \midrule
\multirow{2}{*}{\textbf{CQR}} & Ours-sup   & 0.9381 & \textbf{0.8251} & \textbf{0.7331} & \textbf{0.7764} \\ 
 & Ours-greedy    & \textbf{0.9400} & 0.7992 & 0.6920 & 0.7417   \\     \midrule
\multirow{2}{*}{\textbf{MuDoCo}} & Ours-sup   & \textbf{0.9801} & \textbf{0.7738} & \textbf{0.7103} & \textbf{0.7407} \\ 
 & Ours-greedy  & 0.9692  & 0.6167 & 0.5085 & 0.5574     \\ 
\bottomrule
\end{tabular}}
\caption{Results of predicting positions on 3 datasets. ``Ours-greedy'' means to use greedy algorithm to replace the LCS algorithm.}
\label{tab:results-step1}
\end{table}

\tabref{tab:results-step1} shows the results of our LCS based algorithm and greedy based algorithm in predicting the location that needs to be rewritten (that is, the first phase in the 2-phase framework). They are trained and tested on the sequence annotation data generated by their own methods. We can see that the algorithm based on LCS has better effect.
