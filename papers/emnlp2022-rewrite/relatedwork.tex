\section{Related Work}
%Ruolan should write this part.
Early work on rewriting often considers the problem as a 
standard text generation task,
using pointer networks 
or sequence-to-sequence models %\citep{elgohary-etal-2019-unpack,quan-etal-2019-gecor} 
with a copy mechanism 
\citep{su-etal-2019-improving,elgohary-etal-2019-unpack,quan-etal-2019-gecor}
to fetch the relevant information in the context \citep{gu-etal-2016-incorporating}.
Later, pre-trained model like T5 \citep{2020t5} are fine-tuned with conversational 
query reformulation dataset to generate the rewritten utterance directly. \citet{DBLP:journals/corr/abs-2204-03958} uses Picker which identifies the omitted tokens to optimize T5. We the results in their paper, 
therefore, we did not compare with it in the experiments. In general, these generative approaches ignore the characteristic of IUR problem: rewritten utterances often share the same syntactic structure as the original incomplete utterances.

Given that coreference is a major 
source of incompleteness of an utterance,
another common thought is to utilize a 
coreference resolution or corresponding 
feartures. 
\citet{tseng-etal-2021-cread} 
proposed a model which jointly learns coreference resolution
and query rewrite 
with the GPT-2 architecture \citep{Radford2019LanguageMA}.
By first predicting coreference links
between the query and context,
the performance of rewriting has improved 
while the incompleteness is induced
by coreference.
However, this does not work for utterances
with ellipisis. Besides, the performance of 
the rewriting
 model is limited by
the coreference resolution model.

Recently, some of the work on 
incomplete utterance rewriting 
focuses on the ``actions'' we
take to change the original
incomplete utterance
into a self-contained
utterance (target utterance).
\citet{hao-etal-2021-rast} 
solves this problem  
%imcomplete utterance rewriting
with a sequence-tagging model.
For each word in the input utterance, 
the model will predict whether to
delete it or not, meanwhile, the
span of words which need to be inserted before
the current word will 
be chosen from the context.
\citet{liu-etal-2020-incomplete} 
formulated the problem as 
a syntactic segmentation task
by predicting 
segmentation
operations for 
the rewritten utterance.
\citet{DBLP:conf/icassp/ZhangLWCX22} extracts the coreference and omission relationship directly from the self-attention weight matrix of the transformer instead of word embeddings. Compared with these methods, our framework separates the two phases more thoroughly of predicting the rewriting position and ﬁlling in the blanks, and meanwhile, reduces the difﬁculty of the two phases with the divide and conquer method.
%directly extracting the coreference and omission relationship from the self-attention weight matrix of the transformer instead of word embeddings.
%\textcolor{blue}{Ruolan: could zitong please
%add 1 or 2 utterances to explain our advantage over
%sequence tagging and syntactic 
%segmentation approach?}
%Compared with these methods, our framework more thoroughly separates the two steps of predicting the rewriting position and filling in the rewriting content, and reduces the difficulty of the two steps with the divide and conquer method.
