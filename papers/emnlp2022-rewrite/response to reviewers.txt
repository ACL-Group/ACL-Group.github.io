Response to Review #1:
** Not reporting the BLEU-4 and EM:
Below are the numbers which were not reported previously due to space constraints:
BLEU-4:
	CANARD: HCT 0.5179 RUN 0.5007 T5-small 0.5429 T5-base 0.5278 Ours-rule 0.4861 Ours-sup 0.5905 Ours-gold 0.6546
	CQR: HCT 0.3936 RUN 0.3747 T5-small 0.5572 Ours-rule 0.5786 Ours-sup 0.8086 Ours-gold 0.8487
	MuDoCo: HCT 0.8521 RUN 0.8466 T5-small 0.7510 Ours-rule 0.6885 Ours-sup 0.9075 Ours-gold 0.9375
EM:
	CANARD: HCT 0.2486 RUN 0.1842 T5-small 0.2459 T5-base 0.2389 Ours-rule 0.1764 Ours-sup 0.2669 Ours-gold 0.3940
	CQR: HCT 0.1280 RUN 0.0984 T5-small 0.1823 Ours-rule 0.1308 Ours-sup 0.3598 Ours-gold 0.4206
	MuDoCo: HCT 0.7450 RUN 0.7168 T5-small 0.4618 Ours-rule 0.4738 Ours-sup 0.7998 Ours-gold 0.8627
It can be seen that our framework performs well on these two metrics.

** The correctness of your experimental results:

We used the script score.py provided by HCT (https://github.com/lisjin/hct) to calculate BLEU score. 
If you take CANARDâ€™s test set (reference) and their best experimental result pred_test_05_0.5183.txt (prediction) as input, you will get a BLEU score result similar to that provided by us. The reason for the gap with the original HCT paper is unknown, and the author has not responded to it. As HCT is a newly published paper, the number of citations is not enough to verify.
T5-base in Inoue et al. (2022) against ours. In the e-mails the authors, they only said that please check the segmentation, but did not provide their code.

** Other pre-trained language models: 
The reason why we chose T5 is that it took the pre-training task of filling in the missing parts of the sentence. This is consistent with step 2 in our framework, so we adopted T5 instead of other pre-trained models.
In future work, we will try to compare with other pre-trained models.


** different random seeds and recorded the error bars?
No, our experimental results are the average results obtained by training and fine-tuning three times under the same configuration.

** the meaning of Acc?
Acc means accuracy, which is the proportion of correct tags predicted. Unlike precision, P/R/F1 is the result based on non-O labels. Acc will take into account the labels "O". Since the number of labels "O" is very large, it is easy to predict correctly, so Acc will be close to 100%.


** If the syntax of the rewritten utterance is different from the original's, does the LCS-based method still work?
LCS algorithm will be wrong. Our framework is based on the observation that the syntactic rules before and after rewriting are basically unchanged. This is also the conclusion on which rewriting models are generally based so far.


Response to Review #2:
** the rules for predicting the editing positions:
In Section 2.1.1, we summarized the rules:
	Personal Pronouns
	Interrogatives
	That, This
	The + Noun Phrase
	Other, Another, Else
	Before, After
In the testsets, the proportion of samples to which the rules are applied is:
	CANARD: 0.8632
	CQR: 0.6449
	MuDoCo: 0.5498
The main reason for this phenomenon is that most of our rules are obtained through observation and summary of CANARD, and the generalization ability is insufficient. However, our rule-based algorithm is only a baseline, and its poor performance is expected.

** why BERT+CRF...
Bert+CRF is widely recognized as the best performing method in sequence labeling tasks presently, so we directly adopted this method.
We adopted the idea of division conquer to simplify the rewriting problem. In order to lighten step 1, we did not take the context into account, so some predicted positions to be filled in were wrong. We consider that the context should be included in the input of step 1 of the framework to avoid this phenomenon.


** "T5 model was fine-tuned with 20 epochs"...
After fine-tuning 20 epochs, it converges well. We fine-tune it on the training set. The reason why we chose T5 is that it took the pre-training task of filling in the missing parts of the sentence. This is consistent with step 2 in our framework, so we adopted T5 instead of other pre-trained models.


Response to Review #3:
** T5 model results reported only in Table 2...
In Section 3.4, due to the memory constraints of our experimental environment, we adopt T5-small model in the second phase of our framework. So in baseline, we only use T5-small.
In footnote 3 on page 5, we also explained that we directly adopted the version of T5-base that was fine-tuned on CANARD by others.


** unsupervised method got low results:
The unsupervised method is based on rules, which are observed and summarized according to datasets. Compared with the training based method, it will be insufficient. And the unsupervised method is only for locating the rewriting position. We still need to fine-tune the T5 model. Therefore, we only use it as a baseline. To improve this method, we need to study more rules.


** the LCS method: 
In section 3.7, we show the results of replacing LCS algorithm with simple greedy algorithm. We can find that compared with LCS, the metrics are low.


** error analyses and samples to see the wrong predictions...
Yes, we made error analysis, which mainly focused on the prediction in step 1. In order to lighten the model, we did not take the context into account, so some predicted positions to be filled in were wrong.
For example, the utterance to be rewritten is:
	When did the expedition of tabuk happen?
The predicted result of step 1 is:
	When did the [MASK_i] expedition happen?
Although the phrase "expedition of Tabuk" appears in the context, the model still locates the position to be filled in before "expedition".
We consider that the context should be included in the input of step 1 of the framework to avoid this phenomenon.
However, in the current experimental results, this may introduce new errors, and we are still making further experiments.


General Response to Reviewers:

