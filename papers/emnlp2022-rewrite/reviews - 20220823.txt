The 2022 Conference on Empirical Methods in Natural Language Processing
EMNLP 2022
Author Response

Title:	Incomplete Utterance Rewriting by A Two-Phase Locate-and-Fill Regime
Authors:	Zitong Li, Ruolan Yang, Jiawei Li, Haifeng Tang and Kenny Zhu
Instructions
The author response period has begun. The reviews for your submission are displayed on this page. If you want to respond to the points raised in the reviews, you may do so in the boxes provided below.

Please note: you are not obligated to respond to the reviews.

For reference, you may see the review form that reviewers used to evaluate your submission. If you do not see some of the filled-in fields in the reviews below, it means that they were intended to be seen only by the committee. See the review form HERE.
Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This work focuses on the incomplete utterance rewriting task. To solve this task, the authors propose a two-step rewriting framework, which first finds the potential coreference and ellipsis components and then generates the target spans via the T5 model. Two kinds of locating methods, including an unsupervised one and a supervised one, are presented to finish the first stage. Next, the T5 model is introduced to fill the blanks produced in the first stage. Experimental results on three rewriting datasets (CANARD, MuDoCo, and CQR) confirm the effectiveness of the proposed method.
Reasons to accept
The proposed framework is straightforward and easy to follow.

The good empirical results.

Reasons to reject
Major drawbacks:
The paper is poorly written and organized. Too many typographical or grammatical errors in the paper. Very careful checks of the spelling and grammar should be done when you submit the paper.

Missing the descriptions about how you train the BERT-CRF model, and what the training/dev/test set and training configuration are.

Not reporting the BLEU-4 and EM.

The correctness of your experimental results. The BLEU1/2 scores on CANARD in the original HCT paper are 72.4/70.8 compared to yours 67.9/61.7. The F1/F2/B1/B2/R1/R2 scores of T5-base on CANARD reported in Inoue et al. (2022) are 56.2/44.6/77.8/70.8/83.9/70.2 against yours 52.3/41.2/68.8/62.3/78.9/65.4. Why are such huge gaps among these scores?

If possible, I'd like to see the results on another two commonly tested rewriting datasets, including Restoration 200k (Pan et al., 2019) and REWRITE (Su et al., 2019). As a pro-drop language, Chinese dialogues tend to have more coreference and ellipsis cases. It can also illustrate your method is not language-specific and dataset-specific.

A minor drawback: It is basically a pipeline method, and might suffer the problem of error propagation. However, the supervised locating model is a one-time training model that can be directly used after the first training process, and the performance of the sequence labeling model is seemingly good. So, I do not think the pipeline fashion is a fatal defect.

Questions for the Author(s)
Have you tried any other pre-trained language models as the backbone, like BERT? Since both HCT and RUN use BERT as the backbone, it might be an unfair comparison between your model and these two models.

Have you run your code with different random seeds and recorded the error bars?

In sequence labeling tasks, P/R/F1 are the most used metrics. In Table 9, what is the meaning of Acc?

If the syntax of the rewritten utterance is different from the original's, does the LCS-based method still work?

Missing References

[1] Huang et al., SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration. (It should be a strong baseline method.)
[2] Xu et al., Semantic role labeling guided multi-turn dialogue rewrite. (It is also a two-stage method to finish IUR, and its first stage is to recover the coreference and ellipsis components via SRL.)

Typos, Grammar, Style, and Presentation Improvements
There are a number of typos and grammar errors in the paper. For examples,
In general, the figures and large tables should be located at the top of each page.
Bert-CRF -> BERT-CRF
Line 258-260
Line 266 contains -> contain
Line 276. 4 strong baselines -> four
Line 327-328 samples in CANARD is the most ...
Line 336-338.
Line 345 was -> is
Line 358 2 metrics -> two metrics
Line 359 T5 model generates
Line 363 baseline -> baselines
Line 371-372 The accuracy of ...
Line 374 the results of human evaluation
Line 392 the positions that need to ...
Line 420-421 the decline will be more obvious...
Line 440 with zero blanks -> without any blanks
Reproducibility:	3
Ethical Concerns:	No
Overall Recommendation - Long Paper:	2.5
Overall Recommendation - Short Paper:	

Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposed a two-phase approach for incomplete utterance rewriting by first locating the editing positions and then inserting the missing content by a generative model. For the first phrase, it utilized a sequence labelling model. For the second phrase, it fined-tuned a T5 model for generation. Experimental results show the proposed model obtained improvements on three public datasets.
Reasons to accept
The model outperformed several baselines on three public datasets.
Reasons to reject
Baselines are too weak and not enough. Several recent works were not compared.

Hao et al, RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging, EMNLP 2021.

Xu et al, Semantic role labeling guided multi-turn dialogue rewriter, EMNLP 2020.

Liu et al, Conversational query rewriting with self-supervised learning, ICASSP 2021.

Zhang et al, Self-attention for incomplete utterance rewriting, ICASSP 2022.

For Section 2.1.1, how did you summarize the rules for predicting the editing positions? How much of the percentage can these rules cover? I would expect to see more statistics to support your points.

For Section 2.1.2, why did you use BERT+CRF for sequence annotation? Did you try other models? Table 9 showed that the F1 scores of sequence labeling are not satisfying, what's the influence of these prediction errors?

Line 238, it mentioned that "T5 model was fine-tuned with 20 epochs", why? Did you tune the model on the dev set? Did you try other generative models? e.g., BART? Why did you decide to use T5?

There are lots of typos and grammar errors in the manuscript. I don't think it's ready for publication at the moment.

Line 066, they generally incurrs -> incur

Line 185, the sentences have had a -> please correct the tense

Line 187, coreference and ellipsis ... is extracted -> are

Line 507, We the results?

Questions for the Author(s)
See Reasons to reject.
Missing References
See above.
Typos, Grammar, Style, and Presentation Improvements
See above.
Reproducibility:	3
Ethical Concerns:	No
Ethics Justification
N/A
Overall Recommendation - Long Paper:	2
Overall Recommendation - Short Paper:	2

Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
This paper proposes a method for rewriting incomplete and ambiguous utterances in dialogue systems.
The proposed method is a two-step rewriting framework to predict empty slots, then generate the content to fill the slots. Locating positions (or predicting empty slots) uses two methods: unsupervised rule-based method and supervised common subsequence method. The authors used six rules for generating two kinds of blanks: for resolving coreference and ellipsis (the two sources of incomplete utterances). For blanks filling, the method adds hints for blanks, using T5 model to fill blanks with two optimizations: adding hints and splitting utterance into sub-sentences.

The authors conducted experiments on 3 datasets (CANARD, MuDoCo, CQR), and compared to 4 baselines: T5, RUN (Liu 2020), HCT (Lisa 2022), rule-based method. The authors reported SOTA results when comparing with the baselines. The supervised method obtained the best scores, which are better than the best baseline (T5-small), while their unsupervised methods got worse performance than the baseline in most cases. The authors also conducted several analyses including ablation tests to check the contribution of components in their proposed method, in which the "splitting sentences‚Äù seems to give the most contribution.

Reasons to accept
Interesting method
Good performance
I think this is an interesting work and can contribute to improving dialogue systems.

Reasons to reject
Lacking results: T5-base in other tables
Lacking some analyses and discussions: show in the questions below.
Questions for the Author(s)
Why are T5 model results reported only in Table 2, but not in other tables?
The unsupervised method got low results. Do you have any solutions? you should discuss the reasons and solutions.
How does the LCS method affect the performance? What are the differences between this LCS method and existing methods in literature?
For this pipeline method, can you show error analyses and samples to see the wrong predictions? Is there any solution for future work?
Reproducibility:	4
Ethical Concerns:	No
Overall Recommendation - Long Paper:	4
Overall Recommendation - Short Paper:	1
