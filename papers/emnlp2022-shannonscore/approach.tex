\section{Approach}
\label{sec:approach}

\subsection{Comparison of state-of-the-art Metrics}
The first thing we do is to compare the current state-of-the-art reference-less summarization evaluation metrics, including Shannon Score ~\cite{shannonscore}, BLANC ~\cite{blanc}, SummaQA ~\cite{summaqa}, and SUPERT~\cite{supert}. As shown in Table 1, the first row of the table shows the correlations with human annotations in respectively four different categories (coherence, consistency, fluency, and relevance) of the Shannon Score on the SummEval datasets, provided in the Shannon Score paper ~\cite{shannonscore}, the following rows of show the correlations with human annotations of SummaQA, BLANC, and SUPERT on the SummEval datasets, these results are provided in the SummEval paper ~cite{summeval}, it should be noted that every correlation shown in this work is Kendallâ€™s tau correlation coefficients. We can clearly see from the table that Shannon Score surpasses every other metrics in terms of correlations in every category. 

To ensure that the Shannon Score is truly superior in performance when compared to the other metrics, we ran those metrics to evaluate the TAC 2008 and 2009 datasets, Table ~\ref{tab:tacscore} shows the results. It should be noted that the TAC datasets do not contain four different categories like the SummEval datasets, instead, there is a single human-annotated pyramid score for each entry, in our experiments, we correlate our evaluation scores with the pyramid score to get the correlations. The Shannon Score surpasses the other metrics again in terms of performance even in completely different datasets. Naturally, if we want to choose one metric to work on, the choice would clearly be the Shannon Score.

\subsection{Introduction to Shannon Score}
The Shannon Score metrics replicates the Shannon Game ~\cite{shannongame} by using conditional language models such as GPT-2 ~\cite{gpt2} in place of human to get the probability distribution of documents that could correspond to a given summary S, then the author define the information difference as the difference between the amount of information the model gains from the document D and the amount of information the model gains from the document D if given the information of summary S, from there, they define Shannon Score as the information difference after normalization ~\cite{shannonscore}.

\subsection{The Irregularities of Shannon Score}
In the original paper, the author states that the Shannon Score is biased against highly compressed summaries \cite{shannonscore}, during our investigation, we indeed spotted some mismatched cases where low Shannon Scores were given to highly compressed summaries with high expert scores, our approach to mitigate the bias is shown as following:

\subsection{Shannon Score Enhanced with Compression Difference or Compression Ratio}
We define two factor to quantify the difference of length between the original text and summary using two different ways, one being the length of the original text divided by the length of the summary, naming it compression ratio(CR), the second being the length of the original text minus the length of the summary, naming it compression difference(CD), the usage of those two factors are introduced in the following experiments section.

