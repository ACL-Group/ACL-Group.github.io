\section{Experimental Results}
\KZ{give an preamble at the beginning of every major section if it contains
multiple subsections.}

\subsection{Implementation Details}
We train two models, linear regression model and decision tree regression model, using the Shannon Score and CR or CD as training features, and the human annotations scores as labels. After the initial training, we input the Shannon Score and CR or CD of the test sets to let the model predict the corresponding scores, lastly, we correlate the output scores with human annotations scores. 

\subsection{Introduction to Datasets}
We train our models on the SummEval and TAC 2008 and 2009 datasets respectively. The SummEval datasets consists of 1600 instances of original text and summary pairs, each paired with human annotation scores on 4 different fields: coherence, consistency, fluency, and relevance. Coherence measures how well-structured and well-organized the summary is; consistency determines if the summary contains only statements that are entailed by the original text; fluency checks the grammar and formatting; relevance indicates if the summary only contains important information from the original text.

The TAC datasets consists of instances of original text and summary pairs, each paired only one human annotation scores.

\subsection{Experiments on the SummEval Datasets}
We compare the performance of our models with Shannon Score, SummaQA, BLANC and SUPERT on the SummEval datasets. Table ~\ref{tab:summevalscore} shows our results.
\begin{table*}[th]
	\scriptsize
	\centering
	%\begin{tabular}{lp{1.1cm}rrrcccc}
\begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}} ccccc}
		\hline
		Metrics & Coherence & Consistency & Fluency & Relevance\\
		\hline
		Shannon Score & 0.4118 & 0.6324 & 0.5240 & 0.6029\\
		SummaQA & 0.1176 & 0.6029 & 0.4059 & 0.2206\\
		BLANC &  0.0735 & 0.5588 & 0.3616 & 0.2647\\
		SUPERT & 0.1029 & 0.5882 & 0.4207 & 0.2353\\
		Linear Regression (CR) & 0.6 & 0.7333 & 0.6 & 0.7333\\
		Linear Regression (CD) &  0.6 & 0.6 & 0.7333 & 0.6\\
		Decision Tree Regression (CR) & 0.0666 & 0.4666 & 0.0666 & 0.4666\\
		Decision Tree Regression (CD) &  0.7333 & 0.8666 & 0.4666 & 0.4666\\
		\hline
	%\end{tabular}
\end{tabular*}
	\caption{Correlations with human annotations score in terms of coherence, consistency, fluency, and relevance respectively for Shannon Score, SummaQA, BLANC, SUPERT, output from linear regression and decision tree regression model with CR and CD as training feature on the SummEval dataset.}
	\label{tab:summevalscore}
\end{table*}

\subsection{Experiments on the TAC Datasets}
We compare the performance of our models with Shannon Score, SummaQA and BLANC on the TAC 2008 and 2009 datasets. Table ~\ref{tab:tacscore} shows our results.
\KZ{There has been a number of other referenceless summarization evalutation
work that you need compare, e.g., Yizhu's naacl 22 paper: Reference-free Summarization Evaluation via Semantic Correlation and Compression Ratio. If you need
her code, u can contact her. The results are just one table is too simple.
You need to give some case studies to show why your method performs better
than other scoring methods. Given a text-summary pair, give the diff scores
by different method. And show why ours is better. Also give more detailed 
analysis to convince people that this thing works.}

\begin{table}[th]
	\scriptsize
	\centering
	%\begin{tabular}{lp{1.1cm}rrrcccc}
\begin{tabular}{lcc}
		\hline
		Metrics & TAC 2008 & TAC 2009\\
		\hline
		Shannon Score & 0.303 & 0.432\\
		BLANC &  0.271 & 0.353\\
		SummaQA & 0.263 & 0.367\\
		Linear Regression (CR) & 0.287 & 0.425\\
    	Linear Regression (CD) & 0.291 & 0.429\\
		Decision Tree Regression (CR) & 0.286 & 0.392\\
		Decision Tree Regression (CD) & 0.385 & 0.456\\
		\hline
	%\end{tabular}
\end{tabular}
	\caption{Correlations with human annotated pyramid score for Shannon Score, BLANC, SummaQA, and output from linear regression and decision tree regression model with CR and CD as training feature on the TAC 2008 and 2009 datasets.}
	\label{tab:tacscore}
\end{table}

\subsection{Analysis and Discussion}
On both the SummEval and TAC datasets, models that use compression ratio $Y$ showed better results than models that use compression ratio X. The difference is caused by the difference between division and subtraction as a relation between the length of original text and summary, while the value of compression ratio X fluctuates with the length of the original text, in other words, it is a relative relation between the length of original text and summary, the value of compression ratio Y keeps the constant relation between the length of original text and summary. We could argue that both compression ratio X and Y have their respective usages as a relative relation or a constant relation, however, from our experiment results, Y as a constant relation gives us better results, we deduct that the reason behind this phenomenon is the difference of amount of information carried by original text and summary is directly proportional to the difference in length which is the definition of compression ratio Y.

As stated before, the author pointed out that the Shannon Score is biased against highly compressed summary, the reason for that is clear when we revisit the definition of Shannon Score ~\cite{shannonscore}, where the Shannon Score is calculated using the amount of difference in information between the original text and summary using a pretrained-language model, when a summary is highly compressed, even with the information gained from the summary, the model could still perceive a high amount of information from the original text which the summary does not contain, thus results in the model judging that the summary provides little information, which in turn deduct that the summary deserves a low Shannon Score. However, in some particular cases in the field of summarization, a highly compressed summary does not necessarily mean that the summary has undesirable quality, in fact, some of the best summaries conclude the whole original texts in one or a few sentences, thus a low Shannon Score is not always accurate on such summaries. Considering such factors, compression ratio, in this case, especially compression ratio Y works wonder as a compensating factor, where the difference in amount of information is consistently quantified by compression ratio Y.

