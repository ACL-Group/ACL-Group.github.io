\section{Introduction}
\label{sec:intro}

The development of summarization algorithms is one of the most important fields of study in the NLP community, to improve the performance of such algorithms, automatic and consistent metrics are needed to measure the quality of summaries produced by those algorithms. There are previous studies which focus on such metrics, they could be divided into two types: reference-based or reference-less. Reference-based metrics require human written reference summaries as references to evaluate the summaries produced, human written reference summaries are expensive and time-consuming to obtain. Therefore, recent works have been focusing on reference-less metrics, which do not require human written reference summaries. 

The main focus of developing reference-less metrics is to increase the correlations of such metrics with human annotations without using any references, in other words, the judgement of whether the metrics are satisfiable completely depends on whether the scores they evaluate is similar to human judgement. This task is proved to be difficult, given a source document to summarize, a single correct answer does not exist, the possible correct summaries, in other words, the domain of the target is enormous, for example, when given a summarization task to different human, the summaries they produce could be vastly different. We need to take all the factors such as consistency, fluency, coherence, relevance, and etc. into consideration. Furthermore, the essence of summary is to only include the most crucial information from the original document, this generates other consideration such as how to differentiate the importance of information in the original document and whether the generated summary contains only the important information.

This paper investigates the current state-of-the-art reference-less summary evaluation metrics, the Shannon Score ~\cite{shannonscore}, and proposes an improvement upon it, specifically we train a Decision Tree Regression model using the differences between the length of original text and length of summary and Shannon Score, with the human annotations as target. 

In a word, our contributions are:
\begin{itemize}
	\item Improved the current state-of-the-art metrics Shannon Score.
	\item Achieved state-of-the-art correlations with human annotations on coherence and consistency on the SummEval datasets.
	\item Achieved state-of-the-art correlations with human annotations on the Text Analysis Conference (TAC) 2008 \footnote{\url{https://tac.nist.gov/2008/}}, and TAC 2009 \footnote{\url{https://tac.nist.gov/2009/}} datasets.
\end{itemize}
