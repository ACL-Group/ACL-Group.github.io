\section{Related Work}
\KZ{For a short paper, no need to have a separate related work section. 
Integrate (and shorten) this content into intro.}
\textbf{Reference-based summarization evaluation metrics} such as BLEU ~\cite{bleu}, ROUGE ~\cite{rouge}, ROUGE-WE ~\cite{rougewe}and METEOR ~\cite{meteor} focused on the n-grams between the summary and human written references. Other works such as BertScore ~\cite{bertscore} and MoverScore ~\cite{moverscore} focused on making use of the contextualized embeddings between the summary and human written references.

\textbf{Reference-less summarization evaluation metrics} such as SummaQA ~\cite{summaqa} and QuestEval ~\cite{questeval} makes use of the principle of question-generating (QG) and question-answering (QA), the intuition is if the summary does contain correct information from the original paragraph, the summary should be able to give the same answers as the original paragraph when given questions, to measure the factual consistency of summary with original paragraph. Other works such as SUPERT ~\cite{supert}, and A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy ~\cite {centrality} extracts salient sentences from the source documents to generate pseudo-references, then uses soft-token alignments on the pseudo-references and summaries to measure their semantic similarities. There are also works which used pre-trained language models to help with the evaluation such as BLANC ~\cite{blanc} and Shannon Score ~\cite{shannonscore}. 

