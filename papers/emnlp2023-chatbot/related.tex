\section{Related Work}
\label{sec:related}

\subsection{ChatGPT for Mental Health}
Recently, several studies have assessed the performance of ChatGPT in tasks like depression detection \cite{lamichhane2023evaluation}, emotional conversation \cite{zhao2023chatgpt}, factor detection of mental health conditions, and emotion recognition in conversations \cite{yang2023evaluations}. However, these evaluations were performed on existing datasets using conventional metrics, and did not involve human interaction. What's more, \citet{qin2023read} developed a chat interface using ChatGPT.  
However, they mainly focus on more interpretable and interactive depression detection from social media, while our work focuses on outpatient scenarios, where information should be obtained from conversation, and user experience is the major concern.

\subsection{Doctor Chatbot}
Automatic diagnosis by doctor chatbot has significant practical applications. It enables large-scale screening, alleviates the issue of insufficient medical resources, and provides patients with a more engaged experience than using scales like PHQ-9 \cite{kroenke2001phq}.

While numerous chatbots have been developed to automatically diagnose physical illnesses \cite{Xu2019End, wei2018task}, such chatbots remain relatively uncommon in the mental health domain due to the difficulty in obtaining dialogue data because of ethical concerns. \citet{yao-etal-2022-d4} introduced a depression diagnosis dialogue dataset performed by patient and doctor actors, and a doctor chatbot trained on it. Although the chatbot conduct the diagnostic process correctly, it lacks adequate emotional support and the diagnostic process is inflexible. Another pioneer work \cite{liu2021towards} defines various empathy strategies for mental health support and proposed a meticulously annotated dialogue dataset with these strategies. Recently, \citet{wei2023leveraging} proposed an LLM-based chatbot for information collection, which shares similarities with doctor chatbot, as the latter also need to thoroughly collect the patients' symptoms. 

\subsection{Patient Chatbot}
Recent years, there has been increasing attention to the development of virtual patients for training clinician-patient communication skills \cite{Chaby2022Embodied}. Simulating more lifelike patients can help develop better doctor chatbots \cite{tseng2021transferable}. Additionally, patient chatbots could serve as standardized patients (SPs) in medical education, as currently, actors are hired to portray patients with mental disorders, which is both costly and time-consuming \cite{Gillette2017Cost}. 

Despite this, there are still limited works on developing patient chatbots, and most of them are rule-based\cite{Llanos2021Lessons}. \citet{Duppuy2020Guidelines} provides several guidelines for the design of virtual patient, such as having a reasonable symptomatology and focusing on the abilities needed for psychiatrists (e.g., the virtual patient can show resistance when the doctor ask questions without empathy).

% \subsection{Other Mental Health Related Conversational AI}

% \KZ{More about Woebot and Wysa, etc.?}
