Paper Topic And Main Contributions:
Authors develop doctor & patient chatbots for depression diagnosis. The doctor chatbot mimics a mental healthcare provider, the patient chatbot mimics a patient with a mental health condition. Authors lay out objectives that characterize a "good" doctor and a realistic patient. The chatbots are chatGPT based, and prompts are iteratively improved to include feedback from doctors/patients. Along with human evals, authors perform automated assessments and ablations to study the efficacy of their approach.

Reasons To Accept:
The paper has a fairly solid approach. I especially appreciate that authors clearly lay out what constitutes a good doctor chatbot, and similarly what constitutes a representative patient chatbot. In the patient chatbot, there is special attention to the fact that patients do not conduct goal-driven conversations & may often be reticent or hesitant. Authors address real problems that arise in designing such a chatbot - i.e., chat agents are in fact trained to be goal-driven, therefore requiring them to mimic real patient conversations is at odds. I found the evaluation thorough, both in terms of human & automated evals.

Reasons To Reject:
First, the experiments section is weak. I would have liked to see comparison against strong baselines. There is one CPT based doctor chatbot, but authors don't mention what CPT is and what makes their approach significantly different. Without strong baselines, it's not possible to assess a) how difficult the task is, and b) how much progress authors have made on the task?

Second, the paper doesn't have a "related work" section. Authors haven't reported what techniques were used previously to build chatbots that can mimic doctors / patients in multi-turn conversations. Aside from saying that meeting multiple goals (empathy, effective diagnosis, realistic reporting, etc) is difficult, authors do not situate their work within broader scholarship. There have been multiple efforts to build chatbots for depression help/diagnosis (eg. woebot), as well as virtual patient chatbots. There is no mention of, or comparison with, any of these efforts.

Questions For The Authors:
These are more suggestions than questions -

Would be good to see examples from error classes. For instance, authors mention "P2 has more human-like words and fewer robot-like words"; I would have liked to see examples. Examples of what makes one patient chatbot more colloquial than the other?
I would suggest that you remove the following sentence from your Introduction - "Doctor chatbots can be effective tools for mental disorder screening (Pacheco-Lorenzo et al., 2021) in lieu of official medical diagnosis." The statement is both untrue & harmful. Chatbots can at best assist doctors in arriving at/testing different diagnoses. I am not at all convinced that they can be used "in lieu of official medical diagnosis". This claim is especially alarming, given that your own study shows that it's not (yet) possible to build a doctor chatbot that scores high on all the necessary parameters.
Please provide dataset details. How many patients & doctors were engaged in the human evals? How many different transcripts (i.e. distinct conversations) were evaluated by each, and overall? How many different humans have assessed a single conversation? These results are statistically significant only if a) the dataset size was reasonable, ie. enough conversations were evaluated, and b) each conversation was evaluated by multiple people.
Soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems
Excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.
Reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.
Ethical Concerns: No
Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.



Paper Topic And Main Contributions:
The current paper examines ChatGPT's ability to simulate psychiatrists and patients for depression diagnosis. To this end, the authors collaborated with professionals to, in the first place, identify key objectives of both psychiatrists and patients. Based on the findings, they designed task-based evaluation metrics (which are computed either manually or automatically).

Reasons To Accept:
The current study focuses on an interesting ability of ChatGPT with carefully designed experiments.
I like the idea of identifying key objectives first and using the findings to design evaluation protocols.
The extrinsic evaluations (with both psychiatrists and patients) in this study are very useful and persuasive. The automatic metrics also look reasonable.
The paper has a very detailed introduction of how the prompts were designed.
The reminder looks very useful for prompt design in further (esp. in the medical domain).
The paper has a very good in-depth analysis of the assessment results.
Reasons To Reject:
Can you elaborate more on why talk to psychiatrists only when identifying psychiatrists key objectives? I think patients' opinions are also important.
The automatic evaluation was done using ChatGPT. I don't think this way of using ChatGPT to evaluate ChatGPT is a big problem, but it is vital to discuss the limitations and hit on how reliable it is.
Typos Grammar Style And Presentation Improvements:
016 shows -> show

Soundness: 5: Excellent: This study is one of the most thorough I have seen, given its type.
Excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.
Reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Ethical Concerns: No
Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.



Paper Topic And Main Contributions:
In this paper, the author has created a patient bot and a doctor bot to investigate the potential of using Chat GPT for depression diagnosis. It is an NLP engineering experiment.

Reasons To Accept:
The idea for exploring the use of ChatGPT for depression diagnosis is good. If it's successful, it has a huge impact as accurately diagnosing depression, especially the mild or early onset ones, is a difficult task, even for human.
The metrics/goals used for the functional requirements are good
Reasons To Reject:
The experiment design and evaluation metrics are flawed. As stated in the paper, the main goal of the chatbot supposed to be for getting accurate diagnosis. However, diagnosis accuracy is not one of the human evaluation metrics, it is only an automatic evaluation.
From the paper, it is not clear on who evaluate the patientbot and who evaluate the doctorbot. It seems that depressed patients are evaluating doctorbot and doctors are evaluating the patientbot. As pointed in the paper, depressed patients won't have enough knowledge to know that the questions asked by the doctorbot are good questions. The human doctors should measure this. Similarly, the doctors won't know for sure how the patients would feel with the doctorbots' response.
The authors claimed that their doctorbot seems to have higher level of empathy and higher level of diagnosis accuracy than baseline. As we all know that Chat GPT fluency is SOTA, but its informativeness and factualty might not be great. I fully expect CPT model to not be able to write a fluent and eloquent response compare to Chat GPT. Does thie eloquent response = empathy?? Also, does the reason for CPT low diagnosis accuracy due to it's inability to ask or answer the patients eloquently??
I think the author need to test their Doctor chat bots on actual human depressed patients to know how well is its diagnosis accuracy and empathy level.
There was no discussions on what's next. How can we improve this? What are the limitations?
Questions For The Authors:
Please see the reasons to reject above and try to rebute it
Soundness: 2: Borderline: Some of the main claims/arguments are not sufficiently supported, there are major technical/methodological problems
Excitement: 2: Mediocre: This paper makes marginal contributions (vs non-contemporaneous work), so I would rather not see it in the conference.
Reproducibility: 2: Would be hard pressed to reproduce the results. The contribution depends on data that are simply not available outside the author's institution or consortium; not enough details are provided.
Ethical Concerns: No
Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.