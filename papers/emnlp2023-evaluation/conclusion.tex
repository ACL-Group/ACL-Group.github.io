\section{Conclusion}

This paper focuses on zero-shot faithfulness evaluation for summarization and introduces a novel evaluation metric FFLM which is simply based on the foundation language model. Experiments on both the inconsistency detection benchmark and faithfulness rating datasets show the strong generalization ability of FFLM across various task settings and different datasets. It also shows favorable performance over strong baselines including ChatGPT. 
Using our proposed metric for more fine-grained consistency detection and 
designing more faithful summarization systems are future directions.


\section*{Limitations}

The main idea of this work is to do faithfulness evaluation based on a foundation language model by a combination of different probability changes. FFLM is just a feasible but not perfect metric design. Although it makes improvements over each $\Delta$ on almost all of the datasets in Table~\ref{tab:ablations-fr}, it failed on the errors related to discourse errors on the FRANKCNN and FRANKXSUM dataset according to Fig.~\ref{fig:error}. Designing better aggregation metrics based on specific analysis of different error types will be considered in the future.

Besides, in this work, our FFLM only calculates a single score for the whole 
summary without pinpointing the exact erroneous words or the specific 
error type. Considering the success of CoP~\cite{she2022cop} on 
token-level inconsistency detection and detailed inconsistency category 
evaluation, we hypothesize that our metric FFLM can be also used for these evaluation scenarios by adjusting the aggregation weights or combining it with the prompting approach.

Moreover, we limit our scope to faithfulness evaluation for text summarization in this paper because the definition of faithfulness evaluation for other generation tasks has some non-trivial differences. For example, the chit-chat utterances in dialogue generation~\cite{dziri2022faithdial} are supposed to be acceptable under the evaluation for faithfulness, instead of being regarded as extrinsic hallucinations. The evaluation for sentence paraphrasing~\cite{zhang2019paws} should be bi-directional, i.e., the first sentence has to be consistent with the second one, and vice versa. We consider transferring FFLM with adjustment on the other tasks as future work.
