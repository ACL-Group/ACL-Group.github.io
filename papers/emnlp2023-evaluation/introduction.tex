\section{Introduction}
\label{sec:intro}

Faithfulness evaluation for text summarization aims at measuring if the information in a summary is fully covered by and consistent with the source document~\footnote{We use the words ``faithfulness'', ``consistency'' and ``(without) hallucination'' interchangeably. Extrinsic hallucinations that are correct to the world knowledge are regarded as unfaithfulness in this work.}. Although automatic text summarization has achieved remarkable improvements with pre-trained language models~\cite{zhang2020pegasus,lewis2020bart,LiuJZ21,Liu0Z22Opinion,zhang2023benchmarking} in recent years, especially in the aspect of fluency and informativeness. However, these neural models tend to generate unfaithful summaries. An effective faithfulness evaluation metric not only helps for implementing summarization systems in real applications but also plays a key role in developing more faithful summarization models, such as by data filtering~\cite{matsumaru2020improving} or doing post-hoc corrections~\cite{chaudhury2022x}.

%containing inconsistencies to the source document~\footnote{Faithfulness means that the information in a summary should be covered by and consistent to the source document. We use the words ``faithfulness'', ``consistency'' and ``(without) hallucination'' interchangeably. Extrinsic hallucinations that are correct to the world knowledge are regarded as unfaithfulness in this work.}~\cite{maynez2020faithfulness,huang2021factual}. Such unfaithfulness issues present a serious obstacle for the 
%implementation in real applications, demonstrating an urgent need for 
%more effective faithfulness evaluation metrics, which also play an important 
%role for optimization approaches such as data filtering~\cite{matsumaru2020improving} and post-hoc corrections~\cite{cao2020factual,chaudhury2022x}.

Most previous work for faithfulness evaluation either takes advantage of models trained on related tasks for zero-shot evaluation~\cite{goodrich2019assessing,falke2019ranking,wang2020asking}, or does weakly-supervised evaluation with synthetic in-domain data~\cite{kryscinski2020evaluating}. The former requires transferring out-of-box models to the summarization domain~\cite{mishra2021looking}, which lacks guarantees on the models' performance and suffers from error propagation~\cite{ji2023survey}. The latter one shows poor generalization ability~\cite{laban2022summac} as a result of the limited synthetic rules that couldn't cover various kinds of hallucinations.
Recently, as ChatGPT~\cite{openai2022} has shown amazing generation abilities on various tasks, researchers attempt to do human-like evaluation by designing prompts to query the model in the zero-shot manner~\cite{luo2023chatgpt}. However, such strong language models are still sensitive to nuances, showing unstable performance with different wording of prompts~\cite{gao2023human, chen2023evaluating}.

%Most of previous work for faithfulness evaluation can be divided into three categories. First, out-of-box models trained on other tasks are transferred for zero-shot evaluation, including information extraction~\cite{goodrich2019assessing}, natural language inference~\cite{falke2019ranking} and question answering~\cite{wang2020asking}. This kind of approach suffers from error propagation~\cite{ji2023survey} and \KZ{rephrase: domain transfer from their training data to the summarization texts~\cite{mishra2021looking}}.
%Second, a weakly-supervised way is to construct synthetic datasets to train classifiers~\cite{kryscinski2020evaluating}, while those synthetic guidance strongly relies on expertise by error analysis on a small group of data, showing poor generalization ability~\cite{laban2022summac}.
%it's hard to achieve consistent improvements among different benchmarks either by the chain-of-thought techniques~\cite{weichain}, or designing elaborate prompts with detailed task definitions~\cite{gao2023human}, which also shows the unstable performance among different text prompts. 

Considering the above weaknesses, we think that an ideal faithfulness 
evaluation metric for summarization should be independent of other tasks 
and dataset-specific expertise, be able to generalize among different 
benchmarks and robust for the same document-summary pair. 
\citet{zhou2023lima} concludes that instruction tuning is just to teach the model to produce high-quality output while almost all of the knowledge has been learned during pre-training for large language models. Based on their findings, we wonder: can we get rid of the popular prompting approaches and calculate the faithfulness score simply with a foundation language model, which meets the above expectations?


In this work, we propose a metric named FFLM for zero-shot faithfulness evaluation with a foundation language model. The intuition behind FFLM is that the generation probability of a piece of text will increase when prefixing another piece of consistent text. Following this intuition, we classify different kinds of probability changes into changes with prior probability and changes with conditional probability. The former contains a comparison between the vanilla sequence-to-sequence probabilities of the summary given document and unconditional probabilities of the summary, and a similar comparison by changing the position of the document and the summary. The latter calculates the vanilla sequence-to-sequence probability with another conditional probability by adding a piece of prefix text.
Similar intuition has been considered in previous works ~\cite{she2022cop,son2022harim}. The major differences are that their metrics were carried out on models fine-tuned by summarization data and they only consider a single kind of probability changes. Our FFLM is based on the foundation language model, and we hypothesize that these different probability changes capture different hallucinations (see Sec.~\ref{sec:errortype}) which should be considered as a whole.

On top of these three components of probability changes, we introduce a feasible design of FFLM by re-weighting each token and each component to get the final faithfulness score. We did experiments in both the inconsistency detection setting and the faithfulness rating setting for summarization evaluation. The results show the favorable performance of our FFLM across different settings and datasets~\footnote{The code and dataset for this paper are available at \url{https://github.com/JiaQiSJTU/FaithEval-FFLM}.}. Our contributions are as follows:
%Specifically, following this intuition, \citet{she2022cop} calculates the probability difference of the summary condition on the document or on the summary and the document.
%\citet{son2022harim} is defined as the probability of the summary conditioned on the document subtracted by the unconditional probability of the summary.
%Different from their work considering only a single probability change, which is
%calculated by a FLM model fine-tuned on a summarization dataset, 
%our FFLM is based the foundation language model, and we hypothesis that these two probabilities changes capture different
%hallucinations (see Sec.~\ref{sec:errortype}) which should be considered as a whole. We also introduce a new probability changes by subtracting the unconditional probability of the document from the conditional probability of the document given the summary.

%In a word, the core of FFLM is a comprehensive metric made up of the three probability differences. In detail, we take the logarithm of the probabilities, and weighted-sum the probability differences of each token and the three components to get the final faithfulness score. 


\begin{itemize}
\item We propose to do zero-shot faithfulness evaluation based on a  foundation language model(Sec. \ref{sec:analysis-prompts}).
\item We introduce a comprehensive evaluation metric FFLM by calculating the probability changes of the desired output in different ways(Sec. \ref{sec:approach}) and verify the rationality of our metric design(Sec.\ref{sec:analysis-design}).
\item Experiments on different evaluation settings show that FFLM based on LLaMa with only 7 billion parameters can achieve competitive performances or even outperforms ChatGPT among different datasets(Sec~\ref{sec:results-id} and~\ref{sec:results-fr}).

\end{itemize}

