Reviewer TWe7

We profoundly appreciate the hard work and the precious comments.


"the advances over the state-of-the-art are small"

Response:
1. In terms of method innovation:
The core of this work is to show the feasibility of doing the zero-shot faithfulness evaluation with a foundation language model (instead of a fine-tuned model or prompting methods). 

Besides, we classify the probability changes into two categories and show that different probability changes are complementary to each other to some extent, which can achieve further improvements when considering them all together. 

2. In terms of result improvements:
With our metric FFLM, a 7B-parameter language model can surprisingly outperform ChatGPT on most of the datasets and perform competitively on the others. 

FFLM also shows strong generalization ability among different datasets (see Table 1), tasks (inconsistency detection in Table 2 and faithfulness rating in Table 3), and evaluation setups (summary-level in Table 3 and system-level in Table 4), compared with the baselines.

It should also be noted that the baselines FactCC and SummaC in Table 2 are weakly-supervised methods, while our FFLM is a zero-shot evaluation metric. FFLM can still outperform them on CoGenSum, SummEval, and Frank, while ChatGPT lags behind.

Based on the above two points, we think our work makes considerable improvements over the state-of-the-art.


"Scores for ChatGPT-CoT on the SummaC benchmark"
Response: 
We didn't include it because:
Chen et al.(2023)[1] found that "chain-of-thought prompting hurts performances dramatically compared to vanilla prompting in most cases" as faithfulness evaluation is less reasoning-intensive. Thus, we only listed the results for ChatGPT without CoT in the paper. This has been mentioned in the footnote on Page 3.

[1] Shiqi Chen, Siyang Gao, and Junxian He. 2023. Evaluating factual consistency of summaries with large language models. arXiv.

-----------
Reviewer cG7b

Thank you very much. We will explain the differences between our work and the baselines more clearly.

First, we apologize that there is a typo in Eq. 9 in the Appendix and it should be:

$HaRiM = \frac{1}{m}\sum_{i=1}^{m}(1-p_{y_i}^{s2s})[1-(p_{y_i}^{s2s} - p_{y_i}^{prior})]$


Our metric FFLM is different from HaRiM in three ways:
1. We propose to do faithfulness evaluation with foundation language models while they suggested to evaluate with models fine-tuned on summarization dataset. 
HaRiM_LLaMa is an improved metric with our proposal and performs quite differently from the original HaRiM as discussed in Line 380-391: The original HaRiM tends to prefer summaries generated by abstractive models, while HaRiM_LLaMa doesn't.

2. HaRiM only considers one of the changes with prior probability, i.e., $\Delta_{p_Y}^{prior}$.  Our metric not only adds a back constrained of P(X|Y) additionally to HaRiM, but also considers the probability changes with conditional probability as explained in Line 168-185. 

3. Specifically for the formula, the core of HaRiM and one of the three components in our FFLM are both $\Delta_{p_Y}^{prior}$. Nevertheless, the intuition of adding weights behind HaRiM and our FFLM are different regardless of the function-form variations.
HaRiM's weight $(1-p_{s2s})$ was originally introduced to adjust the loss scale for training better neural machine translation models by Miao et al.(2021). 
In our formula, we adopted the weight $e^{p_{y_i}^{s2s}}$ and the logarithm to pay more attention to low-probability tokens which generally correspond to unfaithful contents according to Goyal et al.(2022).

[Miao et al.] Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, and Jie Zhou. 2021. Prevent the language model from being overconfident in neural machine translation. ACL.

[Goyal et al.] Tanya Goyal, Jiacheng Xu, Junyi Jessy Li, and Greg Durrett. 2022. Training dynamics for text summarization models. findings of ACL 2022.


------
Reviewer df1Z
Thank you for your precious comments. We will follow your advice to improve the paper.

Reject 1:
Yes, it's possible to use smaller-sized models with some prerequisites:

First, the model should be a foundation language model without being fine-tuned on task-specific datasets. The drawbacks of models trained on task-specific datasets are explained in Line 380-391. Flan-T5 was fine-tuned on Muffin corpus containing a large amount of summarization data, thus it's not recommended.

Second, this is highly related to research on investigating the optimal model size and dataset size for training foundation language models as mentioned in Sec. 4.5. 



Reject 2:
Yes, the combination weights are tuned on the dev set if it exists. The weights are tuned on the dev set for the SummaC benchmark (Table 2), and set empirically for the faithfulness rating datasets (Table 3).

Q1:
In Table 5, we mainly show the ablation study of our metric FFLM, and all of the results are calculated between the faithfulness ratings by human annotators and automatic metrics.

The Pearson/Spearman/Kendall correlations(%) between the pairs of the metric components are as follows:

| | FRANKCNN | QAGSCNN | SummEval | FRANKXSUM | QAGSXSUM |
|  ----  | ----  |  ----  | ----  |  ----  | ----  |
| $\Delta_{Y}^{prior}$, $\Delta_{X}^{prior}$ | 44.7 / 46.2 / 32.0 | 32.5 / 35.8 / 23.8 | 29.5 / 34.4 / 23.7 | 28.3 / 31.1 / 20.9 | 28.0 / 24.9 / 17.0 |
| $\Delta_{Y}^{prior}$, $\Delta_{Y}^{cond}$ | 26.5 / 19.9 / 12.9 | 14.7 / 2.3 / 1.2 | 20.1 / 15.6 / 10.1 | 23.4 / 20.1 / 13.4 | -6.2 / -2.4 / -1.5 |
| $\Delta_{X}^{prior}$, $\Delta_{Y}^{cond}$ | 47.2 / 49.9 / 34.6 | 35.2 / 41.2 / 28.0 | 36.5 / 40.6 / 27.9 | 38.7 / 38.3 / 25.9 | -0.4 / 3.8 / 2.5 |

We can see that the correlations vary among different datasets, and none of the pairs show a high degree of correlation. This also indicates that these components may capture unfaithfulness from different aspects.

We will add the results of the correlation of these metric components in the Appendix.



Q2:
If I understand you correctly, when you say "the generation prefixes", you mean the generated tokens before each step, for example, 
$\{y_{<t}\}$ are the prefix tokens of predicting $y_{t}$ for LM(Y). If that is the case, in this work we simply take the average of them (see Eq. 6) and have achieved favorable results compared with the baselines. It should be noted that we calculate the probability of each token under the teacher-forcing strategy (Line 127-128), which won't lead to the exposure bias issue.

If what you mean is "the length of generation contexts", for example, X is the context of Y in calculating LM(Y|X). We didn't consider the effect of it in the current proposed metric, and will consider it in the future.



%%%
According to Table 5, we can see that each metric component performs differently among datasets. Specifically, the top two datasets for them are: {FRANKCNN, SummEval} for $\Delta_{Y}^{prior}$ , {FRANKCNN, QAGSCNN} for $\Delta_{X}^{prior}$, and {QAGSCNN, FRANKCNN} for $\Delta_{Y}^{cond}$. Besides, \Delta_{Y}^{prior}$ and \Delta_{Y}^{cond}$ are more linearly correlated with human scores, while $\Delta_{X}^{prior}$ shows higher Spearman correlations indicating monotonic relationships in most cases.
%%%






