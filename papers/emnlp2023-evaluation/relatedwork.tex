\section{Related Work}
\label{sec:relatedwork}
\subsection{Faithfulness Evaluation for Summarization}

%Traditional evaluation metrics, such as Rouge~\cite{}, BLEU~\cite{} BertScore~\cite{}, doesn't correlate well with human judgments in the faithfulness of summaries~\cite{}. Thus, designing faithfulness evaluation metrics is an urgent need for testing the usability of a generated summary in real applications.

Faithfulness evaluation metrics can be classified into zero-shot ones and 
weakly-supervised ones.

Zero-shot evaluation metrics mainly take advantage of the models trained with related natural language tasks. \citet{goodrich2019assessing} adopted information extraction tools to extract the fact tuples from both the source document and the summary. Tuple mismatches reflect the hallucinations. The intuition behind question-answering-based metrics~\cite{wang2020asking,durmus2020feqa,scialom2021questeval} is that identical answers should be generated when asking the same question to a summary and the corresponding document respectively. 
%Both IE-based and QA-based mainly focused on word-level or span-level content, regardless of higher-level semantics and being less robust to lexical variability~\cite{honovich2021q2}.
Natural language inference also shares commonalities with faithfulness evaluation in the way that information in a consistent summary should be entirely entailed by the source document~\cite{falke2019ranking,mishra2021looking,laban2022summac}. 
%A major obstacle for NLI-based metrics is that the input document for summarization is much longer than the premise in NLI datasets~\cite{mishra2021looking,laban2022summac}.
However, all of these metrics highly rely on the domain-transfer ability of out-of-box models and suffer from error propagation.

Instead, weakly-supervised approaches choose to train classifiers by constructing synthetic in-domain data with heuristics by experts. Different kinds of inconsistency errors are simulated by perturbing the reference document-summary pairs~\cite{kryscinski2020evaluating,utama2022falsesum,yin2021docnli}. The limited heuristic makes it hard to cover all kinds of errors and shows poor generalization ability among datasets~\cite{laban2022summac}.

As language modeling-based metrics~\cite{egan2022play,Liu0Z22Ref} receive more attention, another small group of work for faithfulness evaluation computes probability changes with models fine-tuned on summarization datasets~\cite{she2022cop, son2022harim,xie2021factual}, showing a biased preference for abstractive summaries. Based on this line of work, we propose FFLM based on the foundation language model. Our zero-shot metric doesn't require further training with in-domain or synthetic data and shows a strong generalization ability.

\subsection{Evaluation with Large Language Models}

With orders of magnitude more parameters and extensive training on large-scale 
data, large language models~(LLMs)~\cite{gpt3,touvron2023llama} have exhibited 
surprising abilities that may not be observed in previous small language 
models. 
%The mainstream approach to accessing LLMs for solving downstream tasks 
%is through prompt interface (e.g., gpt-3.5-turbo API) or conducting instruction 
%tuning with crafted human-machine conversational data~\cite{alpaca,vicuna2023}. 
The strong capability in language comprehension naturally spurs research in exploring LLMs as better automatic evaluators for various text generation systems~\cite{wang2023chatgpt}.

There are also some attempts of faithfulness evaluation by prompting large models~\cite{luo2023chatgpt} with different templates and strategies, such as adding detailed definitions~\cite{gao2023human} and chain-of-thought~\cite{chen2023evaluating}. None of these strategies achieve consistent improvements over the original prompt. Besides, neural models are sensitive to the choices of words~\cite{chen2023evaluating}, resulting in unstable performances(See Appendix~\ref{sec:diff-prompts}).

Our FFLM takes advantage of the strong capability of LLMs for faithfulness evaluation in a different way and shows competitive performance requiring a much smaller number of parameters than the well-known ChatGPT~\cite{openai2022}.



