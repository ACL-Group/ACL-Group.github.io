Official Review of Submission1369 by Reviewer TWe7

Official ReviewReviewer TWe709 Aug 2023, 19:42 (modified: 23 Aug 2023, 01:57)Program Chairs, Senior Area Chairs, Area Chairs, Reviewers Submitted, Authors, Reviewer TWe7Revisions
Paper Topic And Main Contributions:
This work is on the faithfulness (factual consistency) evaluation of abstractive summaries. It is an important research direction to improve natural language generation models and reduce their hallucinations. In particular, this work considers the task of zero-shot faithfulness evaluation, in contrast to weakly-supervised methods.
The authors contribute a new evaluation metric FFLM, a zero-shot faithfulness evaluation based on foundation language models (they use Llama). In their experiments, they extensively show that their approach has favorable performance over state-of-the art models, especially ChatGPT-based approaches, even though their model is "small" (7B parameters) in comparison.
Reasons To Accept:
* the authors advance a recently introduced (and thus small) line of work: faithfulness evaluation with probability changes
* the proposed FFLM model achieves SotA results on many datasets and is competitive with other approaches on the other evaluated datasets, even though it uses less parameters. Further, their method achieves consistent results across all datasets as opposed to other approaches (even on the harder XSum based datasets), demonstrating good generalization capabilities over dataset.
* the experiments are extensive. Models are evaluated in two important faithfulness tasks "Faithfulness Rating" and "Inconsistency Detection". Further, all components of their FFLM approach (which consists of three measurements of probability changes and two token weighting schemes) are analyzed in ablation studies as well as error analyses.
* the paper is well structured and clearly written, which makes it easy to understand
Reasons To Reject:
* the advances over the state-of-the-art are small (- this work describes incremental work)

Missing References:
Scores of ChatGPT with Chain of Though reasoning are available for the SummaC benchmark, but not reported in this paper.
Soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments.
Excitement: 4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.
Reproducibility: 5: Could easily reproduce the results.
Ethical Concerns: No
Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
−＝≡
Official Review of Submission1369 by Reviewer cG7n

Official ReviewReviewer cG7n07 Aug 2023, 09:04 (modified: 23 Aug 2023, 01:57)Program Chairs, Senior Area Chairs, Area Chairs, Reviewers Submitted, Authors, Reviewer cG7nRevisions
Paper Topic And Main Contributions:
This paper presents a metric for zero-shot faithfulness evaluation of summaries simply with a foundation language model. The work is very similar to (the same motivation) She et al. (2023) and Son et al. (2022) as cited by the authors with slight changes in the formulation of the actual proposed metric.
Reasons To Accept:
The paper does an excellent job in terms of comparing with a large number of models, on various datasets and correlating across multiple (currently adopted) metrics. This includes comparing with just the output of LLM without the proposed metric.
The metrics show a strong correlation with human ratings pointing to the usefulness of the proposed methods. Further, the formulation is very intuitive, and all the 3 components of the weighted sum make sense intuitively (along with providing a somewhat naive interpretablity).
Reasons To Reject:
The technique proposed is extremely similar to HaRiM, though HaRiM ends up with a more complex formula, the proposed metric only seems to add a back constraint of P(X|Y) additionally to HaRiM's method. Further note, HaRiM_LLaMa in fact outperforms the given metric on human correlation in Table 4.
Questions For The Authors:
Can you provide the main difference to HaRiM conceptually, though the formula is provided in Appendix (and it varies with the formula for the proposed method) from my understanding it seems exactly similar conceptually with an additional constraint.啥意思

Soundness: 3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.
Excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.
Reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Ethical Concerns: No
Reviewer Confidence: 4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
−＝≡
Official Review of Submission1369 by Reviewer df1Z

Official ReviewReviewer df1Z03 Aug 2023, 08:13 (modified: 23 Aug 2023, 01:57)Program Chairs, Senior Area Chairs, Area Chairs, Reviewers Submitted, Authors, Reviewer df1ZRevisions
Paper Topic And Main Contributions:
This paper presents a novel evaluation metric FFLM to solve zero-shot faithfulness evaluation for summarization. FFLM is designed based on the probability output of foundation models. FFLM is a parametric combination of changes with prior probability and changes with conditional probability. Extensive experiments and ablation studies on various benchmarks compared with many recent methods show the effectiveness of FFLM, given a comparatively small backbone model and no fine-tuning required.
Reasons To Accept:
1. The guideline of the metric is clearly motivated: independent of other tasks and dataset-specific expertise.
2. This work contains clear explanation on the concepts and comparison with other work. For example, the authors clarify that extrinsic hallucination is unfaithful in this work, which is often ambiguous in other recent work.
3. The comparison and ablation are nicely designed, for example, the comparison between prompting and instruction-tuning is much appreciated.
Reasons To Reject:
1. FFLM is backed on LLaMa 7B. Is it possible to use smaller sized models such as Flan-T5 to conduct the same experiments?
2. The authors mentioned that the combination weights can be tuned, besides naïve 0.25 0.25 0.5 and all 1/3, mentioned in 4.3. Are there results based on tuning on dev set?


Questions For The Authors:
Q1: what is the correlation of these metric components in Table 5 experiments?

Q2: Can we adjust the score or weighting parameters by removing the effect of different lengths in the generation prefixes of different component metrics?
Missing References:
None as far as I know
Typos Grammar Style And Presentation Improvements:
None as far as I know
Soundness: 4: Strong: This study provides sufficient support for all of its claims/arguments.
Excitement: 3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.
Reproducibility: 4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Ethical Concerns: No
Reviewer Confidence: 3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.
