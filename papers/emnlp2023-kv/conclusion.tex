\section{Conclusion}
In this work, we propose a simple yet effective approach that enables LLMs to summarize the key-value memory of specified span of tokens. Experiments on language modeling and open-ended generation demonstrate that our approach substantially outperforms sparse attention baselines in terms of 
information compression and generation quality.

\section*{Limitations}
In the current evaluation setup, we apply the same strategy used for training to select spans of tokens to be enclosed by \textit{<CL>} and \textit{<CR>} for inference-time context compression. However, different text pieces may display different degrees of importance for downstream tasks. For instance, a grammatical and semantic complete noun phrase can be more compressible than an ungrammatical one that contains only partial linguistic units. Though our input transformation procedure theoretically includes  text spans of all possible linguistic structures, it may still 
benefit from an elaborately designed strategy/algorithm for selecting compression targets in a given context.