@inproceedings{transformer,
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Attention is All you Need},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	volume = {30},
	year = {2017}
}

@misc{gpt2,
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	added-at = {2023-01-14T15:28:29.000+0100},
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, Ilya},
	biburl = {https://www.bibsonomy.org/bibtex/272c31587e067e0041527dabb3a34cdb8/lepsky},
	interhash = {b926ece39c03cdf5499f6540cf63babd},
	intrahash = {72c31587e067e0041527dabb3a34cdb8},
	keywords = {chatgpt kuenstliche_intelligenz},
	timestamp = {2023-01-14T15:33:48.000+0100},
	title = {Language models are unsupervised multitask learners},
	url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
	urldate = {2023-01-06},
	year = 2019
}
@misc{opt,
	title={OPT: Open Pre-trained Transformer Language Models}, 
	author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
	year={2022},
	eprint={2205.01068},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@misc{llama,
	title={LLaMA: Open and Efficient Foundation Language Models}, 
	author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
	year={2023},
	eprint={2302.13971},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{18bert,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{emergence,
	title	= {Emergent abilities of large language models},
	author	= {Barret Zoph and Colin Raffel and Dale Schuurmans and Dani Yogatama and Denny Zhou and Don Metzler and Ed H. Chi and Jason Wei and Jeff Dean and Liam B. Fedus and Maarten Paul Bosma and Oriol Vinyals and Percy Liang and Sebastian Borgeaud and Tatsunori B. Hashimoto and Yi Tay},
	year	= {2022},
	journal	= {TMLR}
}

@misc{longformer,
	abstract = {     Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. },
	added-at = {2020-10-28T11:32:53.000+0100},
	archiveprefix = {arXiv},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	biburl = {https://www.bibsonomy.org/bibtex/22435246630c361e01f31009339c70f41/ghagerer},
	eprint = {2004.05150},
	interhash = {9197cfbaf90615f175718342fe883457},
	intrahash = {2435246630c361e01f31009339c70f41},
	keywords = {bert pre-trained transfer-learning transformer},
	primaryclass = {cs.CL},
	timestamp = {2020-10-28T11:32:53.000+0100},
	title = {Longformer: The Long-Document Transformer},
	url = {https://arxiv.org/abs/2004.05150},
	year = 2020
}


@article{bigbird,
	author       = {Manzil Zaheer and
	Guru Guruganesh and
	Avinava Dubey and
	Joshua Ainslie and
	Chris Alberti and
	Santiago Onta{\~{n}}{\'{o}}n and
	Philip Pham and
	Anirudh Ravula and
	Qifan Wang and
	Li Yang and
	Amr Ahmed},
	title        = {Big Bird: Transformers for Longer Sequences},
	journal      = {CoRR},
	volume       = {abs/2007.14062},
	year         = {2020},
	url          = {https://arxiv.org/abs/2007.14062},
	eprinttype    = {arXiv},
	eprint       = {2007.14062},
	timestamp    = {Mon, 03 Aug 2020 14:32:13 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2007-14062.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{reformer,
	author       = {Nikita Kitaev and
	Lukasz Kaiser and
	Anselm Levskaya},
	title        = {Reformer: The Efficient Transformer},
	journal      = {CoRR},
	volume       = {abs/2001.04451},
	year         = {2020},
	url          = {https://arxiv.org/abs/2001.04451},
	eprinttype    = {arXiv},
	eprint       = {2001.04451},
	timestamp    = {Sat, 23 Jan 2021 01:20:41 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2001-04451.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{rope,
	author       = {Jianlin Su and
	Yu Lu and
	Shengfeng Pan and
	Bo Wen and
	Yunfeng Liu},
	title        = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
	journal      = {CoRR},
	volume       = {abs/2104.09864},
	year         = {2021},
	url          = {https://arxiv.org/abs/2104.09864},
	eprinttype    = {arXiv},
	eprint       = {2104.09864},
	timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-2104-09864.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{19bert,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{rouge,
	title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
	author = "Lin, Chin-Yew",
	booktitle = "Text Summarization Branches Out",
	month = jul,
	year = "2004",
	address = "Barcelona, Spain",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W04-1013",
	pages = "74--81",
}

@article{bertscore,
	author       = {Tianyi Zhang and
	Varsha Kishore and
	Felix Wu and
	Kilian Q. Weinberger and
	Yoav Artzi},
	title        = {BERTScore: Evaluating Text Generation with {BERT}},
	journal      = {CoRR},
	volume       = {abs/1904.09675},
	year         = {2019},
	url          = {http://arxiv.org/abs/1904.09675},
	eprinttype    = {arXiv},
	eprint       = {1904.09675},
	timestamp    = {Wed, 03 Jun 2020 10:08:39 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09675.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{local,
	author       = {Rewon Child and
	Scott Gray and
	Alec Radford and
	Ilya Sutskever},
	title        = {Generating Long Sequences with Sparse Transformers},
	journal      = {CoRR},
	volume       = {abs/1904.10509},
	year         = {2019},
	url          = {http://arxiv.org/abs/1904.10509},
	eprinttype    = {arXiv},
	eprint       = {1904.10509},
	timestamp    = {Thu, 02 May 2019 15:13:44 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gist,
  title={Learning to compress prompts with gist tokens},
  author={Mu, Jesse and Li, Xiang Lisa and Goodman, Noah},
  journal={arXiv preprint arXiv:2304.08467},
  year={2023}
}

@article{lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{wikitext2,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@software{redpajama,
  author = {Together Computer},
  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@inproceedings{keyvalue,
    title = "Transformer Feed-Forward Layers Are Key-Value Memories",
    author = "Geva, Mor  and
      Schuster, Roei  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.446",
    doi = "10.18653/v1/2021.emnlp-main.446",
    pages = "5484--5495",
    abstract = "Feed-forward layers constitute two-thirds of a transformer model{'}s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys{'} input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model{'}s layers via residual connections to produce the final output distribution.",
}

@article{topp,
  author       = {Ari Holtzman and
                  Jan Buys and
                  Maxwell Forbes and
                  Yejin Choi},
  title        = {The Curious Case of Neural Text Degeneration},
  journal      = {CoRR},
  volume       = {abs/1904.09751},
  year         = {2019},
  url          = {http://arxiv.org/abs/1904.09751},
  eprinttype    = {arXiv},
  eprint       = {1904.09751},
  timestamp    = {Sat, 29 Apr 2023 10:09:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1904-09751.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@misc{pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Fixing Weight Decay Regularization in Adam},
  journal      = {CoRR},
  volume       = {abs/1711.05101},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.05101},
  eprinttype    = {arXiv},
  eprint       = {1711.05101},
  timestamp    = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@inproceedings{lineartransformer,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{wp,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@article{speculative,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023}
}