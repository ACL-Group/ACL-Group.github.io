\section{Introduction}
The Transformer architecture~\cite{transformer} has become the underpinning component of modern large language models~(LLMs)~\cite{gpt2,18bert,opt,llama} in recent years. However, the quadratic computational complexity and memory footprint of the attention mechanism have largely limited applying Transformers to increasingly long contexts with constrained computing resources.

To mitigate such issues, prior works~\cite{local,longformer,bigbird,reformer} have explored an assortment of efficient Transformer variants, mostly by replacing the original quadratic attention operation with various forms of linearized approximation. Though promising, large-scale pre-training and specialized CUDA kernels are typically required for these models to achieve performance comparable to off-the-shelf LLMs and fulfill real efficiency gains.

In this work, we aim to improve the efficiency of existing Transformer-based LLMs without any architectural changes. Specifically, we focus on the key-value cache, which accounts for the majority of memory footprint and data movement~(I/O) cost when dealing with increasingly long input using LLMs. We propose a plug-and-play approach to incrementally compress the key-value memory of a contiguous span of tokens into compact ones. Specifically, we introduce a pair of special sentinel tokens \textit{<CL>} and \textit{<CR>} into the vocabulary of LLMs and use them to mark the boundary of the span to be compressed. During training, we modify the causal attention mask such that future tokens after \textit{<CR>} are precluded from attending to tokens between \textit{<CL>} and \textit{<CR>}. By continually training LLMs with the next token prediction objective, the model learns to extract and condense task-relevant information of the bounded span into the ending sentinel token. The reduced context length alleviates both memory and computational costs when processing subsequent tokens, thereby improving system throughput with larger batch sizes and faster decoding speed during inference.

We conduct experiments on the WikiText-2 language modeling benchmark and show that 
our approach is generalizable to LLMs with various sizes~(from 1.3B to 3B) and 
position encoding schemes such as absolute position embedding~\cite{18bert} 
and rotary position encoding~\cite{rope}. Compared to sparse attention baselines, our approach is able to effectively \textit{compress historical key-value cache} with significantly reduced degradation in perplexity.  Moreover, 
we demonstrate that our approach outperforms sparse attention in \textit{zero-shot open-ended document generation} across different compression ratios as 
evaluated by perplexity, ROUGE~\cite{rouge}, and BERTScore~\cite{bertscore}. 
Finally, we empirically demonstrate that context compression is able to confer considerable improvement in \textit{system throughput} for text generation.
