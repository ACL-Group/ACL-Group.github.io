\section{Background and Related Work}
In this section, we present necessary background knowledge as well as drawing distinctions of our approach to existing literatures.
\paragraph{Complexity of Attention Mechanism}
The self-attention mechanism of LLMs produces a query vector for each token to select and retrieve information from all previously computed key and value vectors. Thus, a key-value memory is required at runtime to store past information. Given a pre-trained LLM with $M$ Transformer layers, $H$ attention heads, per-head hidden dimension of $d_{head}$, batch size $b$, and current context length $L$, the model stores key and value vectors computed so far as a tensor of shape $(M,2,b,H,L,d_{head})$. During auto-regressive decoding, the size of key-value cache grows linearly with the context length $L$, leading to significant increase in memory footprint and latency.

\paragraph{Efficient Transformers}
Various efficient variants of Transformers have been proposed to address the problematic complexity of attention. Sparse Transformer~\cite{local} limits the receptive field of each token to a local window. Longformer~\cite{longformer} and BigBird~\cite{bigbird} introduce additional randomly and globally accessible tokens to compensate for the information loss. Linear Transformer~\cite{lineartransformer} reformulates the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $O(N^2)$ to $O(N)$. Nevertheless, these approximations were shown to degrade the expressiveness of full-attention and have been upstaged by GPT-like LLMs recently. In this work, however, we tackle this problem by compressing lengthy input into more compact representations, which is orthogonal to on-going efforts to architectural optimization.

\paragraph{Gisting Tokens}
\citet{gist} explored using gisting tokens to compress textual prompts during instruction tuning. They showed that verbose task instructions can be compressed into much shorter ones. However, their approach is only applicable to prefix text of around 20 tokens. In contrast, we aim for a compression scheme with more flexible compression choices and larger compression ratios.
