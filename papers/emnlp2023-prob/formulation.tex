\section{Preliminary}
\label{sec:formulation}
\subsection{Task Definition}
%\textcolor{red}{Hongru: these are concrete examples, which i think should be given after the problem
%formulation} 
We define an instance $x$ of an NLU task 
dataset $X$ as
\begin{equation}
    x = (p, h, l) \in X, \label{eq:nli}
\end{equation}
\noindent
where $p$ is the context against which to do the reasoning ($p$ corresponds 
to ``premise'' in~\exref{exp:snli});
$h$ is the hypothesis given the context $p$; 
$l \in \mathcal{L}$ is the label that 
depicts the type of relation between $p$ and $h$. 
The size of the relation set $\mathcal{L}$ varies with tasks. 

%There is another type of natural language reasoning tasks which 
%are also in the form of multiple-choice questions, 
%but their choices are a fixed set of labels, as shown below. 

%\begin{center}
%\begin{example}\label{exp:roc}
%A story in ROCStory dataset, with ground truth bolded~\cite{mostafazadeh2016corpus}.
%\begin{description}
%\item{Context:} Rick grew up in a troubled household. 
%He never found good support in family, and turned to gangs.           
%It was n't long before Rick got shot in a robbery.             
%The incident caused him to turn a new leaf.
%\item{Ending 1:} He joined a gang. 
%\item{Ending 2:}  \textbf{He is happy now.}
%\end{description}
%\end{example}
%\end{center}
%We can transform the this case into two separate 
%problem instances, still
%in the same form as in \eqnref{eq:nli}, 
%$u_1=(context, ending1, false)$ and $u_2=(context, ending2, true)$, where $L = {true, false}$.

\subsection{Linguistic Features}
\label{sec:extract}

As demonstrated in previous work~\cite{naik2018stress,checklist2020acl}, 
we consider the following linguistic features: 

\indent\textbf{Word}: The existence of a specific word in the premise or hypothesis of a dataset instance.

\indent\textbf{Sentiment}: The sentiment value of an instance, calculated as the sum of sentiment polarities of individual words.

\indent\textbf{Tense}: The tense feature (past, present, or future) of an instance, determined by the POS tag of the root verb.

\indent\textbf{Negation}: The existence of negative words (e.g.,``no'', ``not'', or``never'') in an instance, determined by dependency parsing.

\indent\textbf{Overlap}: The existence of at least one word (excluding stop words) that occurs in both the premise and hypothesis.

\indent\textbf{NER}: The presence of named entities (e.g., PER, ORG, LOC, TIME, or CARDINAL) in an instance, detected using the NLTK NER toolkit.

\indent\textbf{Typos}: The presence of at least one typo in an instance, identified using a pretrained spelling model.

For multiple-choice datasets, all features except Overlap are applied exclusively to hypotheses.

%\subsubsection{Word} 
%For a dataset $X$, we collect a set of all words 
%$V$ that ever exist in $X$. 
%A word feature is defined as the existence of a word $w \in V$
%either in the premise or the hypothesis. 
%Because $V$ is generally very large, in practice, we may narrow it down
%to words that are sufficiently popular in $X$. That is, we may remove
%words that seldom appear in $X$.
%\subsubsection{Sentiment}
%
%For each data instance $x$, we can compute its sentiment value as:
%\begin{equation}
%S(x) = \sgn(\sum_{w \in x} polar(w),
%\end{equation}
%where $polar(w)$ is the sentiment polarity (-1, 0, or 1)
%of $w$ determined by a look-up from a pretrained sentiment 
%lexicon~\footnote{NLTK: \url{https://www.nltk.org}}.
%We say $x$ has a positive/negative/neutral sentiment feature if $S(x)$ = 1, -1 or 0,
%respectively.
%
%\subsubsection{Tense}
%We say that an instance $x$ has  
%\textit{past}, \textit{present} or \textit{future} tense feature if $x$
%carries one of these tenses, respectively, by the POS tag of the root verb
%in $p$ or $h$. 
%
%\subsubsection{Negation}
%Previous work has observed that negative words (``no'', ``not'' or ``never'') 
%may be indicative of a certain label in NLI tasks for some models.
%The existence of a negation feature in $x$ is decided by dependency 
%parsing~\footnote{Scipy: \url{https://spacy.io}}. 
%
%\subsubsection{Overlap}
%In many models, substantial word-overlap between the premise and the
%hypothesis sentences causes incorrect inference, 
%even if they are unrelated~\cite{mccoy2019right}. 
%We define that an overlap feature exists in $x$ if there's at least one word
%(except for stop words) that occurs both in $p$ and $h$. 
%
%\subsubsection{NER}
%We define the NER feature as the existence of either PER,
%ORG, LOC, TIME, CARDINAL entity in $x$.
%We use the NLTK ner toolkit for this purpose. 
%
%\subsubsection{Typos}
%We say an instance $x$ has typo feature if there exists at least one
%typo in $x$.
%We use a pretrained spelling model~\footnote{\url{https://github.com/barrust/pyspellchecker}} 
%to detect all typos in a sentence. We don't distinguish the types of misspellings here. 
%
%In Example \ref{exp:roc}, we noted that multiple-choice 
%questions are split into two instances with opposite 
%labels (T or F) and identical premises. Thus, 
%detecting features within premises alone is unproductive. 
%For MCQ datasets, all features except Overlap are applied exclusively to hypotheses.
%
%
