Review #1
Thank you for your detailed comment. We will polish the paper for better presentation.  

We provide clarification for (iii) reason to reject and Question C:   
The binary and multi-label settings use the same unified model to detect 7 mental diseases simultaneously, but different datasets for training and evaluation with different perspectives on multi-disease detection. 
Binary setting, follows previous practices in containing only users with one disease or no disease at all (a.k.a. the control users), is an easier task to detect a user from one disease to healthy. 
Multi-label setting, contains postings for all users with zero, one, or more diseases, since some users suffer from >1 disease, is a more factual yet difficult setting. 

For Question A: The brother won't be treated as a positive case. All the self-reported diagnosis patterns we used contain the first-person pronoun (e.g., I, me). Here, we can show some examples of the patterns: "I am diagnosed with", "I got sick and was diagnosed", "my doctor diagnosed me".

For Question B: 16 high-risk posts from each profile are used, which are selected according to symptom probability.

Review #2
Thank you for your comments. 
For the first point in "reasons to reject":
Although our model utilizes both symptom and text stream, it is actually lighter and more efficient than other pure text methods which process the users' whole posting history, because our method only selects 16 high-risk posts to be the text stream.

For the second point in "reasons to reject":
Our model does not have the advantage brought by the keywords for tagging. We use the same keywords as SMHD [1] for tagging the disease label, which can be found at https://ir.cs.georgetown.edu/data/smhd/conditions/. These keywords are mainly the alias or subclasses of each mental disorder, while the risky-post screening method to identify the posts is based on symptoms (e.g., weight and appetite change), which are quite different from the keywords used for tagging. 

[1]  Arman Cohan, Bart Desmet, Andrew Yates, Luca Soldaini, Sean MacAvaney, and Nazli Goharian. 2018. SMHD: a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions.

Review #3
Thanks for your detailed comments and valuable advice. We will polish our paper accordingly. 

Question A: Yes, we will provide more references to support this claim. We have evaluated the random samples manually, confirming that they are indeed the riskiest. We will show some example posts of alternative screening methods in the Appendix for comparison.

Question B: The dataset is collected and labeled using the same method as SMHD. We will share our dataset and DUA in the camera-ready version, along with a description.

Question C: It is very interesting to apply the symptom model to the suicide dataset, which may lead to some new discoveries. We will try to explore this as future work. Thanks for the great suggestion. 

Question D: The keywords we used for tagging the dataset are the same as SMHD, which can be found at https://ir.cs.georgetown.edu/data/smhd/ . We'll add this link to the camera-ready version.

Missing references: 
Thank you for pointing out the two missing references. We will follow your advice and add them to the final version.
In paper 1, the authors employ a multi-task learning framework that uses PHQ-9 categories for more explainable depression detection. Comparing with our work, the common trait is the multi-task learning framework. However, they focus on only 1 disease while our model is designed for the comorbidity scenario (7 diseases),  and the multi-task learning in our work is to leverage shared knowledge across different diseases for better detection performance.
