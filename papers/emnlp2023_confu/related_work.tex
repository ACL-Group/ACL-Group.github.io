\section{Related Work}

Our work focuses on solving problems in PSR task with KG using contrastive learning, so it is relevant to PSR, Knowledge Fusion, Term-to-Term Similarity with KG and contrastive learning.

\subsection{Product Search Relevance} 
PSR is a very important task in e-commerce platforms. For this task, a two-tower architecture is widely adopted to perform candidate retrieval and ranking etc. With a two-tower architecture, query and product can be encoded separately. Thus, products can be encoded offline and search can be accelerated in real-time services\cite{liu2022}.

In order to improve PSR task, many researchers try to optimize this task in different ways. Priyanka Nigam et al. \cite{priyanka2019} study the problem of semantic matching in product search and train a deep learning model for semantic matching using customer behavior data to overcome problems of inverted index. Zheng Liu et al. \cite{liu2022} research on the generalization ability of product search models by performing text similarity pre-training on search click log. Han Zhang et al. \cite{zhang2020} designs a query tower with multi-heads and utilizes user information to achieve better personalized and semantic retrieval. Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model \cite{senli} is proposed to dynamically capture the relationship between user query semantics and personalized behaviors. They train a deep learning model for semantic matching using customer behavior data. Nurendra Choudhary et al. \cite{choudhary2022} formulates PSR as a multi-class classification problem and proposes a graph-based solution to classify a given query-item pair as exact, substitute, complement, or irrelevant (ESCI). Han Zhang et al. \cite{zhang2021joint} propose Poeem to unify embedding learning and index building this two separate steps to reduce additional indexing time and improve retrieval accuracy. Shaowei Yao et al. \cite{yao2021learning} focus on how to train a product relevance model from the weak supervision of click-through data. These works solved different problems in PSR task, but work focusing on injecting KG knowledge into PSR task is still relatively less.

\subsection{Knowledge Graph Fusion} 
For some tasks, especially domain-specific task, external knowledge is necessary. KG contains a wealth of knowledge and is beneficial to those downstream tasks. How to effectively fuse knowledge into model becomes a popular research direction in recent years. KBERT \cite{weijie2019kbert} injects triples into the sentences as domain knowledge. Ning Bian et al. \cite{bian2021benchmarking} enhance Commonsense QA model via concatenation with knowledge extracted from KG. Following these works, we use KGP as one of our baseline models. KI-BERT \cite{faldu2021ki} infuse knowledge context from multiple knowledge graphs for conceptual and ambiguous entities into transformer-based language models during fine-tuning. In e-commerce field, researchers from Alibaba \cite{luo2021alicoco2} propose a multi-task encoder-decoder Knowledge Graph Embedding (KGE) framework to provide representations for nodes and edges from AliCoCo2 and so that downstream tasks can be improved. Inspired by their solution, we adopt KGE-based method as one of our baseline methods.

\subsection{Term-to-Term Similarity with Knowledge Graph} 
Term-to-Term Similarity task is similar with PSR task and some researchers also try to utilize KG to assist this task. Roy Rada et al. \cite{rada1989development} measure the similarity between two terms in KG with the length of the shortest path connecting the two terms. Philip Resnik \cite{resnik1995using} uses information content of the least common ancestor node of the two terms in the tree-structured taxonomy to evaluate their semantic relatedness. Marco A. Alvarez \cite{alvarez2007graph} measuring the semantic similarity between pairs of words by constructing a rooted weighted graph. Peipei Li et al. \cite{li2013computing} research on how to compute term similarity by large probabilistic isA knowledge. They map two terms into the concept space, and compare their similarity in the space. Unfortunately, these methods mainly rely on traditional algorithms and are relatively far away from current PSR technologies. Besides, query and product are naturaly different from simple terms and these methods rarely concerns the application in e-commerece scenario, so they can hardly be smoothly applied on current PSR task. 

% improve PSR model performance by concatenating concept embeddings to the BERT word embeddings. 
% ERNIE \cite{sun2019ernie} ...
% K-ADAPTER \cite{wang2020k} utilizes adapter to inject different kinds of knowledge into large pre-trained models.

\subsection{Contrastive Learning} 
As an effective way to optimize embedding, Contrastive Learning has been successfully utilized in different fields. In SimCSE \cite{gao21}, authors designs a simple yet effective contrastive learning method using dropout. Similarly, ConSERT \cite{yan20} explores various effective text augmentation strategies to generate views for contrastive learning. Self-Tuning \cite{wang21} designs pseudo group contrastive learning to help simultaneously exploring both labeled and unlabeled data. Following their work, the group contrastive learning is also adopted in ConFu. Zhiping Luo et al. \cite{xu2021kge} use contrastive learning to help learn KGE. Yuhao Yang et al. \cite{yang2022knowledge} utilize contrastive learning with KG to alleviate the information noise for KG-enhanced recommendation systems.

