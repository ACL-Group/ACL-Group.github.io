\section{Conclusion}

We observe that models can select correctly without premise and 
pay little attention to premise on attention map. Inspired by a speculation that 
models can short circuit the premises on MCQs and become fragile, 
we propose two data augmentation methods 
\textit{crossover} and \textit{mutation}. 
Our expriment results show that, while the
proposed methods do not always improve
results on the original datasets,
they significantly and consistently increase the
accuracy on stress test. 
They improve the model robustness and generalization capability. 
We also analyze the reason for this improvement 
with detailed stress test, choice-only test and 
case study. We conclude that our data augmentation methods can encourge models to pay more 
attention to the premise of questions. 

%Our experiments verify the existence 
%of short circuit behavior in three fine-tuned strong models.
%We also find that crossover is better than choice-only and 
%human annotation as proxy test to detect short circuit for models. 
%In addition, we try different data augmentation methods and 
%recommend two operators, crossover and mutation, 
%to make models more robust.
