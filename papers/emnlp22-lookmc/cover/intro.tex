\section{Introduction}
\label{sec:intro}
%\KZ{Be careful not to use ``context'' for ``premise''. We stick to the
%terminology ``premise'' and ``choices'' throughout this paper.}
%\KZ{MCNLR is kind of tasks that is made up of
%a context and two or more choices, all in text form.
%Recently, research has noted that some advanced NLP models
%may not be truly solving this type of questions by understanding
%the underlying logical connections between the context and the choices,
%but instead resorting to local signals in the choices alone.
%Such speculations were largely fueled by a kind of
%experiments called ``end-only tests''. Although such tests
%can show that models have some ability to predict correct choices
%without given the context, it doesn't show what the model does
%if it were given the full questions with the context.
%In this work ...  }
%main point:
%1. identify short circuit, dataset have some hints, spurious feature. 
%1. pretrained models perform very well on many tasks but fail on ``stress tests'' especially 
%the cases requiring to know specific relations between context and candidates, like coreference. 
%Thus there is an assumption that model learns a lot only from candidates.
%2. previous work try to explain this by candidate-only test, but it can't really judge if the model really 
%learn from candidates.We design another test called cross test...
%3. In this paper, we further study how to improve models by some simple data augmentation methods 
%based on the weakness of models. We target to teach models to look at both context and candidates. 

Multiple-choice questions (MCQs) are a widely used format  
in Natural Language understanding tasks. 
For example, causal reasoning task~\cite{copa2012}, story ending prediction task~\cite{roc2017},
argument reasoning comprehension task~\cite{arct2018}, and reading comprehension task~\cite{yu2020reclor}
are mostly in the form of MCQs which are made up of a premise and 
two or more choices. Below is an example question taken 
from the COPA~\cite{copa2012} dataset, which tests commonsense causal 
reasoning.

\begin{example}\label{ex:copa}
An MCQ from COPA:\\ \\
\noindent
\textbf{Premise:} The man hurt his back.\\
\textbf{Choice 1:} He stayed in bed for several days.  \checksymbol \\
\textbf{Choice 2:} He went to see a psychiatrist. \crosssymbol 
\end{example}

One of the primary goals of training MCQs models is generalization. 
Mostly, models are trained on training data and tested with the validation-test split standard paradigm. 
While accuracy on held-out data is a useful
indicator, held-out datasets are often not comprehensive 
and contain the same biases as the training
data~\cite{mccoy2019right}. Furthermore, this single aggregate statistic is
 difficult to figure out the robustness and the reason for choosing correctly. 
 There has been speculation~\cite{endingonly1,srinivasan2018simple,zellers2018swag} that many models did not
really ``understand'' the semantical and logical connection between
the premise and the choices, 
but do well only due to spurious statistical features in the choices. 
It may make the models fragile.

From the speculation above, we observe some problems through testing
%Lately, some research has been devoted to 
%analysis the MCQs models by
with  white-box~\cite{vig-2019-multiscale} and black-box~\cite{ribeiro-etal-2020-beyond} methods. 
For white-box test, previous work uses the attention map to explain the good performance
of advanced neural models on such natural language~(NL) reasoning problems. 
We plot the attention map between the words in the full question 
from the final encoder layer of the model. We show
such a plot of \exref{ex:copa} in \figref{fig:att-goodex}.
The diagram clearly shows that there's virtually no connection
between the first choice and the premise (emphasized with the red box) when the model is processing
the full question, while the attention between the words within the
first choice remains the same when the model processes only the choices
without the premise. 
We call the phenomenon ``short circuit'' in
natural language reasoning in this paper.

\begin{figure}[th!]
\centering
\includegraphics[width=0.6\columnwidth]{figure/end_related.eps}
\caption{Attention map showing that BERT short-circuits on a COPA question.}
\label{fig:att-goodex}
\end{figure}

Due to the limited interpretability of the model, 
there are two kinds of black-box tests.
%many stress tests~\cite{} are designed and 
%Such speculations were largely fueled by a kind of
%experiments 
One is called ``ending-only tests'' in some literature~\cite{endingonly1,endingonly2}, 
which we refer to as ``choice-only test'' here since our focus is 
on multiple-choice questions.
For example, BERT, when fine-tuned on the COPA data, can answer
the question in \exref{ex:copa} correctly. When we remove the premise from 
the same question and feed it to the same BERT model, it still
gets the correct answer (Choice 1). This result from the ``choice-only'' 
test seems to suggest that the model can make the correct prediction
without even looking at the premise of the question. 
The other is stress test~\cite{checklist2020acl}, which guides users in what to test, by providing
a list of linguistic capabilities, like taxonomy and negation. It breaks down potential capability
failures into specific behaviors. We generate many stress test cases for MCQs 
%using both existing and two newly proposed operators, i.e., \textit{crossover} and \textit{mutation}, and 
and observed that many models are fragile.
Through testing, we basically confirm that the model only pays attention 
to the characteristics of choices instead of the relationship between the premise and
choices.

%We call the phenomenon ``\textit{short circuit}'' in natural language 
%reasoning in this paper. 

%Despite the previous research advocating the choice-only test,
%we argue that it has an inherent flaw as a test for
%short circuit: just because the model answers correctly without
%the premise doesn't mean the model doesn't look into the premise
%when it's given one. What we need is a test that works with questions
%that are complete with premises and choices.





%\begin{figure}[th]
%\centering
%\includegraphics[width=\columnwidth]{figure/end_unrelated.eps}
%\caption{BERT passes the choice-only test but 
%doesn't short-circuit on another COPA question (left).}
%\label{fig:att-badex}
%\end{figure}
%
%Can we achieve the same purpose using
%the choice-only test? The answer is negative. \figref{fig:att-badex}
%shows another question which is correctly answered by BERT with
%or without the context, but the attention map shows that 
%there exists some attentions between
%the word ``forgot'' in the choice and some other words in
%the context, indicating that the model is not really short-circuiting.

%we first develop an intuitive method to visualize
%the model's attention map to verify if it is shortcircuiting. 

%Manually checking the short circuit of a model using such attention maps
%is tedious and costly. And automating this process would require the access to 
%the code of the models and such approach only works for 
%attention-based models.

%To address these challenges, we design a new operation on 
One straightforward way to improve model robustness is to generate more 
training examples by the types of stress test that the model is struggling with. 
However, many of the stress tests come with certain constraints on the choice
construction, which limits the number of cases that can be generated
automatically, and consequently their ability to serve as a general 
data augmentation method. In contrast, the proposed \textit{crossover} and \textit{mutation} are easily scalable and applicable for generating abundant data to encourage models to 
pay more attention to the relationship rather than choices only. 
%MCQ question instances, 
%called \textit{crossover},
%which takes two MCQs and exchange their
%choices, analogous to how chromosomes swap their segments ino
%the biological reproduction. 
 
%Fortunately, crossover, as well as its counterpart operator \textit{mutation}, 
%do satisfy all the stringent requirements. 
%It imposes a unique challenge on models  
%frequently exploiting short circuit and is able to detect such behavior 
%on real tasks by constructing proxy test cases. 
%Through the lens of crossover test and  
%several other instance-level stress tests, such as named entity replacement, 
%we find evidence of short circuit and brittleness in three recent powerful natural language
%reasoning models reflected by notable declines in accuracy on these tests.
To that end, we applied crossover, mutation and back-translation~\cite{back2019} 
to augment BERT, XLNet~\cite{xlnet2019nips} and RoBERTa~\cite{roberta2019} on ROC~\cite{roc2017}, COPA, ARCT~\cite{arct2018} and RECLOR~\cite{yu2020reclor} and saw
up to 27\% increase in accuracy on the stress tests and 10\% increase in
the original test data.
%\KZ{The following needs to be rewritten: 
%Through crossover test, we further confirm that many classifiers 
%based on pretrained language models do rely heavily on the 
%spurious features of the options on some test instances. 
%In addition, we follow previous work~\cite{} to generate several stress test for 
%model testing. Many models have experienced a significant 
%decline in the correctness of stress tests which also indicates 
%that models that learn short circuits are not robustness.
%}

%Different with previous research~\cite{} which create a more robust model 
%by changing the structure of the model to adversarial training. 
%For the above issue, we mainly investigate two simple data augmentation methods 
%with no changes to models in this paper: 
%\textit{cross data augmentation} and \textit{grammar data augmentation}. 
%Based on the model's excellent performance on various tasks, 
%we believe the pretrained models are capable of learning reasoning knowledge 
%but are affected by spurious features in dataset and perform poorly on stress test. 
%By augmenting more data with these two methods, the pretrained models can consider more 
%about contextual relationships rather than just making judgments through choices. 
%We explain and analyze the reasons for the model improvement by 
%increasing on stress test and visualizing the model's attention matrix. 


This paper makes two main contributions:
\begin{enumerate}
%\item We design ``crossover'' test as a blackbox proxy test to 
%massively and efficiently detect short-circuiting problem.  
%\item We propose a black-box proxy test framework for detecting short circuits, and 
%particularly ``crossover'' test which is shown to be effective in this framework.
\item We propose to use both crossover and mutation operations
to augment training data that teaches the models to consider 
premises in questions. Our experiments confirm the validity 
of this approach and show substantial improvement to model robustness,
not only on the stress tests but also on the original test data.
\item We experimentally explain the behavior
in three fine-tuned strong NL reasoning models and the effectiveness of 
our augmentation method on weakening the impact of short circuits.
\end{enumerate}

%\begin{figure*}[th]
%\centering
%\begin{subfigure}[b]{0.49\textwidth}
%\centering
%\includegraphics[width=\columnwidth]{figure/end_related.eps}
%\caption{Cue ``no'' in MNLI}
%\label{fig:cue_no}
%\end{subfigure}
%\hfill
%\begin{subfigure}[b]{0.49\textwidth}
%\centering
%\includegraphics[width=\columnwidth]{figure/end_unrelated.eps}
%\caption{}
%\label{fig:cue_threw}
%\end{subfigure}
%\caption{Three test examples for distribution comparison with 4 different models}
%\label{fig:cue_result}
%\end{figure*}





%questions to discuss:
%1. how to know if a model only pay attention to candidates only? ending only? cross? 
%2. how to know if a model have a poor performance on stress test?
%word swap  choice swap(premise) reflecility (aug merge)
%3. how to explain typo and synonym? should we use grammar to test? do we have other relational stress tests?
%we only have pronoun, negation, ner now. adverbial, synonym paraphrase right to right, typo
%4. experiment should try on different test cases many times or same cases many times?
