\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[review]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}

\renewcommand{\UrlFont}{\ttfamily\small}


% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{natbib}
\usepackage{caption}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{multirow}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{color, colortbl}
%\usepackage{subfigure}
\usepackage{url}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{verbatim}
%\usepackage[linesnumbered, boxed, ruled]{algorithm2e}
%
\usepackage[noend]{algpseudocode}


\usepackage{algorithmicx,algorithm}
\usepackage{bbding}
%\usepackage{subfigure}
\usepackage{subcaption} 
%\hypersetup{
%	colorlinks=true,
%	linkcolor=blue
%}

\newtheorem{example}{Example}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\Roy}[1]{\textcolor{red}{Roy: #1}}
\newcommand{\crosssymbol}{{\color{red} \XSolidBrush} }
\newcommand{\checksymbol}{{\color{green} \Checkmark} }
\newcommand{\cut}[1]{}

\usepackage{makecell}
%\usepackage[table]{xcolor}
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{138} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}
\definecolor{Gray}{gray}{0.9}

\title{Reducing Short Circuits in Multiple-Choice Natural Language Reasoning Models with Data Augmentation}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}
\hypersetup{draft}
\begin{document}
\maketitle
\begin{abstract}
Statistical biases in the training data may lead to fragility in 
neural models that makes choices in multiple-choice natural language 
reasoning problems without referring to the context or premises. 
To encourage the models to pay more attention to the relations between 
the premise and the choices, we propose two biologically inspired operations 
that can generate new training data that ``forces'' the model
to look at the premises and reducing short circuits. They can augment
any type of multiple choice reasoning dataset, and can be applied to
any supervised learning models. Results show that models trained
with the augmented data become more robust against both stress test  
and original test.
%beating the strong back-translation baseline.
\end{abstract}

\input{intro}
\input{method}
\input{experiments}
\input{related}
\input{conclude}
\newpage
\input{limits}
\bibliography{aaai22}
\clearpage
\appendix
\input{appendix}
%\bibliographystyle{acl_natbib}
\end{document}
