Reviewer #1:
Q1: Can you tell me whether this idea has been explored in NLP...
A1: Mutation(randomly swapping tokens) in one sentence 
is a widely used method for data augmentation in lots of work 
(Artetxe et al., 2018; Lample et al., 2018; Wei and Zou, 2019; Miao et al., 2020). 
These work believes that the semantics of natural language
is sensitive to text order information, while slight order change is still readable for humans. 
Therefore, the mutation 
within a reasonable range can be used as a data augmentation method. 
However in our method, we treat the choices with mutation operation as 
a wrong choice which is more strict than previous work. 
Because we design mutaion to encourage the
model to look into the premise due to its two very similar
choices (same set of tokens)
Besides, it can also make the model more
sensitive to find differences in word orders and enhances the
model’s prior grammatical knowledge (described in Sec 2.3). 
We ensure the correctness of augmented data with this strict operation. 
We sampled 100 samples from the mutated 
samples and annotated these samples by 5 annotators on ROC. The accuracy for these 
questions is always 100%. You may also wonder to know which kind of mutation is better. 
If we believe mutation keeps its meaning and augment data with this operator, 
although it can enhance fault tolerance of models, it does nothing for bias  
elimination which is the main reason for model fragility in MCQ tasks. Because the 
feature distribution based on different label is almost unchanged as the meaning.

Q2: Can you extend this idea to more settings like multiple 
choice machine reading comprehension datasets, such as RACE?

A2: Thanks for your suggestions. We have utilized 4 datasets on different tasks 
to show the effectiveness of our methods including RECLOR which is a widely used 
multiple choice machine reading comprehension dataset. We can extend to more datasets 
in appendix in our revised version. 

Reviewer #2: 

Q1:The second proposed data augmentation method "Mutation" seems a little unconvincing... 
A1: We are so sorry that our description maybe not clear enough that make you confused. 
In fact, we always preserve the right choice. For the generated wrong choices, 
they can be derived from the right choices or wrong choices. For example, In Figure3, The 
right choice can be denoted as R, the wrong choice with color pink can be denoted as W. 
Then the swapped sentence from R and W are R' and W'. R' and W' are both wrong choices because 
they are grammatically wrong. Then we can randomly choose R' or W' as a wrong choice for the 
new aumgmented question A'.

Q2: Stress Test Cases are used for evaluating the data augmentation method... 
A2: We have decribed the reason in the first two paragraphs of Sec 2.2. 

Reviewer #3:

Thanks for your detailed suggestions.

Q1: In Figure 1, is there an impact on the attention map...
A1: We compare the attention map on unsupervised bert model without fine-tuning and Figure 1 in Figure A 
(a and b seperately). The result shows that unsupervised learning (don't train or train unsupervised?)

Q2: In MCQs, questions and answers are rather single-sentences...
A2: Not all tasks contains single questions and single answers, like ROC which contains 4 sentences in questions, and 
RECLOR have a long premise which is a reading comprehension task. 
Although we have compared the three very popular encoders, 
as far as we can, we also try on sentence BERT. The result......

Q3: In table 1, about the adverb operator: how is the adverb selected ?...
A3: The adverb operator is not used for training but testing. 
The size of pool doesn’t change behavior. The size of the pool is ... which is diverse enough for testing. 

Q4: This may be popular in MCQ papers, but the neural architecture used to predict the correct answer is not discussed...
A4: Thank you for your suggestion, we did miss the description in this regard
Thank you for your suggestion, we did miss the description about the strcuture we used to predict the correct answer. 
``During fine-tuning on a specific MCQ task with language model encoders, like BERT, the final
hidden vector corresponding to the first input token is used as the aggregate representation
followed by an extra fully connected layer to compute the probability score of being the right answer.'' We will add this 
description in Sec 3.1 in the revised version.

Q5: In C+M data augmentation scheme, there is an augmentation from crossover and a augmentation from mutations...
A5: In Sec 3.1, we have described that ``The expanded
data volume is equal to the original data volume and
the size of new train dataset has doubled.''
It may be not conspicuous or clear enough. In C+M, 
data volume for C accounts for half of original volume and M is also a half. 
Thus C+M have the same volume with back-translation, C or M alone which is fair for comparing.
For the results of double augmented data, we conduct experiments on the ROC data set...

Q6: In Table 4, the average of the 4 datasets is not a weighted average...
A6: Maybe it is not necessary to use a weighted average score. Many papers also compare with the average score
in this way on GLUE tasks (Liu et al. 2019b; Devlin et al. 2019; Yang et al. 2019).

Q7: The baseline seems stronger when the dataset contains more training...
A7:

Q8: In the introduction, the paragraph starting at "In contrast, the proposed (...) increase in the original test data"...
A8: Thanks for your suggestion. We will revise it to ``In contrast, our proposed ...''

Q9: Please double check the list of references.
A9: Thanks for your suggestion. We will revise the references in the revised version.

Reviewer #4:
It's a pity that you may not understand our work according to the description you make 
in summary part. First, the tasks we refered are all MCQs in natural language reasoning area 
rather than nli tasks. Second, ``crossover'' and ``mutation'' are the methods we proposed and 
``back-translation'' is the strong baseline we compared with. We didn't only ``adopt'' them. 
Third, our experiments show our proposed methods can improve the robustness of vanilla models and 
they are better than back-translation, a well-know data augmentation method. Besides, we
also analyze the reason for this improvement with detailed
stress test, choice-only test and case study. We conclude that
our data augmentation methods can encourge models to pay
more attention to the premise of questions.
Thus, we implore you to read our paper carefully again and thanks a lot.
