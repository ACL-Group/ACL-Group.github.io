\section{Related Work}
%Our work is related to three lines of research:  data augmentation, model probing and spurious feature analysis.


\textbf{Data Augmentation.}~~
%Data augmentation is a widely used technique 
%to enhance the robustness of neural network models. 
Data augmentation refers to strategies for increasing the diversity 
of training examples without explicitly collecting new data. 
It has received active attention in recent machine learning research such as UDA~\cite{xie2020unsupervised}, 
which used back-translation~\cite{sennrich2016improving}, AutoAugment~\cite{cubuk2018autoaugment}, 
RandAugment~\cite{cubuk2020randaugment}, 
and MIXUP~\cite{zhang2017mixup}. These are often first explored in computer vision, 
and it seems secondary and 
comparatively underexplored for NLP. 
It is perhaps due to challenges presented by the discrete nature 
of language, which rules out continuous noise and makes it more difficult to maintain invariance.
To augment more data in NLP tasks, previous work constructed more data 
with one kind of feature or rule have improved 
accuracy on that particular case, but didn't generalize to 
other cases, suggesting that models overfit to the 
augmentation set~\cite{Iyyer2018,Liu2019a}. 
In particular, \citeauthor{mccoy2019right} found that 
augmentation with HANS examples may generalize 
to a different word overlap challenge set, 
but only for examples similar in length to HANS examples. 
We reduce the choice-only short circuit inference 
behavior of models via several simple yet feature-agnostic  
augmentation methods aiming at teaching models to reason over relations between context and choices. 

\textbf{Model Probing.}~~Ever since the emergence of large pretrained language models, 
many works have focused on the analysis of their inner workings. 
As a result, a considerable amount of linguistic properties are shown to be encoded 
in the contextualized representations and attention 
heads~\cite{goldberg2019,clark2019,liu-etal-2019-linguistic,tenny2019}. 
In contrast, we are concerned with the model's higher-level reasoning capability. 
To prob what specific linguistic capabilities models get, 
one approach is to create challenging datasets. 
Some work~\cite{belinkov2019analysis} has 
noted benefits of this approach,
such as systematic control over data, as well as
drawbacks, such as small scale and lack of resemblance
to ``real'' data. Further, they note that the
majority of challenge sets are for Natural Language
Inference. Our stress test which can also be called short-circuit test is not 
aimed to replace the 
challenge or benchmark datasets, but to complement
them to test whether really have the inference capability, 
in particular the short circuiting behavior. 
The behavior is reflected in downstream performance through diagnostic stress tests.

\textbf{Spurious Feature Analysis.}~~Prior studies~\cite{endingonly1,zellers2018swag} 
have discovered that NLP models can achieve surprisingly 
good accuracy on natural language understanding tasks in MCQs form even without looking at the context. 
Such phenomenon is identified via the so-called ``hypothesis-only'' test. \citeauthor{sanchez2018behavior} 
further showed that models sometimes bear insensitivity to 
certain slight but semantically significant perturbations in the hypothesis, 
leading to suspicions that the high hypothesis-only performance 
stems from statistical correlations between spurious cues in the 
hypothesis and the label. Such spurious cues can be categorized 
into lexicalized~\cite{naik2018stress} and unlexicalized~\cite{bowman2015large}: the former mainly contains n-gram and cross-ngram spans that are indicative of certain labels, while the latter involves word overlap, sentence length and BLUE score between the premise and the hypothesis. Instead of unearthing the specific cues in the dataset, we directly diagnose if models are exploiting the short circuit in hypothesis alone 
and mitigate such reasoning behavior accordingly.


