\section{Related Work}

Controllable generation has gained wide research interest recently. PPLM \citep{dathathri2019plug} proposed a plug-and-play framework to control the generation with an extra attribute classifier. Later research progress can be roughly divided into 3 categories. \textit{Reranking} methods leverage attribute classifiers to either simply rank the full generation candidates \citep{thoppilan2022lamda}, or partial generations for the guidance of future outputs \citep{yang2021fudge}. \textit{Integrated} methods integrate attribute-related trainable parameters into the generation model for fine-tuning, such as discrete control codes \citep{keskar2019ctrl} or continuous prompt prefix \citep{qian2022controllable}. \textit{Weighted Decoding} methods leverage token-level attribute classifiers to guide each decoding step. For example, \citet{krause2021gedi} and \citet{liu2021dexperts} utilized one/two additional class conditional language models to provide the attribute discrimination. Director \citep{arora2022director} integrates the attribute classifier as simple linear layers on top of LM hidden states.  

Multi-attribute controllable generation is relatively under-explored now. \citet{lin2021plug} proposed to extend weighted decoding for the multi-attribute case with the simple product of multiple attribute conditional language models. \citet{gu2022distributional} proposed a VAE-based method combined with an intersection-searching algorithm for multi-aspect controllable generation, but their method cannot simply apply to conditional generation tasks like dialogue generation. \citet{mireshghallah2022mix} proposed an energy-based controllable generation method that can combines multiple controls, but are mainly suitable for fixed-length generation. 

Controllable generation techniques are especially important in dialogue systems and the applications of several controlling aspects have been studied. For example, we may condition the generation with dialogue acts for the genuine reflection of the desired behavior \citep{wen2015semantically}, add emotions in the response to enhance the expressiveness of the bot \citep{zhou2018emotional}, and also impose personal profiles like gender \citep{su2020stylistic} and persona \citep{zhang2018personalizing} to establish a human-like companion.
Recent advance in LLMs has pushed the frontiers of dialog generation, 
enabling applications like role-playing with complex personality and 
memory~\citep{park2023generative}. However, their exorbitant
cost and privacy concerns make them less relevant in certain deployment 
scenarios.

Another line of controllable generation utilizes dense persona descriptions \citep{zhang2018personalizing}. This paradigm is capable of expressing rich persona information in free text, such as personal status, hobbies and occupations. The natural language form allows for integration of other language resources for an enhanced generation quality. For example, \citet{song2021bob} disentangles the task of persona consistency learning and response generation, and leverages non-dialogue NLI datasets to help the former and consequently enhance the latter. However, although attributes can also be expressed in free-text descriptions, they can contain noise, and making them less effective than attribute-specific methods, as shown in our experiments (Appendix \ref{sec:desc_control}). It would be promising to further combine the two paradigms for more general controllable generation \citep{tang2023enhancing}.