# Rebuttal

## Reviewer 1

Thank you for recognizing our core contributions and the effort we put into this work. We appreciate your feedback and acknowledge that we may have missed some details regarding staged training in clustering and audio denoising. The implementation of these aspects directly follows the setup of the cited papers (HuBERT (Hsu et al., 2021), AudioSep (Liu et al., 2023)). We will ensure that the final version includes the missing details.

Additionally, we are excited to share our main results with the community by a demo at http://202.120.38.146:8079/.

We appreciate your concern about the suitability of this paper for the ACL community. **However**, we believe our work, which introduces animal communication using computational linguistics methods, will bring fresh insights to and inspire new directions within the ACL community.

## Reviewer 2

Thank you for the inspiring comment. We would like to clarify a few possible misunderstandings.

1. We seek to lay the foundation of understanding canine vocalization from a more phonetic/linguistic perspectives, rather than signal processing and sound events. However, this work makes no attempts to claim the discovery of syntactic or semantic structures in canine vocalization, which is an aspiring and very challenging future work.
2. This work is *different* from sound event clustering because the granularity is different.
We go beyond the clustering and detection of the different dog barking events (or bark types) by PANNS, to discover and analyze the fine-grained composition of the dog barks, in terms of "phones" and "words", which are smaller units. If we were doing sound event analysis, we could have computed the correlation between the activties and the result of PANNS directly. 
3. The term "phone" in this work is not to be confused with "phoneme". Phones are merely distinct (often short) sounds produced by a dog, regardless of its significance in the structure or meaning of the communication. The same definition has been used for human phonetics. Our implementation of HuBERT and clustering exactly follows this definition. Whether some of the phones we discovered in this work are actual "phonemes" will be future work. Such phonetic discovery is inspired and motivated by prior work [2,3] on birds and cats. Our definition of "words" is less vigorous than in human languages, but they share some commonalities [1], including strong causality with usage scenarios. We use a complex unsupervised method to find possible "words", by considering the probability of different "phone sequence combinations", and selecting those that have strong correlation with the scenarios. This is different from traditional linguistic approach that requires a careful definition of a "word" first.

[1] Cartmill, E. A. (2023). Overcoming bias in the comparison of human language and animal communication. Proceedings of the National Academy of Sciences, 120(47), e2218799120.
[2] Engesser S, Crane JM, Savage JL, Russell AF, Townsend SW. (2015). Experimental evidence for phonemic contrasts in a nonhuman vocal system. PLoS biology, 13(6):e1002171.
[3] Schötz, S. (2020). Phonetic variation in cat–human communication. Pets as sentinels, forecasters and promoters of human health, 319-347.

## Reviewer 3

We are impressed by your forward-thinking suggestion regarding the limitations of current classifications of canine activities, which is our on-going efforts. We have invited biologists to collaborate with us to refine factors influencing canine vocalizations. 

We apologize for not clarifying the details of semantics discovery and audio denoising. For semantics discovery, we obtained video clips of dogs barking, established several activity categories (Table 2), and manually labeled 2,534 clips with a balanced number in each category. For audio denoising, we first use PANNs (Kong et al., 2020) and AudioSep (Liu et al., 2023) to obtain clean audio, then use HuBERT to recognize all phones in canine vocalizations, including pauses and trivial sounds. After manually checking each phone discovered, we were able to identify most phones to actually come from dog vocals. We will ensure the final version contains these missing details.


====================Rebuttal Finishs===========================


## Question 1: How to prove you discoverd "phone", "words" and "semantics"?

This is indeed a very challenging problem. We aim to use the relationship between the common phone sequence with activities to support the possibility that canine vocalizations may possess such a language structure. 

We referred to the statement from the HuBERT article, which states that it can be used to obtain phone related units. Given the previous lack of research in this field, we want to use human-related definitions to demonstrate that canine vocalization contain certain patterns and are related to specific events.

This will lay the groundwork for further exploring the regularity or meaning in animal communication. To truly confirm that canines possess a language, we will need additional playback experiments.

## Question 2: Details about clustering training, audio denoising and activity detection.

For clustering training, we followed HuBERT work, *Can xingyuan finish this?*

For audio denosing, we refer to previous work AudioSep to maximally eliminate all sounds other than canine vocalizaitons.

For activity detection, we refer to MMAction-2 model, we manually labeled 13 different canine activities and got a 80% accuracy on video event detection.

## Question 3: Examples

We have already added some examples on the GitHub, please check them. In addition, we also developed an internal web app to help us examine the result of each stage. This demo is already hosted in public, you can access it through http://202.120.38.146:8079/
