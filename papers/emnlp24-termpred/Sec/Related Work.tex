% \vspace{-1em}
\section{Related Work} \label{rw}

%\subsection{Legal Judgment Prediction}

Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases~\cite{chalkidis-etal-2019-neural, an-etal-2022-charge}. These predictions encompass the charges, applicable statutory provisions, and suggested sentencing, including prison terms. Previous studies have predominantly framed these tasks as classification problems. There are three main approaches in LJP research~\cite{DBLP:journals/access/CuiSW23}.

\textbf{Content-based models.} 
These models employ language models to extract semantic features from the case documents and make predictions via MLP or linear layer. BERT~\cite{devlin-etal-2019-bert} and Electra~\cite{DBLP:conf/iclr/ClarkLLM20} stand out as representative language models in the literature. These models can be trained on large corpora. However, recent studies showed that language models may not be as effective in legal judgment prediction tasks compared to large language models, specifically GPT-3 and GPT-4~\cite{shui-etal-2023-comprehensive, wu-etal-2023-precedent}. These findings suggest that the legal domain problems are quite complicated so merely applying a language model does not guarantee high performance~\cite{vats-etal-2023-llms}.

\textbf{Models learning from historical cases.} 
Inspired by case law principles, these models aim to extract valuable insights from historical cases to enhance the prediction accuracy for new cases~\cite{DBLP:conf/cicai/ZhouLWKZW22}. Case relations can be learned by node classification of a graph~\cite{Rformer}, or contrastive learning~\cite{liu-etal-2022-augmenting}. 

\textbf{Models enhanced by legal knowledge.} 
Unlike other models, these models leverage extra legal knowledge. The legal knowledge utilized can be the content of legal rules~\cite{ML-LJP} or the judicial process~\cite{DBLP:conf/emnlp/ZhongGTX0S18,deng-etal-2023-syllogistic}. An innovative example of a law-enhanced approach is RLJP~\cite{Wu2022TowardsIA}, which draws inspiration from the judicial decision-making process and introduce an intermediate step to generate rationale and use this information to the LJP. However, these works often require additional training information, such as reference rationale from judges. Incorporating statutory text directly into prediction models represents another pivotal strategy for embedding legal knowledge, enabling the models to learn directly from the law text, as demonstrated in studies~\cite{neurjudge,ML-LJP}. 

% \paragraph{Few-shot Learning} 
% Although a few prior studies acknowledge the significance of charges with limited case numbers, and claim that it is not correct to directly ignore charges with fewer than 100 cases in evaluation. However, instead of conducting a detailed few-shot analysis~\cite{DBLP:conf/nldb/ZhaoSSA17, hu-etal-2018-shot}, these studies simply lower the threshold from 100 to 10 cases. The mixed results obtained do not definitively demonstrate effectiveness in few-shot learning scenarios.

% \subsection{Graph Representation for Text}

% Text embedding methods like Word-to-vector and Transformer-based model encoding are widely employed across a wide range of tasks in natural language processing (NLP), including Legal Judgment Prediction (LJP)~\cite{Greco2023BringingOI}. These techniques have been foundational in advancing NLP applications by enabling sophisticated understanding and processing of text data. However, recent studies have identified graph representations as a particularly effective approach for capturing the structural information inherent in text. This advantage is mentioned in several tasks, such as text summarization~\cite{DBLP:conf/acl/GaoCLC00Y23} and legal case classification~\cite{wang-etal-2022-d2gclf}, etc.

