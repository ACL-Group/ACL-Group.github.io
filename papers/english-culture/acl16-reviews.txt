============================================================================
ACL 2016 Reviews for Submission #490
============================================================================ 

Title: Mining Word Semantic Difference between American and British English

Authors: Hanyuan Shi, Yizhong Wang, Kenny Zhu and Seungwon Hwang

============================================================================
                            META-REVIEW
============================================================================ 


---------------------------------------------------------------------------
Meta Reviewer's Recommendation
---------------------------------------------------------------------------

Reject


---------------------------------------------------------------------------
Meta Reviewer's Detailed comments
---------------------------------------------------------------------------
============================================================================
                            REVIEWER #1
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

                         APPROPRIATENESS: 5
                                 CLARITY: 4
                             ORIGINALITY: 2
       EMPIRICAL SOUNDNESS / CORRECTNESS: 2
     THEORETICAL SOUNDNESS / CORRECTNESS: 2
                   MEANINGFUL COMPARISON: 1
                               SUBSTANCE: 4
              IMPACT OF IDEAS OR RESULTS: 1
         IMPACT OF ACCOMPANYING SOFTWARE: 1
          IMPACT OF ACCOMPANYING DATASET: 1
                          RECOMMENDATION: 1
                               MENTORING: NO


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

The abstract claims this will help linguists, language educators and learners - that's rather over-optimistic!

The discussion is stated in terms of "cultural differences" but that is not at all the same as looking at dialect differences.  There are far more cultural differences between nineteenth century US and twenty-first century US than there are between nineteenth century US and nineteenth century UK, for instance.  The books you have retrieved are presumably mostly out of copyright and therefore more than 70 years old.

The dictionary definitions will not give you ground truth on word usage, only on differences in senses.  For instance, Wiktionary does not give differences between British English and American English for the word "court" but some uses will be much commoner in BrE than AmE.

Your corpora are far too small to pick up anything other than very frequent uses reliably by a distributional method (including word2vec).  You will get a huge amount of noise just because of variation in subject matter of the books you have chosen.
(Incidentally, it was not a good idea to highlight `The Waste Land' as an example of a UK book, given that Eliot was from the US.)

I note that you don't give examples of actual figures for cosine similarity - you should indicate what sort of range you are finding.
My guess is you are finding numbers which are a lot less than 1, even for the words which are not supposed to show dialect differences.  If so, this is an indication the corpus is too small.

Stopwords - names of persons and places are not generally treated as stop words.  How did you detect them?

Why did you use tf-idf for the `co-occurrence' based algorithm as opposed to the more usual PPMI?

You need to use simple frequency as a baseline method.        Firstly
because there are differences between US and UK English and secondly because frequency has a strong effect on similarity measurements.

To claim that the quality at the top of your list is "very high" is an overstatement. This is nowhere near good enough to be of any use to lexicographers, for instance.

Did you actually look at your corpora?        I believe you would discover,
for instance, that the predominant sense of `ulster' is actually Ulster - one of the provinces of Ireland - and has nothing to do with
coats.        I seriously doubt you will find any use of `rook' meaning
firecracker in your corpus - it's an extremely uncommon use.

You have missed a directly relevant paper:
Peirsman, Geeraerts, Speelman (2010) JNLE

There have also been a series of papers on detecting unattested senses, which is a related problem - see:
Erk 2006
Lau et al 2012
Lau et al ACL 2014

The bibliography is missing information such as book publisher.

============================================================================
                            REVIEWER #2
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

                         APPROPRIATENESS: 5
                                 CLARITY: 5
                             ORIGINALITY: 2
       EMPIRICAL SOUNDNESS / CORRECTNESS: 2
     THEORETICAL SOUNDNESS / CORRECTNESS: 2
                   MEANINGFUL COMPARISON: 3
                               SUBSTANCE: 2
              IMPACT OF IDEAS OR RESULTS: 2
         IMPACT OF ACCOMPANYING SOFTWARE: 1
          IMPACT OF ACCOMPANYING DATASET: 3
                          RECOMMENDATION: 2
                               MENTORING: NO


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

This paper hypothesizes that cosine distance between vector representations of the same word in two different vector-space models (VSMs) can be used to estimate the usage differences between those two words. The hypothesis is evaluated in VSMs built from texts of British English and American English.
Three models are evaluated: TF-IDF, TF-IDF with basic dimensionality reduction, and word2vec. The evaluation data were extracted from Wiktionary, which has annotations for these usage differences. The authors find that word2vec gives the best results of these approaches. 

This paper establishes a novel task with some inherent interest and some promise as a general evaluation metric for vector representations of words. The dataset the authors are making available has potential as well. My concerns, reflected in my questions below, is that the approach is not actually identifying "cultural differences" or even deep usage differences, but rather just picking up on the inevitable differences one sees when comparing two VSMs, especially for the relatively rare words in the evaluation set.

* Do we have evidence that the usage differences that led to the gold data are actually involved in the analysis? Consider the first word in Table 2, 'rook'.
According to Wiktionary, in both British and American English, this can refer to a kind of crow. The usage difference is that, in BE, it can also refer to a firecracker used to scare that bird. Is that usage ever found in the corpus? If so, is the bird usage also present in those contexts? How many of the words in the gold data have this pattern?

* Is this about cultural differences or meaning differences? I guess we can view all word meanings as cultural, but this obscures important distinctions.
Consider Table 3. As a speaker of AE, 'united', 'mummy', and 'wasp' ('WASP') are all ambiguous for me, and I recognize that the less frequent sense might be more frequent in BE usage. The paper addresses concerns in this area at the bottom of page 3, but I feel the need for more clarity.

* What was the dimensionality used for truncating the SVD matrix? This is an important parameter. I would suggest exploring a range of values. It would not surprise me if some choices here led to performance that was as good as word2vec. (I would suggest a similar exploration of different word2vec output vector dimensionalities.)

============================================================================
                            REVIEWER #3
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

                         APPROPRIATENESS: 5
                                 CLARITY: 3
                             ORIGINALITY: 2
       EMPIRICAL SOUNDNESS / CORRECTNESS: 2
     THEORETICAL SOUNDNESS / CORRECTNESS: 3
                   MEANINGFUL COMPARISON: 3
                               SUBSTANCE: 2
              IMPACT OF IDEAS OR RESULTS: 2
         IMPACT OF ACCOMPANYING SOFTWARE: 1
          IMPACT OF ACCOMPANYING DATASET: 2
                          RECOMMENDATION: 2
                               MENTORING: YES

---------------------------------------------------------------------------
---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

In this paper, the authors have addressed the task of linguistic annotation of words which are semantically different due to their usage across different cultures. As stated, this is an initial attempt where the authors have experimented with various models for the purpose of model selection.

Overall, I liked the task addressed by the authors; it certainly has the potential to be applied in various downstream tasks if the results, in future, are promising. However, in my opinion the paper in its present state requires much improvement to be accepted for publication. 

While the authors have shown that their best approach (Skipgram) outperforms the others (SVD, TF-IDF) in the experiment, the results presented are not very good. An accuracy of approximately 0.5 on the best approach essentially implies a random guess at a word being a classified to be culturally different or not (Table 2: Top-n ranked list of Skipgram). Moreover, the outcome that Skipgram vectors are better than SVD+TF-IDF vectors which in turn are better than the vanilla TF-IDF vectors is not unexpected. It would have been interesting to see how the inverse cosine (with Skipgram vectors) model fares as compared to regression based learning and other state-of-the-art models.

Since the end goal seems interesting, I would suggest the authors extend this work by experimenting with other learning based models (along with the current best model), fine tune the models to get the best outputs and evaluate on a downstream task related to automatic linguistic annotation.

Some general observations:
1. Since this is a short paper, optimal space utilization is important. I feel that Section 2, which states common implementation techniques and established models, could have been cut down to a paragraph (with appropriate references).
This, in turn, would have given the authors much needed space to explain their results and observations with greater clarity.

2. With reference to section 2 and 3, the paper fails to establish a good flow for easy readability. I had to skim through these sections a couple of times to figure out what data is being used by which model, what is the outcome and analysis of that outcome. An easier approach is to have small sections dedicated to addressing specific aspects of the experiment individually. For example, Section 2 can briefly describe the models and Section 3 can be divided
in: 1) Data - Corpus used; 2) Experimental Setup - Preprocessing, word-windows and co-occurrence matrices; 3) Model implementation - models and data used by each; and, 4) Results and Discussion.

3. The authors should try and avoid mistakes / omissions, for example, Table 2 title which makes it difficult to comprehend the results. Additionally, more examples explaining the results could have been included if less informative facts, like Table 1, could be reduced/removed.

