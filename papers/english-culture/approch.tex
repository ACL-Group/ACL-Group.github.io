\section{Approach}
This section discusses the preprocessing of our text corpora, our
simple baseline methods as well as the Skip-grams model.

\subsection{Data Preprocessing}

In this study, our input data is English literature written by 
American or British authors, including such categories as poetry, 
science fictions, politics, biography, fables, etc.
We first lemmatize the text and reduce all words to orginal form. 
Then we remove stopwords from the paragraphs. 
Stopwords include names of persons or places \cite{names}, 
prepositions, pronouns, 
conjunctions, numerals and modal verbs, which do not have significant
meanings.
Finally, we apply each of the following algorithms to generate two vector
representations for each word, one from the American corpus and the other
from the British corpus. The inverse of cosine similarity between the
two gives the amount of cultural difference for a word.

\subsection{Co-occurrence Based Algorithms}
Here we use the distribution of co-ocurring words to represent a word. 
%Co-occurrence matrix is a common tool in the field of word embedding.
Given a sequence of words $(w_1, w_2, w_3,...,w_T)$, we use a fixed size
sliding window to obtain the co-occurrence matrix $M_{US}$ and $M_{UK}$ 
from the two corpora. Each entry in the matrix $M_{i, j}$ is the frequency 
of words $(w_i, w_j)$ appearing in the same window. Note the two matrixes
are of the same size because we combine the distinct words from both corpora
to form a global vocabulary.  We assume the cultural difference of a word
can be computed from the two occurrence matrixes. 

Our first algorithm is based on TF-IDF. Each entry $M_{i, j}$ 
in the above matrixes is multiplied by an IDF term, $IDF_{i, j}$, which
is the inverse of the number of paragraphs containing 
both $w_i$ and $w_j$ with in a window. This gives us two new
matrixes $M'_{US}$ and $M'_{UK}$. The cultural difference is computed by
calculating the cosine similarity between $M'_{US, i}$ and $M'_{UK, i}$ 
for $w_i$. This algorithm penalizes very frequent words. 

Our second algorithm is an optimization of the first. It factorizes the
above TF-IDF matrixes by singular value decomposition (SVD), and then compute
cosine similarity accordingly.

\subsection{The Skip-gram Model}

%The Skip-gram model is a high quality and efficient method for learning vector representation from a large data corpus. Its performance has been proven in many other applications. Due to our word embedding purpose, we use the Skip-gram model as one way to generate the vector representation from American and British corpus separately for every word $w_i$.

In the Skip-gram model, the objective is to maximize the average 
log probability

\begin{equation*}
\frac{1}{T} \sum_{t=1}^{T} \sum_{j=-c}^{c} \log p(w_{t+j}|w_t)
\end{equation*}
where $c$ is the size of training window which can be a function of $w_t$ and 
$T$ is the total size of training corpus.

$p(w_i|w_j)$ is defined by a softmax function
\begin{equation*}
p(w_i|w_j)=\frac{\exp(u_{w_i} {}^{\top} v_{w_j})}{\sum_{l = 1}^{V} \exp(u_l {}^{\top} v_{w_j})}
\end{equation*}
where $V$ is the size of the vocabulary, $u_w$ and $v_w$ are the "input" and "output" vectors representing the word $w$.

After we train this model on each monolingual corpus, we have two vector spaces: one for US corpus and the other for UK corpus. These two vector spaces 
are not directly comparable due to unknown meaning of each dimension. 
However, experiments \cite{Mikolov:2013tp} have shown that the relationship 
between vector spaces trained using Word2vec can be possibly captured by 
rotation and scaling, represented by a linear transformation matrix $W$. 
This matrix can be learned using a number of words with {\em little} 
cultural difference and the following optimization problem:

\begin{equation*}
\argmin_{W}\sum_{i=1}^{n}\norm{Wx_{i}-t_{i}}^2
\end{equation*}
where $x_{i}$ is a word in American English while $t_{i}$ is its 
corresponding word in British English and $n$ is the size of 
training samples. 
%We take the most common 5000 culturally same words 
%to learn $W$ in our experiments.
The training process can be optimized according to 
\cite{Mikolov2013distributed}.
