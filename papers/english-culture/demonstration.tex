\section{Evaluation}

In the evaluation part, we first introduce the evaluation method.
Then we describe the results of our four word embedding algorithms,
and finally we give some explanations.


\subsection{Evaluation Method}

We use two online dictionaries as a standard to evaluate four algorithms.
Wiktionary is a collaborative project to produce a free-content multilingual dictionary.
In Wiktionary, there are two categories called American English and British English. Terms in these categories have specific meanings in the corresponding country.
Oxford Dictionary is a worldwide dictionary famous for its authority. In each entry, the editors highlight the unique meanings in British or American.

For each word embedding algorithm, we enumerate every word in the ranking list from top to bottom to calculate precision and recall. And to check whether a word is cultural different, we respectively use
Wiktionary's category, Oxford Dictionary's highlight signal and the combination of them.

\subsection{Results}

For the evaluation task, we choose 50(or 100?) to be the size of sliding window in TF, TF-IDF, TF-IDF-SVD algorithms. We pick words with top10000 frequency in USA corpus and UK corpus, and then take the intersection of them to be the list which should be ranked. Totally there are 7599 words. Among these words 1077 has cultural difference according to Wiktionary, 3149 for Oxford Dictionary, 3465 for the combination.

In pictures 1, we ....
In table 1, we ....
Here is the result.

\subsection{Explanation}

The observation from the figures shows Wiktionary is more proper than Oxford Dictionary for our purposes. Because as an authoritative resource, Oxford Dictionary contains a large number of informal meanings which never appear in publications.
Then we choose the result from Wiktionary for the following explanations.  
For IF algorithm, the result is bad. We think the reason is that some frequency-less words may be more important for supporting the meaning of aim word but this feature is easily ignored because of the overwhelmed number of more frequency words, which influences a great part of cosine similarity. For instance, consider "football" between UK and USA. Assume that football are co-occur with Arsenal 10 times in British corpus and this number is obviously 0 in American corpus. This is a significant feature but are influence-less because of its small weight.
For TF-IDF co-occur algorithm and TF-IDF-SVD algorithm, which is a optimization of TF-IDF algorithm, we introduce TF-IDF of co-occur pairs to our problems. The applying of IF-IDF weakens the weight of common co-occur pairs, which leading to a more reasonable vector. The result shows this changing has a positive effect in the top of the list. However, to get a better result, there is another obstacle which limits IF-IDF's performance. Under this algorithm, the common words tend to have a bigger similarity. But another fact is that there are more cultural differences among common words because both the public and the editors usually pay more attention on them.
For Word2Vec method, skip-gram model is good for grabbing features of words from corpus to generate the representing vectors. Therefore the hit rate is much higher than the former algorithms and this rate is almost monotonic decreasing, which implies more cultural different words are in the top of the ranking list.
