\section{Related Work}
Linguistic study \cite{kovecses2006language} shows that words in different culture may have big differences in meanings. And this phenomenon is especially common in English with the spread of English across the world \cite{lipka1990outline,wierzbicka2006english,jackson2007words}.  In computational linguistics, 
multilingual word representation is an emerging topic of interest. 
There are generally two research directions: graph-based knowledge network or distribution-based vector representation. 

In graph-based models, BabelNet\cite{Navigli:2012dn} has the ambition 
to construct a unified multilingual semantic network (like WordNet) and 
integrates resources like WordNet and Wikipedia to achieve their goal. 
Even though this semantic network can be used to calculate relatedness 
across languages, due to the unified representation of multilingual synset, 
BabelNet can not detect differences for corresponding concepts 
in different languages.

In distributional models, the predominant approach to represent the
semantics of words is word embedding. The embeddings are usually trained 
using co-occurrence matrix, 
matrix factorization\cite{lebret2013word,levy2014neural,li2015word} or 
neural network\cite{Mikolov2013distributed}. 
Traditionally, these vectors are trained on monolingual data and 
the vector spaces of different languages are not directly comparable 
with each other. To solve this, some researchers try to train 
unified representations from multilingual 
corpus~\cite{Klementiev:2012uk,hermann2014multilingual,Vulic:2015to} 
or construct a mapping between the vector spaces of 
different languages~\cite{Mikolov:2013tp}. These vectors are then evaluated 
in tasks such as bilingual lexicon induction or cross-lingual 
word sense disambiguation, and have shown to achieve state-of-art performance.

Our task is similar to bilingual lexicon induction, though we want to 
detect semantic difference instead of finding similar words. 
Tomas Mikolov\cite{Mikolov:2013tp} shows the potential to detect errors 
in bilingual dictionary with Word2Vec and linear transformation among 
different vector spaces. In this work, we have shown that similar techniques
maybe employed to mining semantic differences within the same language.
