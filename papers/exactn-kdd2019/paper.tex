\documentclass[sigconf]{acmart}

%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{booktabs} % For formal tables
%\usepackage{enumitem}
\usepackage{multirow}
\usepackage{url}
\usepackage{paralist}
\usepackage{epsfig}
%\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{footnote}
\usepackage{threeparttable}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{algpseudocode}
\usepackage{color}

%\usepackage[small]{caption}

\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
\setlength{\textfloatsep}{3pt plus 2pt minus 2pt}

\fancyhead{}

\newcommand{\yu}[1]{{\bf \color{red} [[Yu says ``#1'']]}}
\newcommand{\lu}[1]{{\bf \color{blue} [[Lu says ``#1'']]}}
\newcommand{\KZ}[1]{{\bf \color{blue} [[Zhu says ``#1'']]}}

\newcommand\Algphase[1]{%
	\vspace*{-.4\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\columnwidth}{0.4pt}%
	\Statex\hspace*{-\algorithmicindent}\textbf{#1}%
	\vspace*{-.6\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\columnwidth}{0.4pt}%
}

% Copyright
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
%\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}
%\copyrightyear{2016}
%
%
%\acmArticle{4}
%\acmPrice{15.00}

% These commands are optional
%\acmBooktitle{Transactions of the ACM Woodstock conference}
%\editor{Jennifer B. Sartor}
%\editor{Theo D'Hondt}
%\editor{Wolfgang De Meuter}


\begin{document}
%\title{Optimal K-item Set Recommendation}
\title{Exact-K Recommendation via Maximal Clique Optimization}
%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}


\author{Anonymous Authors}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
%\affiliation{%
%  \institution{Institute for Clarity in Documentation}
%  \streetaddress{P.O. Box 1212}
%  \city{Dublin}
%  \state{Ohio}
%  \postcode{43017-6221}
%}
%\email{trovato@corporation.com}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{Anonymous Authors}
%\renewcommand{\shorttitle}{Exact-K Recommendation}


\begin{abstract}
%\KZ{Simplify this to 4 sentences only: problem, why interesting problem,
%your solution, and what's so good about your solution.}
This paper targets to a novel but practical recommendation problem named exact-K recommendation.
It is different from traditional top-K recommendation,
as it focuses more on (constrained) combinatorial optimization which will optimize to recommend a whole set of $K$ items called card,
rather than ranking optimization 
which assumes that ``better'' items should be put into top positions.
Thus we take the first step to give a formal problem definition,
and innovatively reduce it to Maximum Clique Optimization based on graph.
To tackle this specific combinatorial optimization problem which is NP-hard, we propose \emph{Graph Attention Networks} (GAttN) with a Multi-head Self-attention encoder and a decoder with attention mechanism. 
%\yu{The insights.}
It can end-to-end learn the joint distribution of the $K$ items and generate an optimal card rather than rank individual items by prediction scores.
Then we propose \emph{Reinforcement Learning from Demonstrations} (RLfD) which combines the advantages in behavior cloning and reinforcement learning, making it sufficient-and-efficient to train the model.
Extensive experiments on three datasets demonstrate the effectiveness of our proposed \emph{GAttN with RLfD} method, it outperforms several strong baselines with a relative improvement of 7.7\% and 4.7\% on average in Precision and Hit Ratio respectively, and achieves state-of-the-art (SOTA) performance for the exact-K recommendation problem.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010553.10010554</concept_id>
%  <concept_desc>Computer systems organization~Robotics</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
% <concept>
%  <concept_id>10003033.10003083.10003095</concept_id>
%  <concept_desc>Networks~Network reliability</concept_desc>
%  <concept_significance>100</concept_significance>
% </concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


%\keywords{ACM proceedings, \LaTeX, text tagging}

\maketitle

\input{intro}
\input{related}
\input{problem}
\input{approach}
\input{exp}
\input{conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\newpage
\appendix
\section{Naive Node-Weight Estimation Method}
\begin{algorithm}
	\caption{Naive Node-Weight Estimation Method.}       
	\label{alg:naive} 
	\begin{algorithmic}[1]
		\Require Given user $u$ and candidate items set $S$, construct graph $\mathbb{G}(\mathcal{N},\mathcal{E})$ defined in paragraph 2 of Sec. \ref{sec:problem_definition}.
		\State Estimate weight $w_i$ of each node $n_i\in \mathcal{N}$ in graph based on CTR of corresponding item $s_i\in S$.
		\State Initial result card $A=\emptyset$.
		\For {\emph{t = 1 to K}}
		\State Select node $a_t$ with the largest weight in $\mathcal{N}$ and add to $A$.
		\State Remove $a_t$ and nodes in $\mathcal{N}$ which are not adjacent to $a_t$.
		\EndFor\\
		\Return result card $A$.
	\end{algorithmic}
\end{algorithm}

\section{Reinforcement Learning from Demonstrations}
\begin{algorithm}
	\caption{Reinforcement Learning from Demonstrations.}       
	\label{alg:RLfD}
	\begin{algorithmic}[1]
		\Algphase{Phase 1 - Reward Estimator Training}
		\Require reward function $P(r=1|A,u;\phi)$, dataset $P_{data}^{D}(r^*|A,u)$
		\State Optimize $\phi$ with gradient descent by loss function $\mathcal{L}_{D}(\phi)$.\\
		\Return $P(r=1|A,u;\phi^*)$
	\end{algorithmic}
	\begin{algorithmic}[1]
		\Algphase{Phase 2 - Policy Training}
		\Require optimized reward function $P(r=1|A,u;\phi^*)$, dataset $P_{data}^{S}(A^*|S,u)$, policy $P(A|S,u;\theta)$
		\State Optimize $\theta$ with gradient descent by loss function $\mathcal{L}(\theta)$.\\
		\Return $P(A|S,u;\theta^*)$
	\end{algorithmic}
\end{algorithm}

\section{Experimental Settings}
\subsection{Datasets}
%\subsubsection{Statistics}
\begin{table}[h]
	\caption{Statistics of the experimented datasets.}
	%\vspace{-10pt}
	\label{tab:dataset_statistic}
	\centering
	\scriptsize
	\begin{tabular}{c|c|c|c|c}
		\toprule
		\textbf{Dataset} & \textbf{User\#} & \textbf{Card\#} & \textbf{Item\#} & \textbf{Sample\#} \\
		\midrule
		MovieLens(K=4,N=20) & 817 & 40036 & 1630 & 40036 \\
		\midrule
		MovieLens(K=10,N=50) & 485 & 33196 & 1649 & 33198 \\
		\midrule
		Taobao(K=4,N=50) & 581055 & 310509 & 3148550  & 1116582 \\
		\bottomrule
	\end{tabular}
\end{table}
%\subsubsection{Example}
\begin{table}[h]
	\caption{Show case of the dataset.}
	%\vspace{-10pt}
	\label{tab:dataset_example}
	\centering
	\scriptsize
	\begin{threeparttable}
	\begin{tabular}{c|c|c|c|c|c}
		\toprule
		& \textbf{user} & \textbf{card} & \textbf{candidate items} & \textbf{card label} & \textbf{positive item} \\
		\midrule
		sample\#1 & 1 & 1,2,3,4 & 1,2,3,4,...,20 & 1 & 2 \\
		%\midrule
		sample\#2 & 1 & 1,4,5,6 & 1,2,3,4,...,20 & 0 & / \\
		%\midrule
		\multicolumn{6}{c}{...} \\
		\bottomrule
	\end{tabular}
	\begin{tablenotes}
		\footnotesize
		\item (We take $K=4$ and $N=20$ for example. Items and users are represented as IDs here. Card label represents whether the card is clicked or satisfied by user (labeled as 1) or not (labeled as 0). Positive item is the actually clicked item in card by user.)
	\end{tablenotes}
	\end{threeparttable}
\end{table}

\subsection{Implementation and Parameter Settings}
\label{sec:implementation}
Here we report implementation details for the three datasets\footnote{The code and datasets will be released.} (two MovieLens based datasets and one Taobao based dataset),
and our implementation is based on TensorFlow\footnote{\url{https://www.tensorflow.org/}}.
To construct the training and test sets, we perform a 4:1 random splitting as in \cite{wang2017irgan} for all the datasets.
\subsubsection{MovieLens}
\label{sec:implementation_movielens}
Notice both MovieLens(K=4,N=20) and MovieLens(K=10,N=50) share the same parameter settings.
For a fair comparison, all models are set with an embedding size of $16$ for item and user IDs,
and optimized using the mini-batch Adam \cite{kingma2014adam} with a batch size of $32$ and learning rate of $0.001$.
All models are trained for $10$ epoch.
All the trainable feed-forward parameter matrices are set with the same input and output dimension as $32\times32$ (including DeepRank, BPR, and all the RNN cells in both Listwise-GRU, Listwise-MHSA and ours). 
Specifically for our GAttN model, in decoder (in Sec. \ref{sec:decoder}) we use LSTM \cite{hochreiter1997long} cells with units number of 32 and set beam size as $3$, number of heads in encoder (in Sec. \ref{sec:encoder}) MHSA layer is $2$, and the coefficient parameter $\alpha$ in loss function (in Sec. \ref{sec:loss_combination}) is $0.5$.
Number of layers $L$ in both encoder and decoder are set as 2.
For reward estimator model (in Sec. \ref{sec:reinforce}), we set the hidden size in fully-connected layer as 128.

\subsubsection{Taobao}
\label{sec:implementation_taobao}
In this dataset, the feature vectors for user and item are statistic features with size of 40 and 52 specifically, instead of ID features.
Sample statistic features are PV (page view), IPV (item page view), GMV (cross merchandise volume), CTR (click through rate) and CVR (conversion rate) for 1 day, 7 days and 14 days, etc. 
For this dataset, we first transfer the input representation of user and item to 32 dimension, i.e we set $W_I\in \mathbb{R}^{92\times32}$ and $b_I\in \mathbb{R}^{32}$ in Sec. \ref{sec:input}.
And all the other hyper-parameters are set as the same with those on MovieLens based datasets (refer to Appx. \ref{sec:implementation_movielens}).

\end{document}
