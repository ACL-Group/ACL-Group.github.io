\section{Related Works}
%This section surveys the relevant literature found in Top-K Recommendation and Neural Combinatorial Optimization. We open with a brief background presentation of the standard Top-K Recommendation methodology underlying our research. We then review the existing work found at the intersection of combinatorial optimization and machine learning (ML). 
\subsection{Top-K Recommendation}
\label{sec:top_k_related}
Top-K recommendation refers to recommending a list of $K$ ranked items to a user, which is related to the descriptions of recommendation problem and learning to rank methods.
\subsubsection{The Recommendation Problem}
%The explosive growth and variety of information available on the web frequently overwhelmed users, while Recommender Systems (RS) are valuable means to cope with the information overload problem. RS usually provide the target user with a list of items, which are selected from the overwhelmed candidates, in order to best satisfy his/her current demand. 
The key problem of recommendation system lies in how to generate users' most preferred item list.
Some previous works \cite{koren2009matrix} model the recommendation problem as a regression task (i.e. predict users' ratings on items) or classification task (i.e. predict whether the user will click/purchase/\ldots the item).
Items are then ranked based on the regression scores or classification probabilities to form the recommendation list.
Other works \cite{rendle2009bpr,wang2017irgan,he2017neural,he2018adversarial} directly model the recommendation problem as a ranking task, where many pairwise/listwise ranking methods are exploited to generate users' top-k preferred items.
%as the recommendation list.
%From the learning to rank perspective,
%the former methods belong to pointwise models,
%and the latter methods are pairwise/listwise models.
Learning to rank is surveyed in detail in the next subsection.

\subsubsection{Learning to Rank}
\label{sec:ltr}
%Learning to Rank (LTR) refers to a group of techniques that attempts to solve ranking problems by using machine learning algorithms. It can be broadly classified into three categories based on the training data format and loss function: pointwise, pairwise, and listwise models. \\
%%An excellent survey of these models can be found in \cite{liu2009learning}. \\
%%Here we only provide a brief overview.\\
%\noindent \textbf{\emph{Pointwise Model}} assumes that each instance's importance degree is known, and treats the ranking task as a classification or regression task. The loss function usually examines the prediction accuracy of each individual instance. 
%In the field of recommendation system, methods of \cite{koren2009matrix,he2017neural,wang2017irgan} can be summarized as pointwise ranking.
%%Representative algorithms include Subset Ranking \cite{cossock2006subset}, McRank \cite{li2008mcrank}, Prank \cite{crammer2002pranking}, and OC SVM \cite{shashua2003ranking} etc. 
%Note that pointwise models do not consider the inter-dependency among instances, and thus the position of a instance in the final ranked list is invisible to its loss function.\\
%\noindent \textbf{\emph{Pairwise Model}} assumes that the relative order between two instances is known or can be inferred and transforms it to a pairwise classification task. 
%The loss function is to minimize the number of misclassified instance pairs. 
%%Representative algorithms include Ranking SVM \cite{herbrich2000large}, RankBoost \cite{freund2003efficient}, RankNet \cite{burges2005learning}, GBRank \cite{zheng2008general}, IR SVM \cite{cao2006adapting}, Lambda Rank \cite{burges2007learning}, and LambdaMART \cite{wu2010adapting}.
%For recommendation system, representative algorithms include \cite{rendle2009bpr,wang2017irgan,he2018adversarial}.
%Note that the loss function used in the pairwise models only considers the relative order between two instances,
%%When one looks at only a pair of instances,
%however position of instances in the final ranked list can hardly be derived.\\
%%Considering that most IR evaluation measures are position-based, intuitively speaking, there is still a gap between this model and ranking for IR.\\
%\noindent \textbf{\emph{Listwise Model}} provides the opportunity to directly optimize ranking performance criteria.
%Recently \cite{ai2018learning} proposed Deep Listwise Context Model (DLCM) to fine-tune the initial ranked list generated by a base model, which is more effective than many existing listwise methods in terms of the experimental results.
%Other whole-page ranking optimization methods can be found in \cite{bello2018seq2slate,jiang2018beyond},
%which mainly focus on ranking refinement \cite{jin2008ranking}.
%% or slate optimization \cite{swaminathan2017off}.
%As compared to pointwise and pairwise models,
%advantage of listwise model lies in that its loss function considers positions of instances in the ranked list.
%Above are the most related methods to our problem,
%but they either target to ranking refinement or don't consider the constraints in ranking list,
%which are not well-designed for exact-K recommendation.
Learning to Rank (LTR) refers to a group of techniques that attempts to solve ranking problems by using machine learning algorithms.
It can be broadly classified into three categories: pointwise, pairwise, and listwise models.
Pointwise Models \cite{koren2009matrix,he2017neural,wang2017irgan} treat the ranking task as a classification or regression task.
However, pointwise models do not consider the inter-dependency among instances in the final ranked list.
Pairwise Models \cite{rendle2009bpr,wang2017irgan,he2018adversarial} assume that the relative order between two instances is known and transform it to a pairwise classification task.
Note that their loss functions only consider the relative order between two instances,
while the position of instances in the final ranked list can hardly be derived.
Listwise Models provide the opportunity to directly optimize ranking criteria and achieve whole-page ranking optimization.
Recently \cite{ai2018learning} proposed Deep Listwise Context Model (DLCM) to fine-tune the initial ranked list generated by a base model,
which achieves SOTA performance.
Other whole-page ranking optimization methods can be found in \cite{bello2018seq2slate,jiang2018beyond},
which mainly focus on ranking refinement \cite{jin2008ranking}.
Listwise models are the most related to our problem.
However, they either target on ranking refinement or don't consider the constraints in ranking list,
which are not well-designed for exact-K recommendation.

%\textbf{Describe the difference between Top-k recommendation and Exact-k recommendation, and the advantages of the proposed method compared to the pointwise, pairwise and listwise models for the Exact-k recommendation task.} 
%Above LTR methods targeting Top-K recommendation mainly focus on the ranking optimization objective,
%while Exact-K recommendation is regard as a constrained combinatorial optimization problem.
%So we need make a new problem definition formulation and corresponding approach to tackle it.
%To the best of our knowledge, this
%has never been explored before in the domain of IR.

\subsection{Neural Combinatorial Optimization}
\label{sec:nco_related}
Even though machine learning (ML) and combinatorial optimization have been studied for decades respectively, 
%there are few investigations about application of machine learning method in combinatorial optimization problems. The following section take a closer look to ML within combinatorial optimization and discuss the relation between our work and some relevant literature.
%ML can be employed to perform specific tasks that are integral to solution processes originating in operational research (OR). 
%The application of ML to discrete optimization problems can date back to the 1980's and 1990's \cite{smith1999neural}.
%However, very limited success is ultimately achieved and the area of research is left nearly inactive at the beginning of this century. 
%As a result, these NP-hard problems have traditionally been solved using heuristic methods \cite{silver2004overview}.
%Currently, the related works are mainly focused on two areas: supervised learning and reinforcement learning.
%Supervised learning method \cite{vinyals2015pointer} is a first successful attempt to solve a combinatorial optimization problem by using recent advances in artificial intelligence.
%In this work, a specific attention mechanism named Pointer Net motivated by the neural encoder-decoder sequence model is proposed to tackle Traveling Salesman Problem (TSP) problem.
there are few investigations on the application of ML methods in solving the combinational optimization problem.
Current related works mainly focus on two types of ML methods: supervised learning and reinforcement learning.
Supervised learning \cite{vinyals2015pointer} is the first successful attempt to solve the combinatorial optimization problem.
It proposes a special attention mechanism named Pointer-Net to tackle a classical combinational optimization problem: Traveling Salesman Problem (TSP).
%Reinforcement learning (RL) aims to transform the discrete optimization problems into sequential decision problems and becomes popular because of the advent of game of Go \cite{silver2016mastering}.
%Base on Pointer Network, \cite{bello2016neural} develops a neural combinatorial optimization framework with RL, which solves some classical problems, such as TSP and Knapsack Problem.
%Similar works using architecture like Pointer Network can also be seen in \cite{nazari2018deep,kool2018attention}.
%On a related topic, \cite{khalil2017learning} solves optimization problems over graphs using a graph embedding structure and a greedy algorithm learned by deep Q-learning (DQN) \cite{mnih2013playing}.
Reinforcement learning (RL) \cite{silver2016mastering} aims to transform the combinatorial optimization problem into a sequential decision problem
and becomes increasingly popular recently.
Based on Pointer Network, \cite{bello2016neural} develops a neural combinatorial optimization framework with RL, which performs excellently in some classical problems, such as TSP and Knapsack Problem.
On a related topic, \cite{khalil2017learning} solves optimization problems over graphs using a graph embedding structure and a greedy algorithm learned by deep Q-learning \cite{mnih2013playing}.
In domain of IR, reinforcement learning is also applied to search result diversification \cite{xia2017adapting}, LTR \cite{hu2018reinforcement,wei2017reinforcement} and RS \cite{zhao2018deep,chen2018top,zheng2018drn},
which all regard ranking process as a Markov Decision Process (MDP),
but they are still designed for traditional top-K recommendation.
%In this work, we focus on exact-K recommendation, which is transferred into the maximal clique optimization problem.
%There are some works to solve it,
%but they focus on estimation of node-weight \cite{Ion2011Image}.
%The main difference between our work and previous is that we target to directly select a maximal clique rather than search for maximum weighted cliques,
%which brings a grave challenge.
In this work, we focus on exact-K recommendation,
which is transferred into the maximal clique optimization problem.
Some researches \cite{Ion2011Image} also try to solve it,
but they often focus on estimation of node-weight.
The main difference between them and ours is that we target to directly select an optimal clique rather than search for the clique comprised of maximum weighted nodes, which brings a grave challenge.