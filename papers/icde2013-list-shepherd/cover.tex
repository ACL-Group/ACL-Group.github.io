\documentclass[11pt]{article}
\usepackage{times,amsmath,epsfig,subfigure,proof,url}
%\usepackage{fix2col}
\usepackage{cases}
\usepackage{algorithm}
\usepackage{amsfonts, mathrsfs}
\usepackage{algpseudocode}
%\usepackage{algorithmic}
\usepackage{textcomp}
\setlength{\topmargin}{1cm}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.3in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\newcommand{\rv}{\textbf{Revision:~}}

\begin{document}
\noindent {\bf Dear Shepherd},
\vspace*{5mm}

We thank you and the other anonymous reviewers for your insightful and
valuable comments and suggestions about our paper ``Automatic Extraction of
Top-k Lists from the Web''. We have significantly revamped our paper
according on these comments and below summarizes the changes we have made to
the paper. We organize our changes according to the order of reviewer
comments that request improvement.
\\

\noindent
Sincerely yours,\\

\noindent
Zhixian Zhang, Kenny Zhu, Haixun Wang and Hongsong Li

\vspace*{0.5cm}

\subsection*{Review 1}
\begin{description}
\item[W1:]
The problem of extracting information from web pages with top-k lists is
certainly novel and interesting. There is no doubt that the resulting
information is cleaner and has a ``context.'' What the authors have not
clearly articulated is some sample scenarios where the resulting data can be
used. More elaboration of potential applications in search, advertisement,
and general purpose Q/A systems is necessary.

\rv We have added a concise paragraph about how the
extracted list can be used in Q/A system and introduced some
state-of-art top-k query processing system in related work (Section
7, para 6 ``One of the potential use of the extracted top-k
lists...'').

\item[W2:] The system as proposed should be able to handle most top-k lists on a
single web page. Not handling top-k lists that span across multiple web
pages seems to be a glaring omission...

\rv We surveyed large number of web pages and observed that
roughly 5\% of the top-k lists are multi-page/slideshow type. The
number is not insignificant but in this paper we choose to focus on
single page extraction. We mention this point in Section 4.A, para
3 (``Note that a web page with a top-k-like...'').

\item[Detailed comment 1:]
Another mildly troubling aspect of the paper is the lack of references
within the major sections of the paper [II and III] even when they use
non-obvious techniques from existing papers - the core idea for tag path
clustering algorithm being one of them (taken from [5]).

\rv We added references to the relevant parts of Section 3 and 4
(as we have a new Section 2).
\begin{enumerate}
 \item Section 3, para 3: added reference[10][11][12] to other short text
    conceptualization methods.
 \item Section 4.A, para 4: added reference[14] to POS tag.
 \item Section 4.B, para 3: added reference[5] to Tag Path Clustering Algo.
 \item Section 4.D.(3), para 1: added reference[21] to NER tool.
\end{enumerate}

\item[Detailed comment 2:] While it is true that they describe related papers in section V, they do
not make an attempt to explain how they have used it/improved it or how
their application differs from cited papers.

\rv We overhauled the whole related work section (Section 7) and made
efforts to explain the above. We introduced several state-of-art
list extraction techniques and discussed in detail about the
differences between theirs and ours.

\item[Detailed comment 3:] The paper liberally uses percentages without any discussion or
attribution of sources. For example ``2\% of the tables that are relational''
or ``about 1.4\textperthousand{} of total web pages'' and so on.
It would be better if additional details are provided.

\rv We revised statements with percentages and numbers.
\begin{enumerate}
\item We modified Section 1, para 2 and Section 1, para 10 (``Top-k data is of high
 quality..''), with the results from [3]. In [3], they estimated that only
 1.1\% of web tables are relational and their table relation
 extractor shows 41\% precision.
\item In Sec 6.D, para 1, we gave a detailed discussion on the total
 number of top-k lists in the web. We estimated there are about 2.2 millions
 top-k lists (about 1.4\textperthousand{} of total web pages).
 We added references to the early mentions of these numbers (Sec 1, para 9-12 ``Top-k data on the
 web is large and rich...'').
\end{enumerate}
\end{description}

\subsection*{Review 2}
\begin{description}
\item[W1:]
The empirical observations about top-k lists that motivate the feature
engineering need to be backed by either some statistics: ``many top-k lists
contain spatial and temporal information'' is believable, but needs to be
quantified.

\rv We have done our surveys and computed statistics for
these observations. The surveys are mentioned in Sec 6.B(2) and Sec
6.E(3) respectively. And other statistics were added to bullets
para 11(``Top-k data is ranked...'') and para 12 (``Top-k data has
interesting semantics...'') in Section 1 Introduction.

\item[W2:] As mentioned in the review summary, the paper lacks formalisms that
cleanly articulate the input, output and process.

\rv We added a Section 2 Problem definition in which we define the
problem more clearly.

\item[W3:] Much of the paper's quality efforts rely on Probase (SIGMOD 2012)'s data
quality. Is there a way to evaluate the Title Classifier / Content
Extraction phases as independent pieces, say, by training on other
ontologies / factsets, like WordNet or even a 50\% sample of Probase? This
would give a good evaluation of the impact of semantic knowledge on the
system.

\rv We conducted further experiments on the accuracies of
title classification and list extraction module of the algorithm
against subsets of Probase of various sizes and on the
whole of WordNet. The results are presented in Fig. 11(a) and 11(b)
and discussed in detail in Section 6.A, 6.B (bullet 1 and 3).

\item[W4:] The Empirical evaluation is sample based, but details are scarce on the
sampling -- how are the numbers significant? What was the sampling scheme?
Is there a way to validate rank and classification quality objectively at
scale? If not, why?

\rv We redesigned our evaluation plan and defined 4 benchmark
datasets, and this information is presented in Table 6 and we make a detailed
explanation on the size and sampling scheme of each benchmark in
Section 6.A. We enlarged Title-1, Title-2 and Page-1 from the previous version of
the paper to ensure that they contain enough true positives. We then redid
all experiments with the 3 new benchmarks and update experimental results
(most of them changed very little and did not affect our conclusion).

\item[W5:] Fig 11a demonstrates scalability, but there is no commentary on the
outliers. Are there some intuitions as to \_why\_ some items took longer / non
linear?

\rv The main reason for the outliers is that the
time-consuming page swapping happens as the program took up most
physical memory in previous experimental environment (3GB memory).
Therefore, we upgrade the environment to 4GB memory and re-ran
each of our experiments 10 times and took the average time. The resulting
scatter plot (in Fig. 11(c)) has been significantly improved and appears to
be more regular. This discussion is included in Section 6.C.

\item[Detailed comment 1:]
Overall, the paper could use some rigor in the formalisms and overall
intent of the task. The notion of a ``top-k list'', consisting of $k$, where
each item is a bag of attribute value-based concepts is unclear -- is there
a notion of context of the entire list as well? Does it belong to a larger
ontology? Is there a relationship between two lists which contain related
information? Having a well-articulated data model and problem statement
would help answer all these questions.

\rv We have added Section 2 Problem Definition which hopefully answers
the questions above.

\item[Detailed comment 2:]
Further, as a reader, the paper misses a great possible next step (paper
does not acknowledge any possible future work)...

\rv We have added some discussion about future work in
Section 7, para 6 (``One of the potential use of...'') and introduced
some state-of-art top-k query processing systems including the EDBT
2009 paper as well as a couple of other related papers.

\item[Detailed comment 3:]
There are some parts of the paper that could use some proofreading, such
as:
\begin{itemize}
\item ``Known that the recall''
\item ``which make it extracted about''
\end{itemize}

\rv We have proofread the paper and fixed these errors.
\end{description}

\subsection*{Review 3}

\begin{description}
\item [Minor comment 1:]
In the introduction, when some statistics are mentioned, such as 90\% of
tables and 2\% of the tables, their sources should be also included. How are
those figures derived?

\rv This is similar to W2 of Review 1. And we have revised
all affected statements.
\begin{enumerate}
 \item In Sec 1, para 2 and Sec 1, para 10 (``Top-k data is of high quality...''),
    we update numbers and add reference to [5].
 \item In Sec 1, para 9-12(``Top-k data on the
 web is large and rich...''), some statistics and experiment results are
 mentioned, we refer these numbers to corresponding subsections in
 Sec 6 Evaluation.
 \item In Sec 1, para 2, the number 90\% is based on our empirical
 observation.
\end{enumerate}

\item [Minor comment 2:] This problem is related to ...

\rv We have added this citation to our related work section.

\item[Minor comment 3:] In Probase [], the citation should be added.

\rv This has been fixed.
\end{description}

\subsection*{Some Additional Modifications Not Due to Comments Above}

\begin{enumerate}
\item In Sec 4.A, para 5 (``We build a classifier to recognize top-k
titles...''), we give further explanation on why we tune the
classifier for higher recall.

\item In Sec 4.C, para 4 (``The above described approach...''), we
discuss in detail about the learning-based model and provide
equations.

\item Moved Implementation Details out of Sec 4 Evaluation and made it a
stand-alone section. We also gave more details on the generation of the
training data.
\end{enumerate}
\end{document}
