Dear Shepherd,

We thank you and the other anonymous reviewers for your insightful and
valuable comments and suggestions about our paper "Automatic Extraction of
Top-k Lists from the Web". We have significantly revamped our paper
according on these comments and below summarizes the changes we have made to
the paper. We organize our changes according to the order of reviewer
comments that request improvement.

** Review 1.
W1. The problem of extracting information from web pages with top-k lists is
certainly novel and interesting. There is no doubt that the resulting
information is cleaner and has a "context". What the authors have not
clearly articulated is some sample scenarios where the resulting data can be
used. More elaboration of potential applications in search, advertisement,
and general purpose Q/A systems is necessary.

[Revision] We have added a concise paragraph about how the
extracted list can be used in Q/A system and introduce some
state-of-art top-k query processing system in related work (Section
7, para 5-6).

W2: The system as proposed should be able to handle most top-k lists on a
single web page. Not handling top-k lists that span across multiple web
pages seems to be a glaring omission...

[Revision] We surveyed large number of web pages and observed that
roughly 5% of the top-k lists are multi-page/slideshow type. The
number is not insignificant but in this paper we choose to focus on
single page extraction. We mention this point in Section 4.A, para
3.

Detailed comments:
(1) Another mildly troubling aspect of the paper is the lack of references
within the major sections of the paper [II and III] even when they use
non-obvious techniques from existing papers - the core idea for tag path
clustering algorithm being one of them (taken from [5]).

[Revision] We added references to the relevant parts of Sec 3 and 2
(as we have a new Sec 2).
 (a)Section 3, para 3: add reference[10][11][12] to other short text conceptualization methods .
 (b)Section 4.A, para 4: add reference[14] to POS tag.
 (c)Section 4.B, para 3: add reference[5] to Tag Path Clustering Algo.
 (d)Section 4.D.(3), para 1: add reference[21] to NER tool.

(2) While it is true that they describe related papers in section V, they do
not make an attempt to explain how they have used it/improved it or how
their application differs from cited papers.

[Revision] We overhauled the whole related work section and made
efforts to explain the above. We introduce several state-of-art
list extraction technique and discuss in detail about the
difference between theirs and ours.

(3) The paper liberally uses percentages without any discussion or
attribution of sources. For example "2% of the tables that are relational"
or "about 1.4бы of total web pages" and so on. It would be better if
additional details are provided.

[Revision]
 We revise statements with percentages and numbers.
 (a) We modify Sec 1, para 2 and Sec 1, para 10 ("Top-k data is of high
 quality.."), with the results from [3]. In [3], they estimates that only
 1.1% of web tables are relational and their table relation
 extractor shows 41% precision.
 (b) Sec 6.D, para 1, we give a detailed discussion on the total
 top-k list in the web. We estimate there are about 2.2 millions
 top-k lists (about 1.4бы of total web pages). We add references to
 the early mentions of these numbers (Sec 1, para 9-12).

** Review 2.
W1. The empirical observations about top-k lists that motivate the feature
engineering need to be backed by either some statistics: ``many top-k lists
contain spatial and temporal information'' is believable, but needs to be
quantified.

[Revision] We have done our surveys and computed statistics for
these observations. The surveys are mentioned in Sec 6.B(2) and Sec
6.E(3) respectively. And statistics are added to bullet 3) and 4)
in section 1 Introduction.

W2. As mentioned in the review summary, the paper lacks formalisms that
cleanly articulate the input, output and process.

[Revision] We added a Section 2 Problem definition in which we define the
problem more clearly.

W3. Much of the paper's quality efforts rely on Probase (SIGMOD 2012)'s data
quality. Is there a way to evaluate the Title Classifier / Content
Extraction phases as independent pieces, say, by training on other
ontologies / factsets, like WordNet or even a 50% sample of Probase? This
would give a good evaluation of the impact of semantic knowledge on the
system.

[Revision] We conducted further experiments on the accuracies of
title classification and list extraction module of the algorithm
that depends on subsets of Probase of various sizes and on the
whole of WordNet. The results are presented in Fig. 11(a) and 11(b)
and discussed in detail in Section 6.A, 6.B (bullet 1 and 3).

W4. The Empirical evaluation is sample based, but details are scarce on the
sampling -- how are the numbers significant? What was the sampling scheme?
Is there a way to validate rank and classification quality objectively at
scale? If not, why?

[Revision] We redesign our evaluation plan and define 4 benchmarks,
information is presented in Table 6 and we make a detailed
explanation on the size and sampling scheme of each benchmark in
Section 6.A. We enlarge Title-1, Title-2 and Page-1 to guarantee
there are enough true positives. We redo all experiments with the 3
benchmarks and update experimental results(most of them change very
little and do not affect our conclusion).

W5. Fig 11a demonstrates scalability, but there is no commentary on the
outliers. Are there some intuitions as to _why_ some items took longer / non
linear?

[Revision] The main reason for the outliers is that the
time-consuming page swapping happens as the program took up most
memory in previous experimental environment(3GB memory). Therefore,
we upgrade the environment to 4GB memory and re-run each of our
experiments 10 times and taken the average. The resulting scatter
plot (in Fig. 11(c)) has been significantly improved and appears to
be more regular. This discussion is included in Section 6.C.

Detailed Comments:
(1) Overall, the paper could use some rigor in the formalisms and overall
intent of the task. The notion of a ``top-k list'', consisting of $k$, where
each item is a bag of attribute value-based concepts is unclear -- is there
a notion of context of the entire list as well? Does it belong to a larger
ontology? Is there a relationship between two lists which contain related
information? Having a well-articulated data model and problem statement
would help answer all these questions.

[Revision] We have added Section 2 Problem Definition.

(2) Further, as a reader, the paper misses a great possible next step (paper
does not acknowledge any possible future work)...

[Revision] We have added some discussion about future work in
Section 7, para 5-6) and introduce some state-of-art top-k query
processing systems including the EDBT 2009 paper as well as a
couple of other related papers.

(3) There are some parts of the paper that could use some proofreading, such
as:
"Known that the recall"
"which make it extracted about"

[Revision] We have proofread the paper and fixed these errors.


** Review 3.
Minor comments:
(1) In the introduction, when some statistics are mentioned, such as 90% of
tables and 2% of the tables, their sources should be also included. How are
those figures derived?

[Revision] This is similar to W2 of Review 1. And we have revised
all statements related.
 (a)In Sec 1, para 2 and Sec 1, para 10, we update numbers and add reference to [5]
 (b)In Sec 1, para 9-12, some statistics and experiment results are
 mentioned, we refer these numbers to corresponding subsections in
 Sec 6 Evaluation.
 (c)In Sec 1, para 2, the number 90\% is based on our empirical
 observation.

(2) This problem is related to ...

[Revision] We have added this citation to our related work section.

(3) 3: In Probase [], the citation should be added.

[Revision] This has been fixed.

Additional modifications:

(1) In Sec 4.A, para 5 ("We build a classifier to recognize top-k
titles..."), we give further explanation on why we tune the
classifier for higher recall.

(2) In Sec 4.C, para 4 ("The above described approach..."), we
discuss in detail about the learning-based model and provide
equations.

(3) Move Implementation Details out of Sec 4 Evaluation and make a
stand-alone section. And give more details on the generation of the
training data.
