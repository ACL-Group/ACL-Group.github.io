\section{Introduction}
\label{sec:intro}
% table extraction from the web has attracted a lot of attention, however, top-k extraction is more important, for the following reason:

% 1) a lot of data. For table extraction, only Wikipedia table is useful
% 2) data is more clean
% 3) data has more semantics, with ranking info, typicality info, also include other columns (pictures, links, singers for songs, etc)
% 4) data is more interesting, trendy, people want to know top things

\begin{figure*}[th]
        \centering
        \epsfig{file=./pics/dotNetExample.eps,width=1.8\columnwidth}
	\vspace*{-2ex}
        \caption{Snapshot of a Typical Top-k Page
          \cite{dotNetAwards} and its segments}
 	\label{fig:dotNet}
	\vspace*{-2ex}
\end{figure*}

The world wide web is currently the largest source of
information. However, most information on the web is unstructured text
in natural languages, and extracting knowledge from natural language
text is very difficult. Still, some information on the web exists in
structured or semi-structured forms, for example, as lists or web
tables coded with specific tags such as {\tt <ul>}, {\tt <li>}, and
{\tt <table>} on html pages. As a result, a lot of recent work has
focused on acquiring knowledge from structured information on the web,
in particular, from web tables
\cite{googlesets,webtables08,LiuGZ03:MDR,MiaoTHSM09:TagPathClustering,GatterbauerBHKP2007:Towards,FumarolaWBMH11:List,WangWWZ12:Tables}.

However, it is questionable how much valuable knowledge we can extract
from lists and web tables. It is true that the total number of web
tables is huge in the entire corpus, but only a very small percentage
of them contains useful information. An even smaller percentage of
them contains information interpretable without context. Specifically,
based on our experience, more than 90\% of the tables are used for
content layout on the web. Furthermore, a majority of the remaining
tables are not ``relational.'' (We are only interested in relational
tables because they are interpretable, with rows representing
entities, and columns representing attributes of those entities.)
According to Cafarella et al. \cite{webtables08}, 
of the 1.1\% of all web tables that are relational, many
are meaningless without context.  For example, suppose we
extracted a table that contains 5 rows and 2 columns, with the 2
columns labeled ``Companies'' and ``Revenue'' respectively. It is
still unclear why these 5 companies are grouped together (e.g., are
they the most profitable, most innovative, or most employee friendly
companies of a particular industry, or in a particular region?), and how
we should interpret their revenues (e.g., in which year or even in
what currency). In other words, we do not know the extract circumstances
under which the extracted information is useful.

% to extract lists or tables from the web.  None of them targets the
% top-$k$ list extraction that is studied in this work. In fact,
% most of the methods are based on either very specific list-related
% tags \cite{googlesets,webtables08} such as {\tt <ul>}, {\tt <li>} and
% {\tt <table>} or the similarity between DOM trees
% \cite{LiuGZ03:MDR,MiaoTHSM09:TagPathClustering} and ignore the visual
% aspect of HTML documents. These approaches are likely to be brittle
% because of the dynamic and inconsistent nature of web pages. More
% recently, several groups have attempted to utilize visual information
% in HTML in information extraction. Most notably, Ventex
% \cite{GatterbauerBHKP2007:Towards} and HyLiEn
% \cite{FumarolaWBMH11:List} were designed to correlate the rendered
% visual model or features with the corresponding DOM structure and
% achieved remarkable improvements in performance. We will discuss them
% further in \ref{sec:related}.  However, these techniques
% undiscriminatingly extract {\em all} elements of {\em all} lists or
% tables from a web page, therefore the objective is different from that
% of this work which is to extract {\em one} specific list from a page
% while purging all other lists (e.g. (d) in Figure
% \ref{fig:topscientists}) as noise. The latter poses different
% challenges such as distinguishing ambiguous list boundaries and
% identifying unwanted lists.

% \cite{Weninger10:UnexpectedList}.

Therefore, {\it understanding the context} is extremely
important in information extraction.  Unfortunately, in most cases,
context is expressed in unstructured text that machines cannot
interpret. In this paper, instead of focusing on structured data (such
as tables) and ignoring context, we focus on context that we can
understand, and then we use the context to interpret less structured
or almost free-text information, and guide its extraction.


Specifically, we focus on a rich and valuable source of information on
the web, which we call top-$k$ web pages. A top-$k$ web page describes
$k$ items of particular interest.  In most cases, the description is
in natural language text which is not directly machine interpretable,
although the description has the same format or style for different
items. But most importantly, the title of a top-$k$ page often clearly
discloses the context, which makes the page interpretable and
extractable. Some typical titles are:

\begin{itemize}
\item {\it 20 Most Influential Scientists Alive Today}
\item {\it Twelve Most Interesting Children's Books in USA}
\item {\it 10 Hollywood Classics You Shouldn't Miss}
\item {\it .net Awards 2011: top 10 podcasts}
\end{itemize}

The title of a top-$k$ page contains at least three pieces of
important information: i) A number $k$, for example, {\it 20}, {\it
  Twelve}, and {\it 10} in the above example, which indicates how many
items are described in the page; ii) A topic or concept the items
belong to, for example, {\it Scientists}, {\it Children's Books},
{\it Hollywood Classics} and {\it podcasts};
iii) A ranking criterion, for example, {\it
  Influential}, {\it Interesting}, and {\it You Shouldn't Miss} (which
is equivalent to {\it Best} or {\it Top}). Sometimes the ranking
criterion is given implicitly, in which case we make it equivalent to the
``Best''. Besides these 3 components, some top-$k$ titles contain
two optional pieces of information: time and location. For example,
{\it 2011} and {\it USA} in the above example.

In this paper, we develop a system that extracts top-$k$ lists from a
web corpus that contains billions of pages. As an example, Figure
\ref{fig:dotNet} is a typical top-$k$ page \cite{dotNetAwards}, and
Table \ref{tab:sampleoutput} shows the result extracted by our system
from the page.  A top-$k$ page has some interesting features. Figure
\ref{fig:dotNet}(a) is a snapshot of the entire page and Figure
\ref{fig:dotNet}(b-e) are some of its noteworthy segments.  The title,
which is shown in Figure \ref{fig:dotNet}(b), contains $k$, the size
of the list (10), the topic (podcasts) of the list, 
a ranking criterion (top) and time information(2011).  Figure
\ref{fig:dotNet}(c) shows the description of one item in the top-$k$
list, which contains the podcast's name (The Big Web Show) as well as
some additional information, such as who (Zeldman et al.), when (since
April 29, 2010), where (New York City and Austin, Texas), how (weekly,
live, audio, sometimes video, about an hour) and a picture, which can
be treated as the attributes of the item.  Furthermore, note that the
top $k$ page may contain unwanted lists such as those shown in Figure
\ref{fig:dotNet}(d-e), which poses a challenge to the 
extraction algorithm.

\begin{table*}
\centering
\caption{Sample extraction output of ``.net Awards 2011: top 10 podcasts'' \cite{dotNetAwards}}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|} \hline
Index & Name & Image & Url & Hosted by & Recorded in & Running since & Format & ... \\\hline
1 & The Big Web Show & [image] & [link] & Zeldman et al. & NYC \& Austin, TX & April 29, 2010 & Weekly, live... & ...\\
2 & Boagworld & [image] & [link] & Boag et al. & a barn in Hampshire & August 2005 & Weekly, audio... & ...\\
3 & Creative Coding & [image] & [link] & Lee-Delisle et al. & Brighton, Truro... & January 2011 & Every two... & ...\\
%4 & CSS-Tricks & [image] & [link] & Chris Coyier & Chris' home office... & 2009 & Randomish... & ...\\
%5 & Nettuts+ & [image] & [link] & Jeffrey Way &  Chattanooga, TN & 2008 & Weekly, screencast... & ...\\
%6 & This Developer's Life & [image] & [link] & Hanselman et al. & Scott's home office... & 2010 & Pretty much... & ...\\
%7 & The SitePoint Podcast & [image] & [link] & Simoneou et al. & SitePont headquarters & November 10, 2008 & weekly... & ...\\
%8 & Creative Coding & [image] & [link] & Lee-Delisle et al. & Brighton, Truro... & January 2011 & Every two... & ...\\
%9 & Creative Coding & [image] & [link] & Lee-Delisle et al. & Brighton, Truro... & January 2011 & Every two... & ...\\
... & ... & ... & ... & ... & ... & ... & ... & ... \\
10 & Unmatched Style & [image] & [link] & Crawford et al. & Columbia, SC & 2009 & Weekly, pre-recorded... & ...\\


%{\bf Index} & {\bf Name} & {\bf Image} & {\bf Description} & {\bf Wiki. Link} \\ \hline
%1 & {\em Timothy J. ...} & tim-berners-lee... &     who invented the World Wide Web... & [link]\\
%2 & {\em Noam Chomsky} & noam\_chomsky.jpg &        who, though a linguist and... & [link]\\
%3 & {\em Richard Dawkins} & richard\_dawkins.jpg &  whose use of evolutionary biology... & [link] \\
%4 & {\em Persi Diaconis} & persi\_diaconis.jpg &    who in merging the mathematical... & [link] \\
%5 & {\em Jane Goodall} & jane\_goodall.jpg &        whose work on primates has led... & [link] \\
%6 & {\em Alan Guth} & alan\_guth.jpg &              whose idea of inflationary... & [link] \\
%7 & {\em Stephen Hawking} & stephen\_hawking.jpg &  whose work on the nature of... & [link] \\
%8 & {\em Donald Knuth} & donald\_knuth.jpg &        whose work on the theory of... & [link] \\
%9 & {\em Lynn Margulis} & alan\_guth.jpg &          whose ideas about symbiogenesis... & [link] \\
%10 & {\em Gordon Moore} & gordon\_moore.jpg &       who as founder of Intel merged... & [link] \\
%11 & {\em Roger Penrose} & roger\_penrose.jpg &     who has broken new ground not... & [link] \\
%12 & {\em Allan Sandage} & allan\_sandage.jpg &     who continued the work of the... & [link] \\
%13 & {\em Frederick Sanger} & frederick\_sanger.jpg & whose research first revealed... & [link] \\
%14 & {\em Charles Townes} & charles\_townes.jpg &   who invented the laser, which ... & [link] \\
%15 & {\em Craig Venter} & craig\_venter.jpg &       whose completion of the Human... & [link] \\
%16 & {\em James Watson} & james\_watson.jpg &       whose codiscovery with Francis... & [link] \\
%17 & {\em Steven Weinberg} & steven\_weinberg.jpg & whose work on unifying the... & [link] \\
%18 & {\em Andrew Wiles} & andrew\_wiles.jpg &       who in resolving the 300-year ... & [link] \\
%19 & {\em Edward O. Wilson} & edward\_wilson.jpg &  whose work on sociobiology has ... & [link] \\
%20 & {\em Edward Witten} & edward\_witten.jpg &     whose work on the mathematical... & [link] \\
\hline
\end{tabular}

\label{tab:sampleoutput}
\end{table*}



Top-$k$ lists contain rich and valuable information. In particular,
compared with web tables, top-$k$ lists contain a larger amount of
data, and the data is of higher quality. Furthermore, top-$k$ lists
have more meaningful and interesting context, and are more likely
to be useful in search, Q/A, and other interactive systems. In
summary, we target top-$k$ pages for information extraction for the
following reasons.

\begin{enumerate}

\item Top-$k$ data on the web is {\em large and rich}.  We extracted
  1.7 million top-$k$ lists from %  estimate that the total number of top-$k$
  % lists in
  a web corpus that contains 1.6 billion web pages. We estimated that
  the total number of top-$k$ lists in those pages is around 2.23
  million, so our system has a recall of 72.3\% (Section \ref{sec:bigdata}). The scale of this data
  is much larger than any manually or automatically extracted lists in
  the past.  The top-$k$ data is also rich in terms of the content
  acquired for each item in the list. For example, as shown in Table
  \ref{tab:sampleoutput}, each item is described by at least 8 attributes
  (including the ranking, which is an important piece of information),
  while the majority of web tables extracted contain only two columns
  (basically each row is a key/value pair).

\item Top-$k$ data is of {\it high quality}. It is
  generally cleaner than other forms of data on the web.  
  Most data on the web is in free text, which is hard
  to interpret. Web tables are structured, but only a very small
  percentage of them contain meaningful and useful information.  In
  contrast, top-$k$ pages have a
  common style: the page title contains the number and the concept of
  items in the list.  Each item can be considered as an instance of
  the page title, and the number of items should be equal to the
  number mentioned in the title.  As a result, we can correctly
  identify the content of more than 90\% of the top-$k$ lists (Section \ref{sec:bigdata}),
  compared with 41\% of web tables\cite{webtables08}.

\item Top-$k$ data is {\it ranked}. Unlike web tables, which contain a
  set of items, items in a top-$k$ list is usually ranked according to
  a criterion described by the title of the top-$k$ page. Ranking is
  extremely important in information retrieval. Knowing that a term
  ranks 1st or among the top 3 based on a certain criterion is extremely
  useful in search, advertisement, and general purpose Q/A systems.
  \ZZX{Based on our observation(Section \ref{sec:rankedlist}), more than 60\% of top-$k$ lists
 include explicit ranks or indexes
 (e.g., the first column in Table \ref{tab:sampleoutput}),
  while the indexes of the other top-$k$ lists can be easily
  inferred from the layout.}

\item Top-$k$ data has {\it interesting semantics}.  One of the reasons why
  top-$k$ data is valuable is because each list has a context we can
  interpret, and the context is usually an interesting one.  Top-$k$
  lists are often manually composed by domain experts for the general
  public, because people find such information interesting and useful.
  What's more, people are always fascinated about the rankings.  
  Information of this sort is likely to find a large
  audience. Furthermore, many top-$k$ lists contain spatial and
  temporal information (e.g., top 10 vacation destinations in {\it
    North America} of {\it 2012}). 
  According to statistics(Section \ref{sec:evalDate}),
  more than 13\% of the extracted top-$k$ lists contains
  either spatial or temporal information, which provides additional
  valuable contextual information for the lists.

\item Top-$k$ data acquisition is an important step in our bigger
  effort of automatically constructing a universal knowledge base that
  includes a large number of known concepts and their instances.  To
  that end, we have already built one of the largest open-domain
  taxonomy called Probase \cite{WuLWZ12:Probase} which consists of 2.8
  million concepts and many more instances.  The top-$k$ lists we
  extracted from the web can be an important information source for
  Probase.  We are building a Q/A system using the top-$k$ data
  to % , we are able to build an effective fact answer
  % engine \cite{YinTL11:Facto}.  With such an engine, we can
  answer queries such as ``tallest persons in the world'', or ``What
  are best-selling books in 2010'' directly.
  % instead of referring the users to a set of ranked pages like all
  % search engines do today.
\end{enumerate}


In a nutshell, the top-$k$ list extraction
performs three tasks: % We deployed our prototype system on a
% distributed computing platform. % and performed extraction on
% up to 1/10 of a high frequency web snapshot crawled by Bing. Our
% preliminary results showed that the system achieved 90.4\% precision
% and 57.7\% recall. And that equivalent to the correct extraction of
% 129,169 lists from a total of 160 million randomly selected web pages.

 % The input of the system
% is any HTML web page and the output would be the extracted top-$k$
% list of the page, if any.
% \footnote{The actual output
  % is stored in XML format and includes additional information.}.
\begin{enumerate}
\item \textit{Recognize top-$k$ pages}: We identify top-$k$ pages from
  a billion-page web corpus by parsing and analyzing their
  titles. Furthermore, we convert each top-$k$ title into a 5-tuple:
  ($k$, $concept$, $ranking criterion$, $time$, $location$)
  where time and location are optional.

\item \textit{Extract top-$k$ lists}: From each top-$k$ page, we
  extract a list of $k$ items. Note that the page is usually in
  natural language text, and is not formatted using tags such as {\tt
    <ul>}, {\tt <li>}, and {\tt <table>}. % not in may be in If the
  % extracted list items contain additional information, our system can
  % also extract them as attribute columns.
  We will show that knowing $k$ and using a general purpose
  knowledgebase are important to the successful
  extraction of the $k$ items from the text.

\item \textit{Understand list content}: % Since our final goal is to
  % understand the content of ``top-k'' lists we get, we need to process
  % the extraction result.
  Each item in the top-$k$ list might be come with a rich set of
  attributes. Our goal is to extract this information as well as the
  meta-information, i.e., its schema. For example, from a list of
  top-$k$ books, we may first detect and extract information such as
  ``J. K. Rowling'', ``Stephen King'', etc. We then find out that the
  information actually denotes the authors of the books.
\end{enumerate}

The rest of the paper is organized as
follows.  Section~\ref{sec:prelim} introduces
some background information about the the knowledge base we use.
Section~\ref{sec:algo} discusses in detail the
framework of our system.  Sections \ref{sec:imp} discuss a few implementation
details while \ref{sec:eval} presents the evaluation of our system.
Section~\ref{sec:related} describes some state-of-the-art
techniques of information extraction on the
web. Section \ref{sec:conclusion} concludes the paper.

% Then we will discuss in detail the framework of our system (Section
% \ref{sec:algo}) and give the evaluation results (Section
% \ref{sec:eval}), and present a plan for demonstration (Section
% \ref{sec:demo}).  And finally, we will summarize our contributions
% and conclude the paper in Section

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
