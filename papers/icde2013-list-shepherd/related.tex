\section{Related Work}
\label{sec:related}

In this section, we introduce
the state of the art in the field of automatic extraction
of web lists, tables and other structured data.
Although so far none of them have focused on
top-$k$ lists
and their work cannot be directly used in our
extraction system
, we still get a lot of inspirations from their ideas
and approaches.

\cut{%%%%%%%%%%%%%%% BEGIN CUT %%%%%%%%%%
\subsection{Structured Data Extraction from Web Pages}

There has been a large amount of recent work on information extraction
from web page, Although so far none of them have focused on
top-$k$ lists and their work cannot be directly used in our
extraction system, we still get a lot of inspirations from their ideas
and approaches.
}%%%%%%%%%%%%%%% END CUT %%%%%%%%%%

Miao et al. \cite{MiaoTHSM09:TagPathClustering} introduced a
method for record extraction that captures a list of objects based on
the analysis of the tag paths in the DOM tree.  Usually,
list objects have some similar visual styles such as font, size and
background color, which will present a set of distinct tag paths that
appear repeatedly.  In order to estimate how likely several certain
tag paths represent the same list of objects, they introduce the
visual signal which is a vector describing tag path occurrence
patterns. Based on a similarity measure that captures how closely the
visual signals appear and interleave, they perform clustering of tag
paths and rebuild the structure of data in form of sets of tag paths.
They also present some experimental result indicating that this method
achieves higher accuracy than previous methods.


%This paper introduces a new method for record
%extraction that captures a list of objects in a more robust
%way based on a holistic analysis of a Web page. The method
%focuses on how a distinct tag path appears repeatedly in the
%DOM tree of the Web document. Instead of comparing a
%pair of individual segments, it compares a pair of tag path
%occurrence patterns (called visual signals) to estimate how
%likely these two tag paths represent the same list of objects.
%The paper introduces a similarity measure that captures how
%closely the visual signals appear and interleave. Clustering
%of tag paths is then performed based on this similarity mea-
%sure, and sets of tag paths that form the structure of data
%records are extracted. Experiments show that this method
%achieves higher accuracy than previous methods.


Yang et al. \cite{YangZ01:VisualCues} presented a visual cue based
approach to the extraction of the semantic structure of HTML
documents.  Because web pages are normally composed for viewing in
visual web browsers and lack information on semantic structures, this
approach brings semantic information to the traditional DOM tree.
This method can be applied to adaptive content delivery system and
search engine to extract useful information from the whole web page.

Chang et al. \cite{ChangL01:IEPAD} introduce IEPAD, a system that
automatically discovers extraction rules from web pages. The
motivation of the method is from the observation that useful
information in a web page is often placed in a structure having a
particular alignment and order. After encoding token string of web
pages, IEPAD system uses PAT tree for pattern discovery. The
discovered maximal repeats are further filtered by two
measures: regularity and compactness. IEPAD system is quick and
effective.

Lerman et al.\cite{Lerman01:AutomaticData} describe a technique for
extracting data from lists and tables. The approach proposed by this
article develops a suite of unsupervised learning algorithms that
induce the structure of lists by exploiting the regularities both in
the format of the pages and the data contained in them. There are
three steps in this method: (1) Extract all data from lists; (2) Identify
columns; (3) Identify rows. One limitation of this approach is that it
requires several pages to be analyzed before data can be extracted
from a single list.

MDR \cite{LiuGZ03:MDR} is a system proposed to automatically
mine all the data records in a given web page. The method is based on
the observation that a group of similar data records being placed in a
specific region is reflected in the tag tree by the fact that they are
under one parent node. There are three steps in this method. The first
one is building a DOM tree for the given web page. The second step is
mining data regions in the page using DOM tree and string
comparison. The final step is identifying data records from each data
region. The method is quite intuitive but it has some new features
such as discovering non-contiguous data records and high efficiency.

Ventex\cite{GatterbauerBHKP2007:Towards} is an unconventional and
promising approach towards structured information extraction from the
web tables.  Unlike most other web analysis work that utilizes DOM
trees to represent web pages, the approach uses CSS2 visual box
model\cite{CCS2Box}, which describes a web page by a set of
topological and typographical 2-D visual boxes or visualized element
nodes (VENs).  Therefore, it avoids the complexity in code
interpretation (HTML tag structure, CSS, JavaScript code, etc.)  and
works on the higher level of visual features and layouts.  In Ventex,
several rules and heuristics are applied to extract web tables.
Although the idea of Ventex is novel and promising, the experimental
results are preliminary and outperformed by HyLiEn
\cite{FumarolaWBMH11:List} which we will mention later.


%In addition, Ventex have presented a model for representing web table structures
%along with algorithms to derive instances of the model given some arbitrary web pages.

Google Sets\cite{googlesets} is one of the first applications in the
Google Labs \cite{googlelabs}.  Basically it will automatically create
sets of related items from a few examples.  For example, if you type
in ``Honda'' and ``Toyota'', it will returns other Automobile brand
such as ``BMW'' and ``Ford''.  Although, google never reveals the
algorithm behind and closed Google Sets in 2011.  Some
article\cite{howGoogleSetsWorks} in web gives a reasonable explanation
of how the program works.  In brief, Google tries to identify lists on
the web as it crawls pages.  It may look for these lists by
considering several things, including HTML tags (e.g., {\tt <UL> },
{\tt<OL> }, {\tt<DL> }, {\tt <H1>-<H6> }tags), items placed in a
table, items separated by commas, semicolons or tabs.  Items typed
into the Google Sets interface by users are matched up against these
lists, and probabilities are calculated to determine which items might
be a good match for the items submitted by someone using Google Sets.

Cafarella et al.\cite{webtables08} introduce WebTables, a system for
searching and analyzing structured data at search-engine scales.  To
generate the corpus, they first extract raw HTML tables from the the
English documents in Google's main index, which are approximately 14.1
billion in total; then rebuild tables that are reasonable relations
using several recovery techniques, including filtering out tables that
are used for page layout or other non-relational reasons, and
detecting labels for attribute columns.  Finally, they got about 154
million high-quality web tables, each of which can be consider as a
relational database.  WebTables provides search-engine-style access to
this collection of structured data, and presents several applications
to validate the power of the corpus.

HyLiEn \cite{FumarolaWBMH11:List} is an automatic hybrid approach for
extracting web lists from web pages.  We call it hybrid because it is
primarily based on the visual alignment of list items, which is the
same as Ventex\cite{GatterbauerBHKP2007:Towards}, but it also utilizes
non-visual information such as the DOM structure and the size of
visually aligned items.  Basically, there are three steps for HyLiEn
to extract lists from a certain web page.  First, HyLiEn segments a
web page into a set of boxes using the CSS2 visual box
model\cite{CCS2Box}.  Second, using some heuristic visual cues, HyLiEn
generates candidate lists from boxes that are are similar to each
other and aligned visually.  At last, it will filter candidate lists
based on structural similarity in the DOM tree.  Some experiments have
been done to evaluate the performance of HyLiEn and compare it to
Ventex, whose results show that HyLiEn achieves significant
improvement and convincingly outperforms Ventex.

The concept of ``top-$k$'' was first introduced 
by us in a demo paper\cite{ZZX2012KDD}. 
In this demo, we proposed the top-$k$ list extraction problem 
and designed a prototype system. 
The prototype sketches the basic system framework 
and uses a rule-based ranking algorithm. 
But the performance is limited 
due to some bugs in implementation and incomplete functions.
We presented this prototype as a web GUI 
in our project website\cite{list-extractor}.





\cut{%%%%%%%%%%%%%%% BEGIN CUT %%%%%%%%%%
\subsection{Wrapper Induction}

Wrapper induction
\cite{AshishK97:WrapperGeneration}\cite{BaumgartnerFG01:Lixto}\cite{MeccaCM01:RoadRunner}
is instructive to our work. We present these three typical wrapper
induction systems here. They talk about the information extraction
from three different perspectives.

Ashish et al. \cite{AshishK97:WrapperGeneration} introduce a method
for semi-automatically generating wrappers which can be used to
provide database-like querying for semi-structured WWW sources. The
key idea used by this article is to exploit the formatting information
in pages and heuristics from designer.

Baumgartner et al.\cite{BaumgartnerFG01:Lixto} present a new technique
for supervised wrapper generation and automated web information
extraction.  The lixto system based on these techniques has three
modules. The Interactive Pattern Builder module requires user to
provide an extraction pattern from interactive UI and then generates
Elog program which is a specification for the actual extraction
system.  The Extractor module is the Elog program interpreter. The
Extractor module generates a pattern instance base, a data structure
encoding the extracted instances as hierarchically ordered trees and
string, as its output. Finally the XML generator performs translation
from the extracted pattern instance base to XML. The methods and
system mentioned by this paper have distinctive features such as it is
easy to learn and use,it only depends on the sample web page and so
on.

RoadRunner \cite{MeccaCM01:RoadRunner} developed a novel technique to
compare HTML pages and automatically generate a wrapper based on their
similarities and differences. RoadRunner does not rely on priori
knowledge and user interaction and sample web pages.
}%%%%%%%%%%%%%%% END CUT %%%%%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
