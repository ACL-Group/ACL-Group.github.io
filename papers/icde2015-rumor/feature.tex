\subsection{Feature selection}
\label{sec:imf}
To investigate the effectiveness of our new features,
we train several SVM classifiers using different subsets of the features.
%Basically, we consider some subsets of features without one or
%more new features.
We also train a classifier without graph kernel function to show its
usefulness. The small data set (1000 messages)
is divided into training set and test set with a ratio of 2:1.
For each subset of features, we train an SVM classifier
on the training set and test the classifier on the test set.
The results of experiment are shown in \tabref{table:result-features}.
Here (-)X means the whole set of features except feature X. F stands for
``false rumors'' while O stands for ``other messages''.

\begin{table*}[ht]
\centering
\small
\caption{Impact of features}\label{table:result-features}
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multicolumn{1}{c}{\textbf{}}              & Accuracy & F precision & F recall & F F$_1$ & O precision & O recall & O F$_1$ \\ \midrule
(-)TOPIC TYPE                              & 0.865    & 0.836       & 0.911    & 0.872   & 0.900       & 0.818    & 0.857   \\ \hline
(-)SEARCH ENGINE                           & 0.880    & 0.872       & 0.912    & 0.892   & 0.906       & 0.863    & 0.884   \\ \hline
(-)USER TYPE                               & 0.894    & 0.877       & 0.919    & 0.897   & 0.912       & 0.868    & 0.890   \\ \hline
(-)AVG SENTIMENT & & & & & & \\
(-)AVG DOUBT                               & 0.872    & 0.846       & 0.912    & 0.878   & 0.903       & 0.830    & 0.865   \\
(-)AVG SURPRISE & & & & & & \\ \hline
(-)AVG EMOTICON                            & 0.887    & 0.874       & 0.908    & 0.891   & 0.902       & 0.866    & 0.884   \\ \hline
(-)REPOST TIME SCORE                       & 0.892    & 0.874       & 0.919    & 0.896   & 0.912       & 0.865    & 0.888   \\ \hline
(-)Graph Kernel & 0.848    & 0.851       & 0.846    & 0.849   & 0.844       & 0.849    & 0.846   \\ \hline
(-)All New Features & 0.796    & 0.806       & 0.782    & 0.794   & 0.786       & 0.810    & 0.798   \\ \hline
(-)All New Features \& Graph Kernel & 0.761    & 0.748       & 0.792    & 0.769   & 0.777       & 0.730    & 0.753   \\ \hline
With All Features \& Graph Kernel& {\bf 0.904}	&{\bf 0.886} &{\bf 0.929} &{\bf 0.907} &{\bf 0.924} &{\bf 0.877} &{\bf 0.900} \\ \bottomrule \end{tabular}
\end{table*}

The results show that the inclusion of the graph kernel is indeed
very effective, improving the accuracy by 0.056, which is the largest
single-feature improvement among all features.
This clearly indicates that the explicit representation
of propagation tree patterns better models the false rumors than others.
Results also suggest that TOPIC TYPE is the most effective among all
ordinary features. This is because false rumors tend to concentrate on a few
sensitive topics, such as missing persons or health issues.
The features about sentiments also have significant impact on the
result, which means people's opinions, especially when they are doubtful or
surprised, point to possible false rumors. Finally, when all new features
and the graph kernel are removed, the accuracy of the classifier
drops considerably, which again shows that the graph kernel and
our new features, when combined together, provide substantial boost 
for the classifier.
