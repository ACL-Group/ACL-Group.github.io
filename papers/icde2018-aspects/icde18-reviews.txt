View Reviews
Paper ID38
Paper TitleExtRA: A Framework for Extracting Prominent Review Aspects from Online Customer Feedback
Track NameResearch
REVIEWER #1

REVIEW QUESTIONS

1. Overall Evaluation
Weak Reject
2. Originality
Low
3. Importance
Medium
4. Summary of the contribution (in a few sentences)
Summarizing customer reviews often are based on manually provided aspects/features. This paper proposes a framework that tries to extract such aspects automatically and in an unsupervised way. The proposed framework is based on a sequence of steps that are not new. In particular, there are many recent studies in the NLP domain that are not included. The experimental results are also not convincing.
5. List 3 or more strong points, labelled S1, S2, S3, etc.
S1: the problem that they are trying to solve is important. An attempt to try to extract aspects automatically is still an open problem.

S2: The proposed framework is a reasonable combination of various steps in the process. The authors clearly understand the main steps involved.

S3: For the most part the paper is well written and easy to understand.
6. List 3 ore more weak points, labelled W1, W2, W3, etc.
W1: While I pointed out in S2 that the proposed framework includes all the main steps, one concern I have is that the various steps are not novel. There are many recent studies in the broader NLP literature that deal with many of these issues. See item D1 below for some examples. 

W2: It is also well known in the NLP domain that extracting keywords as topic labels are generally inadequate. Topic phrases are now believed to be more advanced. This paper still operates under the paradigm of keywords; see Table V for instance. 

W3: The experimental results are not convincing. The datasets summarized in Table IV are still in product domains that can be constructed manually. What the paper needs would be datasets where aspects are hard to be constructed manually, which is the original motivation of the paper.
7. Detailed evaluation, labelled D1, D2, D3 etc.
D1: the various steps of the proposed framework are hardly novel. They are reuse or re-sequencing of various tools in the literature. There are various studies in the broader NLP literature that extracts aspects automatically. E.g., the 2014 study by 
Carenini et al. on summarization of product reviews using discourse structure. 

D2: the topic cluster ranking in stage 4 seems very arbitrary. There needs to be stronger justification on why this is better than ones in the literature, of which there are many.
REVIEWER #2

REVIEW QUESTIONS

1. Overall Evaluation
Accept
2. Originality
Medium
3. Importance
High
4. Summary of the contribution (in a few sentences)
The paper proposes a comprehensive framework for extracting feedback from review. The feedback is provided with respect to different aspects. The paper has experimental results analyzing various aspects of the approach and to experimentally compare the approach against others.
5. List 3 or more strong points, labelled S1, S2, S3, etc.
S1. The paper is well written
S2. The paper has some novel contributions
S3. The paper also shows how the framework analysis results would look like on a user mobile phone.
6. List 3 ore more weak points, labelled W1, W2, W3, etc.
W1. The approach requires tuning a lot of parameters. The authors indicate in the paper how they have selected the values for the parameters. However they do not discuss whether their results would still be valid for different values of these parameters. Also they do not discuss how such parameters would be tuned when the framework is used in a real setting. Would the parameters be set by the end-user?
7. Detailed evaluation, labelled D1, D2, D3 etc.
The paper is fine. 

D1. I appreciate particularly the use of ontology to improve the clustering. One question, though, is how to obtain/generate the ontology in real settings.
8. Required changes for a revision, if applicable. Labelled R1, R2, R3, etc. (Please mark the requests clearly.)
R1. The paper should address R1 (if space is needed, the application demo part can be reduced)

R2. The paper should discuss approaches for D1.
REVIEWER #3

REVIEW QUESTIONS

1. Overall Evaluation
Weak Accept
2. Originality
High
3. Importance
High
4. Summary of the contribution (in a few sentences)
There is a body of work on summarizing user reviews of products and
services along the key features or aspects of the products. Much of
the work takes the aspects as granted and aims at extracting the
opinions related to them.

In the current paper, the authors present a framework for automatically 
extracting the key aspects from a corpus of user reviews. The framework
comprises a complex workflow of word embedding of sentence or word
collections, k-means clustering of the vectors, topic modeling and
ranking. The framework comes in two flavours, a first where aspects are
single words, and a second one, where aspects can also be phrases.

For their experiments, the authors created six new data sets,
consisting of product, hotel, or restaurant reviews, respectively, and
as ground truth for each corpus, a sets of aspect terms created by
humans. An instantiation of the framework, is evaluated against several 
topic modeling approaches and alternative approaches for aspect extraction. 
The new approach emerges as significantly superior.

As an application the authors present a technique for aspect-based
sentiment analysis of user reviews relying on the extracted aspects.
5. List 3 or more strong points, labelled S1, S2, S3, etc.
S1: The paper addresses the problem of summarizing user reviews, which
is becoming more relevant as new products and services are being
reviewed and the focus of attention of reviewer communities
changes over time to new aspects.

S2: The paper presents and effective technique for the problem based
on complex workflow that combines state-of-the-art techniques. 

S3: By way of using word embeddings, the approach takes into account
the semantic distance/closeness of aspect terms.

S4: Overall, the paper is well-written and easy to read.
6. List 3 ore more weak points, labelled W1, W2, W3, etc.
W1: The work consists of a combination of established techniques,
rather than containing an entirely new idea.

W2: The paper leaves a number of crucial details open. For one, the
mapping of words to the vector space is not explained. The second
version of the approach, which includes the extension to phrases,
is only sketched.

W3: The new datasets and the open-source implementation, which are
listed as contributions of the paper, are not accessible.

W4: The measure of of accuracy used in the experiments is unintuitive
(see below).
7. Detailed evaluation, labelled D1, D2, D3 etc.
D1: The paper presents a complext and thoughtful pipeline of
operations:
- word embedding into vector spaces for sentences
- clustering of sentences
- topic modeling on sentence clusters
- word embedding of topics, using top terms of topics 
- topic clustering
- selection of top-k most distinctive topics
- selection of most distinctive terms of those topics,
where the techniques for each step have been chosen judiciously.
In addition, the paper defines new ranking functions to express
weight and distinctiveness of terms.


D2: The description lacks some important details. There is no
explanation as to the mapping of words to vectors. Several
embedding techniques for sentences are mentioned, but they paper
does not say, which one has been chosen.

In addition, the description of the AspVec variant, which allows
for the extraction of aspect phrases is only sketched. For
instance, Fig. 3 shows that for this variant, word vectors and and
quality phrases are mined, which are processed differently from
the sentences in the first variant. 

There is still some space left on the 12 pages to provide such
details. If that is insufficient, the part on the downstream
applications and the corresponding images could be condensed.


D3: The measure of accuracy is unintuitive. An accuracy of 100% is
only reachable if the human annotators agree completely on their
terms. It would be better to have a measure that makes it clear
how close a method is to what could be achieved in principle.

For instance, one could count the total number of occurrences of
the five most frequent terms supplied by the human experts and
take that as a benchmark. 

Since that would still have the drawback that missing one of the
terms could lead to a significant reduction, the authors may want
to think further about a more appropriate measure.


D4: The explanation of the word embeddings and topic modeling should
also explain why the terms in topics and clusters are always
nouns. Is the approach only applied to nouns? Also, why is it the
case that words expression opinions do not appear there? Is the
reason just that they are less frequent than objective nouns?


D5: When opinions about aspects are extracted in the downstream
analysis, the authors used apparently not only the aspect terms,
but also other terms of topic cluster. How were those terms
extracted so that interference between clusters was minimized?


D6: There are some typos that should be corrected.
8. Required changes for a revision, if applicable. Labelled R1, R2, R3, etc. (Please mark the requests clearly.)
R1: Extend the description of the AspVec variant so that it becomes clear how phrases are generated. Extend experiments to the phrase generation variant. Show examples of phrases.

R2: Provide more details about the word embedding used in the experiments. Which 
framework did you use and why (e.g., Google's word2vec vectors trained on the Google 
News corpus)? 

R3: Find a measure for accuracy that allows a method to achieve 100%
accuracy, even if the annotators did not agree on the terms and 
express the performance using that measure.

R4 : Explain how the clusters for opinion extraction were created.

