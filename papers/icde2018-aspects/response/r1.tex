\section{Response to Reviewer 1}
\begin{itemize}
	\item [Q1:] 
	%[W1] 
While I pointed out in S2 that the proposed framework includes all the main steps, one concern I have is that the various steps are not novel.

\item [A1:] Some of the NLP techniques included in our framework, 
such as sentence vector representation, sentence clustering and 
topic modeling, are not new. However, our framework is not a simple 
``re-sequence'' of these techniques. We propose innovative methods to 
combine those techniques to better represente the aspects. 
For example, in Stage 3, instead of solely utilizing the word distributions 
after topic modeling, we propose to cluster the topics (word distributions) 
by representing them as ``topic vectors.'' Those topic vectors are 
constructed by incorporating word embeddings as external information.  
Another example is our AspVec. We eventually represent each potential aspect 
as an aspect vector, which can 1) better compute the aspect similarity,  
2) keep the representations in original word vector space, \KZ{What do you mean?} 
3) and thus enables extracting aspect phrases at the same time.
	
\item [Q2:] 
%[W2(1), D1] 
It is also well known in the NLP domain that extracting keywords as topic labels are generally inadequate. There are various studies in the broader NLP literature that extracts aspects automatically. E.g., the 2014 study by Carenini et al. on summarization of product reviews using discourse structure. 
\item [A2:] Carenini et al. (2014) leverages the discourse structure and 
discourse relations in the product reviews to select important aspect words 
for the further abstractive summarization. We discuss this related work in 
the revised version.
	%The paper "Abstractive Summarization of Product Reviews Using Discourse Structure"
	
\item [Q3:] 
%[W2(2)] 
Topic phrases are now believed to be more advanced. This paper still operates under the paradigm of keywords; see Table V for instance.
\item [A3:] Our framework can extract both aspect terms and phrases. 
The description of the phrase extraction method is in "AspVec-based Extraction", stage G, of sec. II. 
We asked the human annotators to provide words as groundtruth in Table V for
two reasons: 1) to enable easy comparison between our model and other 
strong baselines (e.g. MG-LDA, shown in Table VII ) that can only extract 
aspect words; 
	2) to ensure higher inter-annotator agreement among the five annotators. 
\item [Q4:] 
%[W3] 
The experimental results are also not convincing. The datasets summarized in Table IV are still in product domains that can be constructed manually. What the paper needs would be datasets where aspects are hard to be constructed manually, which is the original motivation of the paper.
	\item [A4:]
Our experiments were performed on both product and service 
(e.g. restaurant) reviews. Although such aspects can be extracted by 
human efforts, it would be very expensive to do this for thousands of 
product or service types. 
Our experiment is actually a proof of concept with 
an evaluation metric purposed designed to compare human and algorithmic
performance. Our experiments also show that our framework and algorithm 
can be applied on larger text corpora and on other 
domains where aspect terms need to be mined.
	
\item [Q5:] 
%[D2:] 
The topic cluster ranking in stage 4 seems very arbitrary. There 
needs to be stronger justification on why this is better than ones 
in the literature, of which there are many.
\item [A5:] 
We rank the topic clusters by their distinctiveness scores in stage 4. 
This score indicates the amount by which a cluster overlaps 
with others. This can help remove redundant top clusters as shown in Table II. 
\end{itemize}
