\section{Response to Reviewer 3}
\begin{itemize}
\item [Q1:] 
%[W2(1), R2] 
The mapping of words to the vector space is not explained. Several	embedding techniques for sentences are mentioned, but they paper does not say, which one has been chosen. 
	\item [A1:] In our framework, we used word2vec (skip-gram) model to embed the discrete words from the vocabulary of the product reviews into a low-dimensional vector space, commonly called word embedding space. The word vectors are trained using the whole corpus of review sentences. Also note that the word vectors, phrase vectors as well as aspect vectors showned in Fig.3 are simple linear combination of word vectors stated above (i.e., weighted average/sum, etc.), 
thus they are all in the same embedding vector space, where they can be 
safely compared against each other.
	
For sentence embedding techniques, we mainly compared two different methods, 
Paragraph Vector and LSTM-based AutoEncoder (as stated in II.C Stage 1). 
Actually the experimental results of both techniques are shown in later experiments (TABLE VII and VIII), dubbed ExtRA-PV and ExtRA-LSTM. 
Since the evaluation of this task is more open ended, we regard both 
variants as possible approaches.
	
\item [Q2:] 
%[W2(2), R2] 
The second version of the approach, which includes the extension to phrases, is only sketched. The description of the AspVec variant, which allows for the extraction of aspect phrases is only sketched. For instance, Fig. 3 shows that for this variant, word vectors and and quality phrases are mined, which are processed differently from the sentences in the first variant. 

\item [A2:] As for the phrase-capable version of AspVec, we first mine quality phrases using SegPhrase/AutoPhrase (Shang et. al.) from review corpus as candidates. Then, by combining the words in a given phrase by averaging their word embeddings, we can easily obtain the phrase embedding while ensuring that they are in the same embedding space. Now the vocabulary has been expanded with newly added candidate quality phrases, along with original words. The rest of the process are the same since the phrases are just tokens with embedding vectors from now on (See Fig 3). We have revised this part of the paper to make it clearer and more detailed.
	
\item [Q3:] 
The new datasets and the open-source implementation, which are listed as contributions of the paper, are not accessible.
\item [A3:] At the time when we submitted the paper initially,
we were working on organizing the datasets and the code base (adding comments, readme, dataset introduction, etc.) We believe that reproducibility is highly important and will make the datasets and code publicly available by the time
the revised paper is submitted.

\item [Q4:] 
%[W4, D3, R3] 
The measure of accuracy used in the experiments is unintuitive. An accuracy of 100\% is only reachable if the human annotators agree completely on their terms. It would be better to have a measure that makes it clear	how close a method is to what could be achieved in principle.
For instance, one could count the total number of occurrences of the five most frequent terms supplied by the human experts and take that as a benchmark. 	Since that would still have the drawback that missing one of the	terms could lead to a significant reduction, the authors may want to think further about a more appropriate measure.
\item [A4:] In the revision, we adopt a new measure for accuracy which 
achieves the reviewer's requirement and allows a method to achieve 100\% 
accuracy, even if the human annotators did not completely agree on their terms. We follow the reviewerâ€™s advice to select the five most frequent terms 
provided by the human annotators and take those as the ground truth. 
Because our ground truth consists of only 5 terms, counting the 
number of matches makes the accuracy score very discrete and coarse-grained.
To solve this problem, we use the semantic similarity between terms as a
soft accuracy measure. First, we align each generated aspect with one golden 
aspect. Then we calculate the semantic similarity of each aspect and its 
corresponding golden aspect term as the soft matching score (from 0 to 1) 
by using the standard pretrained word2vecs (i.e., Glove).
We use the summation of such soft matching scores to represent the 
accuracy of our model.
Such measure for accuracy can achieve 100\% even when the human annotators 
do not completely agree with each other.
	
	
\item [Q5:] There is no explanation as to the mapping of words to vectors. Several embedding techniques for sentences are mentioned, but they paper does not say, which one has been chosen. In addition, the description of the AspVec variant, which allows for the extraction of aspect phrases is only sketched. For instance, Fig. 3 shows that for this variant, word vectors and and quality phrases are mined, which are processed differently from the sentences in the first variant. 
\item [A5:] Please refer to A1 and A2 above.
	
\item [Q6:] 
%[D4]
The explanation of the word embeddings and topic modeling should also explain why the terms in topics and clusters are always nouns. Is the approach only applied to nouns? Also, why is it the case that words expression opinions do not appear there? Is the	reason just that they are less frequent than objective nouns?
\item [A6:] 
Our framework mainly aims to extract the prominent aspects from different kinds of custom reviews. 
Such expected aspects are usually nouns or noun phrases (B. Liu et. al.). Instead of explicitly extracting the opinion words from topics for  sentiment analysis in the downstream application, we use a LSTM-based neural network model trained on the Stanford Sentiment Treebank to compute the sentiment score for the target aspects. 
We give an extended discussion on this topic in the revised paper. 
	
\item [Q7: ] 
%[D5:]
When opinions about aspects are extracted in the downstream analysis, the authors used apparently not only the aspect terms, but also other terms of topic cluster. How were those terms extracted so that interference between clusters was minimized?
\item [A7:] 
In the downstream application, the sentiment or opinion terms (e.g., in
Fig. 9) targeting a particular aspect can be automatically extracted from
the same sentence containing the aspect term in question. We achieve this 
in this paper by a LSTM-based neural model trained on Stanford Sentiment 
Treebanks. This is, however, not the focus of this paper. \KZ{can you
sleep on the above a bit more?}
\end{itemize}


\KZ{Is the following really necessary? I thought we have said in the abstract
that we will bold the changes in the revision? No point repeating what they
asked for?}
\subsubsection{Reviewer 2}

R1. The paper should address R1 (if space is needed, the application demo part can be reduced)

R2. The paper should discuss approaches for D1.

\subsubsection{Reviewer 3}

R1: Extend the description of the AspVec variant so that it becomes clear how phrases are generated. Extend experiments to the phrase generation variant. Show examples of phrases.

R2: Provide more details about the word embedding used in the experiments. Which framework did you use and why (e.g., Google's word2vec vectors trained on the Google 
News corpus)? 

R3: Find a measure for accuracy that allows a method to achieve 100\% accuracy, even if the annotators did not agree on the terms and express the performance using that measure.

R4 : Explain how the clusters for opinion extraction were created.


