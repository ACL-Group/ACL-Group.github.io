\section{Introduction}

%Following End-to-End paper style
%Target: 1.5 page

%========

%Part 1: What is KBQA
%KB intro
%KBQA Target

%1. KB introduction
Structured knowledge bases (KB), such as Freebase~\cite{bollacker2008freebase}, YAGO and DBpedia,
are the graph: with manually predefined edges, connecting massive unique named entities in the real world.
%2. KBQA task
KBQA: more and more attention
Goal of KBQA: taking NL language as input, return answers or query structures
and then leveraging query language like SPARQL.
%3. Interesting and open research task
The task is an open research task, attracting interests from both NLP and IR communities.
The benchmark datasets such as Free917, WebQuestions, ComplexQuestions and SimpleQuestions
are widely used in most of the recent KBQA work.
%========

%Part 2: Examples, talk about our focus: NN-based approach to answer the question.
%what's the second longest river in USA?
%1. what's simple
There are two main steps in the KBQA task: focus recognition and relation detection.
The first one recognizes the focus entities of the question and link it to the KB,
and the latter step aims at identifying the semantic relationship between the focus entities and the target answer.
The relationship is usually represented by the predicate in the KB.
We call a question ``simple'', if it can be answered based on a single KB predicate connecting
one focus entity and the answer.
%2. what's complex
However, the KBQA is not a trivial task, because a large number of questions
hold richer semantic information.
Except for the single focus entity in the question,
we can find extra focus entities, target entity types, numbers, datetimes,
or even ordinal and aggregative indicators.
Therefore, we call them ``complex'' questions.
%3. percentage of complex questions
Around 15\% questions in WebQuestions dataset belong to the complex category,
and the ComplexQuestions dataset is fully built upon this kind of questions.
%4. real example
For example in Figure \figref{fig:complex-example},
the question ``What is the second longest river in China?'' is a typical complex question,
including one focus mention ``China'', the target type mention ``river''
as well as the ordinal indicator ``second longest''.
%5. detail semantics
In order to perfectly answer this question, three different semantic information should be inferred:
1) the answer is contained by China;
2) the answer is a river;
3) the answer ranks 2nd by the river length attribute.
%corresponding to different semantic components in the complex query graph.
The corresponding query graph is the structural combination of these semantic components.

%========

%Part 3: KBQA Branches
%Semantic Parsing: Focus on Explicit Query Structure
%Information Retrieval

%1. Two parts
   The state-of-the-art methods for the KBQA task can be categorized into two branches.
%2. Semantic parsing (show papers)
   The first branch is based on semantic parsing (SP), which focuses on constructing
   a semantic parsing tree or equivalent query structure that represents the semantic meaning of the question.
   The semantic parsing tree can be translated into SPARQL queries, hence target answers can be generated,
   and the structure is also human-readable, (better explanation)
   Due to the lack of annotated semantic parsing trees of questions, 
   SP-based approaches are usually distant supervised.
%3. Information Retrieval (show papers)
   The second branch is based on information retrieval (IR).
   The IR-based methods directly
   search the target answers from the KB,
   and capture the semantic associations between the NL question and
   the information encoded in the neighbourhood of the answer entity, including relations, types, ....

%========

%Part 4: NN into QA model
%IR: model answer, contexts
%SP: model structure: 

%1. NN put into use
%Recently, with the progress of deep learning,
%neural network-based (NN-based) methods have been introduced to the KB-QA task.
Recently, neural network models, especially CNN and RNN, gain much success in NLP fields.
The NN-based approaches are also adapted to the KBQA task.
%2. basic point of view: encoding
The key idea under the NN approach: representing both of the questions and the answers (entity or SPT)
as hidden vectors in some semantic space.
The best answer is selected by applying a scoring function between q- and a- vector,
which measures the similarity or relatedness.
The NN-based methods avoid feature engineering in both SP- and IR- framework,
and performance better than traditional methods.

%========

%Part 5: Detail point: schema encoding
%8-10 sentences
%1. introduce the focus: schema representation
In this work, we pay attention to the combination SP and NN approach.
%and focus on finding the best query graph of a given question.
%We propose a neural network approach to compute the similarity score between a question and a candidate query structure.
%2. semantic structure encoding: target
The cruical step is to compute the the similarity score between q representation and SP represeantation.
%3. previous:
Previous work have taken efforts to the problem.
The first method (Luc, Yu) focus on the predicate path connecting the focus and the entity, 
where the recurrent modes like Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM)
show the effectiveness of modeling the representation of SP.
This method is straightforward, but it's mainly used in simple question scenarios.
The additional constraints can be added to the query graph, but the semantic information
is never used in the encoding step.
The second approach (Yih,Bao) takes all semantic components into the computation step.
The authors didn't encode the rep. of the entire structure, as instead,
proposed a CNN-based module to measure the semantic similarity between the question
and various components of the query structure, including its focus entity, type, predicates.
However, the overall model is more likely to be feature-based (with handcrafted rules),
and when computing the similarity between q and compoenents, the subsentence of q is
selected through hard rules, which is inflexible and may cause error propagation problem.
%and the semantic similarities serve as some feature values of the <q, sc> pair.

%========

%Part 6: Ours and Challenge
%1. semantic rep for the entire query graph.
In order to combine the advantages of the previous methods,
we present a neural network method for modeling the semantic vector of the entire query graph.
%2. intuition: skeleton split
The main intuition behind our approah is to regard the complex query graph
as a set of simple skeletons.
%3. what's skeleton
Generally speaking, a skeleton is a predicate path starting from a fixed node
(entity, type, number, datetime or ordinal value) to the target answer.
%4. what to represent skeleton
Each skeleton describes one semantic aspect of the question,
and the overall representaion of a query graph is the
dynamic combination of the skeleton vectors within the graph.
%5. attention 1: each skeleton focus on different words in the question;
Since different skeletons focus on different words in the question,
we adopt a word-level attention module to learn a flexible representation
of each skeleton.
Besides, a skeleton-level attention module helps adjust the weight between different skeletons,
producing the final representation of the whole graph.
It's worth mentioning that our method learns focus recognition and relation detection jointly,
which makes our model more robust. \KQ{need examples for clarification}

%========

Part 7: contribution
\begin{itemize}
\item schema split to skeletons
\item skeleton encoding, including ordinal/type/time constraints
\item cross-attention
\item outperform
\end{itemize}
