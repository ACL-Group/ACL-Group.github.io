\section{Related Work}

Information retrieval(IR) is one popular method for question answering problems, which tries to extract the entities in the problem and obtain the knowledge base subgraph centered on those entities. Each node or edge in the subgraph can be used as a candidate answer, and the problem vector is extracted by observing the problem according to certain rules or templates. The final answer can be selected from candidate answers with the help of problem vector. Recent work concerning IR includes (Yao et al., 2014~\cite{yao2014information};Clark et al., 2016~\cite{clark2016combining};Xu et al., 2016~\cite{xu2016question}).

%semantic parsing
Semantic parsing, which focuses on constructing a semantic parsing tree or equivalent query structure that represents the semantic meaning of the question is a key component in Knowledge Base question answering task. Prior work has mainly focused on general knowledge base senmantic parsing framworks construction. For example, (Yih et al., 2014~\cite{yih2014semantic}; Golub et al., 2016~\cite{golub2016character}; Yu et al., 2017~\cite{yu2017improved}) are based on Freebase.

One drawback of traditional semantic parsing method to sovle KBQA problem is that it needs to generate queries based on large amount of entities and predicates in the KB, a large proportion of which are unseen during the training process. Therefore, some research has focused on solving this issue. One solution is embedding based method, which represents entities and relations in vector space and predict soundness of candidate triplets from these latent vectors, such as  
TransE(Bordes et al., 2013)~\cite{bordes2013translating}, TransH~\cite{wang2014knowledge}, TransR~\cite{lin2015learning} and other enhanced but similar models. The general idea behind these models is training embedding vectors of entities and relations by the formula $h+r\simeq t$. Besides, recent work has also focus on character-level model to enhance the performance, such as (Golub et al., 2016). Golub uses character-level modeling to handle traditional out-of-vocabulary(OOV) problem. Furthermore, there are some research on combination of knowledge base and open vocabulary to improve semantic parsing performance. For example, (Gardner et al., 2017)~\cite{gardner2017open} try to leverage the information contained in both a formal KB and a large corpus to avoid the limit of the schema of the underlying KB. However, most of these approaches can only deal with single-relation questions, having no ability to deal with ordinal questions and multi-relation questions, such as WEBQUESTION. 

Previous multi-relation questions requires a recognition of a core chain (Yih et al., 2015~\cite{yih2015semantic}; Bao et al., 2016~\cite{bao2016constraint}; Yu et al., 2017) and  they regard other constraints as variables added to this main relation. The MulCG (Bao et al., 2016) handles multiple constraints systematically and expands non-entity constraints which develops the staged graph (Yih et al., 2015). However, they all treats constraint a variable added to a node of main relation, while our model doesn't have to detect the main relation and treats constraints and relations parallel. Moreover, our model is word-based, in other words, we don't need KB information to generate our schema. Our model represents every path a word sequence ended with the final answer entity and merges them together to form an overall schema. The word sequence path only takes word features into account thus concise and uniform thus prevent an overfit problem.

In terms of schema generation, the MulCG (Bao et al., 2016) is not flexible enough by mapping explicitly the exact word proof to detect and match constraints, while our model applies the attention mechanism which can collect other textual evidences to help detecting constraints.

For the model and the ranking step, the MulCG (Bao et al., 2016) embedss question and schema together in feature-based then uses learning to rank, which requires feature engineering thus not flexible. And our model embeds respectively question(word-based RNN) and schema then trains with neural networks to get a score.

The MCCNN (Dong et al., 2015~\cite{dong2015question};Xu et al., 2016~\cite{xu2016question}) proposed a dependency tree based method to handle multi-relational questions. They decompose the original question into simple sub-questions using six dependency patterns, and the intersection of answer sets of all the sub-questions will be selected as the final answer. The latent problem is (1) six patterns are not adequate to support all kinds of multi-relational questions, (2) answering all sub-questions separately can be redundant and (3) they can't solve ordinal questions until the final answer refinement step. However, this final step adds an additional resource wikipedia to select one answer from possible candidate answers, which is not only slow but easy to mismatch. Compared to the MCCNN, our model avoids redundant work to answer sub-questions successively by fully representing the multi-relational question in a uniform way. Besides, our schema treats the ordinal constraints and other constraints equally thus don't need extra work like the refinement step. 

The attention mechanism is widely used for question answering in recent years(Yin et al., 2017~\cite{yin2017type}). IARNN (Wang et al., 2016)~\cite{wang2016inner} is an inner attention RNN that the attention was imposed directly to the input therefore can avoid biased attention problem caused by outer attention RNN. Nevertheless, it is unidirectional  and it doesn't consider the mutual influnce of the question and the answer.
 
The ABCNN model proposed by (Yin et al., 2015)~\cite{yin2015abcnn} integrates attention into CNNs in order to solve sentence pair modeling problem. This ABCNN model sets up mutual influence between the sentence pairs into CNNs, therefore the output embedding of each sentence contains the information of its counterpart. This ABCNN model gives an intuition of cross attention which takes mutual influence into account.
 
(Hao et al., 2017)~\cite{hao2017end} proposes another cross attention model focus on KBQA task. This cross-attention mechanism consists of two parts: one answer-towards-question attention part and one question-towards-answer attention part. It focuses on different answer aspects of each question word and calculates the similarity score of the question and the four different candidate answer aspects. The final score will be composed of the weighted sum of these four similarity score. This model well develops the attention mechanism, however, the mutual interaction is still not fully established. The reason lies below:  The four similarity vectors between question and answer aspect are independent to each other thus the mutual influence can't be fully expressed. 
 
Our cross attention mechanism aims to represent both the question and the schema in a dynamic way. We take each question word's embedding and each skeleton embedding as input and we import an attention matrix. For the attention matrix, each item $e_{ij}$ of this matrix denotes the weight of the \emph{i}-th skeleton and the \emph{j}-th question word. Thus the \emph{i}-th row represents the contribution of each question word to the \emph{i}-th skeleton, while the \emph{j}-th row represents the contribution of the \emph{j}-th question word to each skeleton. This cross-attention mechanism fully takes the mutual influence of the question and the schema into account, thus generates a more precise and meaningful representation of the schema. 

In conclusion, our model has a uniform framework, and it is concise and flexible,also puissant to answer ordinal questions.