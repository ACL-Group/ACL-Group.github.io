\section{Introduction}
%The commonsense causalities are causal dependencies between common events or actions.
Commonsense causal reasoning is the task of inferring commonsense causalities, which are causal relations
between common events or actions. 
For example, ``Amanda feels hot'' can be a reasonable cause for the effect sentence ``Amanda turns on the fan''. 
There is a causal relationship between such two events.
In this work, we aim to design models to capture such
causalities and use them to perform commonsense 
causal reasoning.
It is also crucial to many other natural language processing applications, such as text understanding and question answering , etc.

%In this work, we mainly focus on generating the causal relationships between sentences. For example, ``Amanda feels hot'' can be a reasonable cause for the effect sentence ``Amanda turns on the fan''. 

Gordon et al.~\cite{roemmele2011choice} formulates commonsense causal reasoning as a selection problem.
Given a cause (or effect) as the premise, 
it requires the model to select its reasonable effect (or cause) 
from a manually labeled effect (or cause) candidates for the premise.
Previous works~\cite{roemmele2011choice, LuoSZHW16} propose various causal strength metrics to rank those candidates and reason the more plausible alternative for the premise.
However, those ranking models reason about commonsense causality rely on human labeled candidates, which is not feasible for many generation scenarios in NLP, such as question answering and dialog completion.
Thus, we reformulate the commonsense causal reasoning task as a generation problem, which given a cause (or effect) sentence as the premise, requires the model to generate its reasonable effects (or causes) sentence called targets. 
We call the cause-to-effect inference process ``forward reasoning''. For example, given the cause sentence ``Amanda feels hot'', possible effect sentence can be ``Amanda turns on the fan'' or ``Amanda takes off her coat'', etc. Backward reasoning, in the contrary, treat the input sentence as an effect and try to infer the cause. An ideal cause output for the effect ``Amanda feels hot'' can be ``The air-conditioner stops working'' .

Previous methods~\cite{roemmele2011choice,goodwin2012utdhlt,
	jabeen2014using, LuoSZHW16} 
for selection-based causal reasoning can easily adapt to the generation-based causal reasoning.
We first automatically generate a candidates set for the premise, then perform the selection-based methods on those candidates to reason about causalities.
% talk about the limitations or drawbacks
There are two limitations of the selection-based methods: 1) It requires an extra on-line step to build a proper candidates set for each premise, which is very expensive. 2) The model can only select the targets from the candidates, which is not flexible enough for causal reasoning task.
In this paper, we propose a novel model based on the convolutional sequence to sequence framework~\cite{gehring2017convs2s}, equipping with the causal attention fusion mechanism which powers the generation-based causal reasoning task.
In the encoder-decoder sequence to sequence framework, the attention mechanism suppose to learn the semantic dependency between the current decoding hidden state and each of the encoding hidden states. 
Such semantic dependency are interpreted as
the semantic alignments in machine translation and summarization, 
and are interpreted as the causal dependency in causal reasoning.

We propose syntactic causal patterns to automatically harvest the causalities from text corpus. 
However, commonsense causalities embedded in texts are sparse and ambiguous. 
The scarcity and the noise of the training pairs raises the question of whether the CNN seq2seq model can learn a strong enough causal dependency model for causality generation.
To ameliorate this limitation, 
we aim to complement the model with global causal dependency guidance for causality generation. 
We first propose a causal attention mechanism which leverages external causal knowledge, CausalNet~\cite{LuoSZHW16}.
Furthermore, we propose a novel attention fusion mechanism which combines the global causal dependency guidance from the causal attention with the local causal dependency from the soft attention mechanism at decoding time steps.

 CausalNet is a large network with words or terms as nodes. It contains more than 62 million causal evidences (represented by directed edges between causal words) and their frequency of co-existing with causal relationships in a large web corpus. For example, the word ``rain'' is a strong cause of ``umbrella'' in CausalNet with frequency 123, and even stronger of ``wet'' with frequency 399. 
 In this work, we compute the global word-level causal strengths based on the causal evidences in CausalNet and use the computed causal strengths as the global guidance for the causality generation.
The extensive results show that our causality fused attention mechanism outperforms all the other competitive baselines for the causality generation task.

In sum, this paper makes the following contributions:
\begin{itemize}
	\item We define a new problem, commonsense causality generation, which benefits many NLP applications such as question answering.
	\item We propose a method to automatically create a cause-effect pairs corpus which facilitates the training process for commonsense causality generation.
	\item We propose a novel causal attention fusion mechanism which introduces the global causal dependencies observed from external knowledge source. Extensive experiments show that our approach outperforms multiple strong baselines by a substantial margin.
\end{itemize}

%Luo et al.~\cite{} extracts the cause-effect pairs from text corpus to harvest the large scale causal knowledge, such as CausalNet~\cite{}.


% sequence to sequence model
