\section{Experiments}
\label{experiments}
\begin{table*}[th]
	\centering
	%\small
	\begin{tabular}{@{}llcccccc@{}}
		\toprule[1.5pt]
		&               				& \multicolumn{2}{c}{\textbf{4-class}} & \multicolumn{2}{c}{\textbf{6-class}} & \multicolumn{2}{c}{\textbf{13-class}} \\ 
		& & Acc & F1-macro                 & Acc & F1-macro                  & Acc & F1-macro \\
		\midrule
		\multirow{6}{*}{\textbf{Session-level}}&Random   &23.0$\pm$3.56 &22.67$\pm$3.71 &17.33$\pm$2.62 & 15.80$\pm$3.00& 8.33$\pm$2.62& 6.63$\pm$2.12  \\		
		&Majority    &31.00$\pm$0.00 &11.80$\pm$0.00 &31.00$\pm$0.00 &7.90$\pm$0.00 &26.00$\pm$0.00 &3.20$\pm$0.00 \\
		&LSTM    &29.80$\pm$1.28 &22.87$\pm$1.24 &30.83$\pm$1.16 &11.10$\pm$0.08 &28.50$\pm$1.44 &4.63$\pm$0.45 \\
		&CNN    &42.67$\pm$2.93 & 33.27$\pm$6.63&37.80$\pm$1.31 & 31.40$\pm$6.67 &32.33$\pm$2.46 &9.20$\pm$4.97 \\
		%&CNN    &44.93$\pm$1.23 &39.70$\pm$1.31 &39.03$\pm$0.90 &34.10$\pm$2.18 &30.93$\pm$1.30 & 6.53$\pm$2.31 \\
		&BERT   &47.10$\pm$1.28 &44.53$\pm$1.10 &41.87$\pm$0.81 &39.40$\pm$0.85 &39.40$\pm$0.36 &20.40$\pm$0.67 \\
		&Human &56.00$\pm$6.00 &55.20$\pm$6.30&50.00$\pm$9.00&53.00$\pm$8.10&38.50$\pm$5.50&40.75$\pm$8.15 \\ 
		\midrule
		\multirow{6}{*}{\textbf{Pair-level}}&Random   &28.20$\pm$9.30 &26.90$\pm$9.24 &17.93$\pm$7.89 &16.2$\pm$7.54 &6.43$\pm$2.76 & 5.73$\pm$2.64 \\		
		&Majority    &23.10$\pm$0.00 &9.40$\pm$0.00 &23.10$\pm$0.00 &6.20$\pm$0.00 &19.20$\pm$0.00 & 2.50$\pm$0.00\\
		&LSTM    &25.63$\pm$2.76 &13.13$\pm$5.06 &22.67$\pm$0.61 &6.40$\pm$0.29 &19.20$\pm$0.00 &2.57$\pm$0.05 \\	
		&CNN    &47.47$\pm$2.76 &35.03$\pm$5.80 &38.47$\pm$4.21 &30.40$\pm$9.06 & 22.20$\pm$6.08& 7.07$\pm$6.04\\	
		%&CNN    &52.13$\pm$4.97 &45.00$\pm$4.34 &34.63$\pm$3.63 &30.70$\pm$5.56 & 19.20$\pm$1.06 & 4.27$\pm$1.39\\
		&BERT   &58.13$\pm$0.61 &52.00$\pm$0.86 & 42.33$\pm$2.76&38.00$\pm$1.14 &39.73$\pm$1.79 &24.07$\pm$0.63 \\
		&Human & 75.65$\pm$3.85 &73.00$\pm$4.40 & 72.40$\pm$4.50&73.55$\pm$5.45 &63.45$\pm$1.95 &54.40$\pm$3.00 \\ 
		\bottomrule[1.5pt]
		
	\end{tabular}
	\caption{The classification results(\%) on session-level tasks and pair-level tasks.}
	\label{tab:results}
\end{table*}

In this section, we introduce the baseline models for predicting the relation type of each dialogue session, human evaluation settings and evaluation metrics.

\subsection{Baseline Models}
We introduce two naive baseline methods, Random and Majority, and three strong neural
baseline models, CNN, LSTM and BERT. 
%We re-implemented these three neural models 
%by \textit{Pytorch-Lightning}~\footnote{https://github.com/PyTorchLightning/pytorch-lightning}
The source code is avaliable at Github.~\footnote{http://Anonymous.com.}

\textbf{Random:} A relation type is randomly assigned to a dialogue session or a pair of interlocutors.

\textbf{Majority:} The most frequent relation type seen by the model during training
is assigned to a dialogue session or a pair of interlocutors.

\textbf{CNN:}
TextCNN, proposed by Kim ~\cite{Kim14}, is a strong text classification models 
based on convolution neural network. We expect that by moving the convolution
kernel along the dialogue, TextCNN can capture some semantic information that strongly
indicates the relation between the two speakers. For example, \textit{dad} obviously indicates
the speakers are \textit{Child-Parent} and \textit{honey} strongly indicates the speakers are
in close relation, possibly \textit{Lovers} or \textit{Spouse}.

All of the utterances in a dialogue session are concatenated as the input to the 
embedding layer, where 300-dimension pre-trained Glove~\cite{pennington2014glove} 
embeddings are used and freezed 
during training. Following the setting of Kim ~\cite{Kim14}, we used three convolution layers
with kernel size 3, 4, and 5 to extract semantic information from the dialogue. A dropout layer with 
probability $0.5$ is attached to each convolutional layer to prevent overfitting.
Finally, a linear layer and a softmax function are set for the final predictions. 
We use the negative log likelihood as the loss function. Stochastic gradient descent is 
used for parameter optimization with the learning rate equaling to $0.01$. 

\textbf{LSTM:}
Long Short-Term Memory(LSTM) ~\cite{DBLP:LSTMs} is a strong variant of recurrent neural 
network, which is commonly used to process sequential data like natural text sentences.
Attention mechanism allows the model to capture dependencies without regard to their distance in the input sequences.
~\cite{DBLP:journals/corr/BahdanauCB14}~\cite{DBLP:conf/iclr/KimDHR17}.

We introduce a novel LSTM-attention model by Zhou et al.~\cite{ZhouSTQLHX16} as another
neural baseline for this task.
The input dialogue sequences are firstly encoded by the same pre-trained Glove embeddings 
we mentioned above in the embedding layer. Then the last hidden states of both directions 
are concatenated as the query for the self-attention among the input tokens. Finally, the weighted summed 
feature vector can be used to characterize the whole session and used for final relation 
classification with a linear layer and a softmax function. Following Zhou et al.~\cite{ZhouSTQLHX16},
we use Adadelta~\cite{DBLP:adadelta} as optimizer but with learning rate $3e-4$.

\textbf{BERT:}
BERT ~\cite{DevlinCLT19} is a contextualized language model pre-trained on large amount of corpus.
BERT can be used as a encoder of the natural text input.
It is a strong baseline models for varied of text classification tasks.

We fine-tune the uncased base model of BERT released by \textit{Transformers}\footnote{https://huggingface.co/transformers/}
All of the utterances in a dialogue session 
are also concatenated with the special token [CLS] as the start of the sequence.
Following the general procedure of fine-tunning BERT, we pass the output hidden state of 
[CLS] token into a full connected layer for classification and use Adam as optimizer
with learning rate $1e-6$. We fine-tune the dataset for 32 epochs with early stopping
patience equal to 3.

Research found that using a turn embedding in dialogue related task can lead for better performance.
We tried to use the token type embedding in BERT as the turn embedding, that is, 0 for speaker A
and 1 for speaker B. But we found that this method will hurt the overall performance with about
4-5 drop in accuracy. Therefore, we remove the turn embedding in BERT baseline models and treat
the dialogue as a whole sentence.

\subsection{Human Evaluation Settings}

To give an upper bound of our proposed DDRel dataset, we hired human annotators to do the relation classification tasks on the test set. Since given the 13-class classification results, the 4-class or 6-class classification results are obvious for human. We only asked annotators to do the 13-class interpersonal relation classification tasks.

We asked 2 volunteers to do the 13-class relationship task on session-level samples.  Each session are showed individually and volunteers are required to choose the most possible relation type. 

Another 2 volunteers are hired to do the 13-class relationship task on pair-level samples. All of the dialogue sessions between a pair of speakers are given to the volunteers to inference the relation types.


\subsection{Evaluation Metrics}
\label{sec:metrics}
Each classification model are trained separately for relation classification tasks on different granularity. 

We use accuracy and F1-macro scores to evaluate the session-level performance.

To evaluate the pair-level performance, we did following calculation based on the results for each model: We first calculate the MRR metric of each relation type for each dialogue session. Then, given a pair of interlocutors with multiple sessions, the confidence score for each relation type can be regarded as the average MRR among sessions. Finally, the relation type with the maximum confidence score is regarded as the final prediction for this pair. Similarly, accuracy and F1-macro scores are used for evaluating the pair-level performance.


