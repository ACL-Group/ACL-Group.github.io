\section{Preliminary Definition}
\label{sec:prelim}
%\textcolor{red}{Hongru: these are concrete examples, which i think should be given after the problem
%formulation} 
\KZ{Check the use of $x$ with later sections.}
In this work, we define a NLR multiple-choice question (MCQ) as: 
\begin{equation}
mcq = (p, \{h_1, h_2, \ldots, h_n\}) 
\label{eq:nli}
\end{equation}
\noindent
where $p$ is the context against which to do the reasoning ($p$ corresponds 
to ``premise'' in~\exref{exp:roc});
$h_i$ is a choice, a.k.a., a hypothesis given the context $p$; 
$n$ is the number of choices in this questions. There is only \textbf{one
correct choice} in a question.

 
%The size of the relation set $\mathcal{L}$ varies with tasks.
%We argue that 
%most of the discriminative NLR tasks can be formulated into this general form. 
%which will bring us much convenience to evaluate the cues of a dataset. 
%For example, an NLI question consists of a \textit{premise}, a \textit{hypothesis} 
%and a \textit{label} on the relation between premise and hypothesis. 
%The formulation of this task will be $p =$ \textit{premise}, $q =$ \textit{hypothesis} and $l =$ \textit{label}. 
%$|\mathcal{L}| = 3$ for three different relations: 
%\textit{entailment}, \textit{contradiction} and \textit{neutral}. 
%We will talk about how to transform into this form in \secref{sec:dynamic}. 
%The ROCStory~\cite{mostafazadeh2016corpus} dataset
%such as in~\exref{exp:roc}, 
%consists of a \textbf{context} and two possible story \textbf{endings}. 

%We will formulate this task by setting $p = $ \textit{context}, $h = $ \textit{ending1/ending2}. In this case, $|\mathcal{L}| = 2$. because $l$ is ``true'' or ``false'' indicating whether 
%the ending is a plausible ending of the story.

%\subsection{Transformation of MCQs with dynamic choices}
%\label{sec:dynamic}
%So far the multiple-choice questions we targeted such as NLI problems
%are actually classification problems with fixed set of choices. 
%There is another type of natural language reasoning tasks which 
%are also in the form of multiple-choice questions, 
%but their choices are a fixed set of labels, as shown below. 
%\begin{example}\label{exp:roc}
%A story in ROCStory dataset, with ground truth bolded~\cite{mostafazadeh2016corpus}.
%\begin{description}
%\item{Context:} Rick grew up in a troubled household. 
%He never found good support in family, and turned to gangs.           
%It was n't long before Rick got shot in a robbery.             
%The incident caused him to turn a new leaf.
%\item{Ending 1:} He joined a gang. 
%\item{Ending 2:}  \textbf{He is happy now.}
%\end{description}
%\end{example}
%
Most NLR models handle the above MCQ as $n$ binary classification problems: 
($p$, $h_1$, \{$true$, $false$\}), ($p$, $h_2$, \{$true$, $false$\}), ..., ($p$, $h_n$, \{$true$, $false$\}), 
and returns the choice with the highest true probability. Therefore, for all practical purposes,
an NLR dataset contains a set $X$ of problem instances $x = (p, h, l)$, where $l \in \LL$ and $\LL = \{true, false\}$.
%We can transform the this case into two separate problem instances, still
%in the same form as in \eqnref{eq:nli}, 
%$u_1=(context, ending1, false)$ and $u_2=(context, ending2, true)$, where $L = {true, false}$.
%For better take advantage of the bias score to choose the right choice. We also use two linear model: SGDClassifier and 
%logistic regression. The inputs of the models for instance $e_n$ is the concatenation of bias scores for each label which %can express as :  $input(e_n) = [ f_{\mathcal{F}}^{(w_{n_1}^{l_1})},..., f_{\mathcal{F}}^{(w_{n_d}^{l_1})},..., f_{\mathcal{F}}^{(w_{n_1}^{l_v})},..., f_{\mathcal{F}}^{(w_{n_d}^{l_v})}]$. The corresponding target is the correct label $l_{gold}\in{L}$.

In this work, we only consider linguistic features in the hypothesis since the features in the premise  
are common to all labels or choices and will not affect model's choices. 
Given an NLR question instance ($p$, $h$, $l$), a feature $f$ in $h$ is extraneous if 
($p$, $h_{-f}$, $l$) also holds where $h_{-f}$ is a variant of $h$ with $f$ removed.
Next we will give detailed definition of linguistic features and how to remove them from a sentence.
