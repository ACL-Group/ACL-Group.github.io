\section{Introduction}
\label{sec:intro}
Deep neural models have showned to be effective in 
a large variety of natural language inference~(NLI)
tasks~\cite{bowman2015large,wang2018glue,mostafazadeh2016corpus,roemmele2011choice,zellers2018swag}. Many of these tasks
are discriminative by nature, such as predicting a class label or
an outcome given a textual context, as shown in the following example:

\begin{example}\label{exp:snli}
An example in SNLI.\\
\textbf{Premise}: A swimmer playing in the surf watches a low flying airplane headed inland. \\
\textbf{Hypothesis}: Someone is swimming in the sea.\\
\textbf{Label}: a) Entail. b) Contradict.  c) Neutral.
\end{example}

The number of candidate labels may vary. Humans solve such questions by
reasoning the logical connections between the premise and the hypothesis,
but previous work~\cite{naik2018stress,schuster2019towards} 
has found that some NLP models can solve the question
fairly well by looking only at the hypothesis (or ``conclusion'' in some work)
in the datasets.
It is widely speculated that this is because in many datasets, 
the hypothesis are manually crafted and may contain artifacts that
would be predictive of the correct answer. 
Such ``hypothesis-only'' tests can identify problematic questions
in the dataset if the question can be answered correctly without 
the premise. While such a method to evaluate the quality of
a dataset is theoretically sound, 
it i) usually relies on training a heavy-weight model such as Bert, which
is very costly, ii) does not provide explanation why the question is 
a culprit, and iii) cannot be used to evaluate a model since a model that
can make a correct prediction using only the hypothesis is not necessarily a
bad model: it is just not given the right data.  


%argumentation~\cite{niven2019probing}, commonsense reasoning~\cite{}, 
%reading comprehension~\cite{lai2017race}, question answering~\cite{talmor2019commonsenseqa} 
%and dialogue analysis~\cite{lowe2015ubuntu}. 

Inspired by black-box testing in software engineering, 
CheckList~\cite{checklist2020acl} is a framework that assesses the weakness of 
models without the need to know the details of the model. It does so by
providing additional stress test cases according to predefined 
linguistic features. Unfortunately, CheckList requires manual annotation 
of templates to generate test cases, which is not a fully automated process. 
Furthermore, it can only tell you what the model is incapable of doing but can't tell 
you what the model has learned from the data 

\begin{figure}[th]
\centering
\includegraphics[width=0.6\columnwidth]{picture/cue_def.png}
\caption{Example of a {\em cue}.}
\label{fig:cue_def}
\end{figure}

In order to provide researchers in the community with a standard, open and easy-to-use method
to evaluate {\em data set} and {\em model} for
discriminative NLI tasks {\em fully} automatically, we introduce ICQ (``I-see-cue''), 
an open source evaluation framework~\footnote{\url{http://adapt.seiee.sjtu.edu.cn:3308/eval/dataset}}.
%which
%In this paper, we propose a light-weight framework called
%can evaluate both the {\em data set} and the {\em model} for
%discriminative NLI tasks {\em fully} automatically. 
Recent study~\cite{gururangan2018annotation,sanchez2018behavior,poliak2018hypothesis} 
found that 
statistical bias, linguistic features which have unbalance appearance with different labels, such as 
sentiment, repetitive words and even shallow n-grams in 
benchmark datasets can be predictive of the correct 
answer. We call such biases artificial spurious \textit{cues} when
they appear both in the training and test datasets with a similar distribution
over the prediction values.
We illustrate this in~\figref{fig:cue_def}. 
Once these cues are neutralized (in our work, we flatten the distribution) in the test data, 
resulting in what we call ``stress test'',
previous successful models may degrade substantially, suggesting 
that it has taken advantage of the biased feature and hence not as robust as desired.

%Inspired by the black-box testing in CheckList, 
%%whwith various of perturbation strategies, like typos, 
%we propose to generate test cases by just 
%filtering the original test data into new subsets according to 
%specific linquistic features, like word, NER, Negation, and sentiment, etc. 
%%in constructing the spurious cues. 
%We can show the cases distribution on each label indicates whether 
%the dataset is balance on this feature. In another word, we can 
%find the bias and cues intuitively. 
%Meanwhile,  we can prob what bias features the model really sensitive to.
%We even out the filtered test date on different labels. The even out version 
%can be used as a ``feature test'' to estimate whether a model consider 
%bias features in training data to make prediction.
%
%Our framework can be used to identify simple but effective biases and cues  
%in a broad range of multiple choice NL reasoning datasets.
%Though not all multiple choice questions in these dataset involve 
%all three components, i.e., a premise, a hypothesis and a label, 
%we will describe in~\secref{sec:formulation} how to normalize them into
%the standard form. 

%The relative size of hard part over the whole
%data indicates the quality of the dataset .
%\KZ{pls quickly finish the remaining part of intro. 
%The contributions part is critical.}
%Thus we can
%separate the original dataset into easy and hard part with 
%the bias score feature of cues
%any   dataset into easy and hard part
%and thus we can evaluate the ``biasness'' and ``quality'' of the dataset. 



%using simple and cost-effective linear classification models 
%on the simple cue features,
%which trained with unbalanced score of cues to several benchmark datasets across 
%various tasks and domains. 
%We evaluate the effectiveness of our methods with the 
%deviation between our results and random selection results. 
%Simultaneously, the test set can be divided into two parts: \textbf{easy} and \textbf{hard}. 
 %\textbf{balance} and \textbf{imbalance} are relative rather than absolute. 
%\KZ{Improving the neural models by splitting the training set.} 

%The training data can be separated into $n$ parts, test on one part and training 
%the model on the rest for each time. 
%The filtered part contains the instances 
%which can not be correctly chosen.
%The filtered data will be used to train a better model for target tasks. 

In summary, this paper makes the following contributions:
%\KZ{These contribs need to be revised right?}
\begin{itemize}
\item we provide a light-weight but effective method to evaluate to what extent 
NLI datasets contain statistical biases and cues;
%(\secref{sec:result}).

\item by filtering the test data into different stress test sets, 
we can assess whether a given model has taken advantage of some
spurious cues to make predictions;

\item our demonstration system makes the evaluation results visual and intuitive.
\end{itemize}
% (\secref{sec:result}).

%\item We filter the training data and get a better performance on the \textbf{hard} dataset.
%to get a high quality training dataset which
 %is possibly closer to the intended task. 

















