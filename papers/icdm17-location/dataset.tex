\section{Datasets}
\label{sec:data}

This section describes the process of creating our benchmark data sets.
Both data sets will be made public at 
\url{http://anonymized.due.to.blind.review}.

\subsection{Physical Objects}
We create a vocabulary of single-word physical objects by the 
intersection of all entities that belongs to ``physical object'' class
in Wikidata\footnote{\url{https://www.wikidata.org}} and 
all the ConceptNet concepts. We then manually filtered out some 
words that have the meaning of an abstract concept, 
such as ``story'' and ``dream''. This leaves us with
1169 physical objects in total.

\subsection{Text corpus}
We use a cleaned subset of the Project Gutenberg corpus~\cite{lahiri:2014:SRW} 
in this work\footnote{\url{https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html}}, containing 3,036 English books written by 142 authors.
The reason for choosing this corpus rather than other well-known
corpora is that sentences in novels are more likely to describe scenes 
involving physical objects. 
We compare the density of \lnear~ relations in Gutenberg with other 
widely used corpora, namely Wikipedia, 
used by~\citeauthor{mintz2009distant} and New York Times corpus, 
created by~\citeauthor{riedel2010modeling} and 
used by~\citeauthor{Lin2016NeuralRE,hoffmann2011knowledge,surdeanu2012multi}. 
In English Wikipedia dump, out of all sentences which mentions at least two
physical objects, 32.4\% turn out to be positive. 
In New York Times corpus\footnote{\url{http://iesl.cs.umass.edu/riedel/ecml/}},
the percentage of positive sentences is only 25.1\%. 
In contrast, that percentage in the Gutenberg corpus is 55.1\%, much higher 
than the other two corpora, making it a good choice for \lnear~ 
relation extraction.

\subsection{\lnear~Sentences Dataset}
\label{lsd}
From our corpus, we identify 107,026 pair of physical objects that
co-occur within a sentence, out of which 
15,193 pairs occur in least 10 sentences.
We call these ``frequent pairs''.
Among frequent pairs, we randomly select 500 object pairs and 
10 sentences for each pair for the annotators to label true or false. 
%We do not distinguish between \lnear\ and \textsc{atLocation} relations,
%where the latter typically refers to objects which are adjacent to or
%contained by each other, e.g., {\em room} and {\em door}.
Each sentence is labeled by three annotators who are college students
and proficient in English. The final truth label of a sentence is decided
by a majority vote from the three annotators. 
The Cohen's Kappa among the three annotators is 0.701 which suggests
substantial agreement. 
This dataset will be used to train and test models for relation
classification.
We compare the statistics of our \lnear\ sentence
dataset with a few datasets on other well known relations in 
\tabref{tab:datasets}.
One can see that our dataset almost double the size of those most
popular relations in the SemEval task, and the sentences in our
dataset tend to be longer with more words.
From a total of 500 physical object pairs with 10 sentences each, we create the training and test set by splitting 500 pairs, that is,
we randomly choose 400 pairs along with their 10 sentences each as the training set and the remaining 100 pairs with sentences as the test set.

\begin{table*}[th]
\centering
\begin{tabular}{|l|c|c|c|c|} \hline
Data set & Frequency & Percentage & Words per entry & Chars per word  \\ \hline \hline
\lnear & 2,754 & 55.1 & 18.6 & 4.51  \\ \hline
Not \lnear& 2,246 & 44.9 & 19.1 & 4.32  \\ \hline\hline
Cause-Effect & 1,331  & 12.4 & 17.3 & 4.71\\ \hline
Component-Whole &1,253 & 11.7 & 17.9 & 4.12 \\ \hline
Others &1,864 & 17.4 & 17.8 & 4.34 \\ \hline
\end{tabular}
\caption{Comparison between our \lnear~ dataset and 
the most popular relations from SemEval 2010 Task 8 dataset for 
relation classification}
\label{tab:datasets}
\end{table*}

\subsection{Commonsense \lnear~object pairs}
We randomly sampled 500 pairs of objects according to the distribution of
the number of their occurrences in the corpus, that is, the number of
sentences they appear in. This tends to give us pairs that are more popular.
We again ask the same annotators to label whether each pair of objects
are likely to co-locate in real world. Majority votes determine the final
truth label as before.
The inter-annotator agreement here is {0.703}. 
This smaller data set can be used to evaluate the 
scoring and ranking of relation triples.

