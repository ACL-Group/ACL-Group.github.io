Title:	Automatic Extraction of Commonsense LocatedNear Relationship from Text
Authors:	Frank Xu, Bill Lin and Kenny Zhu
Instructions

The author response period has begun. The reviews for your submission are displayed on this page. If you want to respond to the points raised in the reviews, you may do so in the box provided below (limit: 500 words).

Please note: you are not obligated to respond to the reviews.

Review #1

Appropriateness:	5
Clarity:	3
Originality:	3
Soundness / Correctness:	3
Meaningful Comparison:	3
Substance:	3
Impact of Ideas / Results:	3
Impact of Accompanying Software:	1
Impact of Accompanying Dataset / Resource:	3
Recommendation:	3
Reviewer Confidence:	3
Comments

The paper presented the problem of extracting commonsense LocatedNear relation between physical objects, explored in two settings. The first setting is dependent on sentential context, and the second setting is more context independent (or dependent on some global context) reflecting general belief/knowledge about the LocatedNear relation among these objects. The problem is interesting and well-motivated. The work accompanies a newly created data set with good inter-rater agreements and could be beneficial for the research community. The authors also presented two methods intended to establish some baseline results on these data sets, which include an SVM classifier with feature engineering, and an LSTM-based neural network with customized architecture for this problem setting.

Following are my comments:

the rationale behind the formulation of the second task as a ranking problem instead of a classification problem is not very clear to me. I am also not entirely sure what a ranked list of physical object pairs would mean here. Should it mean that a higher ranked pair is physically closer/nearer to each other than a lower ranked pair? The current ranking methods presented in the paper would not reflect that regardless, and I cannot think of any other explanation that could justify a ranking formulation.

The initial description of the second task (lines: 140-144) uses the outcome/result of the first task as a pre-condition (excerpt: "One is a binary classification problem which detects two co-located physical objects in a sentence. The other one is given result of the above classification on large number of sentences, how to produce a ranked list of LOCATEDNEAR facts as a knowledge base."). In my opinion, the task (second) should be formulated independently. It would have made more sense to me if the authors presented the use of the classifier of the first task as part of the solution method for the second. Although, this might be just a writing issue in lines 140-144 only.

The authors should clarify in the paper (or define) what is meant by “physical scene” (line 194) and/or “real life scene” (line: 518), etc. (perhaps give examples)

Precision/recall/F1 scores are typically calculated per category or micro/macro-averaged over all categories. The authors should clarify in the paper if the precision/recall/F1 results in Table 5 are with respect to the LocatedNear class.

Since the authors are introducing these data sets for the first time, for comparison with the presented method, I would also like to see results for a random baseline, and a majority class (LocatedNear) baseline in Table 5.

I also feel that Table 6 should have baseline results for ConceptNet or any other existing source of LocatedNear relation, which could be made to work in a classification task, but I am not sure how that could be done in a ranking formulation.

Minor:

Line 180: previous -> previously

Line 294: “word sequence of as” -> “word sequence as”

Line 322-323: verb missing in the sentence

Line 526: Extra space between “corpus” and footnote 7.

Line 543: proficient with -> proficient in (“with” is typically used for an instrumental/tool concept)

Line 572-573: the authors mentioned that they sampled 500 pairs of objects by the number of sentences they appear in. This statement is confusing and needs a revision.

Line 628: we set initialize -> redundant use of verbs

Review #2

Appropriateness:	5
Clarity:	4
Originality:	3
Soundness / Correctness:	2
Meaningful Comparison:	4
Substance:	4
Impact of Ideas / Results:	2
Impact of Accompanying Software:	3
Impact of Accompanying Dataset / Resource:	3
Recommendation:	2
Reviewer Confidence:	4
Comments

When two objects are mentioned in text, this paper aims to classify whether they are located near each other or not. It is not finding a “located near” relation explicitly expressed by the text itself, but rather an implicit relation suggested by it. The aim is to then observe many such mentions and determine if two objects are often LocatedNear each other in the world.

I like the overall setup of the task, data collection, and annotation. The topic is rather specific, though, and might not be of interest to a broad set of readers. The model suggested is not evaluated in a general manner, so there is limited takeaway from its results. That said, the topic is interesting and I always like new datasets and evaluations. However, the experiments have a couple problems.

Train and Test split. There are 500 pairs with 10 sentences each. The train/test split was to choose 1000 random sentences for test. This is not a great way of doing things, and at worse, it inflates your reported results. For instance, what if an object pair’s 10 sentences are all virtually the same? You now have pollution of training. Even if the sentences are not similar, the learners can still memorize the answers, rather than learn a general language pattern to detect LocatedNear. The proper way to do this is to choose 100 random PAIRS and then include all 10 sentences of each in test. This gives you a true proper split of the data. I strongly suggest you do this before a camera-ready or a resubmission.

The SVM baseline is not given a fair chance. The LSTM is given the sentence pattern, but the SVM only has a dependency path between the two objects. You at least need to have a predicate feature that gives the verb/noun under which the objects appear! Your own example: “A dimly lit room with a dining table, plates, and glasses.” The shortest path will just be plates->and->glasses. The key here is being under an NP (headed by ‘room’) and under a ‘with’ preposition. The SVM isn’t given the chance to use this information, whereas the LSTM was given a custom “NORM” sentence input with that critical piece. Can this be given to the SVM in shortest-path form?

CommonSense baseline. The common-sense results are perplexing to me because your best cutoff was just a summation of scores. If you are evaluating on the 500 pairs in section 3.4, then each pair was seen a different number of times. There is an obvious baseline that you need: # of sentences observed. Forget SVM and the LSTM. Just add up how many sentences you saw an object pair. How does that perform? This is a required baseline based on your “best accumulative score”.

The above changes should not be too much to do. Create a new train/test split, add a common-sense baseline, and spend just a little more time on that SVM. The paper will be MUCH stronger with these changes.

Review #3

Appropriateness:	5
Clarity:	4
Originality:	2
Soundness / Correctness:	4
Meaningful Comparison:	3
Substance:	4
Impact of Ideas / Results:	3
Impact of Accompanying Software:	1
Impact of Accompanying Dataset / Resource:	4
Recommendation:	3
Reviewer Confidence:	3
Comments

1121: Automatic Extraction of Commonsense LOCATEDNEAR Relationship from Text

This paper deals primarily with the task of identifying LocatedNear relationships, and describes the process of creation of two datasets associated with the task. Baseline models for the task are described.

As a paper introducing new datasets, this is a well-written paper. It covers all of the key questions expected of a resource description paper.

The baseline systems described here are interesting -- covering, both, a feature engineering approach, and a neural (LSTM-based) approach. However, the purpose of the experiments is not entirely clear. I was struggling to identify the research questions underlying the experiments in section 4.

The motivations for some decisions taken in the paper seem somewhat convoluted. For instance, in Section 2.1.2:

"We will find that the important words “lead” and “into” are not attached more importance than the other words. Also, the semantic differences between irrelevant words, such as “king” and “criminal”, “beautiful” and “poor”, are not useful to the co-location relation between the “dog” and the “garden”, and thus tends to be noise in training a classifier."

"Considering such problems, we can use POS (part-of-speech) tags instead to capture a little more syntactical information and reduce the vocabulary size..."

The motivations in the above section to not simply use word based models, but instead do a much more complex model -- combining lemma, pos, dependency -- based solely on intuition, seems rather odd. Readers would be much more convinced of the superiority of the complex model, when presented with their results alongside those of simpler word-based models.


