\section{Approach}
\label{sec:approach}
\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/learning_reasoning}
	\caption{Overview of the rule learning framework and causal reasoning system.}
	\label{fig:approach}
\end{figure*}
In this section, we first introduce the proposed rule learning framework including rule instance extraction and rule acquisition, as shown in the above two parts of Figure \ref{fig:approach}, and then introduce the reasoning system in the bottom part of Figure \ref{fig:approach}.
\subsection{Rule Instance Extraction}
This part is about how to extract rule instances from the online financial text in Chinese.

\textbf{Pattern Matching}
Causality, expressed by natural language texts, can be identified by linguistic patterns known as causal cues \cite{Chang2005}.
We design a set of causal patterns to extract cause span and effect span from the sentence, and this approach is also adopted in \cite{Luo2016a,Zhao2017,Radinsky2012}. 
Causal patterns can be divided into 3 groups based on the pattern structure, as shown in Table \ref{tab:pattern_statistics}. 
The priority of each group represents the order of matching when multiple causal patterns can match one sentence. 
Specifically, the stricter the regular expression is, the earlier it matches, and the higher the priority is. 
In addition, we found that such kind of patterns can not discover the conjunction of multi-cause events because of editors' loose writing habits in the news. 
%\TD{mention conjunction, segment firstly}
%	However, in our experiment, we found that many sentences that matched in the patterns with conjunction are not really conjuncted. \TD{an example}, so we regard them as the normal cause or effect spans.
%
%Alternatively, we can use a more sophisticated way to catch the causality such as \cite{zhao2016event}. In this study, however, we focus more on the precision of causality extraction rather than the recall using more complex patterns, also we think a more sophisticated method used on such noisy text may lead to more errors.
%
%	With these curated patterns, we then use them to match the Chinese \TD{check previous mention about Chinese data} sentences which are segmented firstly to reduce the ambiguity since Chinese has no natural separator. For example,  unsegmented text "由于是" can match both "由于"(cause cue) and "于是"(effect cue). 

%为此###	 短期 来看 ， 近期 的 反弹 主要 是 受到 “ 金 九 银 十 ” 传统 消费 旺季 预期 的 影响 ， 因为 此时 下游 钢材 市场 出现 回暖 迹象 ， 而且 临近 国庆 长假 ， 钢厂 有 补库 需求 ， 所以 铁矿石 需求 端 得到 支撑 ， 目前 主力 合约 的 价格 重回 400 元 / 吨 上方 ， 并且 在 节前 曾 最高 上 冲 至 426 元 / 吨 。

%于是### 四川省 食品 发酵 研究所 白酒 专家 余乾伟 告诉 记者 ， 茅台酒 由于 是 酱香型 的 发酵 工艺 ， 发酵 后 储存 需要 3 年 的 工艺 ， 同时 只有 在 茅台镇 茅台 酒厂 所在地 方圆 几 公里 的 地方 才 有 茅台酒 的 酿造 微生物 环境 ， 因此 产能 很 难 扩大 ， 也 决定 了 茅台酒 资源 的 稀缺性 。

%于是### 对于 是否 可能 导致 “ 囤气 ” 的 行为 ， 林 伯 强 认为 可能性 小 ， 因为 “ 囤气 ” 成本 较 高 ， 且 天然气 占 能源 的 比重 很 小 ， 只有 百分之三点几 ， 现在 商品 市场 又 很 开放 ， 所以 企业 将 成本 转嫁 到 商品 的 可能性 不 大 ， 反而 会 促进 企业 节能 减排 。

%	 we use existing dependency parser tool\cite{manning2014stanford} to parse the whole sentence tokens sequence to extract the cause events and effect events to constitute the rule instances.  For cause events, we firstly extract the verbs from the cause tokens span as the predicates. Then extract the subjects and objects of the verbs with the roles of 'nn:subj' and 'nn:dobj' in dependency relations. Lastly, we try to extract the modifiers of these subjects and objects by role of 'nn:compound'. 
%	 After extraction, we can construct many quintuples as cause events with predicate, subject, object and the modifiers of subject and object.	It is the same for effect events. Then we combine these cause events and effect events with a "product" approach to get lots of rule instances. 

%	  Since most sentences have no casual cues and parsing is a very time-consuming operation, we firstly filter out the sentences without casual patterns as a preprocessing.

%	This submodule will be illustrated using the following examples. Consider the following sentence:

%	Feed each sentence into parser tool, then join the  segmented	 output tokens with blanks. For sentence (1), We will get "上 个 月 智利 曾 遭受 8.8 级 大地震 的 袭击 ， 导致 铜 价格 上涨 近 6\% 。". Then we exploit the elaborate patterns to match the padded sentence and we can get cause tokens span "上 个 月 智利 曾 遭受 8.8 级 大地震 的 袭击 ， " and the effect tokens span "铜 价格 上涨 近 6\% 。".

\textbf{Rule Instance Extraction}
To extract the rule instances,
first, we use these causal patterns to extract cause span and effect span.
Then, we further extract the quintupled cause events and quintupled effect events from these two spans via existing dependency parser.
The events are mediated by predicates, so we regard each verb in the span as a predicate. 
Then, find out the subject and object corresponding to each predicate, and get their modifiers. Meanwhile, we also consider the negative dependency of the verb and mark it behind the predicate, which is a practical implementing trick. 
For example, `occur\_1' indicates `occur', while `occur\_0' means `not occur'. 
Since there may be more than one predicate in each span, which means many events exist in cause span or effect span, we do a Cartesian product of these cause events and effect events as the extracted rule instances. As the comment of philosopher David Hume (1711–1776) about causality that the frequency of causal events will be higher than those which are non-causal, filtering will alleviate the noisy rule instances. Thus, we do next step: rule instance distillation.

%The events in rule instances are mediated by the predicate, namely verb, so we regard each verb in the tokens span as a possible predicate. Then, we find the corresponding subject and object of each predicate. 
%Here, we can get subject and object of each predicate beyond the scope of its corresponding event's tokens span which is an advantage of parsing the whole sentence before matching the cause and effect spans since it can alleviate the parsing error by feeding more context information into the parser.
%Last, we get the modifiers of the subject and object with the dependency relation of 'compound:nn' or 'nmod:assmod', which can be also beyond the scope of its belonging token spans. Meanwhile, we also consider the neg relation of the predicate, we mark it behind predicate, which is a small implement trick. '\_0' means the predicate is negative, while '\_1' means the predicate is positive. 
%These compound nouns are placed at the event as the same order appeared in their origin sentence. 
%Since it may be more than one predicates in cause tokens span or effect tokens span, which means we can get more than one structured events from cause tokens span or effect tokens span, we do a Cartesian product of these cause events and effect events to get more combined rule instances. as the comment of philosopher David Hume about causality, The frequency of causal events will be higher than these are non-causal.  
%We believe that the right rule instances will be appeared more often than the corrupted. 
%Also, later, we will rank the low frequency pairs into the back and filter the undesired rule. From sentence (1), we can get many rule instances, one desired rule instance is ('国际 石油', '价格', '攀高\_1', '', '')$->$('橡胶', '价格', '上升\_1', '', ''). From sentence (2), we can get many rule instances, ('', '乙醇', '增长\_1', '', '')$->$('玉米', '价格', '上涨\_1', '', '') is our desired. Meanwhile we also consider the neg relation of the predicate, we mark it behind predicate, which is a small implement trick. '\_0' means the predicate is negative, while '\_1' means the predicate is positive.

\textbf{Rule Instance Distillation} Some heuristics can be used to discard bad rule instances. 
For example, 
a) The frequency of predicate pair is less than 2. 
b) Pronouns appear in events of the rule. 
c) Some verbs should not be predicates, such as `say', `state', etc. 
d) The events in the rule should ensure semantic integrity, which should have at least one subject or object.
% c)the length of each role(subject, object and so on) of the event should be limited because the parser can cause errors.

\subsection{Rule Acquisition}
This part is about how to exploit external knowledge bases to generalize rule instances into rules.

\textbf{Preliminary Work}
Available Chinese taxonomic and common sense resources are scarce. 
Most existing Chinese taxonomic knowledge bases, such as CN-Probase \cite{Xu2017} and zhishi.me \cite{Niu2011}, are constructed from online encyclopedias, which suffer that the concepts inside are much fewer than Probase \cite{Wu2012a} and they have no probabilistic characteristic. 
To tackle this problem, we translate English Probase into Chinese \zhpro. 
Meanwhile, to our knowledge, there exists no large-scale Chinese commonsense knowledge base, so we translate the English part of ConceptNet in Chinese and merge it with the original Chinese part into \zhcon.

\textbf{Predicate Normalization}
Predicates in different rule instances may express the same meaning, such as `raise', `rise', `increase', etc. We use Ciling\footnote{ \url{ http://www.bigcilin.com/}}, the largest word-level Chinese synonym resource, to normalize the predicates. We ignore word sense disambiguation to reduce the complexity of the whole framework.

%	Then, we would like to generalize these rule instances into candidate rules shown in the Figure \ref{fig:overview}'s middle part. This rule generalization component mainly contains two steps. The first step is predicate generalization and the second step is argument generalization. Predicate generalization is trying to normalize the similar predicates into the unified one. For example, "raise, rise, soar, increase, gain, enhance" have the same meaning, we need give a unified predicate to represent this group of predicates with the help of Cilin\footnote{ \url{ http://www.bigcilin.com/}}. Argument generalization step is trying to conceptualize the arguments in the rule instances which need to exploit the taxonomy in our built knowledge base. Since each candidate rule is derived by observing a cluster of similar rule instances, we need divide rule instances into several clusters. in each cluster, we generalize these rule instances to a candidate rule, and we will elaborate it in Section \ref{sec:approach}.  

\textbf{Constraint Relations}
As analyzed in section \ref{sec:intro}, we use \zhcon to add constraint relations. 
We choose \zhcon because the objects we care about in finance are always common objects, such as corn, copper, alcohol, gasoline, etc. 
If two events are causal, they must be related. 
Thus, we try to find the relations between the cause event and effect event from \zhcon.
Meanwhile, multiple edges between two nodes in \zhcon are preprocessed and removed with only one edge keeping according the the semantics of these relations. For example, if two relations `madeOf' and `relatedTo' exist in two nodes at the same time, we will remove `relatedTo', since `madeOf' is richer than `relatedTo', semantically. 
	
%	 It is in fact because that one event cause another event, Some connection must exist these two events,which is the Do-theory in causality test theory.TODO(give more explanation and citation)} 

%	Here, we exploit our built knowledge base to find all the relations between each pair arguments separately in cause event and effect event.

%	Take an example, there are many arguments $a_1, a_2$ in subject position of cause event, and $a_3, a_4$ in object position of effect event. We will firstly find all the relations between one argument in $a_1,a_2$ and the other argument in $a_3,a_4$. Then, we will fetch the most commonly appeared relations as one of the relation constraints between subject in cause event and object in effect event.
%	After adding get these relations constraint into the candidate rules, we will get the final rules.


% Sometimes these relations may not be the same literally, but they are semantically duplicated, which will also regarded as the same relation to increase the count. After adding get these relations constraint into the candidate rules, we will get the final rules.


%For example, Conceptualizing one argument to a concept make the rule much more general, ('国际 石油', '价格', '攀高@攀高', '', '') $-->$ ('橡胶', '价格', '上升@升高', '', '') to('X0', '价格', '攀高', '', '') $-->$ ('X1', '价格', '升高', '', '') where 'X0' IsA'化石燃料''X1' IsA '产品', For instance, coal IsA '化石燃料',phone IsA '产品', We instance X0 and X1 respectively with coal and phone. Then the rule means  raising the price of coal would lead to the rubber's rising, Which is obviously unreasonable. 

%So we hopefully find the relation constraint between X0 and X1, also we want to discover the relation between X0, X1 is X0 'madeof' X1. and mostly the relations are commonsense relation. 
%
%Here, we find all the relations between each pair arguments in cause event and effect event from our built knowledge base, which may consist many pair of arguments, and many relations of one pair arguments. since one rule contains many rule instances, Here, We would like the intersect these relations which include the relation's semantic duplication, the remaining relations will be kept as the relation constraint in this rule. 

\textbf{Rule Induction}
Generalizing rule instances into rules should balance generality and specificity, which is consistent with MDL principle trying to find the best hypothesis H (rules) that can semantically describe or compress data D (rule instances).
 
\textbf{a) Generalization}
We generalize the event roles (subject, object or their modifiers) in rule instances using \zhpro. A pre-built lexicon (See detail in Section \ref{sec:experiment}) help us to determine which one can be generalized. For example, the lexicon includes concrete things, such as corn, oil, copper, etc., and does not include abstract things, such as price, yield, sale, etc. Generally, the more general concepts in Probase a rule has, the more general it is. Formally, let $RI$ and $R$ denote the entire rule instances and the rules, $ri$ and $r$ denote one of them, $i$ and $c$ denote an instance and a concept, respectively.
\begin{equation*}
\begin{split}
L(RI|R)&=\sum_{r \in R}\sum_{ri \in RI(r)}{f(ri)*(-\log(f(ri|r)))} \\
f(ri|r)&=\avsum_{c\ in\ r, i\ in\ ri}{f(i|c)}, \ \ \ \  f(i|c)=f(c,i)/f(c)
\end{split}
\label{equ:mdl_1}
\end{equation*}
where the frequency of ($c$,$i$) pair is $f(c,i)$, 
obtained directly from \zhpro, and $f(ri)$ is the frequency of rule instance $ri$.

\textbf{b) Specialization}
As analyzed in Section \ref{intro:Causal_Knowledge_Mining}, we try to control the generality of the rules using the entropy of rules. Formally,
\begin{equation*}
\begin{split}
L(R)&=\sum_{r \in R}{f(r)*(-\log(f(r)))}\\
f(r)&=\sum_{ri \in RI(r)}{f(ri)f(r|ri)}=\sum_{ri \ (\ ri\ to\ r)}f(ri)
\end{split}
\label{equ:mdl_2}
\end{equation*}
Balancing generalization and specialization, we will get:
$$R=\arg\min_{R}(\alpha L(R)+(1-\alpha)L(RI|R)\label{eq:objective_function}$$
The parameter $\alpha$ controls the relative importance of generalization and specialization.  
Last, we adopt a simulated annealing (SA) algorithm \ref{alg:rule_induction} to search the optimal rules ($R$). 
\begin{algorithm}[htb]
\caption{Rule Induction\label{alg:rule_induction}} 
\textbf{Input}: Rule Instances(RI), \zhpro \\
\textbf{Output}: Rules(R)
\begin{algorithmic}[1]
	\STATE Initialize a rule($r$) for each rule instance($ri$).
	\WHILE {There is no change of L in the last $\beta$ iterations}
	\STATE	Generalize a randomly picked rule instance (ri) to a new rule $r^{t+1}$
	\STATE	Calculate $L^{t+1}$ ($L=\alpha L(H)+(1-\alpha)L(D|H)$)
	\STATE	Accept this rules(R) with the probability:
	\STATE	\begin{equation*}\begin{split}p=\left\{\begin{array}{rcl}1 & & {L^{(t+1)} < L^{(t)}}\\e^{(L^{(t)}-L^{(t+1)})/t}& &{L^{(t+1)}>= L^{(t)}}\end{array} \right.	\end{split}\end{equation*}
	\ENDWHILE
	\STATE \textbf{Return} Rules(R) 
\end{algorithmic}
\end{algorithm}	


%Algorithm \ref{alg:alg2} describes the procedure, mainly consists of two components, clustering and conceptualization.

%The input of this overall algorithm is rule instances, and our built knowledge base, our desired output is candidate rules.

%\begin{enumerate}
%	\item \textit{Clustering.} This procedure is shown in Algorithm \ref{alg:alg2}. We firstly iterate each rule instance, and find its similar rule instances , then gather them to make up a cluster. Variable RICs reserves all the rule instances clusters. As for the $Similar$ function, which is used to decide whether two rule instances should be gather together into one cluster. The basic idea is that if the arguments can't be conceptualized, They must be the same literally, else we use the word embedding to calculate their similarity. Only when the similarity value is larger than a specified threshold, we gather them together. 

%Given two rule instances, If both arguments in the same argument type(subj,obj, compound nouns,predicate) can't be conceptualized, if they are the same, these two rule instacnes are similar, else they are not similar. If both arguments can be conceptualized, We will exploit the word embedding to measure their similarity. specifically, we concatenate all the conceptualized arguments, last we calculate the cosine similarity. If it is great than a threshold, we regard these two rule instances similar, otherwise they are not similar.   

%\item \textit{Argument Conceptualization.} After get clusters of rule instances, we need generalize each cluster of rule instances into one candidate rule. Algorithm \ref{alg:alg2} line 10 to line 24 shows this process. the generalization can be divided into several argument conceptualization part. Figure \ref{fig:argument_generalization} shows one of them. in the left part is a cluster of given arguments and their respective conceptualization, and in the right part is common concepts satisfy all the arguments and also we remove the very general concepts. After doing several argument conceptualizations in this rule instances cluster, we can get a candidate rule.

%We use all the news text we crawled to train the word embedding with word2vec tool \footnote{ \url{ https://code.google.com/archive/p/word2vec}}, the output vector dimension is 400. 
%\end{enumerate}
%which includes single rule instance argument generalization in the left part, and arguments generalization in rule instances cluster in the right part. the blue part is the least general generalization in concept space of the argument. 


%\textbf{argument generalization of single rule instance.} rule instance argument generalization use external knowledge to generalize the entities in the rule instance, Here We also make a lexicon, which can prevent some abstract entities from conceptualization, For example, ('橡胶', '价格', '上升@升高', '', '') we want generalize '橡胶' rather than price, So we make a concrete lexicon which consist all the leaf nodes of the IsA relations in ConceptNet5, Since we observe the entities in ConeptNet5 are more concrete.

%\textbf{argument generalization of rule instances cluster.}one candidate rule is always derived after observing a cluster of similar rule instances. So we hope to gather clusters of generalized rule instances to generalize candidate rules. Algorithm 2 describe how to generalize each candidate rule from each cluster of generalized rule instances. Generally, we first generalize every rule instances, Then for each generalized rule instance, we find the similar rules which can be clustered together, then we generalize cluster of rule instances to one candidate rule, Repeat until process all generalized rule instances, Last we can get the candidate rules. 

%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=0.9\columnwidth]{figures/argument_generalization}
%	\caption{Argument Generalization.}
%	\label{fig:argument_generalization}
%\end{figure}

%\begin{equation}
%\label{relation_saliency}
%RS_{ij}=\frac{Count(rel_i,R_j)}{Count(R_j)}
%\end{equation}
%pick $rel_i$ for $R_j$ as relation constraint, where $i=argmax_i RS_{ij}$
%	\subsubsection{Rule Subsuming.}
%	after we put the rule into the rule base, if the frequency of this rule reach a threshold, the system will trigger the rule subsuming function, which will generalize this rule together with previous rule to get a more general rule.
%		\begin{algorithm}[htb]
%		\caption{Online Causality Rule Learner.\label{alg:oclr}} 
%		\KwIn{ free text} 
%		\KwOut{First-order logic Rules}
%		rule instances extractions
%		generalize all the rule instances' predicates.\\
%		\For{ rule instances set I with the same predicate}{
%		R=Rule\_Induction(I)\\
%		add the relation constraints for these rules
%		}
%	\end{algorithm}	
\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.99\linewidth]{figures/pipeline_example}
	\caption{An Example Showing Various Stages of the Framework}
	\label{fig:stages_in_an_example}
\end{figure*}

\textbf{Rule Confidence}
Knowledge discovered from noisy data is uncertain. Therefore, we combine the following features of the learned rules to give each rule a confidence value. For a given rule, we consider:
\begin{itemize}
\item $F_1:$ the frequency of the predicate pair in cause event and effect event.
\item $F_2:$ the frequency of event roles in this rule.
\item $F_3:$ the correlation between the cause event and effect event in the rule, as correlation is a necessary condition for causation. (we assemble the structures of both cause and effect in the rule into two short texts, respectively, e.g. the cause event in Rule (1) will be converted into `金属价格上涨/the price of metal rises'. Then, we use the Bert language model \cite{devlin2018bert} to encode two short texts into dense vector, respectively, and calculate their cosine similarity value as the correlation.)
\item $F_4:$ the average lengths of the sentences from which this rule is extracted and generalized.
\item $F_5:$ the number of rule instances extracted from the sentences in $F_4$
\end{itemize}

$w$ represents the linear combination of all these features: $w=\sum_{i\in[1,5]}w_i*rank(F_i)+w_0$. We take Mean Average Precision (MAP) \cite{baeza1999modern,cao2007learning} as the evaluation measure and use grid search, trying to search optimal weights to make the ranking best in a small number of sample rules, which means good rules rank ahead of bad rules as much as possibly.
Finally, we transform $w$ into [0,1] by linear normalization as the rule confidence. A specific example in Figure \ref{fig:stages_in_an_example} shows the process of rule acquisition step by step.

\subsection{Causal Reasoning}
%The specific reasoning process is as follows. Extract the query event from a given query sentence, search related rules in the learned rules with the depth-first search algorithm, look up related facts in Probase and ConceptNet to reduce the size of the facts and rules to speed up reasoning, and convert the found rules and facts into Prolog code together. 
%To implement uncertain reasoning, we put rule confidence into the generated Prolog code and 
%%use multiplication to decline the confidence in reasoning.
%use multiplication to simulate the decline of the confidence in reasoning.
%Uncertain reasoning should reduce confidence as reasoning deepens. We set the threshold $\gamma$ to cut off more in-depth reasoning. 
%%We choose Prolog for reasoning with uncertainty because it is a mature and reliable tool, instead of ProbLog \cite{de2007problog} and PSL \cite{bach:jmlr17}.
%Small modification of Prolog code generalization is enough to reason with uncertainty, so we choose the mature and reliable Prolog, instead of ProbLog \cite{de2007problog} and PSL \cite{bach:jmlr17}.
%After generating the Prolog code, we query the query event in the Prolog code format and finally return the top K results sorted by the confidence.
The bottom part of Figure \ref{fig:approach} sketches the causal reasoning system.
Given a sentence containing an event, our reasoning system will output the events that this event will lead to (prediction) or the events that cause this event (explanation). 
The specific process is as follows:
%\begin{figure*}[htbp]
%	\center
%	\includegraphics[width=0.95\linewidth]{figures/reasoning}
%	\caption{Causal Reasoning with Uncertainty }
%	\label{fig:causal_reasoning}
%\end{figure*}

\textbf{Event Extraction:} Given a Chinese sentence, typically, a financial news title, parse it with Stanford CoreNLP tool \cite{Manning}, and extract the structured event. 
\textbf{Search for Related Rules/Facts:} For this structured event, firstly, search for the related rules and facts in order to reduce inference time in Prolog. When searching for rules to predict (or explain), specific event extracted from the input sentence will try to match the cause (effect) in the rule via \zhpro. For example, extracted specific event ``suffer(`', Thailand, earthquake, attack)" matches ``suffer(`', X, Y, attack), isA(X, country), isA(Y, disaster)" in rule (1) by instantiating X to Thailand and Y to earthquake. In this way, we can acquire rule (1). Then, we use the depth-first search algorithm to search for other related rules of rule (1). Last, we search for all facts that can used in these related rules from \zhpro and \zhcon.
\textbf{Prolog Code Generation:}
We convert all retrieved human-readable rules and facts into standard Prolog code and also add some auxiliary code, see an executable example in Section \ref{sec:experiment}.
\textbf{Reasoning:}
To implement uncertain reasoning, we use multiplication to simulate the decline of the confidence in reasoning and set a threshold $\lambda$ to prevent the Prolog from reasoning too deeply, which will lead to unreasonable predictions. This threshold also can avoid infinite reasoning loop since our rules learned automatically may lead to each other between two events. A small modification of Prolog code 
is enough to reason with uncertainty, so we choose the mature and reliable SWI-Prolog\footnote{\url{http://www.swi-prolog.org/}} \cite{Wielemaker2010}, instead of ProbLog \cite{de2007problog} and PSL \cite{bach:jmlr17}.
\textbf{Ranking:}
Finally, it returns the top K events (or event chains) sorted by the confidence (if it is a event chain, this confidence will be averaged by these confidences in the chain).