\section{Introduction}
\label{sec:intro}
Causal reasoning, the core challenge to artificial intelligence, which aims to understand the causal dependency between events, is receiving more and more attention \cite{Pearl2009}.
In the reasoning process, causal knowledge and reasoning mechanism play a critical role in people's daily behavior and decision-making \cite{waldmann2013causal}.
It is of great interest in many domains, including finance \cite{Dunietz2017}, where understanding causality can provide significant opportunities for economic benefits. 
For example, consider the following,
\begin{enumerate}
	\item If a large disaster happens in a country and this country is rich in certain metal, the price of this metal will rise. \label{intro:natural-language-rule-1}
	\item If the price of some kind of metal rises, the price of the products made of this metal will also go up. \label{intro:natural-language-rule-2}
\end{enumerate}
Above causal knowledge expressed in the form of natural language is easy for us to understand and reasoning based it has a great practical value in real life. 
For example, if there's an earthquake in Chile, according to rule \ref{intro:natural-language-rule-1}, it is easy to infer the price of copper will rise. Further, we can infer the price of the household appliances, such as air conditioners and refrigerators, will also rise via above rule \ref{intro:natural-language-rule-2}.

However, it is a daunting task for humans, especially financial traders, to learn the overwhelming number of such kind of rules and reason based on them in a real-time manner. Therefore, we hope machines can learn these rules automatically and reason quickly. 
In order to achieve this goal, we face two challenges: how to represent causal knowledge in a machine-actionable way and how to automatically acquire a large amount of this type of causal knowledge. We will explain these two aspects separately.
	
\subsection{Causal Knowledge Representation and Reasoning}
To get some inspirations on how to represent causal knowledge, we first review some previous causal knowledge representation schemes. Currently, there are two general directions towards causal knowledge representation. 

One direction is the \textbf{\textit{Neural or numeric}} form. Such neural form scheme can represent both causal knowledge and non-causal knowledge in the knowledge graph by a unified graph-embedding method \cite{Li2016a,Bordes}. However, it has problems with interpretability and reusability, which means that new data requires complete training from scratch.
The other direction is the \textbf{\textit{symbolic}} form, which also includes two directions. One is the specific description of causal knowledge. In this way, both the cause and the effect of the causal knowledge are specific events or actions, expressed by terms, phrase or sentence, such as CausalNet \cite{Luo2016a}, ConceptNet \cite{Speer2016}, and BECauSE 2.0 \cite{Dunietz2017}. However, these representation schemes do not have abstractive structured events, which are crucial for inferring unseen events. Besides, this kind of representation scheme usually lacks expressiveness, which, specifically, can be defined as generality and concreteness. Here, generality means the scope described clearly by the representation \cite{oei121992meta} and concreteness means how clearly and completely the event can be described by the representation \cite{wand1993ontological}.
Moreover, representing the event in the causal knowledge with sentences such as BECauSE 2.0 and ATOMIC \cite{sap2018atomic} is hard for the machine to process, since machines are keen on structured data.
The other direction is towards the general description of causal knowledge. In this manner, both the cause and the effect of the causal knowledge are abstract events, expressed by structures with abstract concepts, such as the frame in FrameNet \cite{Baker1997} or a pair of abstract words \cite{Zhao2017}. However, both of them are too general to figure out the events within the causal knowledge clearly. 
%they are not concrete enough to express the events clearly, thus lacking expressive power, and FrameNet requires a pre-defined and hand-crafted ontology, which is not suitable for universal knowledge representation.
Besides, all the above representation methods lack two crucial components: constraint and confidence score, which are critical to reasoning. The detailed drawbacks of current representation method will be demonstrated in Section \ref{sec:related} and Section \ref{sec:experiment}. \TD{mention this in later sections} 
To tackle these flaws mentioned above, we propose a novel and powerful representation scheme with logical form, which contains abstractive open-world structured events, constraints, and confidence. Here, we use the following logic rule (1), equivalent to above natural language rule \ref{intro:natural-language-rule-1}, as an example to illustrate how this idea evolves. 

上涨\_1/rise\_1(Z, 价格/price, `', `'):-遭受\_1/suffer\_1(`', X, Y, 袭击/attack), isA(X, 国家/country), isA(Y, 自然灾害/disaster), isA(Z, 金属/metal), atLocation(Z, X) conf:0.842 \ (1)

\textit{First,} we try to use the tuple structure to represent the events in the cause part and effect part of the causality. A simple and effective event representation is the structured form \textit{Predicate(Subject, Object)} as used in \cite{ding2015deep}, also known as SPO. To capture richer event information, we extend the SPO form to \textit{Predicate(Modifiers of Subj, Subj, Modifiers of Obj, Obj)}, such as 上涨\_1/rise\_1(铜/copper, 价格/price, `', `'). It can not be done simply in SPO form by 上涨\_1/rise\_1(`铜价格/the price of copper', `') since it is hard for the machine to automatically manipulate this specific event into abstractive event. For example, It is hard to recognize and convert the 铜/copper into 金属/metal to generalize this event into 上涨\_1/rise\_1(`金属价格/the price of metal', `') .
We call this event pair a rule instance, see (2) (following the convention of Prolog \cite{Wielemaker2010}, we put the cause part in the head position).

上涨\_1/rise\_1(铜/copper, 价格/price, `', `'):- 遭受\_1 /suffer\_1(`', 智利/Chile, 地震/earthquake, 袭击/attack)\ (2)

\textit{Second,} using such a specific rule instance (2) to represent causal knowledge is not general and lacks the ability to infer the unseen since it is likely that some specific causal event pairs that we can recognize in this world are not written in the
corpus, no matter its size \cite{ha2015gener}. Instead, we use the abstractive rule (3).

上涨\_1/rise\_1(Z, 价格/price, `', `'):-遭受\_1/suffer\_1(`', X, Y, 袭击/attack), isA(X, 国家/country), isA(Y, 自然灾害/disaster), isA(Z, 金属/metal)\ \ (3)

\textit{Third,} reasoning using rule (3) without some constraints may result in some errors. For example, we can get one inferred result from rule (3): 上涨\_1/rise\_1(铁/steel, 价格/price, `', `'):- 遭受\_1/suffer\_1(`', 智利/Chile, 地震/earthquake, 袭击/attack). This is unreasonable because Chile is a large copper producer rather than a large steel producer. Therefore, some constraints are also in our causal representation scheme, such as atLocation(Z, X), to exclude unreasonable inference.

\textit{Last,} after previous steps, we assign a confidence value to the rule to enable reasoning with uncertainty. Finally, we get rule (1).

\textbf{\textit{To sum up}}, 
the designed causal knowledge representation scheme is novel and powerful, which contains three key components: abstractive open-world structured events, constraints, and confidence.
%the knowledge representation scheme we proposed is more expressive and friendly for machines to reason. 
%Moreover, it is suitable for open-domain knowledge representation.
%Moreover, it can be easily extended to the complex conjunction of multi-cause events by adding multiple cause events to the header of the rule. 
%To the best of our knowledge, this is the first work about studying uncertain causal reasoning with the logical rule based on abstract events. 

\subsection{Causal Knowledge Acquisition}
\label{intro:Causal_Knowledge_Mining}
After developing a machine-actionable causal knowledge representation scheme with the logic rule, we need to tackle the second challenge that how to automatically obtain a large number of rules.  
WWW (World Wide Web) is a large treasure trove of knowledge, but most of the content is unstructured and noisy.
Rule learning has been studied extensively in Inductive Logic Programming (ILP) \cite{Quinlan1990,Muggleton1997}. However, the rule instances extracted from Web text are noisy and incomplete, with explosive and emerging concepts and relations, which makes these systems weak for large-scale Web knowledge acquisition. Moreover, the lack of negative examples cannot make the Closed World Assumption typically made by ILP systems. Finally, we also face the challenge of the scarcity of Chinese resources, since our dataset are online financial news in Chinese. Instead, this paper presents a new ILP system with the Open World Assumption, which adopts a bottom-up approach with two stages:

\textbf{1) From unstructured text to structured rule instances} We first derive the sentences with causal relation through the designed patterns. Then, we use event extraction technique to extract the structured rule instances, such as above (2).
	
\textbf{2) From specific rule instances to general rules} With a large number of rule instances, we generalize them into rules, such as above (1), while balancing generality and specificity:
	
\textbf{Generality} We try to represent given rule instances semantically with as few general rules as possible under the help of Probase \cite{Wu2012a}. For example, we can generalize two rule instances fall\_1(corn, price, `', `'):-rise\_1(corn, yield, `', `'), fall\_1(soybean, price, `', `'):-rise\_1(soybean, yield, `', `') into the rule fall\_1(X, price, `', `') :- rise\_1(X, yield, `', `'), isA(X, food).


\textbf{Specificity} Overgeneralized  rules may lead to errors. For example, generalizing above two rule instances into the rule fall\_1(X, price, `', `'):-rise\_1(X, yield, `', `'), isA(X, thing) is unreasonable, because when the machine instantiates thing into air, the inferred events ``the yield or price of air rises" do not make sense.

Generalizing rule instances to rules can be seen as a semantic compression procedure, which accords with the MDL principle \cite{rissanen1978mdl}. In our case, hypotheses (H) accord with rules and data (D) accords with rule instances. Thus, we propose a rule induction algorithm, which takes MDL as the evaluation criterion and regards this rule induction procedure as an optimization problem, see details in Section \ref{sec:approach}. This procedure is similar to \cite{Cui2016} and \cite{Zhu}. Finally, each learned rule will be automatically assigned a confidence value by considering some heuristics.
	
\textbf{Contributions}
In this paper, we present a full stack solution for causal knowledge representation, acquisition, and reasoning. More precisely, our contributions are as follows:
First, we design a novel and powerful causal knowledge representation scheme based on the logic rule , which contains abstractive open-world structured events, constraints, and confidence.
Second, we propose a rule learning framework for obtaining rules from large unstructured text.
Third, extensive experiments show that the rules learned are reasonable and effective. 
Specifically, we learned 41,695 rules and humans' evaluation shows that the top 10,000 rules of the highest confidence reach 65.0\%(good), 26.0\%(fair), 9.0\%(bad). Last, we release the rules, \zhpro and \zhcon and provide an interactive demo to show the reasoning process.
%and the application of futures price triggering. \TD{second and last repeat?}
	
The remainder of the paper is organized as follows. Our proposed rule learning and reasoning approach are presented in Section \ref{sec:approach}. Section \ref{sec:experiment} reports the experimental observations. An executable reasoning example is demonstrated in Section \ref{sec:experiment} followed by a review of related works in Section \ref{sec:related} and a brief conclusion in Section \ref{sec:conclusion}.

%	\section{Problem Definition}
%	\label{sec:problem}
%	
%	In this section, we will formally state the focal problem to be solved.
%	\begin{equation}
%	\begin{split}
%	&p_e('', X_1, no_e, o_e):-p_c(X_0, s_c, no_c, o_c)\\
%	&isA(X_0, c_1),isA(X_1, c_3)\\
%	&relation(X0,X1) 
%	\end{split}
%	\label{equ:rule_notation}
%	\end{equation}
	%The list of major symbols and notations in this paper is summarized in the following table.
	
	%\begin{table}[htbp]
	%	\caption{Table of notations }
	%	\begin{center}
	%		\begin{tabular}{|c|l|}
	%			\hline
	%			\textbf{Notation}&\textbf{Definition}\\
	%			\hline
	%			rule instance & structured causality proposition pair\\
	%			\hline
	%			$E_{c_i}$ & the instances of $i-th$ concept \\
	%			\hline
	%			\multirow{2}{*}{rule}
	%			& generalized rule instance with concept constraints \\
	%			&and relation constraints \\
	%			\hline
	%		\end{tabular}
	%		\label{tab1}
	%	\end{center}
%	\end{table}	
	
	%our goal:
	%	We try to make reasoning interpretable via symbol not neural network. 
	% 
%	\textbf{Problem Statement.} Mining the causality knowledge in the form of first-order logic rule \ref{equ:rule_notation} from large online free text.