\section{Evaluation}
In this section we evaluate our model. Although we only implement our model on Java language, readers can also use our model to mine other programming language code.
\subsection{Data Set}
We download five open source code repositories from GitHub, which are guice, jna, libgdx, zxing adn cocos2d. Then we select all functions in the repository except the constructor function of every file. Because there may be many constructor functions in one file and the name of these constructor functions are the same of their own class which means that constructor functions' name can't represent the subject of this function. And we split every function's name into many individual English word and make these words as the groundtruth tags of this function. After splitting, we can get many $(T,C)$ pairs, $T$ is the sequence of words that split from function's name and $C$ is the corresponded internal codes. For example, we have a function XXX as shown in Fig.4, then we can get the pair as shown in Fig.5.

For evaluation, we will choose 100 functions from repository into evaluation data set. Then we sample other 49 function's name as distractor tags for every evaluation function. And because we split all functions' name into individual words, every function may have almost 100 tags as its candidate tags. Our evaluation task is to retrieve the true tags(true tags are split from function's own name) from all candidate tags.
\subsection{Baseline}
To show the effectiveness of our model, we also construct two kinds of baseline. These two baselines are all based on bags of words from source code and don't utilize any structural information such as parse tree and so on.
\subsubsection{TF-IDF}

\subsubsection{Topic Model}

\subsection{Result}

\subsubsection{Accuracy}

\subsubsection{MRR}
