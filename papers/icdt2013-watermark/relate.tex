\section{RELATED WORK}
\label{sec:related}
%\KZ{Divide all the papers into a few categories or types, discuss them separately,
%then summary the work within a categorie with its pros and cons and compare with
%our work. More focus should be given to the two methods that we compared with in
%the evaluation.}
In recent decades many efforts have been made to protect the copyrights of 
multimedia products such as images, movies and music \cite{BoneyTH96,ChangA01}. 
Watermarking techniques, 
which helps the IP owner to declare the rightful ownership of a product, 
have attracted researchers' interests. 
Multimedia data usually includes redundant information with respect to 
the human perceptual system. Thus, much bandwidth is available in these 
datasets to hide small pieces of extra information, or watermarks. 
A wide variety of watermarking techniques 
have since been developed in many of domains. 
Digital road maps is a special vector graphics data which contains important
geographical and spatial information. In the rest of this section, we will
focus on GIS spatial data watermarking but will also briefly touch on related
techniques in some other domains.
The technique proposed in this paper benefits from or is inspired 
by some of these methods.

\subsection{GIS Spatial Data Watermarking}
Several studies have touched on the watermarking of GIS spatial data such as digital 
vector maps. Even though digital vector maps are represented by node coordinates which 
can also be treated as relational tables, techniques developed for relational data 
ignore the spatial properties of road maps such as spatial distances and geo-localities.
These distinct features of GIS spatial data call for special treatments in watermarking
techniques. According to Niu \cite{Niu06:Survey}, the existing methods of 
digital vector map watermarking is classified into two categories: 
algorithms in spatial domain and algorithms in frequency domain. 
Depending on whether to require the original map 
for detection, these methods are also classified as the not blind and the blind.
In addition, algorithms can also be categorized into {\em global} or {\em local}
depending on whether global information of the map is needed when computing each
watermark.

The spatial domain algorithms embed watermarks based on the geometric properties of 
polyline and polygon objects. It is often easier to control the amount of distortion
added by the watermarks. They are also more robust against rotation, scaling 
and noises, while preserving the utility of a map. 
Existing methods in spatial domain usually lack of robustness against massive cropping and merging. 
%\KZ{I don't really understand the above sentence. Please elaborate. Also say
%how our method avoided these drawbacks.}
The method proposed in this paper falls into the category of spatial domain algorithms. 

The frequency domain algorithms essentially transform the original data into
frequency domain, usually by Discrete Fourier Transform (DFT) 
\cite{Kitamura01:DFT,DFT,Illustration} or Discrete Wavelet Transform (DWT)
\cite{Liyuanyuan03}, add noises in the frequency domain and then transform it
back into the spatial domain.  
The critical drawbacks of frequency domain techniques are the 
difficulty of controlling the amount of distortion in the spatial domain
and the lack of robustness against data cropping and data reordering, due to
the fact that it is actually a global watermarking scheme. 
 %\KZ{Check the above to see if it's correct. Also we didn't mention
%local algorithms. Maybe mention a bit here also?} 

\subsubsection{Non-Blind Methods in Spatial Domain}
Some watermarking methods \cite{Han06:Adaptive,OhbuchiUE02,OhbuchiUE03} 
require the original map or watermarked map as a reference to detect watermarks 
from a suspicious map. This kind of methods are called ``non-blind'' algorithms.
The main weakness of these methods is that road maps can be updated over time
and a map producer may have many versions of the same map in its database.
Moreover in the same database, there could be many maps which have overlapping
regions, say map of the Twin Cities, map of Minnesota, map of the United States, etc.
When the map database is big, and when the suspicious map could have been
subjected to cropping and merging, it is not easy to identify the original map.
%which are not realistic because it is very time-consuming to find 
%a corresponding digital map from a map database.
%An improved adaptive watermarking algorithm\cite{Han06:Adaptive} subdivides the map into some 
%uniform size rectangular blocks and insert the watermark into each block. Before extraction,
%it does not compare the the watermarked map with the primitive map, but the original 
%watermarked map. This is not a blind watermarking algorithm. 

Ohbuchi et al.\cite{OhbuchiUE02} partitions the space of the digital map 
into rectangles such that every rectangle contains almost equal node numbers. 
Then the $v^{th}$ vertex inside 
the rectangle is modified to include a watermark. In another method \cite{OhbuchiUE03}, 
all vertices are connected into a single mesh by Delaunay triangulation. 
Then a mesh Laplacian is formed and the mesh is partitioned into patches using 
the same method as in the former one\cite{OhbuchiUE02}. 
Mesh-spectral coefficients are calculated for every patch and the watermark is embedded into 
these coefficients. 
%\KZ{rephrase the above sentence.} 

Both of the above algorithms disperse watermarks locally and have 
some robustness against crop attack. 
However, them are highly dependent on the validity of the original data. 
When extracting the watermark, the suspicious map is aligned onto the original map. 
All inserted nodes have to be deleted, and all removed nodes have to be correctly 
recovered.

\subsubsection{Blind Methods in Spatial Domain}
Some other watermark framework are proposed which required no original or watermarked map
as a reference. Some of these methods \cite{Huo10:Polyline,KangKC01,YanLW11,Spr:scheme} 
insert watermark globally. 
%The work by S. Khanna \cite{KhannaZ00} watermarks 
%digital road maps by altering the shortest path with smallest node numbers. 
%The assumption is that the shortest path will not be affected by attacks. 
%\KZ{Shortest path between which two points?}
Yan et al. proposed a key point based algorithm \cite{YanLW11}. 
Key points are those nodes in the map which have more important geometric aspects 
than the other ones, for example cross joints and those in sharp curve. 
They are not likely to be removed or edited by the attackers because removing them
will render the map useless.  This method uses a dynamic programming 
algorithm \cite{DOUGLASD.H.:1973} both in insertion and detection steps. 
Key points in the map are used for inserting and detecting watermarks. 
This kind of algorithms are robust against some attacks like noise and vertex simplification, 
but are still vulnerable to crop and merge attacks.  %\KZ{If it's a local algorithm, why can't it
%detect the remaining watermarks?} 
% \KZ{Don't understand the above method.}

%The scheme for polyline \cite{Huo10:Polyline} 
%proposed an algorithm to calculate the length and perimeter of the polylines and polygons in 
%a map and cluster polylines polygons to some group with the uniform step of length and 
%perimeter dynamic range. And embed the watermark bit to local mean of lengh/perimeter of 
%polylines/polygons in a suitable group. Then embed the watermark bit to the points coordinates. 
%In this approach, the group is clustered by length/perimeter of the objects and these objects 
%is probability located in any position of the map. 

Other blind watermarking algorithms
\cite{Vogit:2003,Schulz:2004,PuDJ06,Bird09:Shape,Kim11:copyright} 
provide some limited resistance against crop attacks to some extent. 
Voigt et al.\cite{Vogit:2003} 
proposed a feature based watermarking algorithm which is relies on statistical detection. 
This method partitions a map into small rectangular regions called ``patches''. 
It then randomly selects two subsets of the patches called set $A$ and set $B$,
respectively. Next, it calculates a reference point for each patch in the two set.
During watermark insertion, all nodes in set $A$ are shifted {\em toward} the reference point
in their respective patches, while all nodes in set $B$ are shifted {\em away} from
the reference point. The amount of shift is governed by an $F$-distribution 
\cite{abramowitz1965handbook}
%\KZ{Give a citation here cos not everybody knows what it is.}
which is also used to detect the watermark.
%ware relative coordinates against the bottom-left corner of each patch.
%For each set, it computes a relative reference point
%inside the patches to a relative value w.r.t. some original vertices inside the patches. 
%During insertion of watermarks, the distance of vertice to a 
%reference line in one patch list is increased while the distance to the line in
%the other patch list decreased. The ratio of standard error of point
%corrdinates in two patch list follows a $F$ distribution, 
%which is used to decide whether the map is watermarked. 
%This algorithm is robust against several attacks, including cropping to some extent.  
%Another high capacity robust 
%scheme\cite{Schulz:2004} is also proposed . They divide the original map into many vertical 
%or horizontal strips with a certain width. The vertices belonging to each strip are shifted 
%to special locations to represent bit 1 or 0. The scheme has the robustness to additional 
%noises within torlerance and map shifting, and to some extent map cropping. However, this 
%method almost changed all points of a map, which will introduce too much distortion.
Pu et al. proposed a blind algorithm \cite{PuDJ06} which 
divides the map into mesh segments, and then
embeds the watermark in each segment with a fixed order. 
In the detection step, each segment is evaluated by a correlation parameter, which
is a linear correlation of the watermark and the watermarked data. 
%\KZ{Elaborate a bit on the correlation param?}
To survive crop attack, a global correlation based detection method is 
applied as an optimization. This algorithm watermarks maps according to 
their topological relations. 

While these algorithms present some resistance against cropping attack, 
they cannot survive cropping at larger scale.
Take Voigt\cite{Vogit:2003} as an example, when the watermarked map is 
massively cropped, many patches that are marked may be
removed. Thus $F$ distribution of watermarked points may be destroyed. 
Therefore, the method is not robust enough against ``massive crop''. 
%\KZ{Above is repetition of the earlier discussion but it's
%still not clear why removing ``patches'' causes the algo to fail.} 
Furthermore, if the map is cropped and then merged with others, 
the information added by the merge will defeat the watermarking more easily.

\subsubsection{Methods in Frequency Domain}
 
Solachidis et al.\cite{DFT} proposed a blind watermarking scheme embedding 
a single bit into a polyline by modifying the discrete Fourier coefficients 
of polyline’s coordinate sequence.  This method embeds the watermark in 
the magnitude of the curve’s Fourier descriptors to exploit 
its location, scale, and rotation invariant properties. Due to the amplitude frequency 
features of discrete Fourier transform, the algorithm is inherently robust to many attacks 
such as map translation, rotation, scaling and changing start vertex. Similar to the DFT 
method\cite{DFT}, Li et al. proposed a blind scheme \cite{Liyuanyuan03} embedding 
multi-bits into a vector map in DWT domain. 
These frequency domain methods rely on the integrity
of map and order of data points. When part of map data is removed or the order 
of data points is changed, the coefficients will 
also be changed, which makes them extremely vulnerable to crop 
and merge attacks.
%\KZ{Don't understand the above sentence, rephrase?}


\subsection{Watermarking in Other Domains}
%Watermarking techniques are widely used to protect the copyrights of media 
%such as image, video, and audio data . 
Because of their success in protecting multimedia data, 
watermarking techniques have been extended to other
fields. Text/Language watermarking has only low bandwidth to include watermark 
and at the same time must provide acceptable level of resilience to different attacks. 
A natural language watermarking technique \cite{NLW} inserts watermarking by 
constructing a text meaning representation tree.

Some research attention has been given to relational data\cite{AgrawalK02,SionAP03}. 
Rakesh Agrawal proposes a framework of watermarking relational databases\cite{AgrawalK02}. 
According to this approach, a primary key is stored in some significant 
tuples of relational data. The altered attribute index and bit 
index for the selected attribute are randomly 
selected according to secret keys. Their approach is highly dependent on the primary key 
of the data tuple and randomly selects a subset of tuples to watermark. This approach in 
fact disperses one bit watermark into different position of relational data. It provide 
some resistance against crop and merge attack to some extent. 
Some ideas in this framework actually inspired the work in this paper, e.g.,
modifying least significant bits and computing the confidence of detection.
However, this framework is for relational data and hence does not consider 
the special properties of spatial attributes in GIS spatial data. In our method,
information which is adjacent to each other in geographical space collaborate to
decide the watermarking positions. However, in relational data, different tuples
are completely independent to each other, even though they are stored together,
because they form a set.
%\KZ{You discussed many details of the above algo but not enough discussion
%about the connection with our method, most importantly the key differences
%between the two (other than that it was for relation data. Any other
%differences?}
Other work\cite{SionAP03} extends the framework\cite{SionAP02} 
to relational databases. In that paper, numeric dataset are first hashed 
to another dataset with a secret order.  Then the new dataset is grouped 
into different chunks according to their order. 
Finally, the average value and standard deviation is calculated for every chunk. 
According to these statistics and watermarking data, 
a small number of data points are altered. 
This approach still disperses the watermark globally. 
One of the serious drawbacks for this global watermarking approach is that 
it completely fails a "massive crop" attack.

