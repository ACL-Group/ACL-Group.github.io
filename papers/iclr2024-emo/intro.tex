\section{Introduction}
The dominant paradigm of natural language generation systems hinges on probabilistic neural language models~\citep{gpt2,opt}, which permit evaluating the probability of any given text sequence as well as generating novel ones using various decoding strategies upon learned distributions~\citep{topp,meister2023locally}.
Language modeling, the process of aligning model distribution with that of human language, is usually formulated as a sequence prediction task in which maximum likelihood estimation~(MLE) is typically adopted as the training objective owing to its simplicity and intuitiveness.

However, various text degeneration phenomena with incoherent and nonsensical~\citep{lebrun2021evaluating,topp} content are still widely observed in text generated from language models pre-trained on massive amounts of human data. This indicates that the model distribution $Q_{\theta}$~(parametrized by $\theta$) 
learned by MLE still differs substantially from the human language distribution $P$, despite having a seemingly low training loss~\citep{meister2023locally}. From a distributional view, training with MLE is equivalent to minimizing the \textit{forward cross-entropy} between $P$ and $Q_\theta$:
\begin{equation}
    \text{CE}(P, Q_{\theta})=-\mathbb{E}_{x\sim P}[\log{Q_{\theta}(x)}]
\end{equation}
We argue that the forward cross-entropy has inherent limitations as a metric for matching model distribution and that of human language. Firstly, forward cross-entropy is recall-prioritized~\citep{meister-etal-2023-efficacy}. At each time step, it focuses exclusively on increasing the model likelihood of the ground-truth next token. This can result in poor precision of the learned model distribution when training data is noisy or slow convergence even when sufficient amounts of high-quality text corpus are available. Secondly, when used in language model pre-training, forward cross-entropy faces the negative diversity ignorance issue~\citep{li2019data} where all non-ground-truth next tokens are deemed as equally incorrect. However, some tokens might be less incorrect or even plausible alternatives to the ground truth than other 
tokens. Capturing these latent negative diversity can assist language models in enhancing their modeling of the human language distribution. Thirdly, the form of forward cross-entropy is inconsistent with how language models are evaluated~\citep{pang2020text}. Such a train-test objective mismatch makes MLE a less reliable indicator of modeling quality.

To alleviate the aforementioned limitations of MLE, we direct our attention towards an alternative distance metric, namely the Earth Mover Distance~(EMD)~\citep{kantorovich1960mathematical}. EMD is initially discussed in the context of optimal transport problem~\citep{oldandnew} 
and then incorporated as a distance metric for implicit generative modeling, e.g., WGAN~\citep{wgan} and WAE~\citep{wae}. 
The appeal of EMD lies in (1) it takes into account both precision and recall during modeling; (2) it acknowledges the varying degrees of correctness in data samples, enabling more nuanced training signals. (3) its mathematical formulation permits better consistency between the training and testing phases. Given these properties, we incorporate EMD as a better token-level probability distance measure into language modeling. However, computing the exact value of EMD requires external solvers that are 
detached from the computation graph and block gradient back-propagation. We overcome this issue by developing an differentiable upper bound of EMD~(DEMD) that can be optimized in an end-to-end manner without resorting to external specialized solvers. Combined with a semantically informed transport cost function, we present EMO (\textbf{E}arth \textbf{M}over Distance \textbf{O}ptimization) for training auto-regressive language models.

We first evaluate the effectiveness of the proposed method on the task of language modeling across diverse domains and show that EMO yields generations with significantly higher distributional closeness~(6.2 points on average measured by MAUVE) with human text. We further demonstrate that, by applying EMO in a lightweight fine-tuning stage using several orders of magnitude fewer tokens than pre-training, pre-trained LLMs' performance on a range of downstream language understanding tasks can be significantly boosted, e.g., an average improvement of 4 points across 8 datasets. By progressively increasing the volume of data utilized for continual fine-tuning, EMO also demonstrates superior scaling properties compared to existing methods.
