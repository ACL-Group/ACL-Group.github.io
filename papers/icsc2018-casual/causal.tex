\documentclass[conference]{IEEEtran}

\usepackage{rotating}
\usepackage{color}
\usepackage{epsfig,subcaption, multicol,multirow}
\usepackage{epstopdf}
\usepackage{amsmath,algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{url}
\usepackage{lingmacros,framed}
\usepackage{setspace}

\newcommand{\bi}[1]{\textbf{\textit{#1}}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}

%\newenvironment{sequation}  
%{\small\begin{quation}}  
%	{\end{equation}}  

\newenvironment{sequation}  
{\small\begin{equation}}  
{\end{equation}}  

\newcounter{amsthm}

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Word Embedding for Commonsense Causal Reasoning}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{\IEEEauthorblockN{Yuchen Sha}
%\IEEEauthorblockA{School of Electronic Informationl and Electrical Engineering\\
%Shanghai Jiao Tong University, 
%Shanghai, China\\
%Email: sycbelief@sjtu.edu.cn}}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%\boldmath
This paper is a preliminary attempt to represent words of two roles (cause and effect) using vectors, in which way we can measure the amount of causality between words. Beside, vectors can be used for high level application such as sentence representation. We modify the idea of word2vec and generate two vector matrixes for the two roles of words. In the cause-effect space we built, it is easy to track the most possible effect word given cause word and vice versa. 
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
Commonsense causality is the causality between actions or events that is acknowledged by human beings. Commonsense is one thing that shared by nearly all people. It is embraced in large amount of text corpus but hard to mine out because of its sparsity. Commonsense causal reasoning aims to find out the possible causality between events, measuring whether one event can lead to another.
Current work on this area can be summarized into two categories. One is manual annotation such as ConceptNet \cite{conceptnet}. Knowledge using these methods is accurate but quite expensive and rare. It can hardly be used in large scale domain reasoning tasks. The other one is data-driven method, which tries to capture the statistical features between causal events \cite{kr2016}. These methods give a strength or score for each causal pair but can only be applied on word-level or event-level applications. We name the amout of causality between events as causal strength in this paper.

Our method belongs to the second category. However, we are the first to generate the vector representations of cause/effect role for each word and those vectors can be used to not only calculate the word-level scores directly but also use for further sentence-level reasoning. For a word $w$, we believe that it plays a cause(effect) role when it appears in a causal(effect) span in one cause-effect pair. 
For example:
\begin{itemize}
	\item[(1)] In 1998 the Pont du Gard was hit by major \textbf{flooding} which caused widespread damage in the area.
	\item[(2)] Rainfall is occurring as heavy downpours that cause \textbf{flooding}.
\end{itemize}
In the first sentence, ``flooding'' plays a cause role while in the second sentence it plays an effect role. Therefore, we propose two vectors, $\overrightarrow{c_w}$ and $\overrightarrow{e_w}$ to represent word $w$ for the two roles.
Our main contributions are:
\begin{itemize}
	\item We are the first to represent one word with two vectors of different causal roles (cause and effect).
	\item Word vectors can be used to calculate causal strength directly or applied to further applications.
	\item The dataset we generated shows good reasoning ability.
\end{itemize}

\section{Approach}
In this section, we describe our model, which generates word vectors of cause and effect role simultaneously. 

\textbf{Causal-bias Corpus Extraction} We first extract sentences using causal cues following the method provided by Luo et al. 2016~\cite{kr2016}.  One such sentence with $m+n+k$ words where $m$ is the size of cause span, $n$ is the size of effect span and $k$ is the size of causal cue, is denoted as $S = \langle w_1, ..., w_m, w_1, \dots, w_k, w_1, \dots, w_n\rangle$. Causal cue splits the sentence into cause and effect spans, which consists of the causal-bias span pairs. For sentences in Section 1, ``cause(d)'' is the causal cue which splits the sentence as follows:\\
 \underline{In 1998 the Pont du Gard was hit by major flooding which} [\emph{cause span}] {\bf caused} [\emph{cue}] \underline{widespread damage in the area.} [\emph{effect span}]\\
\underline{Rainfall is occurring as heavy downpours that} [\emph{cause span}] {\bf cause} [\emph{cue}] \underline{flooding.} [\emph{effect span}]

\textbf{Vector Representing} We modify the idea of word2vec \cite{word2vec}, and use cause span to predict effect span and vice versa. For word $w_i$, vector for cause role is denoted as $\overrightarrow{c_i}$ and vector for effect role is denoted as $\overrightarrow{e_i}$. For the causal-bias span pairs, we use $C_{1 \sim m}$  and $E_{1 \sim n}$  to represent the context of cause and effect span respectively. And the corresponding vector representations are $\overrightarrow{C_{1 \sim m}}$ and  $\overrightarrow{E_{1 \sim n}}$ where 

\begin{spacing}{0.6}
\begin{sequation}
	\overrightarrow{C_{1 \sim m}} = \frac{1}{m} \sum_{i=1}^m{\overrightarrow{c_i}}, \quad
	\overrightarrow{E_{1 \sim n}} = \frac{1}{n} \sum_{i=1}^n{\overrightarrow{e_i}}
\end{sequation}
\end{spacing}
As introduced in Luo et al.(2016)~\cite{kr2016}, causality between events appear in both sufficiency and necessity ways. In the example (Section 1), causal pair (\emph{flooding}, \emph{damage}) encodes more of sufficiency causality because the effect \emph{damage} can happen by not only making \emph{flooding} as its cause. Similarly, causal pair (\emph{rainfall}, \emph{flooding}) encodes more of necessity causality.
Consider the possibility of sentence $S$ being one contains causality given causal or effect span, we estimate the probability from both sufficiency and necessity sides. To simplify the problem, we suppose that all words of the other span are generated conditional independently. Thus, we have:
\begin{spacing}{0.6}
\begin{sequation}
\begin{aligned}
&P_{nec}(S) = P(c_1, c_2, ..., c_m|E_{1 \sim n}) = \prod_{i=1}^{m}{P(c_i|E_{1 \sim n})}  \\
&P_{suf}(S) = P(e_1, e_2, ..., e_n|C_{1 \sim m}) = \prod_{j=1}^{n}{P(e_j|C_{1 \sim m})} \\
\end{aligned}
\end{sequation}
\end{spacing}

Modify the idea of CBOW(Continuous Bag Of Words) in word2vec and the negative sampling algorithm, we define the objective function as follows:

\begin{spacing}{0.6}
\begin{sequation}
\begin{aligned}
l_{suf} &= -\log P_{suf}(S) = - \sum_{j=1}^{n}{\log P(e_j|C_{1 \sim m})} \\
& \quad-\sum_{j=1}^{n}{[\log\sigma(\overrightarrow{e_j} \cdot \overrightarrow{C_{1 \sim m}})} + \sum_{k=1}^{K}{\log(1 - \sigma(\overrightarrow{e_k} \cdot \overrightarrow{C_{1 \sim m}}))}]\\	
l_{nec} &= -\log P_{nec}(S) = -\sum_{i=1}^{m}{\log P(c_i|E_{1 \sim n})} \\
&= -\sum_{i=1}^{m}{[\log\sigma(\overrightarrow{c_i} \cdot \overrightarrow{E_{1 \sim n}})} + \sum_{k=1}^{K}{\log(1 - \sigma(\overrightarrow{c_k} \cdot \overrightarrow{E_{1 \sim n}}))}] \\
\end{aligned}
\end{sequation}
\end{spacing}

Since we have two loss functions towarding cause or effect span as premise, we finally get two sets of word matrixes, one of which pays more attention to the cause role while the other concentrates on the effect role. We define them as sufficiency set $(W_c, W_{eneg})$ and necessity set $(W_{cneg}, W_e)$ where each of them models the sufficiency and necessity of causality respectively.
Besides, we define the causal strength as the cosine similarity between word vectors of two roles.

% \begin{spacing}{0.6}
% \begin{sequation}
% \begin{aligned}
% &CS_{suf}(w_i, w_j) = \frac{\overrightarrow{c_i} \cdot \overrightarrow{e_j}}{||\overrightarrow{c_i}||_2 ||\overrightarrow{e_j}||_2} \quad \overrightarrow{c_i} \in W_c  , \overrightarrow{e_j} \in W_{eneg} \\
% &CS_{nec}(w_i, w_j) = \frac{\overrightarrow{c_i} \cdot \overrightarrow{e_j}}{||\overrightarrow{c_i}||_2 ||\overrightarrow{e_j}||_2} \quad \overrightarrow{c_i} \in W_{cneg}  , \overrightarrow{e_j} \in W_{e}
% \end{aligned}
% \end{sequation}
% \end{spacing}

\section{Experiments}
We follow the method mentioned by Luo et al. \cite{kr2016}, and extract around 45 billion sentences (nearly 4.5G, 530 million words) from web snapshot containing causality knowledge. 

To show the effectiveness of our method, we sample several words to track its top effects or causes. The result is shown in \tabref{table2} compared with the results of Luo et al. \cite{kr2016}. 
Most of our results are reasonable, but it seems weird for pairs like (\emph{rainfall}, \emph{drought}) and (\emph{accident}, \emph{fatal}). An explanation is that rainfall usually ease the drought situation and accident often leads to fatal outcomes. It could be a little bit hard to understand these cases since we only output words rather than phrases, so the full meaning in the original sentence is cut off.
As for the dataset of Luo et al., most of their high-rank results are rare words. It is because when calculating the score between words, rare words suffer less punishment and thus pop up fast, which is a disadvantage of their work.

\begin{table}[!ht]
	\caption{Sample of causal pairs}
	\label{table2}
	\centering
	\resizebox {0.5\textwidth}{!}{
	\begin{tabular}{|l|l|l|}
		\hline 
		Cauase & Top Effects of Embedding& Top Effects of Luo et al. \cite{kr2016} \\
		\hline
		rainfall & flooding, flood, drought& roundheaded, litchee, epipactis\\
		accident & fatality, die, death, fatal& distrait, crippling, aviatrix\\
		virus & hepatitis, infected, viremia&  ecballium, hominoidea, discase\\
		drown & death, swim, suffocation& equaphobia, diminutiveness\\
		pregnant & abortion, fetus, baby&jotunn, pycnanthemum, lopid \\
		\hline 
		\hline
		Effect & Top Causes of Embedding& Top Causes of Luo et al. \cite{kr2016} \\
		\hline
		damage & magnitude-5, waterspout&reliance, tornade, vehicular\\
		happy & love, joy, wonderful, hope&offend, flag, reproach, goosy\\
		light & illumination, bright, lamp& air, refractive, phototropism\\
		hurt & feel, break, cheat, love& dacoity, screwing, padding\\
		die & complication, accident& complication, accident, sustain\\
		\hline
	\end{tabular}}
\end{table}

We also evaluate our results on COPA (Roemmele et al. 2011\cite{COPA}) which contains 1000 multiple-choice questions which ask for correct answer using commonsense causal reasoning.
We solve this problem in two directions.

\textbf{Word-level Causal Strength} We sum up the cosine similarity of each word pair combination, within which one word is from the premise and the other is from the alternative sentence:
\begin{spacing}{0.6}
\begin{sequation}
CS_{word}(S_1, S_2) = \frac{\sum_{c_i \in S_1, c_j \in S_2}{cosine(c_i, e_j)}}{|S_1|+|S_2|}
\end{sequation}
\end{spacing}

\textbf{Sentence-level Causal Strength} We firstly generate sentence vector by averaging the vectors of words in that sentence and then calculate the cosine similarity:
\begin{spacing}{0.6}
\begin{sequation}
CS_{sen}(S_1, S_2) = cosine(\frac{1}{|S_1|}\sum_{c_i \in S_1}{\overrightarrow{c_i}} , \frac{1}{|S_2|}{\sum_{e_j \in S_2}\overrightarrow{e_j}} )
\end{sequation}
\end{spacing}

In both directions, we calculate the causal strength combining the sufficiency and necessity together:
\begin{spacing}{0.6}
\begin{sequation}
CS(S_1, S_2) = \lambda CS_{suf}(S_1, S_2) + (1-\lambda) CS_{nec}(S_1, S_2)
\end{sequation}
\end{spacing}
We set $\lambda$ as $0.65$, which performs best in COPA task.
Our baseline method is using traditional word2vec in the same task. We also compare with Luo et al. on this task which is reported in \tabref{table1}. Our result outperforms the baseline but is worse than the state-of-the-art. It is clear that word2vec only encode the similarity information while our vectors encode the causal signal.
The score of word pair in Luo et al. works well because in COPA task, we fix one side of word pair and try to compare the causal strength changing the other side. Their dataset is good at choosing one out of two options while ours can recommend several words given one. Our system may be misleaded by the man-made differences but it also turns out that the result will get improved after training more iterations. 

\begin{table}[!ht]
	\caption{Result on COPA}
	\label{table1}
	\centering
	\begin{tabular}{|l|c|c|}
		\hline 
		Approach & Word-level Acc. & Sent-level Acc. \\
		\hline
		word2vec(baseline)  & 0.512 & 0.562\\
		\hline
		Luo \cite{kr2016} (state of art) & 0.702 & - \\
		\hline
		Suf.+Nec.+$\lambda$=0.65 & 0.604 & 0.596 \\
		\hline 
	\end{tabular}
\end{table}



\section{Future Work}
Our future work will focus on  a) better modeling sentence vectors. It is shown in experiments that sentence-level causal reasoning doesn't take full advantages. One well-designed DNN encoder may be a solution.  b) adding high-frequency phrase vector representations, since phrases may be able to describe more complete meaning than words.




% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{conceptnet}
Havasi C, Speer R, Arnold K C, et al. Open Mind Common Sense: Crowd-sourcing for Common Sense[C]//Collaboratively-Built Knowledge Sources and AI. 2010.

\bibitem{kr2016}
Luo Z, Sha Y, Zhu K Q, et al. Commonsense Causal Reasoning between Short Texts[C]//KR. 2016: 421-431.

\bibitem{COPA}
Roemmele M, Bejan C A, Gordon A S. Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning[C]//AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning. 2011: 90-95.

\bibitem{word2vec}
Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[C]//Advances in neural information processing systems. 2013: 3111-3119.


\end{thebibliography}




% that's all folks
\end{document}


