\section{Our Approach}

\subsection{Running Example}

Fig \ref{figure:sourceCodeExample} is two examples of source code method, and this method is from repository rhino and the source file's relative path is rhino/src/org/mozilla/javascript/IdScriptableObject.ja\\va.


%\KZ{\textcolor{red}{Use a code or algorithm environment for the source code. It looks too
%ugly now.}}


\begin{figure*}[!htp]
\center
% \footnotesize{
% public static void setBestExposure(Camera.Parameters parameters, boolean lightOn) \{
%
%  $~~~~$  int minExposure = parameters.getMinExposureCompensation();
%
%  $~~~~$  int maxExposure = parameters.getMaxExposureCompensation();
%
%  $~~~~$  float step = parameters.getExposureCompensationStep();
%
%  $~~~~$  if ((minExposure != 0 $||$ maxExposure != 0) \&\& step $>$ 0.0f) \{
%
%  $~~~~$ $~~~~$   \textcolor{blue}{ // Set low when light is on}
%
%  $~~~~$ $~~~~$   float targetCompensation = lightOn ? MIN\_EXPOSURE\_\\COMPENSATION : MAX\_EXPOSURE\_COMPENSATION;
%
%  $~~~~$$~~~~$    int compensationSteps = Math.round(targetCompensation / step);
%
%  $~~~~$$~~~~$    float actualCompensation = step * compensationSteps;
%
%   $~~~~$$~~~~$  \textcolor{blue}{ // Clamp value:}
%
%  $~~~~$$~~~~$    compensationSteps = Math.max(Math.min(compensationSteps, max\\Exposure), minExposure);
%
%   $~~~~$$~~~~$   if (parameters.getExposureCompensation() == compensationSteps)\{
%
%    $~~~~$$~~~~$$~~~~$    Log.i(TAG, "Exposure compensation already set to " + compen\\sationSteps + " / " + actualCompensation);
%
%    $~~~~$$~~~~$  \} else \{
%
%     $~~~~$$~~~~$$~~~~$   Log.i(TAG, "Setting exposure compensation to " + compensation\\Steps + " / " + actualCompensation);
%
%     $~~~~$$~~~~$$~~~~$   parameters.setExposureCompensation(compensationSteps);
%
%    $~~~~$$~~~~$  \}
%
%   $~~~~$ \} else \{
%
%   $~~~~$$~~~~$   Log.i(TAG, "Camera does not support exposure compensation");
%
%   $~~~~$  \}
%
%  \}
%
%
% }
\includegraphics[width=16cm]{img/codeExample.pdf}
 \caption{\label{figure:sourceCodeExample}source code example}
\end{figure*}

%\KZ{\textcolor{red}{Replace the above example with something a bit more complex but including
%all the features we have such as function call, comments, abbrev etc. It
%should contain several methods calling each other. Then you can show
%the call graph of them.}}


%\KZ{\textcolor[rgb]{1,0.1,0.1}{Label all the sections, figs and tables and refer to these labels in your
%text!}}

\subsection{Parse Tree}

%\KZ{\textcolor{red}{The parse tree should be a simplified AST to save space. Right now, many of
%the nodes in the parse tree are redundant. E.g. you can use If (cond, thenpart,
% elsepart) to represent a if statement. Then recursively define the cond,
%thenpart and elsepart. The tree doesn't have to be the real Java parse but
%something that reflects the commonalities of all programming languages.}}

%\KZ{\textcolor{red}{Also simplify the description of the parse tree. Omit implementation details
%such as JavaParser. This work should not be limited to Java code only even
%we only evaluated on java repos.}}

The first problem that we have to solve is how to represent source code. The basic format of source code is string, and we won't catch any structural information if we just use it as string. When source code compiler compiling a source code file, it convert the source code into a sequence of tokens and then establish a parse tree to represent the code. So the parse tree can tell us much information about the structure of source code.
\subsubsection{Original Parse Tree}
%We use the java library called JavaParser to establish the parse tree of Java code. And because we use JavaParser, the programming language mentioned in this paper is Java language. With the help of JavaParser we can create the parse tree as Fig \ref{figure:parseTree1} shows. And Fig \ref{figure:sourceCodeExample} is the source code example of Fig \ref{figure:parseTree1}.
Fig \ref{figure:parseTree1} is the original parse tree of the source code in the left part of Fig \ref{figure:sourceCodeExample}. And our algorithm can be applied to all programming languages. By the way, the suspension points in Fig \ref{figure:parseTree1} means there are also some other nodes here and we don't show it to save space, readers should complete these parts when implementing the parse tree.
\begin{figure}[!htp]
 \centering
 \includegraphics[width=\linewidth]{img/parseTree1.pdf}
 \caption{\label{figure:parseTree1} original parse tree}
\end{figure}

\subsubsection{Our Parse Tree}\label{sec:ourParseTree}
Comparing to Fig \ref{figure:sourceCodeExample} and Fig \ref{figure:parseTree1}, there are many new internal nodes appears in the parse tree. These internal nodes are specific to programming language and the expression or statement the source code is. So internal nodes hold much structural information. And the leaf nodes of parse tree are all appeared in the string of source code, they are all separated from the sequence of tokens.

But this kind of parse tree isn't fit for source code repositories very much. In source code repositories, most functions will call other functions to achieve their goals. If we want to know the subject of one function we must know the subject of functions that are called by this function. For example, we have two source code methods in Fig \ref{figure:sourceCodeExample}. When we want to know the subject of method initValue, we have to know the subject of method initSlot first. And because of this relation among methods, we add the methods' call graph into our parse tree. This means we replace the method's name in parse tree to the subject of method. Like word embedding, we represent the subject of method as a vector. And this subject vector is calculated from the parse tree of the called method(like initSlot in the method initValue's parse tree).

%\KZ{\textcolor{red}{Change the following discussion once you change the example.}}
At the same time, we also found that in Fig \ref{figure:parseTree1}, the node 31 is "constructorAttrs", but it isn't a correct English word it is combined by "constructor" and "attrs". If we use "constructorAttrs" directly, we may can't catch the subject of function exactly. So we split "constructorAttrs" into "constructor" and "attrs", and add a new internal node called "CombineName". This new internal node means there is identifier and this identifier is combined by the children of "CombineName" node. Meanwhile, "attrs" isn't a correct English word neither. The word "attrs" is an abbreviation of the word "attributes", so we use "attributes" at last instead of "attrs". So in our new parse tree, we use "constructor" and "attributes" to replace "constructorAttrs". Readers can read more details in Section \ref{sec:identifier} about the process of identifiers.

After adding the call graph of functions and the new internal node(CombineName), the new parse tree of Fig \ref{figure:sourceCodeExample} can be seen in Fig \ref{figure:parseTree2}. The bold and lean nodes are different nodes between Fig \ref{figure:parseTree1} and Fig \ref{figure:parseTree2}. And node 12 is the representation vector of method checkValidAttributes.

\begin{figure}[!htp]
 \centering
 \includegraphics[width=\linewidth]{img/parseTree2.pdf}
 \caption{\label{figure:parseTree2} new parse tree}
\end{figure}

%\begin{figure}[!htp]
%\footnotesize{
% $~~~~~~~~~~~~~$   int getBufferSize(int bufferSize)\{\\
% $~~~~~~~~~~~~~$  $~~~~$   if (bufferSize $>$ 0)\\
% $~~~~~~~~~~~~~$  $~~~~~~~~$     return read();\\
% $~~~~~~~~~~~~~$  $~~~~$    else\\
% $~~~~~~~~~~~~~$  $~~~~~~~~$      return 0;\\
% $~~~~~~~~~~~~~~$   \}
% }
%    \caption{\label{figure:functionExapmle} function example}
%\end{figure}

\begin{table}[!htp]
\centering
\caption{\label{table:splitID} Example of Split Identifiers}
\begin{tabular}{|c|c|}
\hline
Identifier & Words \\
\hline
contextInitialize & context, initialize\\
\hline
apiSettings & api, settings\\
\hline
buildDataDictionary & build, data, dictionary\\
\hline
add\_result & add, result\\
\hline
\end{tabular}

\end{table}

\begin{table}[!htp]
\centering
\caption{Example of Split Identifiers}
\label{table:abbr}
\begin{tabular}{|c|c|c|}
\hline
Abbreviation & Origin & Context\\
\hline
val & value & key.value()\\
\hline
cm & confusion, matrix & new ConfusionMatrix()\\
\hline
conf & configuration & context.getConfiguration()\\
\hline
rnd & random & RandomUtils.getRandom()\\
\hline
\end{tabular}

\end{table}

\subsection{Call Graph}

%\KZ{\textcolor{red}{Tone down java... This paper is not only for java! Avoid the use of
%``as we all know..'' etc. Show the call graph of the running example.
%And that is it! Everybody understands it.}}

%\KZ{\textcolor{red}{The following para sounds like implementation details. Present only
%principle stuff here.}}

Call Graph is one representation of invocation information in the source code repositories. The good running of repositories is based on the good running of every methods. And the whole repository is constructed over the calling of all methods in the repository.

In our algorithm, we replace the string of method name to the vector representation of method in the parse tree. If we want to implement this changes into the model, we need to know the calling sort of all methods and then we can calculate the vector representation follow that sort. So that every time we use the vector representation, it's the newest version.
And the call graph among all methods can help us to gain this calling sort. After we constructing the call graph, we can get the topology sort of this graph and this topology sort is the calling sort that we need to utilize. Fig \ref{figure:callGraph} is an example of call graph about the source code in Fig \ref{figure:sourceCodeExample}.
%To utilize invocation information, we have to generate call graph between all methods. As we all know, every java file is a java class and every java class has its own package name and many classes that it import.We firstly utilize the information of package name and import to generate the full name of every method. For example, if method A was called like a.A() and a is the objective of class com.github.abc.ClassA, then the method A's full name is com.github.abc.ClassA.A. And we can know the variable a's class's full name from package name and import information. In one word, the format of method's full name is (Package Name).(Class Name).(Method Name). In the previous example, com.github.abc is package name, ClassA is class name and A is the method name.

%After generating full name for every method, we can begin to build the call graph of all methods in the source code repository. For example, if method B is called in method C, there will be a direct edge from B to C. When we look through all methods in the repository we can build the call graph, and this call graph can be used to improve the performance of parse tree. And Fig \ref{figure:callGraph} is an example of call graph.
%\KZ{\textcolor{red}{Better use the running example here. You can
%simplify the names of the methods to save space.}}

\begin{figure}[!htp]
 \centering
 \includegraphics[width=0.8\columnwidth]{img/callGraph.pdf}
 \caption{\label{figure:callGraph} Call Graph Example}
\end{figure}

\subsection{Identifier Semantics}\label{sec:identifier}
%\KZ{\textcolor{red}{This section needs to be expanded significantly. I still don't understand
%Table \ref{table:abbr}. If you use rules, you should explicitly say what those
%rules are to recover the original words from the abbreviations. If you use
%some algorithm, better include the listing of the pseudo-code.}}

According to the description in last section, some names of identifiers are phrases, which are meaningless since they are combined by
two or more meaningful words together. They are quite important since they are the semantic information in this parse tree, showing the objective of why
they are creative or what problems they solve. So we want to utilize such kind of information, instead of just keep the combination of them which makes no
sense. There are two kinds of method to improve the semantic information. One is to split all the phrases to multiple words and the other one is to retrieve the
full version from some abbreviated parameters.

From table \ref{table:splitID}
%\textcolor{red}{[Really? I suppose that this is listed in table II]}
, we see some examples of identifiers in source code. They are defined in the combination of some words, using the alternation of capital and small
letters or splitting two words with a underscore. For example, when we name a parameter whose function is to add two integers, we can name it as ``addScore'',
changing the first letter of the second word to a capital one. When we encounter an identifier, we can search the alternation of capital and small letters, or
observe the appearance of underscores.

Another situation is that many identifiers are also meaningless since they are the abbreviation of one or more words. Table \ref{table:abbr} shows some abbreviations and
their original version. However, we can get their full name by checking their context. As we observed, we can split the context to words. Then, check the
initials of these words to see whether they make up of the identifier we are operating. Take ``cm'' in Table \ref{table:abbr} as an example. When searching its context we can
find ``confusion'' and ``matrix'', whose initials can make up of ``cm''. So we can believe the full version of ``cm'' is ``confusion matrix''.

When parsing the code, adding such two kinds of operations can add more semantic information into the parse tree, which serves the goal that combining
semantic and structural information together.

%\textcolor{red}{
Here let's talk about how we find out such original information from the abbreviations. As many programmers doing in their works,
many parameters are named according to its type or assignment expression. So we get the original version of the parameter through its definition. We use the %following definition statement
source code in Fig \ref{figure:indentifier} as an example.
%}

\begin{figure}[!htp]
 \centering
\textcolor{blue}{ Matrix} dm \textcolor{blue}{= new DoubleMatrix}(confusionMatrix);
 \caption{\label{figure:indentifier} Identifier Example}
\end{figure}
%\textcolor{red}{Matrix dm = new DoubleMatrix(confusionMatrix);
%}

%\textcolor{red}{
Firstly we split all the combined phrases, which exist in the type or the assignment expression, to some independent words. All the words coming from one phrase are put in one list. The rule we apply to split words is simple. People usually use the alternation of upper cases and lower cases to combine two words and form a phrase. Sometimes the underscores are also used to segment two words. We just split the phrases according these rules. In the example, we split the type ``Matrix'' into a list which contains only one word ``matrix'' and split the expression
``DoubleMatrix(confusionMatrix)'' into two lists. The first list contains ``double'' ``matrix'' and the second contains ``confusion'' ``matrix''.
%}

%\textcolor{red}{
Then we compare the parameter, namely the identifier, and the list above to see whether the identifier's name is a substring of
some word from the list, or is the combination of the initial of the words in the list. In the first situation, the list contains only one
word. We just find if the identifier is part of the word. If so, then we can make sure the identifier is the abbreviation of the word at a higher probability. In the second situation, we can collect all the initials of the words in the list together to see whether the identifier is part of this collection and do the same operation as the first situation.
After this process we can get most of the full names of the parameters.
In the example, we search for the original words of the identifier ``dm'' in the following steps. At beginning, we see ``dm'' is not the substring of any word in any list we establish. Then we collect the initials of the words in one list and get three collections ``m'' ``dm'' and ``cm''. Here we see ``dm'' is an abbreviation of ``DoubleMatrix''.
%}


%According to the description in last section, some names of identifiers are phrases, which are meaningless since they are combined by
%two or more meaningful words together. They are quite important since they are the semantic information in this parse tree, showing the objective of why
%they are creative or what problems they solve. So we want to utilize such kind of information, instead of just keep the combination of them which makes no
%sense. There are two kinds of method to improve the semantic information. One is to split all the phrases to multiple words and the other one is to retrieve the
%full version from some abbreviated parameters.
%
%From Table \ref{table:splitID}, we see some examples of identifiers in source code. They are defined in the combination of some words, using the alternation of capital and small
%letters or splitting two words with a underscore. For example, when we name a parameter whose function is to add two integers, we can name it as ``addScore'',
%changing the first letter of the second word to a capital one. When we encounter an identifier, we can search the alternation of capital and small letters, or
%observe the appearance of underscores.
%
%Another situation is that many identifiers are also meaningless since they are the abbreviation of one or more words. Table \ref{table:abbr} shows some abbreviations and
%their original version. However, we can get their full name by checking their context. As we observed, we can split the context to words. Then, check the
%initials of these words to see whether they make up of the identifier we are operating. Take ``cm'' in Table \ref{table:abbr} as an example. When searching its context we can
%find ``confusion'' and ``matrix'', whose initials can make up of ``cm''. So we can believe the full version of ``cm'' is ``confusion matrix''.
%
%When parsing the code, adding such two kinds of operations can add more semantic information into the parse tree, which serves the goal that combining
%semantic and structural information together.

\subsection{Adding Comments}

\KZ{may add preprocessing of comments if there's any theoretical value in there.}

We also found that there are many comments in source code repository which is different from the source code snippet. And comments are all natural language that can provide much useful information for human to understand source code. So we add these comments information into parse tree. For example, there is a comment shows in Fig \ref{figure:commentExample}, and Fig \ref{figure:nodeOfComment} is its representation in the parse tree. We can see that we put all words as children node of the internal node "Comment". And the node '......' means the other part of parse tree that is generated by the approach mentioned before.

\begin{figure}[!htp]
\footnotesize{
\textcolor[rgb]{0,0,1}{// Maybe selected auto-focus but not available, so fall through here:}

    \textcolor[rgb]{0.7,0.1,0.1}{if} (!
    \textcolor[rgb]{0,0,1}{safeMode} \&\&
    \textcolor[rgb]{0,0,1}{focusMode}
     == \textcolor[rgb]{0.7,0.1,0.1}{null}) \{

     $~~~~$
        \textcolor[rgb]{0,0,1}{focusMode}
         = findSettableValue(
         \textcolor[rgb]{0,0,1}{"focus mode"},
        \textcolor[rgb]{0,0,1}{supportedFocusModes},

      $~~~~$
        Camera.Parameters.\textcolor[rgb]{0,0,1}{FOCUS\_MODE\_MACRO},

      $~~~~$
        Camera.Parameters.\textcolor[rgb]{0,0,1}{FOCUS\_MODE\_EDOF});

    \}
 }

 \caption{\label{figure:commentExample} comment example}
\end{figure}

\begin{figure}[!htp]
 \centering
 \includegraphics[width=\linewidth]{img/comment.pdf}
 \caption{\label{figure:nodeOfComment} node of comments}
\end{figure}

\subsection{Bimodal Modelling of Source Code and Natural Language}
%\KZ{\textcolor{red}{This section relies too much on the ICML paper. You cannot assume
%that the reader has read ICML 2015 before reading your paper.
%Avoid citing ICML papers too many times (once will do!)
%This section needs to be rewritten significantly.}}

We joint source code functions and tags base on a bimodal model mentioned in~\cite{allamanis2015bimodal}.
%Allamanis et al's model can be used to match source code snippets and natural language query. But the code snippets must has less than 300 words, while there are many functions that have more than 300 words in our model. And natural language query is also different from the tag. In our model, tags are many individual words and don't have any relationship with each other while training. But the words in the same query sentence must be trained together.

%So we change some aspects of~\cite{allamanis2015bimodal} to make our model more suit to code repository and the new parse tree is one of changes.
\subsubsection{Notation}
We let $I$ be the set of internal nodetypes (not leaves) and $K$ be the set of tokens (leaf nodes). And a parse tree can be represented as $C\ =\ (Nd,ch,val)$ where $Nd=\{1,2,\cdots,N\}$ is set of all nodes($Nd=I\cup K$) and $ch$ is a function that map the node to its children nodes. $ch(1)=\{2,3\}$ means that node 2 and 3 is the children of node 1. The last one $val$ is a function that match the index of node and the value of this node. For example, i is the index of node "read", then val(i) equals to "read". By the way, we index the nodes of parse tree by left-to-right depth first traversal of tree. Fig \ref{figure:parseTree1} and Fig \ref{figure:parseTree2} have shown some indexing examples.
\subsubsection{Model Overview}
We use a generative model to train our data. $P(C\ |\ T)$ is the probability of generating parse tree C on condition of the tag T. And our goal is to maximum this probability.
\begin{align}
    P(C\ |\ T) & = \prod_{n\in Nd:ch(n)\neq\phi}^N P(val(ch(n))\ |\ T,C_{\leq n}) \label{equa:probability}
\end{align}
Equation \ref{equa:probability} tells us how to calculate $P(C\ |\ T)$, and it means if we want to generate a parse tree C based on tag T, we have to sequentially generate a child tuple for node n conditional upon the tag T and the partial tree $C_{\leq n}$. $C_{\leq n}$ is the partial tree of $C$ that the index of all nodes are less than n.

We also define $supp(i)$ and $S_{\theta}(v,T,C_{\leq n})$ to construct our model. $supp(i) = \{v\ :\ v=val(ch(n)) \bigwedge val(n)=i\ for\ some\ n\ in\ dataset\}$ is the set of all children tuples that appear as the children of a node whose type is $i$. And we can convert scoring function $S_{\theta}(v,T,C_{\leq n})$ to probability by exponentiating and normalizing.
\begin{align}
    P(v\ |\ T,C_{\leq n}) & = \frac{\exp s_{\theta}(v,T,C_{\leq n})}{\sum_{v^{'}\in supp(val(n))}\exp s_{\theta}(v^{'},T,C_{\leq n})}
\end{align}
where $\theta$ is the parameter of our model.

\subsubsection{Scoring Function}
The scoring function in our model is $s(v,T,C_{\leq n}) = (t \bigodot c)^{\top} r + b$ and $\bigodot$ is elementwise multiplication. $t$ is the representation vector of tag that is unique to every word and c is the representation vector of partial parse tree $C_{\leq n}$. $r$ is the vector that is unique to each parent-children pair $(i,v)$. $b$ is also unique to each parent-children pair, but $b$ is a scalar number.

Because the parse tree is always too big to consider all nodes into scoring function, we just choose some nodes of parse tree as feature nodes to extract the representation of partial parse tree. And the two main features that we use are based on the 10 previous tokens following the indexing sort and 10 previous internal node types that in the path from node n to the root node. We let $c_{\varPhi_j}$ be the representation vector of one token that appears at jth position of all feature nodes and $H_j$ be the context matrix of jth feature node and the size of $H_j$ is $20\times20$. So that we can get c with the equation: $c=\sum_{j=1}^{J} H_{j} c_{\varPhi_j}$.

All parameters appear in scoring function need to be trained and modified. And we have to initialize all these parameters before begin training. We initialize them randomly around the center 0 with some small additive noise except $H_i$. We initialize $H_i$ as a diagonal matrix and the diagonals of $H_i$ are $\frac{1}{J} $, $J$ is the number of feature nodes and in our paper it is 20.

\subsubsection{Training}
Our aim is to maximum $P(C\ |\ T)$, and because normalizing cost much time, we use noise-contrastive estimation method \cite{gutmann2012noise} to train our model. The objective function can be written as in Mnih et al \cite{mnih2013learning}:
\begin{align}
  \nonumber  E_{(T,C_{\leq n},v)\sim D}[\log\Delta(\triangle s(v,T,C_{\leq n}))]\ +\ \\ kE_{(T,C_{\leq n},v^{'})\sim noise}[\log(1-\Delta(\triangle s(v^{'},T,C_{\leq n})))]
\end{align}
where $d$ is the distribution of data and $nosie$ is the distribution of the noise data. The noise data's distribution is the posterior PCFG of the training data. And $k$ means that every pair $(T,C_{\leq n})$ has $k$ noise data.
%And we choose the same initialization strategy as~\cite{allamanis2015bimodal} says.

We also use AdaGrad method that is proposed by Duchi et al \cite{duchi2011adaptive} to optimize our training method to gain better results.

By the way, because we use invocation information, we need to sort all methods in one source code repository based on the call graph. And we decide to use the topology sort of the call graph, so that we can guarantee that when we meet a method in the parse tree we have get this method's newest representation vector. For example, there is a method initSlot was called by method initValue in Fig \ref{figure:sourceCodeExample}
and the sequence of methods that are trained is initSlot and initValue. This sequence makes the representation vector of method initSlot is always the newest one when we calculate the vector of method initValue. And if there is a cycle in the call graph, we just break it randomly when generating topology sort.
