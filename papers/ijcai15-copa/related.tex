\section{Related Work}
\label{sec:related}

We start by discussing previous work to extract causal relation term
pairs from text and briefly mention the general task of relation
extraction. Then we present various past attempts to solve the
commonsense causal reasoning problem. Common ingredients in these
approaches are word association or similarity measures, which are
discussed last.

\subsection{Causal Relation Extraction}
Previous work on causal relation extraction is relatively sparse.
The existing approaches use hand-coded and domain-specific patterns
to extract causal knowledge. Girju et al. \cite{girju2003automatic}
were the first to work on casual relation discovery between
nominals. They semi-automatically extracted causal cues, but only
extracted noun category features for the head noun. Chang et al.
\cite{ChangC04} developed an unsupervised method and
utilized lexical pairs and cues contained in noun phrases as
features to identify causality between them. Both of them ignored
how the remaining causal text span between noun phrases
affects the semantics. We proposed numeric features based on that,
and get better results. Blanco et al. \cite{blanco2008causal} used
different patterns to detect the causation in long sentences that
contain clauses. And most recently, Do et al. \cite{do2011minimally}
introduced a form of association metric into causal relation
extraction. They used discourse connectives and similarity
distribution to identify event causality between predicate, not noun
phrases, but achieved a F1-score around 0.47.

\subsection{General Relation Extraction}
Besides causal relations, much work has been done on extracting many
other types of relations from text, e.g., hyponymy (isA)
\cite{Etzioni:Web,12MSRA:Probase}; meronymy (part-whole)
\cite{GirjuBM06}, metaphor \cite{LiZW13}, relatedness
\cite{Zhang15:Assoc} as well as general relations
\cite{Banko:TextRunner,S:YAGO,S:YAGO2,fader2011identifying,NakasholeWS12}.
Relation extraction generally involves identifying the target terms
or entities in text and then annotating the relations properly.
Previous approaches are either supervised or semi-supervised.

Supervised approaches usually treat the extraction as a
classification problem, where the input is the sentence with marked
target entities/terms, and the output is the classification into one
of the predefined relations or none. Marking the entities often
relies on syntactic patterns or named entity recognition. These
approaches require labeled data and hence cannot be easily extended
to new types of relations. They also make heavy use of NLP tools
such as POS tagger and dependency parser which are all error-prone.
Semi-supervised method often starts with a seed set of entity pairs,
and uses a bootstrapping strategy to accumulate more pairs either by
gradually discovering contextual patterns that represent the target
relation \cite{Etzioni:Web}, or by using a fixed set of strong
patterns and some logical rules to determine the plausibility of a
pair in each iteration \cite{12MSRA:Probase}. Our extraction of
causal pairs is completely unsupervised. enables allows us to
harness a web-scale evidences though with noises, which we eliminate
using statistical evidences.

\subsection{Commonsense Causal Reasoning}

Commonsense causal reasoning is a grand challenge in artificial
intelligence. Earlier attempts on the problem were largely
linguistic, for example, developing formal theories to capture
temporal or logical properties in causal entailment
\cite{LascaridesAO92,lascarides:asher:1993a}. These approaches were
not effective due to the difficulty in handcrafting the theories for
board-ranging open domain reasoning.

Recently, the NLP community has explored knowledge based approaches
and show substantial potential. One approach toward this goal is to
accrue common sense knowledge through crowdsourcing. A prominent
example along this line is the Open Mind Common Sense (OMCS) project
by MIT \cite{singh2002open}. Some of the knowledge such as ``effect
of'' relation in the ConceptNet \cite{liu2004commonsense} which is a
sub-project under OMCS can be used to identify causal discourse in
COPA task. However, the scale of such human curated knowledge
suffers from scalability bottleneck. In fact, the ConceptNet is only
a fraction of our causal network by size after 15 years of community
efforts.

More successful efforts are centered around using correlational
statistics \cite{gordon2012copa} such as pointwise mutual
information (PMI) between unigrams (words) or bigrams from large
text corpora \cite{Mihalcea2006:CKM}. Corpora attempted include LDC
gigaword news corpus \cite{goodwin2012utdhlt}, Gutenberg e-books
\cite{roemmele2011choice}, personal stories from Weblogs
\cite{gordon2011commonsense} and Wikipedia text
\cite{jabeen2014exploiting}. Previous research show that the type of
information source has significant impact on the accuracy of such
knowledge based approach. This paper falls into this category of
research, but instead proposed to compute a generalized PMI measure
\cite{Washtell09:CWW} not from the plain text corpus but from a
causal relation graph induced from large web text. In addition,
instead of fixing the target language units in the discourse
sentences to either word or n-gram, we dynamically construct events
which contain internal causality information and make use of these
multi-word events in the computation of the final causal strength
between two sentences.

\subsection{Word Association Metrics}
Our generalized causality is inspired from association strength measure 
proposed by Wettler et al.~\cite{Wettler:1993} and Washtell~\cite{Washtell09:CWW}, 
which introduced parameter $\alpha$ being $0.66$ and $0.5$ respectively.
Our causality strength considers both directions of the causality
relation. Causality strength is similar to
association strength to some degree, since association between term pair $(u,v)$ which also asymmetrical treated $u$ and $v$. We introduced an generalized
formula by introducing $\alpha$ and $\beta$ following the same intuition
for discounting high frequency causes and effects respectively.
%(Mention SCI?)
%parameter $\alpha$ PMI is lexical order based. 

%SCI has a param of 0.5.

