R1:
"demo system doesn't match my commonsense"
Causal strength involving rare English words are less reliable due to
low occurrences. We revised CausalNet demo (same URL) by showing
the most frequent English words on web corpus.

"..which aspect of the system ontributes to.."
1) We conducted COPA evaluation using Gordon's 
PMI method on our web corpus. Results: Gordon's accuracy = 61.2%; 
Our accuracy = 68.8%. This shows it's our framework and not the data 
that contributes to the improvement. 
We will include this additional result in the revised paper.
2) Larger data doesn't necessarily translate to better results:
Gordon's PMI method achieves 65.2% on 1.9GB data and 65.4% on a 37GB dataset.  
3) The previous state-of-the-art result was achieved by specialized personal
story corpus. Our work shows that a more intricate framework can leverage
a much bigger but more general web corpus to achieve better results.

"data source issue"
Although it's not possible to share the whole web corpus, 
the extracted causal pairs will be shared for academic research.  

R2:
"Using conditional probabilities..." 
Unlike causal roles in PMI determined solely by the order of words, we identify accurate roles and 
produce more effective asymmetric measures.

"In equation (7).." 
We believe that a word that triggers a causality within a sentence
is likely to cause an effect in another sentence.

"How to sample negative pairs.." 
From causal relations in ConceptNet, some causal relations are false, which are marked by volunteers into negative scores. Negative pairs are sampled from such relations (see Section 3.1)

"Is all this at the level of words..."
It is based on words (see section 2.1)

"How about replacing PMI scores...?"
If we set parameter \alpha=0, \beta=0, 
the causal strength is exactly the product of two conditional probabilities
(see Eqn 3).

"I found puzzling effects" 
A possible explanation is that it doesn't take
marriage to have love but it does take a marriage to have a divorce. 


R3:
"Experiment concern"
Yes, we agree with your suggestion. Please refer to second response of R1.

"Define 'span' in section 2.2"
Word, phrase, and sentence constitue "span" in this paper.

"implicit knowledge issue"
Although the causal cues cannot detect some of the implicit causal relations 
in text, the large scale of the input data makes up for this deficiency. 

"Did the corpus undergo any kind of preprocessing ..." 
The only preprocessing was lemmatization of the words in sentences. 
