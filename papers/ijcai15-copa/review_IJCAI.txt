#R1
Comments to author(s)
I tried the demo system following the URL presented in the paper and the result (part of it shown below) doesn't match my commonsense. I hope the author could give an explanation of that.

1	knock	archerfish	0.16423735509295
2	knock	bannock	0.06955535762316092
3	knock	mina	0.052899544840364264
4	knock	fluidness	0.019434404420965883
5	knock	tatter	0.01667527315568233

In footnote 1, the authors address the reason that they anonymized the experiment search engine "to honor double blind policy". Whether the authors realized or not, this footnote restricts the author identity to the small set of institutions who could have access to "the commercial search engine snapshot". 

Besides, it makes me worry that if the experimental improvements largely benefits from the unique lexical resources they can access, what would be the authors' plan to make the work benefit academia in general. 

Summary of review
This paper studies the problem of commonsense causal reasoning via a text-driven approach. It presents a sequence of approaches to extract and refine a weighted word-based causality network ("CausalNet") from large amount of web text, as well as the method of reasoning causal relations between two pieces of text that have longer spans. To construct CausalNet, first it build the network skeleton based on text patterns that implies causalities; then it compute directional weights of the edges in the network. With the constructed word network, the causality score between two sentences are computed as a function of the word causality scores from the network, as well as the causality role scores computed based on the syntactic structure of the two sentences. In the experiment session, the statistics of CausalNet is presented and the proposed framework is evaluated on a causal reasoning question task and a causality detection task. And the proposed framework performs better than previously published results in both tasks.

Overall, the presentation of the approach is nice and clear, but the experiment set up fails to show which aspect of the system (framework or data?) contributes to the improvement.

There are several pros and cons of the paper.

Pros: 
1. The presentation of the approach and system is clear and therefore easy to understand.
2. Graph representation of the commonsense background knowledge (if not completely novel) is an efficient and flexible way to approach this problem.
3. When doing causal reasoning between sentences, the combination of intra-sentence causal information and the background knowledge from the network is interesting. 

Cons: 
There are two major concerns I have against the paper. 

First, it presents a framework to address both causal knowledge acquisition and sentence-wise causal reasoning, however, the framework appears to be more like an assembly of a series standard NLP problems the field have been trying to solve, without much innovation w.r.t. each module or an interesting combination.

Second, the background network is constructed from an enormous text corpus which general academia does not have access to, and the authors didn't run previous approaches on this dataset -- they might perform better. Therefore it is unable to judge what "leads to" their improvement over past systems (session 3.2 & 3.3) -- the way larger and more diverse data resource or some novelty in the algorithm? Since the paper is trying to address one of the fundamental problems of AI -- commonsense causal reasoning, I would like to see more qualitative analysis in the experiment session. It would also be great if the author could visualize a small fraction of this huge network instead of only presenting toy examples for illustration.

#R2
Comments to author(s)
The paper presents a method for mining, from a large Web crawl, pairs of causally related words/concepts, and how to use this for a benchmark task on causal reasoning (SemEval COPA).

The problem and the approach are interesting, but the paper has many unclear points that leaves one puzzled about how this works and what it really achieves. Also, the part about computing causality strength does not really seem original. Using conditional probabilities for an asymmetric measure of causality is so obvious - I would be extremely surprised if this has not appeared in the ample prior literature on causality
(but I am not an expert on this area).

Here are some examples of unclear points that left me puzzled:

1) In equation (7), the event-enhancement weights of words are based on the procedure described in Section 2.3. This procedure seems very ad-hoc to me. What are the underlying principles here?

2) The ambiguity penalty factor seems very crude. In WordNet, some concepts have a huge number of senses because the WordNet creators spent a lot of intellectual energy on them while others have few because there was less effort on them. Also, the number of senses does not tell you anything about the distribution of the senses.
For example, man has 11 senses, but only 2 are frequently used. Woman has 4 senses, but real language almost always uses only 1 of them.
Plant has 4 senses as well, but two of them are frequently used.
None of this is reflected here.

3) The experiments use positive and negative samples of causal concept pairs. How do you sample negative pairs (in a meaningful way)? Just take random pairs from the complement of the causes relation? That would produce all kinds of wild pairs (e.g. sampling from partOf or isa). Does this make sense at all?

4) Is all this at the level of words (or noun phrases?) or at the level of concepts, that is, word senses?
Especially with English, there is a big difference as many nouns are ambiguous. 

5) In the evaluation, all the baselines seem to have PMI-based causality strength measures. How about merely replacing PMI scores with an asymmetric score based on conditional probabilities (or relative entropy)? How would such a method perform? This is a very natural question to understand the specific nature of the paper's contributions. The paper does not address this.

6) I tried out the online demo. While I generally appreciate that the authors make this available, I found some puzzling effects here as well. For example, the score for cause/effect "love/hate" is higher than for "love/marriage" and that for "marriage/divorce" is orders of magnitude higher than for "marriage/love", "marriage/happiness", "marriage/children", "marriage/child" etc. 
I certainly realize how difficult it is to get robust results for such a challenging task, but I'm still wondering how much of a bias is obtained from the coverage of events in Web contents. At least I would like to see these issues discussed, but the paper merely emphasizes that it outperforms its baseline competitors. 


Summary of review
very interesting work, 
but many unclear points

#R3
Comments to author(s)
This is a well written paper that describes a pipeline for extracting (from a large corpus of raw text) causal word pairs via causal cues such as “consequently” and “because”, constructing a directional graph of terms where arc weights represent causality strength, syntactically analyzing queries in a Choice of Plausible Alternative (COPA) problem, and heuristics to aggregate causality strength across word pairs in a query. 

While the approach is plagued with heuristics, a few interesting ideas are may be of interest to other researchers working on causal reasoning (I don’t):
constructing a directional causality graph between terms.
cleverly combining conditional probability in both causation directions to define the arc weights in the causality graph (equation 2).
extracting potential cause/effect events from query sentences.

The main concern I have about this paper is in experimental results. The proposed approach outperforms strong baselines (state of the art?) by 2.2~3.4 points, which is a reasonable improvement. However, the data used to obtain these results are about 200 times as large as the data used to train the most competitive baseline (Gordon et al. 2011). Therefore, it is not clear whether the improved results are due to using a larger corpus or using a better formula for estimating causality strength. This issue can be addressed by applying the method proposed in (Gordon et al. 2011) on the larger corpus. It is possible that the method used in (Gordon et al. 2011) won’t scale to the amount of data used in this paper. If this is the case, the author should illustrate this, e.g., using a figure that compares runtime while varying the data size.

Minor comments:
Define what constitutes a “span” in section 2.2
People would often say “I’m not hungry. I just had lunch.” rather than “I’m not hungry because I just had lunch.” In other words, commonsense cause-effect pairs are typically expressed without causal cues, which undermines the reliance on causal cues to extract cause-effect pairs.
Did the corpus undergo any kind of preprocessing, e.g., language identification, boilerplate removal ...etc? If yes, please describe in order to allow other researchers to reproduce your results.


Summary of review
Some interesting ideas but hard to conclude if they are effective due to lack of a properly controlled experiment.