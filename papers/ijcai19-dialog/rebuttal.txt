Review #211019

Thank you very much for your comments and suggestions.

1. In Table 4, the model Distance showed that distance is a key feature for the task. So, intuitively, we add this information at the last prediction layer for preserving this information to a great extent. Moreover, as defined in Part.3, "distance" is the absolute distance of a Q-NQ pair. It is a 10-dimensional one-hot vector, where the index of the position with 1 refers to the absolution distance. And the last dimension is 1 when the distance >=10. We don't have relative distances in our method.

2. The public dialogues on this website do not contain any personal information about patients. Besides, other papers such as "Task-oriented Dialogues System for Automatic Diagnosis" also collected dataset from similar website: http://muzhi.baidu.com. So, our dataset is appropriate for research use.

3. The number of QA pairs that fall in each bin was shown in Table 1. We performed a z-test on the results, which shows that the results on the longer distance QA are statistically valid. For example, HM is significantly better than RPN on distance >=5 with the significance level of 0.01 (z=2.84 > z_0.01=2.325).

4. According to Table 5, HM achieves better performance on long-range QAs and DM achieves better performance on short-range QAs (The accuracy in the first column is close to each other). We think there is a trade-off between history information and distance information in the whole model HDM. HDM still outperforms the other models overall according to Acc and F1 in Table 4. We mentioned it at the end of sec 5.2 and 5.3.

5. The results of our models are averaged over three runs. We will add this information in revised version.

6. This has been answered in 3 of Review #254651.

7. Small dimension sizes are also commonly used in other previous work due to the limitation of computation resources.

Review #254651

Thank you for the precious comments.

1. You might have looked at the wrong place in the forum. Here is an example, among many others: https://www.120ask.com/shilu/0cjdemaflhj6oyw9.html

2. We only bolded the best accuracy among NN-based models as explained in sec 5.2.

3. As far as we know, this is the first paper concerning QA pairs with variable distances. Besides, similar works haven't published their datasets, e.g., [He et al.,2019]. The similar medical dialogue dataset published by the paper "Task-oriented Dialogues System for Automatic Diagnosis" focuses on dialogue policy learning and doesn't preserve the original utterances, hence is not useable for us.

4. Both the dataset and the model are the contributions of this paper. As for the dataset, we are the first to publish a such real-world labelled dialogue dataset full of questions and answers to the research community. As for the model, we prove that it works for QA matching problems especially for long-range QAs.

Review #256645
Thank you for your suggestions and the issues raised about our manuscript.

1.We made this assumption based on that the occurrence of such utterances is only 0.24%.
2.The models named Distance, mLSTM, DM, HM and HDM are ablation models. Their results are discussed in the last paragraph of sec 5.1 to 5.3. We will reorganize this part in the revised paper.
3.We will add the variance of the results in revised version. Our experiments show that 1) for short-range QAs our results are on par with the baselines; 2) for longer-range QAs, HM and HDM perform significantly better than NN-based baselines, such as RPN, as shown in Tab 5. 
4.We will correct the minor mistakes in the revised version.

P1
-1 It is U3 and U4, a QA pair.
P2
-6 A NQ from a person is paired with every earlier Q from the other person as the input sample for the model. 
P3
-4 Both roles (Patient or Doctor) could ask question and answer as shown in fig 1, so we defined the new notation: role_Q and role_P in sec 3.1. 
-6 We use H_RP to represent the history information related to NQ through the model, but it changes between different layers. H_RP in "Combining H_RP into Q" represents the encoded embeddings mentioned at the end of sec 3.1.
-7 We naturally use 0.5 as our threshold since it's a two-class classification problem.
P4
-3 Testing set.
P5
-2 "Quality" refers to the overall accuracy and F1 on the testing set shown in Tab 4.
-4 Please refer to 3 in #211019 and 3 in #256645. We implement the significance test on our results. It shows that HM is not significantly better than RPN on the first column.
P6
-1 For example, in Figure 1, we define the distance between U1 and U11 as 10 instead of 9.
-2 DM.
-4 "can be inaccessible".
-5 As stated in sec 5.2 and Tab 5, our methods work better than previous NN-based methods on variable distance matching. As stated in sec 5.3, our methods significantly increase the accuracy from 22.13% obtained by RPN to 36.07% with history information.

Review #257096
Thank you very much!
1. IQA is the abbreviation of Incremental QA instead of long-distance QA.
2. The IQAs which are made up of the first question by patient and the last answer by doctor account for about 28.70% of all labeled data.
3. Around 13.74% of the distance of the labelled QA pairs are larger than 3 according to Table 1. Our proposed model not only makes great progress on such long-range QA pairs as shown in Table 5 and Table 6, but also improves the overall performance compared with the baselines in Table 4.

