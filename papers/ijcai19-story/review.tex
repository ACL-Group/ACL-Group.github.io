%%%% ijcai19.tex



% These are the instructions for authors for IJCAI-19.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai19.sty is NOT the same than previous years'


% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
% \usepackage[colorlinks,
% linkcolor=black,
% anchorcolor=black,
% citecolor=black]{hyperref}
%\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage[noend]{algpseudocode}

\urlstyle{same}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}
\renewcommand\appendix{\setcounter{secnumdepth}{-2}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\shanshan}[1]{\textcolor{blue}{ #1}}
\newcommand{\ssemp}[1]{\textcolor{red}{ #1}}
\newcommand{\scite}[1]{\citeauthor{#1}~\shortcite{#1}}

\usepackage{verbatim}
\usepackage{stfloats}
\usepackage{graphicx,amssymb,amstext,amsmath}



\begin{document}

Review1:

Comments to Authors.

I found the paper to be rather interesting but limited in its scope of what constitutes knowledge representation (and commonsense knowledge) per se. The paper limits KR to the identification of features of interest and additional information that may be gleaned from a repository such as ConceptNet. This is certainly a view of KR, but is decidedly a machine learning centric one. It would be of interest to examine works done by Lierler, Inclezan, and Gelfond regarding the application of Action Languages and logic-based KR approaches for the purpose of textual understanding as well. This perspective also limits the discussion under the related works section when the authors mention that commonsense knowledge has been shown to be effective for many inference tasks.

From a machine learning perspective though I found the paper and its approach to be of interest and the criticism of the traditional approach towards solving the "SCT story ending predicting task" sound. The proposed remedy was also of interest.

Review2:

Comments to Authors.

This paper proposes a way to use an existing structured resource such as ConceptNet to aid story modeling, approached through completion or identifying the sentiment with which a story ended. The paper extracts concepts (a graph structure) from the provided story and uses the concepts in two ways: first by obtaining dense representations of them, and second by using the concepts to help "simplify" the text of the story; this simplified text is then embedded. These two embeddings are combined into an RNN, whose final hidden representation is used to perform the classification.

The main strength of this method is that, at its core, it is a relatively simple idea that nevertheless achieves decent performance. However, this paper needs substantial editing, additional explanation of the ideas and results, and exploration of the model itself. The following lists areas for improvement:

Technical contribution:

The paper uses negative sampling to train the model, yet erroneously refers to this as a semi-supervised method.

\shanshan{Thank you for pointing out this mistake on our part. Our approach is indeed supervised classification using negative sampling. We will modify the wordings in the revised version and also adjust the headers in Table 4 to "Unsupervised" and "Supervised" respectively.}

 \ssemp{This paper relies on ConceptNet and Numberbatch; it's unclear what exactly in that representation is helping performance. Put another way, is the idea of concept extraction and text simplification a generalizable method (and ConceptNet provided the means), or is there something specific to ConceptNet and Numberbatch?}
 
 \shanshan{The framework proposed in this paper, that is, text simplification plus structured commonsense knowledge is general and can be applied to other commonsense knowledge graph than ConceptNet. We used ConceptNet here due to its comprehensive coverage and relatively high quality. Our contribution here is this framework that makes good use of the knowledge in ConceptNet. If there were a similarly structured but better knowledge source, we are certain it could be plugged into the framework and yield better results. For example, when we use half of ConceptNet in a separate experiment, the results are.... }
		
Clarity:

	\ssemp{The paper needs to spend much more time describing the underlying resources used (ConceptNet and NumberBatch), the algorithm used (what does "word length" for a concept mean? see Sect 2.1), and the impact of their modeling decisions.
	The paper needs thorough proof-reading: there are systematic grammatical errors (e.g., the omission of determiners) that make the paper difficult to read.} 
	\shanshan{We will describe ConceptNet and NumberBatch more clearly in the revised version and proofread the whole paper carefully. 
``word length'' means the number of words in a concept. We will rephrase this term to ``length of a concept''.}

Scholarship:

\ssemp{The paper mischaracterizes at least one work: SemLM (Peng \& Roth, 2016) relies on the output of automated frame and entity tools.}	

\shanshan{Actually in Sec 4, para 2, we were discussing a number of end-to-end techniques for SCT using "manually chosen features". 
SeqMANN is one such technique making use of a feature called SemLM, which is extracted from external resources. So what we are categorizing is
really SeqMANN, and not SemLM. We will rephrase the sentence to make it clearer in the revision.}  

This paper covers story cloze tasks well, but does not sufficiently consider larger implications/connections of the method. For example, other methods of event simplification (Orr et al., 2014 AAAI; Ferraro \& Van Durme 2016, AAAI; Picchota \& Mooney 2016, AAAI).

\shanshan{As the title of the paper suggests, we are limiting our research to representing stories using commonsense knowledge. Orr, Ferraro, Picchota ,
}

Review3:

Comments to Authors.

The paper proposes a model that leverages common sense knowledge for story end prediction. In particular, the facts from ConceptNet are used to simplify the story by extracting a sequence of ConceptNet concepts from the sentence and then, the latent relationship among the key ideas in the story is modeled and finally, a GRU based classifier is used to predicts the correct ending. The model is evaluated on ROC Story Cloze Test data set.
The paper is poorly written and not well-motivated. Also, the technical novelty of the paper is very limited. Here are some concerns/issues regarding the work as listed below–

1.    In Sec 1., authors say- "Predicting “what happens next” in narrative stories is an important but challenging task called commonsense reasoning in artificial intelligence."  – The statement provides a narrow definition of commonsense reasoning problems that exits in AI and is not quite appropriate.

	\shanshan{Our abstract says ``Predicting ending for narrative stories is a grand challenge for machine commonsense reasoning,'' and this is what we really wanted to say.  We will fix this erroneous setence in the introduction in revised version.}
	
2.    In Sec1,  ``Instead, we propose to define the SCT story ending predicting task as a semi-supervised learning problem" – Why you consider the task as semi-supervised learning? More elaboration is required to clearly define the task and explain the notion of  ``semi-supervised learning".

	\shanshan{Please refer to R2...}

3.   The proposed approach is dependent on simplifying the sentence representation by extracting key concepts and mapping them using ConceptNet…What happens if none of the concept in the given sentence or end sentence is found in ConceptNet? How the model deals with Out of Vocabulary (OOV) concepts?…Any empirical analysis on how it affect performance when there is more OOV concepts in the story.

	\shanshan{Our preliminary study shows that only 0.034\% of the sentences from the ROCStories data do not contain any ConceptNet concepts. 
Furthermore, fewer than 0.017\% of the ending sentences of a story contain no concept at all. Therefore we consider the OOV problem to be statistically 
insignificant and choose to ignore it. We add the above discussion to the Sec. 3.} 

4.  In Sec 3,..  ``Then we present a preliminary analysis on the ROCStory dataset to invalidate previous approaches…" – Do you mean previous approaches are all invalid for the task? ..why?

	\shanshan{As we pointed out in Sec 3.2, the previous methods are not invalid, but because they were all trained on the validation set which has information leak, their evaluation results are therefore invalid.}

5.  In Sec 3.2,.. ``The fact that humans score substantially worse than some of the `top' algorithms indicates that these algorithms are not using “commonsense” but rather the patterns leaked in the training data." --- ISCK model combines information like sentiment, commonsense and narrative sequence representation for story end prediction. Why ISCK model is not included in Table 3? Based on the explanation of information leak, have you considered comparing ISCK without sentiment information? From the ablation study conducted by Chen et.al. 2018, it seems like ISCK is still a strong model considering only commonsense knowledge version on ROC stories corpus.

\shanshan{In Table 3, we show the result for models trained only with story endings. However, ISCK requires sentiment and commonsense features which cannot be trained using the last sentence of a story alone. Thus it is not included in Table 3. Table 3 is there to illustrate the information leak problem and is not our main result. We did compare with ISCK in Table 4, which is our main results. We didn't compare with ISCK without sentiment because according to Chen 2018. that setup is worse than the ISCK model itself.} 

6. The results of FTML on SCT dataset in the paper differs substantially from that Radford et. al. -- any reason? Why results on ROCStories Test set have not been reported?

	\shanshan{What we report in this paper are indeed results on the SCT test set, same as in previous work on the task. The reason for the different results of FTML was explained in Sec 3, mainly due to the use of different training data.} 
	
7. In sec 3.3., ``In fact we logged 5\% improvement over RNN-BC which was the previous state-of-the-art." – which model is RNN-BC ..I can not see any information on Table 4 and sec 3.1. 

\shanshan{RNN-BC should be SKBC. This is a typo.}

Review4:

Comments to Authors.

This paper proposes and implements a methodology for determining
the end of a short story, by selecting one among two possible
endings. The process (seems to) identify concepts in the sentence
that also appear in ConceptNet, identify relations in ConceptNet
that relate those concepts, and then use learning over a training
set of data whose negative labels are created automatically to
learn how to predict the correct ending in a given testing set.

The problem seems interesting, in that it has potential to be
extended to question answering in stories, which itself is a an
important problem for commonsense reasoning. To my understanding,
the solution is also potentially interesting in that it combines
symbolically-represented knowledge from ConceptNet with deep
learning techniques to create a predictive model for the task.

\ssemp{I found it very difficult to follow the paper. It presents the
technical parts of the methodology at a very high level, making it
hard to appreciate how various things have been implemented. A
running example and many more technical details would seem
necessary, especially since the paper has extra space that it could
use. As it stands, the paper cannot be said to be self-contained.}

Also, there is a lot of work in the literature that makes the point
that the use of symbolically-represented knowledge is a central
component in both human and computational story understanding
(e.g., work by Michael and colleagues on ``Story Understanding...
Calculemus!" and ``Knowledge Activation in Story Comprehension"). In
particular, the literature seems to suggest that causal knowledge
(which does exist in ConceptNet, but might be only a small
fraction) is the key form of knowledge used in story understanding.
The authors might want to consult this literature and see how to
adapt their approach to get even better results, or to offer
evidence for or against the suggested role of causal knowledge.

The paper must be checked for the proper use of English.

A minor point: Claiming that the words not highlighted in Figure 1
are not important in the understanding of stories is simplistic.
Pronouns, for example, sometimes play a key role in story
understanding, and cannot simply be ignored. You may want to
rephrase the claim to say that you simply ignored those words.

\shanshan{Thank you for the suggestions. We will carefully proofread the 
paper, improve English and clarify important details in the revised version.
In this paper, our main contribution is in fact the discovery 
that commonsense concepts plays an important role in understanding 
the logic of stories. Hence we choose to ignore other parts of 
the sentences such as pronouns and person names.}
		
\end{document}
