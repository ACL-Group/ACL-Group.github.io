\section{Conclusion}
\label{sec:conclude}
We presented a distributed attention mechanism to modify existing CNN seq2seq model 
and were able to train a model that produces 
summaries without repetition that are fluent and coherent. 
Compared with the existing summarization methods, we show that our model has the ability to 
generate summaries without repetition on its own using its internal state.
The summaries generated by our model are more accurate and get higher ROUGE score. 
%We find that the basic CNN seq2seq model 
%still has some problems, such as generating repeated word sequence. 
%We also argue that ROUGE is not a perfect evaluation metric for the abstractive 
%summarization. Our future work will focus on these two aspects.
