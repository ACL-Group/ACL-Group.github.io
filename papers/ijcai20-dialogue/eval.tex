\section{Experimental Setup}
\label{sec:eval}
In this section, we introduce the datasets, baselines and implementation details of our model\footnote{All of the source codes
can be found in http://Anonymous.}.

\subsection{Datasets}
Our experiments are performed on three datasets:
UbuntuV2, DSTC 7 and DSTC 8. %\cite{}
\itemsep0em
\begin{itemize}

\item \textbf{UbuntuV2}~\cite{LowePSCLP17} consists of two-person conversations extracted from the Ubuntu chat logs.
%is a large popular corpus for research in unstructured
%multi-turn dialogue systems, which 

\item \textbf{DSCT7}~\cite{gunasekara2019dstc7} refers to the dataset for subtask1 consisting of two-party conversations.
%is the sentence selection track using a new Ubuntu corpus\cite{KummerfeldGPAGG19}. We use the dataset variant in subtask1 consisting of two-party conversations extracted from the char logs.

%The corpus consists of two-person conversations extracted from Ubuntu chat logs.
%There are 7,659 training pairs, 1,787 validation pairs and 1,710 test pairs in DSTC 7.
%We need predict the response based on conversation from 100 options.

\item \textbf{DSCT8} refers to the dataset for subtask 2, containing dialogues between multi-parties.
%is an extension of DSTC7 track1 where the dialogues are more difficult. We use the dataset for subtask 2, which containing dialogues between multi-parties without doing dialogue disentanglement.
\end{itemize}
More details of theses three datasets are in Table \ref{tab:dataset}.

\begin{table}[th]
	\centering
	\scriptsize
	\begin{tabular}{lrrr}
		\toprule[1pt]
		\textbf{} & {UbuntuV2}& {DSTC7}& {DSTC8} \\ 
    	\midrule[1pt]
		{Train} & 957,101 &100,000&112,262\\
		%\hline
		{Valid} & 19,560 & 5,000&9,565\\
		%\hline
		{Test} &18,920 &1,000&9,027\\
		%\hline
		{Candidates per dialog} &10& 10&100\\
		%\hline
		{Correct Response per dialog}&1&1&0/1 \\
		%\hline
		{Turns per dialog} & 3-19&3-75& 1-99\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{The statistics of the datasets used in this paper.}
	\label{tab:dataset}
\end{table}



%we focus on is task 2, which is the extension of DSCT 7 track 1.
%Unlike DSTC 7, DSCT 8 has the conversations with more than 2 participants in the Ubuntu data.
%It contains XXXX training pairs, XXXX validation pairs and XXXX test pairs.
%We need predict the next utterance in a conversation out of a set of 100 options,
%which in some cases will not contain the correct answer at all.



\subsection{Baselines}
We introduce several state-of-the-art baselines to compare with our results as follows.
\begin{itemize}
	 \item \textbf{DAM}~\cite{WuLCZDYZL18} is a hierarchical model based entirely on self and cross attention mechanisms.
	\item \textbf{ESIM-18}~\cite{abs-1802-02614} and \textbf{ESIM-19}~\cite{abs-1901-02609} are two sequential models, which are the modifications and extensions of the original ESIM~\cite{ChenZLWJI17} developed for natural language inference. The latter one ranked top on DSTC7 tasks.
	\item \textbf{IMN}~\cite{GuLL19} is a hybrid model with sequential characteristics at matching layer and hierarchical characteristics at aggregation layer.
	\item \textbf{Bi-Encoder} (Bi-Enc), \textbf{Poly-Encoder} (Poly-Enc) and \textbf{Cross-Encoder} (Cross-Enc)~\cite{humeau2019poly} are the state-of-the-art models based on pretrained model. 
\end{itemize}

%\begin{table}[th]
%	\centering
%	\scriptsize
%	\begin{tabular}{|l|l|}
%		\hline
%		\textbf{Abbrev.} & \textbf{Description} \\ \hline
%		\textbf{ESIM 2018} & decription and citation  \\
%		\hline
%		\textbf{ESIM 2019} &  \\
%		\hline
%		\textbf{IMN} & \\
%		\hline
%	    \textbf{DAM} & \\
%		\hline
%	    \textbf{Bi-Enc}	& \\
%		\hline
%	    \textbf{Poly-Enc} & \\
%		\hline
 %      \textbf{Cross-Enc} & \\
%		\hline
%	\end{tabular}
%	\caption{Baselines}
%	\label{tab:baselines}
%\end{table}


\subsection{Implementation Details }
%To develop a dialogue parsing base on the dialogue parsing model\cite{ShiH19} as described in Section \ref{sec:DSA} \YZ{?}, 
According to Section \ref{sec:DSA}, we firstly transform the dialogue disentanglement dataset~\cite{KummerfeldGPAGG19} with ``reply-to'' labels. This is also the new ubuntu dataset adopted by DSTC7 and DSTC8. Turns are clustered if there exits a ``reply-to'' edge, 
and we obtain 4,444 training dialogues from the original training set and 480 test dialogues 
from the original valid set and test set. Only 7.3\% of turns have multiple parents. 
Since the parser can only deal with dependency structure with single parent, 
we reserve the dependency relation with the nearest parent in these cases. 
We trained a new parser on this new dataset. The results on the new test set are shown 
in \tabref{tab:parser}. It shows that in-domain data are useful for enhancing the 
results for dialogue dependency prediction.

\begin{table}[th]
	\centering
	\scriptsize
	\begin{tabular}{lccc}
		\toprule[1pt]
		\textbf{} & {Precision}& {Recall}& {F1} \\ 
		\midrule[1pt]
		{Trained on STAC} & 67.37 &64.43&65.86\\
		%\hline
		{Trained on the new dataset} & 71.44 & 68.32&69.85\\
		\bottomrule[1pt]
	\end{tabular}
	\caption{The results of the dialogue dependency parser.}
	\label{tab:parser}
\end{table}


Our model is trained with Adamax optimizer. The initial learning rate and learning rate decay is $5\mathrm{e}{-5}$ and $0.4$ respectively. The candidate response are truncate at $72$ tokens, covering more than $99\%$ of them. The last $360$ tokens in the concatenated sequence of each thread are reserved. The BPE tokenizer were used. 
We set the batchsize as $64$ and evaluate the model on valid set every $0.5$ epoch. 
Training terminates when learning rate is $0$ or the hits@1 on validation no longer increases within $2.5$ epochs. 
The threshold in the Algorithm~\ref{alg:DSA} is set to $0.2$.
%The threshold is set to $0.2$ for the dialogue segmentation algorithm which providing more reasonable number of threads and utterances in each thread \YZ{?}. 
%For DSTC8 dataset, to deal with the non-existence of correct response, we follow the operations in official baseline\footnote{\url{https://github.com/dstc8-track2/NOESIS-II/}} with the "None" candidate\footnote{This may not be the best way to deal with the non-existence of the correct response. For this dataset, we mainly implement the results of the state-of-the-art baselines with pre-trained language model. These models are trained in the same way as our own model so that the results are fair and comparable.}. 
As some coversations in DSTC8 do not have correct response, we follow the operations in official baseline and provide the candidate response ``None''. 
%to these dialogues. 

%~\footnote{\url{https://github.com/dstc8-track2/NOESIS-II/}} 
