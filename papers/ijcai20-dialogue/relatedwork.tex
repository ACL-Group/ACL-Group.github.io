\section{Related Work}
\label{sec:relatedwork}
\subsection{Dialogue dependency parsing}
Discourse parsing has been researched by scientists especially in linguistics for decades. 
%RST~\cite{mann1988rhetorical,taboada2006rhetorical} and PDTB~\cite{prasad2008penn} are the most widely accepted discourse theories in recent years with well-known treebanks. However, these theories are not suitable for dialogues where non-adjacent discourse units may have direct relations and the discourse structures is non-projective. 
Asher and Lascarides~\shortcite{0031949} proposed the SDRT theory with the STAC Corpus~\cite{AsherHMBA16} which made great contribution to the discourse parsing on multi-party dialogues. Shi and Huang~\shortcite{ShiH19} proposed a sequential neural network and achieved the state-of-the-art results on this dataset. Another similar task is dialogue disentanglement~\cite{DuPX17}. This task isn't focusing on developing discourse theories but trying to segment the long dialogues according to topics. It takes each turn in the dialogue as a unit, and only care about whether there is a relation between two turns, which is called ``reply-to'' relation. Due to the scarcity of annotated dialogues across domains under SDRT theory, the predicted dependency relations has never used for down-streaming tasks, such as response selection and dialogue summarization. In this paper, we take advantage of both the simplicity of the ``reply-to'' relation and the sequential parsing methods~\cite{ShiH19} to do dialogue dependency parsing. 
%\KZ{Maybe you can say discourse parsing has been previously used in xxx, but has never been
%applied to dialogue response selection.}
%This theory takes a clause-level units as elementary discourse units (EDUs), and define 16 relation types to depict the relations between each pair of EDUs. Annotating dialogues under such theory requires experts and time-consuming. 

%Methods including maximum spanning trees~\cite{AfantenosKAP15}, integer linear programming~\cite{PerretAAM16} and neural networks~\cite{ShiH19} are develop for multi-party dialogue parsing and achieve reliable results. 
%Besides the feature based methods \cite{WangO09,ShenYSC06}, many statistical approaches \cite{DuPX17} were also proposed, including deep learning ones \cite{TanWGWPGCY19}. However, these methods mainly focused on the performance of dialogue segmentation while ignored the efficiency of dependency parsing. 
%Methods designed for this task mainly focus on the performance of dialogue segmentation while ignore the practicality in real time. 


%This corpus relating to games takes several years being annotated by linguistics who knows the SDRT theory (including 16 different relation types between discourse units) well and is nearly impossible to reproduce in other domains by crowdsourcing.



\subsection{Multi-turn response selection}
\label{sec:mtrs}
Multi-turn response selection task was proposed by Lowe et al.~\shortcite{LowePSP15} and the solutions for this task can be classified into two categories: the sequential models and the hierarchical models. To begin with, the sequential models~\cite{LowePSP15} were directly copied from the single-turn response selection task since we can regard the multiple history turns as a long single turn.  Considering the multi-turn characteristic, Wu et al.~\shortcite{WuWXZL17} proposed the sequential matching network (SMN), a new architecture to capture the relationship among turns and important contextual information. SMN beats the previous sequential models and raises a popularity of such hierarchical models, including DUA~\cite{ZhangLZZL18}, DAM~\cite{WuLCZDYZL18}, IOI~\cite{TaoWXHZY19}, etc. The ESIM~\cite{abs-1802-02614}, which is mainly based on the self and cross attention mechanisms and incorporates different kinds of pretrained word embedding, changed the inferior position of the sequential model, making it hard to say which kind of architecture is better.

% pretrained language models
Due to the popularity of the pretrained language models such as BERT~\cite{DevlinCLT19} and GPT~\cite{radford2018improving}, the state-of-the-art performance on this task were refreshed~\cite{vig2019comparison}. Work such as \cite{abs-1908-04812} and \cite{humeau2019poly} further shows that we can enhance the response selection performance by pretraining or fine-tuning the language models on open domain dialogue datasets such as Reddit~\cite{abs-1904-06472}, instead of single text corpus such as BooksCorpus~\cite{ZhuKZSUTF15}. These models can be also regard as the sequential models because they concatenated all the history turns as the input to the language model while ignoring the dependency relations among the turns. Inspired by these work, we incorporate the dependency information in the dialogue history into the response selection model with the pretrained language model on dialogue dataset.
%also use the pre-trained language model on dialogue dataset as a basic component in our model while incorporating the dependency information. 



