\section{Approach}
\label{sec:approach}

Given a natural language relation with its training relation
instances, our inference model first generates candidate 
schemas from its training relation instances, and then constructs a
probabilistic distribution over all the candidates.
Due to the lack of direct $\langle relation,\, schema \rangle$
training data, our learning model is distant supervised, we design
a data-driven function to label each candidate with ``silver'' 
score, and then perform the learning step based on ``silver'' labels.

% talk about model here
Suppose we have $N$ training relations in total.
We define $GEN(r)$ as the generation set of candidate schemas for 
the relation $r$.
The conditional probability of a schema $s \in GEN(r)$ is produced 
by log-linear model:
\begin{equation}
  p(s|r; \vec{w}) = 
    \frac { 
      exp \{ \vec{f} (s, r) \cdot \vec{w} \} 
    } { 
	  \sum\limits_{s' \in GEN(r)} { 
	    exp \{ \vec{f} (s', r) \cdot \vec{w} \} 
	  }
    },
\end{equation}

\noindent
where $\vec{f} (s, r)$ is the feature vector extracted from the
relation and the schema, and $\vec{w}$ represents the vector of
feature weights, which we are going to learn.
As mentioned above, we define the silver labeling function on a 
schema ranging from 0 to 1, which is denoted by $lb(s, r)$.
The silver labeling function approximates the correctness of one
schema, and the goal of training is to minimize the negative 
log-likelihood function of correct schemas over all the candidates.
The formal loss function is shown below:
\begin{equation}
J(\vec{w}) = \lambda \| \vec{w} \|_2^2 \! - \!  
  \frac {1} {N} \! 
  \sum\limits_{i=1}^{N} {
    log \! \sum\limits_{s \in GEN(r_i)} { 
	  lb(s, r) p(s|r; \vec{w}) 
	}
  },
\end{equation}

\noindent
where $\lambda$ is L2 regularization parameter.
In the following sections, we describe generation set, silver 
labeling function and feature functions in detail.


% then 3 step in detail
% 1. candidate generation (BFS + DFS)
\input{candgen}
% 2. silver labeling (ratio function)
\input{label}
% 3. learning step (feature selection)
\input{feature}
