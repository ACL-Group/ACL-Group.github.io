\section{Experiments}
\label{sec:eval}

The following experiments are needed for ACL submission:
\begin{itemize}
  \item KBC evaluation on standard dataset. This is what we've done
        now. We need to show that our schema-base approach performs
		as good as state-of-the-art.
  \item KBC evaluation on biased dataset. We need to eyeball PATTY
        relations and select those complex relations. Then we can
		compare skeleton-based and schema-based approach.
  \item QA evalution on biased dataset. Based on complex relations,
        we find factoid questions (and make some simple modifications
		if needed) from Yahoo! Answers. And we need to find a way
		to plug our model in some existing QA systems.
  \item Relation simialrity dataset to show the expressiveness of
        compact and human-readable representations, instead of
		massive features or purely data-driven model. 
		We first extract PPDB relation pairs and eyeball selecting 
		good pairs (no same stemmings in PPDB and corresponding 
		relations in PATTY could get enough entity pairs).
		This time, ``co-occurrence $\geq$ 1'' should be removed, 
		otherwise we couldn't find enough pairs.
		Baselines could be word-based w2v, or simply counting EP
		overlap between different PATTY relation instances.
\end{itemize}

And we have a list of ablation tests:
\begin{itemize}
  \item What's the result if we won't add any constraints?
  \item What's the result if we change a naive labeling function?
        Maybe version 4, version 1 and naive ``positive ratio + 
		negative ratio'' could be baselines.
  \item What's are the running times and results when we change a
        different budget?
  \item (Optional) compare the results generated from different
        negative entity pairs. Maybe some generating strategy can
		show a better difference of results between skeleton-based
		and schema-based methods.
\end{itemize}


In our experiment, we present experimental results to evaluate the schemas
that we learned. We first evaluate the quality on a manually labeled
complex relation dataset on Freebase, then we compare with state-of-the-art systems
on an Open IE relation dataset, showing the result on the noisy dataset.




\subsection{Experimental Setup}
To evaluate the quality of paraphrasing, we build two relation datasets containing 
positive and negative $\langle e_1, r, e_2 \rangle$ relation triples, and perform
paraphrasing task over Freebase.
In our experiment, we build a small version of Freebase dump (June 2015).
We pick 2,000,000 different entities with highest popularity score (counted by
number of triples the entities occurs), along with intermediate entities that
connect at least 2 popualr entities. All ordinary relations between these entities
are kept, and all isa relationships between entities and types are kept.
\tabref{tab:fb-size} shows the detail size of this knowledge graph.

\begin{table}[ht]
	\centering
	\caption{Size information of Freebase}
	\begin{tabular}{|c|c|}
		%\toprule
        \hline
		Entities	 & 2,000,000 \\
        \hline
        Intermediates	 & 6,983,095 \\
		\hline
		Nodes & 8,983,095 \\
		\hline
		Distinct Relations & 4412 \\
        \hline
		Types & 2063 \\
		\hline
	\end{tabular}%
	\label{tab:fb-size}%
\end{table}


The first relation dataset is manually created. We picked 10 natural language relations
that have correct schemas in Freebase. Each relation is labeled with at least one
golden schema. Then we perform schema query over Freebase, producing a golden
list of $\langle e_1, e_2 \rangle$ pairs, where each entity are already linked to Freebase.
Entity linking is a fundamental task in NLP and is not the main contribution of works
in paraphrasing (both ours and state-of-the-art works), we use golden entity links 
to factor out this issue. Since the golden entity pairs are generated by querying
knowledge graph, each positive case is known to be correct in this dataset.
Therefore, we regard it as a \textit{clean} dataset.

The other relation dataset comes from PATTY \cite{nakashole2012patty} which is an Open IE system.
PATTY used lexical-syntactic patterns to extraction relations from natural
language sentence in Wikipedia, and group similar patterns into relation synsets with Freebase
type signatures at both domain and range.
We merge different relation synsets together, if they share the same lexical-syntactic
pattern, but differ from type signatures. 
There are 201,816 different relation synsets after merging, and around 1800 popular
relation synsets have more than 500 positive entity pairs.
Each entity in PATTY is disambiguated as a Wikipedia concept, then we apply a simple
conversion, resulting entity pairs linked in Freebase.
Compared to the previous dataset, PATTY dataset more close to real world scenario,
where both information extraction and entity resolution could bring noisy data.
We randomly pick 100 synsets out of 1800 popular relation synsets, and build the 
\textit{noisy} dataset.


\subsection{Adding Negative Entity Pairs}
Obtaining negative entity pairs for each relation is a crucial task.
While both dataset only contains positive entity pairs right now, we build 
negative entity pairs based on the close-world assumption.
For the clean data, the negative example comes from 3 strategies: i) querying auto-generated
negative schemas, by just removing all branches from its original gold schemas;
ii) searching neighbours of $e_1$ in Freebase within the distance of 2 predicates, 
collecting entities that have same (or similar) type with all positive $e_2$ in this relation;
iii) just sampling randomly from popular entities which also satisfy same (or similar) type constraint.
The ratio of negative entities generated by these 3 parts is about 1:2:3.
For the noisy data, since we do not have golden schema, we use the strategy ii) and iii)
mentioned above to automatically build negative entity pairs.

The ratio of numbers between positive and negative entity pairs is 1:6 for
both clean and noisy dataset. 
Afterwards, for each relation, we split entity pairs into training and testing data
under the ratio 4:1. In addition, all entity pairs with the same $e_1$ are put
together (either in training or testing set), making all $e_1$ unseen
in the training set. Followd by Matt's work, we evaluate our systems by predicting whether
$\langle e_1, e_2 \rangle$ entity pair is correct for the relation or not.
And we use MAP score to measure the quality of results.



\subsection{Paraphrasing on Clean Dataset}
We compare our work with two state-of-the-art systems. First one is the work of
Matt et al. \cite{gardnerefficient}, they extract subgraph features to learn the connections
between NL relation and knowledge graph structures. Due to variuos kinds of features
are introduced in their work, we perform 3 tests over different feature settings;
SFE-Bigram (subgraph + bigram feature), SFE-OneSide (subgraph + one-side unary feature) and
SFE-AnyRel (subgraph + ``ANYREL'' wildcard feature). Another system is proposed by
Zhang et al. \cite{zhang2012ontological}, where they built an Markov Logic Network for 
mining the importance of different inference rules among relations and a candidate schema.
Their original model included entity linking from NELL entities into Freebase, 
so we re-implement their algorithms and remove linking-related rules, such that the MLN
approach is adapted to our dataset. 

The evaluation results for each relation and overall MAP score are shown in \tabref{tab:result-clean}.
Note that Zhang et al's work doesn't output a score for each 
We can see that, for this data, the MLN framework get a relatively poor result.
It may caused by the constraint in building paths, where different predicates pointing from (or
pointing to) one node must share the same relation type signature at domain (or range) side.
Compared with SFE series, our work reaches the best score, which shows the point that
information beyond simple paths could increase the paraphrasing quality.
Meanwhile, it's interested to see that SFE-Anyrel could also get a nearly perfect score
on cases like ``hasMother'' and ``hasGrandFather''.
Intuitively, it's impossible for the system to reach such high score, since SFE-Anyrel
actually do not extract any features out of the path. 
However, we find a special predicate ``sports.sports\_team.gender'', where in our Freebase
data, only the gender female is connected to some sports teams via this predicate.
Therefore, a biased knowledge graph would help these path-oriented approaches reaching a
higher result.


% 1. we best
% 2. all good
%3. interesting point


\begin{table*}[ht]
	\centering
	\caption{Evaluation result on clean dataset. Our method reduces
	the error rate by 51.9\%}
	\label{tab:result-clean}
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline
			Relation & SFE-Bigram & SFE-Anyrel & SFE-OneSided & Zhang 2012 & Our Approach \\
		\hline
			characterCreated & 1.000 & 1.000 & 1.000 & 0.973 & 1.000 \\
		\hline
			hasGrandFather & 0.970 & 0.995  & 1.000 & 0.734 & 1.000 \\
		\hline
			hasMother & 0.987 & 0.988 & 1.000 & 0.756 & 1.000 \\
		\hline
			lakeInState & 0.750 & 1.000  & 1.000 & 0.615 & 1.000 \\
		\hline
			playIn & 0.519 & 0.429 & 0.551 & 0.756 & 0.726 \\
		\hline
			presidentOf & 1.000 & 1.000 & 1.000 & 0.696 & 1.000 \\
		\hline
			receiveDoctorDegreeFrom & 0.629 & 0.733 & 0.633 & 0.405 & 1.000  \\
		\hline
			riverFlowsThroughCity & 0.592 & 0.564 & 0.622 & 0.681 & 0.689 \\
		\hline
			stateCapitalOf & 1.000 & 1.000 & 1.000 & 0.688 & 1.000 \\
		\hline
			teamHasForward & 0.216 & 0.342 & 0.887 & 0.037 & 0.949 \\
		\hline
			MAP Score (F1 for AAAI 2012) & 0.766 & 0.811 & 0.869 & 0.701 & \textbf{0.937} \\
		\hline
	\end{tabular}
\end{table*}

\subsection{Paraphrasing on Noisy Dataset}
We perform experiments on the PATTY relation dataset, evaluting the quality
of our system on a noisy data.
Following the clean data test, we compare our results with SFE-Anyrel, SFE-OneSided
and Zhang.


%1. alternative of MDL, check difference on principles
%2. ablation test, DFS/ without DFS
%3. state-of-the-art, find new papers
%4. VELVET 2012 implementation
%5. entity linking (self, others)
%6. make bfs faster. robustness even in noisy situation
%(optional) 6. down-streaming application (QA?)
