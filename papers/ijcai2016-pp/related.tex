\section{Related Work}

%\KZ{First discuss AAAI 2012 and EMNLP 2015, their pros and cons and how
%we stack up with them. Then discuss other less similar work. Finally
%applications that can benefit from this work, eg. QA, etc.}

Recently, more advanced approaches have been proposed that attempt to solve paraphrasing tasks. One of the important paraphrasing problem is mapping target relations in natural language to a large background knowledge base (KB).

% introduce AAAI2012, analyze pros and cons
Representative works like \textit{VELVET} \cite{zhang2012ontological} is based on such a procedure called ontology mapping. Given a user-specified relation along with its labeled instances, ontology mapping actually generates several complex SQL expressions over types and relations on the KB's schema. This procedure is quite difficult since the space of possible SQL views can be extremely large. In order to reduce the search space and select the best views, the authors first generates several constraints (hard rules) described in Markov Logic. This step actually is the a procedure to generate simple candidates schemas. Then, the probability of mappings is described using Markov logic Network after adding different rules into the network. Through weight training and relaxing the optimization problem to a linear problem, those candidate schemas with a high probability form to the mapping result. This work is able to show a set of best schemas for a target relation. However, the authors add several hand-crafted soft rules to Markov Logic Network which limits the dimension of feature space. Besides, the complex SQL views can actually be transformed to simple schemas (paths) which can not be able to handle complex relations in natural language form.

Other feature based supervised works \cite{gardnerefficient,gardner2014incorporating,lao2011random} also map a relation to several background KB schemas. The goal of these works is to complete the imperfectly extracted knowledge base \textit{NELL} \cite{carlson2010toward} by predicting all concept $b$ which potentially have the relation $R(a, b)$ given a concept $a$. \cite{lao2011random} used a random walk path finding algorithm to inference new relation instances by mapping the target \textit{NELL} relation to a join of several basic relations. \cite{gardnerefficient} examines the disadvantage of Path Ranking Algorithm and proposes a technique called subgraph feature extraction (SFE). It first do local search to characterize the graph around each input node of KB. Then SFE runs a set of feature extractors over these subgraphs to get the feature vectors for each node. SFE outperforms other state-of-art KB completion methods since it explores deeply on improving PRA. However, These work only consider the simple schemas, and can not show explicit resulting schemas for a target relation. Comparing to this work, our method adopt complex schemas forms with additional attributes, which can also describe relations in natural language form. Natural language relations always have more complex meaning than KB predicates and using our system, a target human raised relation can be mapped into an explicit readable schema graph.

To solve the paraphrasing problem between natural language relations and KB predicates, we aim to represent a human raised relation with several explicit schemas. One major difference between our technique and others is that during the procedure of generating candidate schemas for target relations, we do not limit the schemas to be simple only. We fully utilize the information of KB, adding extra constraints to the simple schemas and resulting in more complex schemas. Specifically, we perform a breadth-first search to construct the skeleton of a specific relation schema which is similar as the path finding procedure in the previous works \cite{gardnerefficient,gardner2014incorporating,lao2011random,zhang2012ontological}. Beyond relation path, we use a depth-first search to further add more information attributes to the relation path generated in the first step and transform it into a more specific and complex graph form, under the guidance of \textit{Minimum Description Length} (MDL) \cite{fisher2008dirt,grunwald2007minimum} principle. MDL principle is used as a trade-off since it measures the cost of transmitting both schemas and entity pairs.

Our work is also related to unsupervised systems \cite{zou2014natural}, which calculates scores of candidate schemas using TF-IDF and then choose the best one to represent the target relation.
As for concrete mapping format, MapOnto \cite{an2006discovering} used Horn clauses when produces mapping rules between two schemas. Others \cite{zhang2012ontological} generates complex SQL queries consisting of operations like join, union, project and select as mappings.
On the part of schema graph search, \cite{zou2014natural} interpret a natural language question as a semantic query graph where each vertex represented an argument and each edge is associated with a relation phrase. Comparing to them, our schema graph is more complicated since we divide edges into three categories and.

Besides, our work is similar to query synthesis. Given an example input and output of how the database should be queried, query synthesis is a task of automatically synthesizing a SQL query which can be applied to any other databases with a similar schema, and the synthesized SQL query should reproduce a corresponding result that is similar to the example output. This problem is similar to ours since the input and output examples of query synthesis is similar to the input entity pairs extracted by the target relation in our paraphrasing problem and both of our goals is to generate a new schema (SQL query) to best cover the input examples. \cite{zhang2013automatically} addresses query synthesis using a three steps technique. First the authors create an incomplete query skeleton which captures the basic structure of the result query and then complete the skeleton by adding some concrete and accurate rules and generates a list of candidates. These two steps are with the same goal of our candidate schema generation procedure where BFS is to find the skeleton of schema then DFS is to add more attributes. The last step is to rank the candidates though different strategies are used. \cite{cheung2012inferring,cheung2013optimizing} also presents an algorithm to infer relational specifications from imperative code fragments which retrieves data and then transform into SQL queries, while others \cite{niehren2013query,das2010synthesizing} try a different way.

Paraphrasing has been proven useful in areas like question answering (QA) \cite{harabagiu2006methods}, relation extraction \cite{romano2006investigating}, machine translation and so on. For instance, \cite{berant2014semantic} attacked semantic parsing by mapping natural language utterances into logical forms to be executed on a KB using a paraphrase model and furthermore improved QA performance. Graph-based representations \cite{reddy2014large} is usually used in exploiting structural and conceptual similarity between natural language and KB. Similar works \cite{berant2013semantic,fader2013paraphrase,kwiatkowski2013scaling} also apply paraphrasing techniques into question answering system. In the previous mentioned works, \textit{VELVET} \cite{zhang2012ontological} learned a relation extractor after paraphrasing between different ontologies. \cite{gardnerefficient,lao2011random} are able to improve KB completion result after paraphrasing target relation to several other relations in KB. 
