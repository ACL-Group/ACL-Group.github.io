\subsection{Mining Cross-cultural Differences of Named Entity}
\label{sec:mcdne}
This task is to discover and quantify cross-cultural differences of concerns and topics towards name entities. We first explain how we obtain the ground truth, then present several baseline methods to this problem and finally 
show our experiment results in detail.

\subsubsection{Ground Truth}
Lacking settled measures of cultural differences in named entities, we propose to apply the distributional hypothesis of Harris~\shortcite{harris1954distributional}, which states that the meaning of words is evidenced by the contexts 
they occur with. 
Similarly, we assume that the cultural properties of entity can be evidenced 
by the terms they co-occur with. 
Thus, for each named entity, we present four human annotators with two list of top 20 words ranking by their occurrence with the named entity, from Twitter and Weibo respectively. We select 700 named entities for annotators to label. These entities
are the most frequently mentioned both in Twitter and Weibo. 
Annotators rate relatedness between the two word lists from 1 to 5, 
where 1 indicates the lists are very different and 5 means they are 
the most similar. The ground truth similarity score 
for each entity is averaged over all annotators' scores. For classification problem, an entity 
is considered culturally similar if the score is larger than 3.0, and culturally different otherwise.
The inter-annotator agreement is 0.531 by Cohen's kappa coefficient, suggesting moderate correlation.
%\BL{ (0.531 without hanyuan)}

\subsubsection{Baseline and Our Methods}

We propose five baselines that can be categorized into \emph{distributional} and \emph{transform}-based
work.
First category comprises of three baselines, comparing the lists of surrounding
 English and Chinese terms, denoted as $L_E$ and $L_C$, by computing
 the cross-lingual relatedness between the two lists, though different baselines differ in terms of word selection and similarity computation.
Second category computes the vector representation in English and Chinese corpus respectively
then trains a transformation.
% of words, known $L_E$ and $L_C$. The differences of these methods are the selecting method of terms and %the computation method of two word lists.  ii) the second type of baseline methods are first obtain the %comparable vectorial representation of the English title and Chinese title of the given entity, and then just %calculate the similarity between two comparable vectors.

The first three baselines fall into the distributional category:
\begin{itemize}
	\item {Bilingual Lexicon Jaccard Similarity (BL-JS)}
%	The $L_E$ and $L_C$ of both BL-JS and WN-WUP  are the same as the lists that annotators judge.
 BL-JS uses the bilingual lexicon to translate $L_E$  to a Chinese word list $L_E^*$ as a medium and then calculates the Jaccard Similarity between $L_E^*$ and $L_C$ as $J_{EC}$. Similarly, we can compute $J_{CE}$. Finally, use $\frac{J_{EC}+J_{CE}}{2}$ as the cross-cultural similarity of this given name entity.
	\item {WordNet Wu-Palmer Similarity (WN-WUP)} 
	Instead of using the bilingual lexicon and Jaccard Similarity, WN-WUP uses Open Multilingual Wordnet~\cite{wang2013building,bond2013linking} to calculate the average similarity of two lists of words from different languages.
	\item {Word Embedding based Jaccard Similarity (EM-JS)}
	EM-JS is very similar to BL-JS, except for that its $L_E$ and $L_C$ are generated by ranking the similarities between the name of entities and all English words and Chinese words respectively. 
\end{itemize}

The second type baseline methods are as follows:
\begin{itemize}
	\item {Linear Transformation (LTrans)}
	We follow the steps in Mikolov et al.~\shortcite{Mikolov:2013tp} to train a transformation matrix between \textit{EnVec} and \textit{CnVec}, using 3000 translation pairs with confidence of 1 in the bilingual lexicon. Given a named entity, this solution would simply calculate cosine similarity between the \textit{EnVec} of its English name and the \textit{transformed} \textit{CnVec} of its Chinese name. 
	\item {Bilingual Lexicon Space (BLex)}
		This baseline is similar to \textit{SocVec} but it does not utilize socio-linguistic vocabulary and simply projecting \textit{EnVec} and \textit{CnVec} by the similarities to all the word pairs in lexicon.
\end{itemize}

Given a named entity with its English name and Chinese name, our method is to simply regard the similarity between their \textit{SocVec}s as its cross-cultural difference. 

\subsubsection{Experimental Results}
\tabref{tab:mcdne_res_4} shows some of the most culturally different named entities obtained from our method. The listed hot and trending topics on Twitter and Weibo are manually summarized from our ground truth words for presentation purpose. It is obvious that the listed entities all have large divergence on topics over Twitter and Weibo messages, thus reflecting the mined cross-cultural differences.
\begin{table}[ht]
	\scriptsize
	\centering
	\caption{\small Selected culturally different named entities, with Twitter and Weibo's trending topics manually summarized}
	\begin{tabular}{|L{1.2cm}|L{3cm}|L{3cm}|}
		\hline
		\textbf{Entity} & \textbf{Twitter topics} & \textbf{Weibo topics}
		\\ \hline
		Maldives & coup, president Nasheed quit, political crisis & holiday, travel, honeymoon, paradise, beach \\ \hline
		Nagoya & tour, concert, travel, attractive, Osaka & Mayor Takashi Kawamura, Nanjing Massacre, denial of history\\  \hline
		Quebec & Conservative Party, Liberal Party, politicians, prime minister, power failure & travel, autumn, maples, study abroad, immigration, independence   \\ \hline
		Philippines & gunman attack, police, quake, tsunami & South China Sea, sovereignty dispute, confrontation, protest  \\ \hline
		Yao Ming & NBA, Chinese, good player, Asian  & patriotism, collective values, Jeremy Lin, Liu Xiang, Chinese Law maker, gold medal superstar   \\ \hline
		University of Southern California & college football, baseball, Stanford, Alabama, win, lose & top study abroad destination, Chinese student murdered, scholars, economics, Sino American politics \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_4}
\end{table}

In~\tabref{tab:mcdne_res_1}, we evaluate the baseline methods and 
our approach with three metrics: Spearman correlation and 
Pearson correlation on the ranking problem, and Mean Average Precision (MAP)
on the classification problem.  
Monolingual word vectors are trained with 5-word context window and 150 dimensions.
We choose cosine similarity as the \textit{sim} function to compute the similarity within the \socvec~space.
The \textit{BSL} of \textit{SocVec:opn} uses only OpinionFinder as English socio-linguistic vocabulary, while \textit{SocVec:all} also uses Emapth lexicon. All models uses ``\textit{Top}'' pseudo-word generator
(see \secref{sec:model}).
Results show that our \textit{SocVec} models perform best and 
the more lexicon are devised the correlation is better.
\begin{table}[th]
	\small
	\centering
	\caption{\small Comparison of Different Methods}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Method} & \textbf{Spearman} & \textbf{Pearson}  & \textbf{MAP} \\ \hline\hline
		BL-JS& 0.276 & 0.265 & 0.644   \\ \hline
		WN-WUP  & 0.335 & 0.349 & 0.677 \\ \hline
		EM-JS & 0.221 & 0.210  & 0.571\\ \hline
		LTrans& 0.366 & 0.385  & 0.644  \\ \hline
		BLex& 0.596 & 0.595  & 0.765 \\ \hline\hline
		 SocVec:opn& 0.668 & 0.662   & \textbf{0.834} \\ \hline
		 SocVec:all& \textbf{0.676} & \textbf{0.671}  & \textbf{0.834}\\ \hline
	\end{tabular}
	\label{tab:mcdne_res_1}
\end{table}

\begin{table}[th]
	\centering
	\small
	\caption{\small Evaluation of Different Similarity Functions}
	\label{tab:mcdne_res_2}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Similarity} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline\hline
		PCorr. & 0.631 & 0.625 & 0.806\\ \hline
		L1 + M & 0.666 & 0.656 & 0.824 \\  \hline
		Cos & \textbf{0.676} & 0.669 & \textbf{0.834} \\ \hline
		L2 + E & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
\end{table}

\begin{table}[th]
	\centering
	\small
	\caption{\small Evaluation of Different Pseudo-word Generators}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Generator} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline \hline
		Max. & 0.413 & 0.401 & 0.726\\ \hline
		Avg. & 0.667 & 0.625 & 0.831\\ \hline
		W.Avg. & 0.671 & 0.660 & 0.832 \\  \hline
		Top & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_3}
\end{table}
Following the above setups, we also evaluate the effect of four different similarity options in \textit{\socvec},
namely, Pearson Correlation Coefficient (\textit{PCorr}.), L1-normalized Manhattan distance (\textit{L1+M}), 
Cosine Similarity (\textit{Cos}) and  L2-normalized Euclidean distance (\textit{L2+E}).
%It is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking.
We show the results in~\tabref{tab:mcdne_res_2}. We conclude that among these 4 options, \textit{Cos} and \textit{L2+E} perform the best. 
%Although it is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking, we can find that 
\tabref{tab:mcdne_res_3} shows effect of using four different 
pseudo-word generator functions (see~\secref{sec:pg}). We can conclude 
that ``\textit{Top}'' generator function performs best for 
it reduces the noise brought by ambiguity. 






