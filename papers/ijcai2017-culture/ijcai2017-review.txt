Review #20228
Comments to Authors.



This paper studies bilingual word representations obtained via social media  
and a bilingual dictionary. This allows them to obtain better results in
translating slang terms than with traditional automatic translation tools.



The paper is quite well-written, describes a thorough investigation of the topic,
studies interesting problems of cross-cultural differences and slang in
automatic translation, and the results presented seem to indicate a
significant improvement over other methods.



For a non-expert there are some details which could have been better
explained. For example, what is a word-vector? It must be a vector of
numerical values, but how are these numbers calculated? Is "embedding" a
synonym for the calculation of these vectors? The reader who is not familiar 
with Word2Vec has no idea what these vectors look like. 
The scale of the numbers given in Table 7 are not explained. Are they percentages?
Do you restrict attention to socio-linguistic terms because of the particular
applications or is it reduce lexicon size, or both? Would you have used
a different and specific lexicon for different applications (medical, legal, etc.)?




There are just a few presentation problems:



(1) Although the author's English is, in general, very good, there are some
systematic mistakes. For example, 'slang' is uncountable and therefore
'slangs' should be replaced either by 'slang terms' or 'slang' whenever
it occurs. The plural of corpus is corpora. There are many missing articles:
you need to add 'the' before: existing models, Nanjing Massacre, English
social word "fawn", Twitter corpus, First category, Second category,
effect of using, above-mentioned explanations, original glossary, target language,
average over all of them, similarity score, top 5 words, Jaccard similarity
coefficient, average cosine similarity, jth word, Linear transformation model,
literature.

Some other English corrections:
in such projection --> in such a projection
tool of building --> build for building
compute a cross-lingual similarity --> to compute a cross-lingual similarity
remains to be 1 --> remains equal to 1
in which are many-to-many --> in which many are many-to-many (?)
properties of entity --> properties of an entity
two list --> two lists
except for that --> except that
type baseline --> type of baseline
simply projecting --> simply projects
the more lexicon are devised the correlation is better: needs rewriting
evaluate bilingual slang lexicon --> evaluate a bilingual slang lexicon
a better metrics --> a better metric
These work aims --> Such work aims
they fails --> they fail
model are --> model is
Bilingual socio-lingusitic lexicon--> A bilingual socio-lingusitic lexicon
of bilingual lexicon --> of a bilingual lexicon



(2) 2 of the references are incomplete (Klementiev, Pennebaker 2016)


Review #22886

Comments to Authors.

This paper proposes a method to build bilingual embeddings that take into account sociocultural differences between words. The main goal is to provide a similarity measure between words in two different languages while incorporating these differences in sociocultural contexts. This is done by defining a shared space and projecting monolingual embeddings into this space using bilingual dictionaries and lists of socio-linguistic words. Experiments on mining sociolinguistic differences between entities and extraction of Internet slangs show improved performance compared to simpler baselines.



Most of the manuscript is understandable and the motivation is quite clear. But the paper has some issues, which I detail below:



- It is not clear why SocVec can actually be interpreted as a shared semantic space between the two languages. The vectors are built by taking cosine similarities between a word vector and all vectors for words in the socio-linguistic lexicon, if I understood correctly. I fail to understand why such vectors built on similarities are in the same space. If this is indeed the case this should be made explicit in the text.

- In the first paragraph of Section 2, the paper argues that the method can be used in any language pair. This is not exactly true since it needs lexical resources for the pair, like the socio-linguistic lexicon. The sentence should be rewritten to put the method in a better perspective.

- For both tasks, is there any overlap in terms of entities/slangs and the socio-linguistic lexicon used in the model? This is not clear from the text. If there is some overlap this might explain the better performance compared to the baselines. The key aspect is that the proposed model uses more information compared to the baselines (the socio-linguistic lexicon). By itself this is not a problem but if there's an overlap then your better performance could be explained just by plugging the additional information and not because your model is necessarily a good one.

- The manuscript has some bold sentences, such as "State-of-the-art machine translation systems are inadequate in translating such emerging slangs", in Section 1, and "... exact translation of them <slangs> are always missing from all dictionary". These are quite strong statements and need citations (or a through explanation) or should be properly rephrased.

- Finally, the manuscript would benefit from better proofreading as there are some style and grammar erros in some parts of the text.



Overall, I think the idea is an interesting one but unfortunately the issues I pointed above push the paper to a more borderline score, in my opinion.



Review #26453

Comments to Authors.

This paper proposes an approach to bring into the same space terms from different languages, such as English and Chinese. The new space is used for two experiments: one aimed at identifying different interpretations of the same term by different culture, and the other aimed at identifying translations of slang expressions.

The paper is to me a little bit confused and not easy to follow, which probably prevented me from grasping the significance of its contribution.
For instance, the connection of the proposed approach to the two experiments used to test it is not straightforward, nor is the relationship between the two experiments. The former experiment might well be run on texts about the same topic drawn from different sources in the same language, which would make irrelevant a translation step. The latter experiment compares an approach purposely developed to consider slang to other approaches designed to be more standard.


DETAILED COMMENTS

It is unclear to me why "the perception toward a concept or entity can be captured by opinion-related context". Different perceptions may rely on objective or neutral basis, opinion is not necessarily present. Giving the paper a specifically opinion-oriented perspective significangly reduces its range of applicability and relevance.

As regards the second task, it seems quite unrelated from the former, and much less related to "cross-lingual socio-linguistics".

The proposed approach is claimed to be general but applied only to English-Chinese. Is there any strategic reason why this pair of languages was specifically selected? On what basis this pair of languages can be considered representative, so that the generality claim is sufficiently backed? If representative, is it representative of any pair of languages or of just a subset of pairs (e.g., Anglo-Asian)?

Sec. 2.1 mixes symbols W and U, which I guess are the same (e.g.: "cosine similarity between W and U'" is it "U and U'")?

Sec. 2.2: "English and Chinese common words" what words are in common between English and Chinese?

Fig. 1: the explanation of this figure in Sec. 2.2 is very superficial (e.g., C_U and E_W are never defined).

As regards BSL, I guess that the same procedure shown in Sec. 2.3 for English words is also applied to Chinese words? Then, is there a way in which the final BSL maintains a distinction of originally English or Chinese words?

What is (N) in the Max formula for Pseudo(...)? Is it exactly the same as N the number of translations?

Sec. 3.2:
 - which nation and cultural background are associated to the four annotators?
 - is a kappa coefficient of 0.531 sufficient for basing the ground truth on it?
 - the relationship of this experiment with the approach to translation proposed in earlier sections is unclear to me. I mean, the comparison of two lists of correlated terms extracted from different sources may be well applied to sources in the same language. What is the added value or particular interest of having sources in different languages, and thus the need for a translation? On the other hand, in what respects the results of this experiment confirm that the proposed translation approach is good and relevant?

Sec. 3.3:
 - I am unsure this experiment can prove the claimed hypothesis. It seems clearly skewed towards its target: it exploits a system that is purposely trained on slang translation resources, while general translators return the most standard translation of a term. A related question arises: given a term that has both a "standard" and a "slang" translation, such as in Table 5, who can tell whether the "standard" or "slang" interpretation was intended? I.e., couldn't it be that the general translators are correct, and the proposed approach is wrong? Maybe I am missing something, but probably this means that the explanation provided in the paper is insufficient.


PRESENTATION ISSUES

English:
 - Page 1: "is one of the most fundamental computational task" -> tasks; "Each of existing models" -> the existing
 - page 4: "two list" -> lists

 Review #27738
 Comments to Authors.

The paper present an approach for bilingual word representation targeting differences across cultures. This is an understudied problem, which was previously addressed in a monolingual setting only.





Detailed comments:



- Generally, it would have been better to have some more motivation in section 2.3: Why these pseudo-word generation functions? Why the particular final projection to SocVec?



- BTW, there are plenty of existing cross-language projection methods, and thus motivation for the choice the authors made is really needed, see here for an overview:



http://sebastianruder.com/cross-lingual-embeddings/



- In section 2.1, the authors discuss why translation is not good for sentiment projection across languages, focusing on two reasons. Reason (ii) says that sentiment would be preserved in translation. This is not quite true as this work shows:



http://www.jair.org/papers/paper4787.html



- In section 3.1, I do not understand how the bilingual lexicon was constructed: if translation was done with Bing, where is the confidence score coming from? Also, where are the different translation options coming from?



- In section 3.1 again, what does thsi sociolinguistic vocaulary contain? Motivate: why is this needed? Why this vocabulary?



- I am concerned about the way the ground truth was generated. Humans are known to be very bad at assigning absolute scores. This is why for tasks such as machine translation evaluation, the research community has abandoned those for years and has switched to relative judgments: is this translation better than that one? I think this would be very appropriate here as well: have pairs to be ranked by humans, rather than ask them to assign absolute scores for each example.



- How many human annotators were there?



- Overall, the agreement looks somewhat low.



- I do not understand why MAP would be an adequate evaluation measure here: the input is a named entity and the gold truth is a human judgment on an absolute score between 1 and 5. This looks like a regression problem, not a ranking one. Am I missing something here? (Spearman and Pearson correlations are fine.)



- One general issue with I have with section 3.3 is that it is assumed that slang is always used as a slang. Instead, I believe this should be context-dependent. In some contexts, "fruitcake" might have the literal meaning of "cake made with fruits". The dependence on the context should at least be acknowledged.





Typos and spelling suggestions:



- p.1: each of ** existing models -- the



- p.4: two list



- p.5: all the dictionary -- all dictionaries



- p.5: the the



- p.5: a better metrics



- p.5: the a



- p.6: these work aims



- p.6: they fails

