Many thanks to the valuable comments.
Below are our responses to the questions.

R1:

We will correct all grammatical errors in the revision.
The "singer" example is a typo, which should be "performed in".


R2:

We didn't show the comparison with embedding approaches because:

First, this paper means to evaluate KBC on "complex relations", 
suggested in Sec 4.1. By complex relations, we mean natural language relations,
and not simple relations (i.e. Freebase predicates) evaluated by
most other KBC works. Therefore, the comparison, if any, has to be done on
complex relations. The entities involved in complex relations are more diverse
and may touch large portions of Freebase. However, existing embedding methods 
can't scale to such a large KB (instead they used FB15k).
 
Second, the main objective of our experiments is to demonstrate 
the advantage of tree over path as the structured representation of 
complex relations. Both of these representations are
explicit and explainable, as opposed to embeddings (implicit and hard to
understand). Although both structured representations and embeddings can be
used to in KBC, our goal isn't to beat the best KBC methods 
out there, but to focus the comparison on structured approaches. 
BTW, few papers about KB embedding compares with structured approaches.

In sec 4.2 and 4.3.3, we offered discussions about relations 
yielding no explicit structures. Some of them are "trivial relations" 
of little interest to KB; others are complex, but the necessary constituent
predicates are missing from the KB.

R3:

Compared with rule induction such as AMIE+, 
our difference is that we infer "rules" with probabilities,
by maximizing likelihood function (1). This approach can
achieve a balance between general and specific schemas.

CPRA by Qang Wang requires clustering similar relations
before extracting features, but the complex relations 
in our evaluation come from PATTY and have little semantic overlaps. 
If CPRA were used on our data, it would degenerate into
the original PRA approach, which was outperformed by SFE and
hence worse than our approach. Therefore we didn't compare with CPRA.

Regarding the formulas, (1) is the likelihood of probability distribution
over all candidates. In (2), sc_j is the j-th candidate schema, 
with a corresponding probability \theta_j.
In order to learn the parameters, we maximize the likelihood of generating 
the object, given the relation and the subject.
(2) shows how the likelihood is generated: first picks a (hidden) schema based 
on \theta distribution, and then randomly output an object from 
all results queried by this schema, which is defined in (3) and is a uniform 
distribution. The idea comes from GMM, if it makes it easier to understand.

Regarding comparative evaluation on AMIE+, on complex relations,
the Macro/Micro/Average F1 results are 
0.547/0.440/0.470 (ours) vs 0.393/0.301/0.335 (AMIE+).
On ordinary relations, they are
0.423/0.307/0.367 vs 0.474/0.249/0.343.

Evaluating the quality of schemas by manual labeling has two challenges:
First, the evaluation is too subjective.
Second, the labeling task requires extensive knowledge of Freebase, 
which renders the process very costly. 
Instead, we give some real schemas output from our approach.
