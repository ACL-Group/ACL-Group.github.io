R1: (1478 chars, limit 1500)

1. If we want to handle negated relations, we can simply allow p_s (in Def. 2) to support
negated predicates in the KB. For example, "capital_of" is a predicate with pairs like (Paris, France).
Its negation "not_capital_of" is also a predicate with more pairs like (London, France) and
(NYC, UK) where the subjects are cities and objects are countries, but not connected by "capital_of".
Hence the schema can represent a negated semantics.

2. It is true that schemas sharing the same path are dependent on each other. However, what
we meant by cond. independence in Eq. 2 is: 
- Given schema distribution \theta, the prob. of generating j-th schema is independent of 
the subject entity: p(sc_j|s, \theta) = p(sc_j | \theta) = \theta_j.
- Given the subject and the j-th schema, the prob. of generating the object entity by using sc_j
is independent of the schema distribution: p(o|s, sc_j, \theta) = p(o|s, sc_j). 

3. Our approach works regardless of the path length. However, we observe that path length beyond 3 
costs significantly more time with no substantial benefits. 

4. Gamma is a hyperparam defined in sec 3.1 ("To ensure ...").
We tuned its value in sec 4.1, and selected gamma=10% in all experiments.


R2:
Embedding methods can't scale from FB15k to FB3m because 
i) the num of params (entity vectors) increases by at least 200x, 
while the training data (triples) increases by only 100x; 
ii) it takes ~2h for TransE to iterate just once, much slower compared with 20s in FB15k.
Our approach scales better with large KBs, as we control the search space regardless of the size of the KB.

Algo 1:
Initially, Q is empty and S is a skeleton obtained by BFS (para 2 of Sec 3.1). B and gamma are
fixed hyperparams. SchemaExpansion() takes S as input and returns a list of new schemas,
each by adding one constraint to S.

Sec 3.2: Eq.1 should be P(o|s)*P(s|o).

Sec 4.2: The four examples in Fig. 3 are just for illustration purpose. 
The quantitative evaluation is shown in Tab 1, which supports our conclusion. 
The evaluators are 3 non-author students who are familiar with Freebase.
It would be unfair to SFE and AMIE+ if we compare with them in Tab 1. 
The reason is in para 2 of Sec 4.2.

Sec 4.3: We will specify all hyperparams in the revision.
Our goal is to demonstrate the ability to represent complex semantics by primitive predicates.
Thus we created the datasets from PATTY (complex relations) and non-primitive predicates from FB15k.
Following your suggestion, we evaluated our model on FB122.
We achieve the MRR score of 0.385(sc)/0.373(sk)/0.299(KALE) on raw setting,
and 0.743/0.705/0.523 on filtered setting, much better than the results in KALE's paper.


R3: (1436 chars)
Thanks for pointing out the TransG paper.
We think our approach is competitive against TransG because:
i) TransG models a relation by multiple semantic components (vectors). Our approach learns a
probabilistic distribution of schemas (with different semantics) for a given relation, 
which effectively achieves similar goal.
ii) TEKE is similar to TransG in that it also mines multiple semantics of one relation. TransG 
outperforms TEKE by 20% (hits@10) on FB15k, while our method beats TEKE by 30% (hits@10) on FB15k-37.

By the way, the released code of TransG (https://github.com/BookmanHan/Embedding) is broken as is,
but we will try to add the comparison in the revision of the paper.

R4: 
Thank you and we appreciate your support!
