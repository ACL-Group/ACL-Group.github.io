\section{Related Work}

%\xusheng{Slot filling and sequence labeling basics}

Slot filling is considered a sequence labeling problem that is 
traditionally solved by generative models 
such as Hidden Markov Models (HMMs) \cite{wang2005spoken},
hidden vector state model \cite{he2003data}, 
and discriminative models such as 
conditional random fields (CRFs) \cite{raymond2007generative,lafferty2001conditional} 
and Support Vector Machine (SVMs) \cite{kudo2001chunking}.
In recent years,
deep learning approaches have been explored
due to its successful application in many NLP tasks.
Many neural network architectures have been used such as
simple RNNs \cite{yao2013recurrent,mesnil2015using}, 
convolutional neural networks (CNNs) \cite{xu2013convolutional},
LSTMs \cite{yao2014spoken} 
and variations like encoder-decoder \cite{zhu2017encoder,zhai2017neural} 
and external memory \cite{peng2015recurrent}.
In general, these works adopt a BiLSTM as the major labeling architecture
to extract various features, 
then use a CRF layer \cite{huang2015bidirectional} to model 
the label dependency.
We also adopt a BiLSTM-CRF model as baseline and claim that
a multi-task learning framework is working better than directly 
applying it on Chinese E-commerce dataset.
%\KZ{Nobody has applied multi-task learning to the slot filling problem? Are
%we the first? If so we should say it.}
Previous works
only apply joint model of slot filling and intent detection \cite{zhang2016joint,liu2016joint}.
Our work is the first to propose a multi-task sequence labeling model with 
deep neural networks on slot filling problem.

%\xusheng{Multi-task exploration in sequence labeling}
Multi-task learning (MTL) has attracted increasing attention
in both academia and industry recently.
By jointly learning across multiple tasks \cite{caruana1998multitask}, we can
improve performance on each task and reduce the need for labeled data.
There has been several attempts of using multi-task learning on 
sequence labeling task \cite{peng2016multi,peng2016improving,yang2017transfer},
where most of these works learn all tasks at the out-most layer.
SÃ¸gaard and Goldberg \shortcite{sogaard2016deep} is the first to 
assume the existence of a hierarchy between the different tasks in a stacking BiRNN model.
%and to make lower level tasks affect the lower levels of the representation in a stacking BiRNN model.
Compared to these works, our DCMTL model further improves this 
idea even thorough with cascade and residual connection.


