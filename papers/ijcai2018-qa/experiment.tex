\section{Experiments}

In this section, we introduce the QA datasets and state-of-the-art systems
that we compare.
We show the end-to-end results of the KBQA task,
and perform detail analysis to investigate the importance of different modules
used in our approach.



\subsection{Experimental Setup}

\noindent
\textbf{Knowledge Base and Dataset}

\noindent
We use the full version of Freebase as the backend knowledge base,
which contains 46M entities and x,yyy predicates\footnote{
We remove predicates in \textit{user}, \textit{base} and \textit{freebase} domain,
as well as predicates whose objects are neither entities or literals, 
like \textit{common.topic.article} and \textit{common.topic.image}.
}.
We host the knowledge base with Virtuoso engine\footnote{\url{http://virtuoso.openlinksw.com}.}.
We use the WebQuestions~\cite{} and ComplexQuestions~\cite{} datasets in our experiment.
Webquestions contains 5,810 questions collected from Google Suggest API,
with manually labeled answers by Amazon Mechanical Turk.
The questions are split into 3,778 training and 2,032 testing QA pairs.
ComplexQuestions contains 2,100 complex questions collected from Bing search query log,
split into 1,300 training and 8,00 testing questions.
For both datasets, we split the training questions into 80\%:20\% for validation use.
We use the average $F_1$ score as the evaluation metric,
which is computed by the official evaluation script
\footnote{The script is available at \url{http://www-nlp.stanford.edu/software/sempre}.}.

\noindent
\textbf{Implementation Detail}

\noindent
For the step of negative QA pair sampling, we blabla \KQ{not sure what the final version is}.
Besides, we tune the following parameters in our model:
\begin{itemize}
\item The dimension of knowledge base embedding $D_K$ in \{0, 25, 50, 100, 200\},
\item The number of hidden units of RNN (for both word, name and skeleton encoding) in \{50, 100, 200\},
\item The hinge loss margin $\lambda$ in \{0.5, 1.0, 1.5, 2.0\},
\item The batch size $b$ in \{32, 64, 128, 256\},
\item ......
\end{itemize}

\subsection{End-to-End Results}

1. show table
with constraints / without constraints
2. talk about comparsion systems (follow End-to-End paper)
important: talk about Yih / Xu, why their scores are high.

\subsection{Model Analysis}

1. negative sampling strategy: random pick
2. ablation on schema side
   GRU/LSTM
   FC/Average/MaxPooling
   No word-lvl attention
Also show us the heat map.


\subsection{Error Analysis}
Analyze the cases where not the highest result is returned.
take compQ as example.

1. entity linking error
several small categories

2. relation matching error
heat map

3. not perfect (missing edges)


percentage
example:
what's wrong
what's right
reason

