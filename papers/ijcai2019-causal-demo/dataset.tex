\section{Knowledge Bases}
Facts and rules in Prolog come from factual knowledge bases and rule base separately.
\paragraph{Factual Knowledge}
Commonsense knowledge base provides factual triples, such as (rice, atLocation, Thailand). There exists no Chinese one, so we translate the English ConceptNet into Chinese called Zh-ConceptNet.
To our knowledge, most existing Chinese taxonomic knowledge bases, such as CN-Probase\cite{Xu2017}, zhishi.me\cite{Niu2011}, are constructed from online-encyclopedia, which suffer that the number of concepts is far less than Probase\cite{Wu2012a} and they have no probabilistic characteristic. Therefore, we translate English Probase into Chinese called Zh-Probase.  
Specifically, we concatenate two terms in the triple with comma, ignoring the relation, into one phrase, throw this phrase to Google translator and split the translated result string into two separate Chinese terms.
We randomly sample 500 triples from Zh-Probase and Zh-ConceptNet respectively and the accuracies after human evaluation are 87.8\%(close to the accuracy of Probase 92.6\%) and 91.6\%.

\paragraph{Rule Base}
Each rule is a logical form with confidence, see rule (1), which means if country X suffers disaster Y, the price of product Z, rich in X, will rise with confidence of 0.84.
The rule base is automatically learned from massive unstructured Chinese financial news texts. 
The rule learning framework adopts a bottom-up approach with two general stages: causal event extraction via the designed patterns\cite{Chang2005} and rule induction via Zh-Probase.	

%The rule learning system adopts a bottom-up approach with two general stages:	
%\textbf{1)Extraction: from unstructured text to structured rule instances} Online text is massive but noisy. We first derive the sentences with causal relation through the designed patterns \cite{Chang2005}. Then, we use open event extraction technology to extract the structured rule instances, e.g., (2).
%rise(rubber, price, `', `') :- suffer(`', Thailand, earthquake, attack) (2) (where events follow the form "predicate(modifiers of subj,subj,modifiers of obj,obj))
%\textbf{2)Induction: from specific rule instances to general rules} With a large number of rule instances, we generalize them into rules, such as above rule (1), meanwhile balancing generality and specificity with the help of Zh-Probase.

%((Z,price,rise,`',`') :- ('',X,suffer,Y,attack), isA(X,country), isA(Y,disaster), isA(Z,product), atLocation(Z,X) conf:0.84 ruleID:936) (2)

We manually evaluate each sample rule in the rule base into three categories: `good', `fair' and `bad'. 
The `good' rules are causally correct, `bad' ones are causally incorrect, and `fair' means that the causality existing in the general rule is too vague to be recognized. Totally, the framework learned 42,037 rules. For evaluation, we ranked the rules by their confidence and sampled 200 rules from top 10,000 rules. Results show that: `good': 50.5\%, `fair': 39.5\%, `bad': 10\%.