\section{Introduction}
\label{sec:intro}
Causal reasoning, the core challenge in artificial intelligence, which aims to understand the causal dependency between events, is receiving more and more attention \cite{Pearl2009}.
In the reasoning process, causal knowledge plays a critical role in people's daily behavior and decision-making \cite{waldmann2013causal}.
It is of great interest in many domains, including finance, where understanding causal relationships can provide significant opportunities for economic benefits. 
%, which is crucial in many natural language research applications, such as event prediction, question answering and so on. It has also aroused great interest in many areas of real life, including finance, where understanding causality between events can provide significant opportunities for economic interests. 
For example, consider the following,
\begin{enumerate}
	\item If a large disaster happens in a country and this country is rich in certain metal, the price of this metal will rise. \label{intro:natural-language-rule-1}
	\item If the price of some kind of metal rises, the price of the products based on this metal will also go up. \label{intro:natural-language-rule-2}
\end{enumerate}
Above causal knowledge expressed in the form of natural language is easy for us to understand and has great practical value in real life through simple reasoning. For example, if there's an earthquake in Chile and we know above rule \ref{intro:natural-language-rule-1}, it is easy to infer the price of copper will rise. Further, we can infer the price of the household appliances, such as air conditioners and refrigerators, will also rise via above rule \ref{intro:natural-language-rule-2}.

However, it is a daunting task for humans, especially traders, to learn many of these rules and use them for real-time reasoning in the real world.  
We hope machines can learn these rules automatically and reason quickly with them to help us get rid of the heavy burden of rule learning and real-time massive information processing. 
In order to achieve this goal, we face two challenges: how to represent causal knowledge in a machine-actionable way and how to automatically acquire a large amount of this type of causal knowledge. We separately explain these two aspects.
	
\subsection{Causal Knowledge Representation and Reasoning}
To get some inspirations on how to represent causal knowledge, we first review some previous causal knowledge representation schemes. Now, there are two general directions about causal knowledge representation. 
One direction is \textbf{\textit{Neural or numeric}} form. Such neural form scheme can represent both causal knowledge and non-causal knowledge by a unified graph-embedding method \cite{Li2016a,Bordes}. However, it not only has the problems of interpretability and reusability but also has the problem of weak reasoning ability.
% such as First Order Predicate, Production, Semantic Network, Framework, etc. among these, causality knowledge exists in FrameNet, ConceptNet,  
%For the way of First Order Predicate, it uses a predicate with several corresponding arguments to represent knowledge. 
%Production is like IF A Then B such kind of knowledge is good at procedural knowledge, which is also based on rule, But such rules need to be elaborated by human. 
The other direction is the \textbf{\textit{symbolic}} form, which also includes two directions. One is the specific description of causal knowledge. Both the cause and the effect of the causal knowledge are specific events or actions, expressed by terms or short text, such as CausalNet\cite{Luo2016a}, ConceptNet\cite{Speer2016}. However, this kind of knowledge is usually less expressive and less informative. The other is the general description of causal knowledge. Both the cause and the effect of the causal knowledge are abstract events or actions, expressed by structures, such as frame \cite{BakerCollinFandFillmoreCharlesJandLowe1997} or a pair of abstract words \cite{Zhao2017}. 
%For example, FrameNet \cite{BakerCollinFandFillmoreCharlesJandLowe1997} regards frame as the cause part and effect part to express the causal knowledge. \cite{Zhao2017} regards the a pair of abstract verb and noun as the cause part and effect part to express the causal knowledge. But these schemes 
These structures are usually too abstract or vague to be understood. Additionally, existing symbolic representation schemes are unfriendly for machines to reason.

Previous causal knowledge representation schemes encounter problems, such as uninterpretability, insufficient expression ability, unfriendly reasoning.
Therefore, we propose a novel and powerful representation scheme with logical form, which takes structured events as the basic key components and can reason with uncertainty. 
Here, we use the following logic rule (1), equivalent to above natural language rule \ref{intro:natural-language-rule-1}, as an example to illustrate how this idea evolved. 
\TD{mark -1}
(Z,`价格/price',`上涨/rise',`',`'):-(`',X,`遭受/suffer',Y,` 袭击/attack'),isA(X,`国家/country'),isA(Y,`自然灾害/ disaster'),isA(Z,`金属/metal'),atLocation(Z,X) conf:0.842 \ \ (1)

%\begin{align}
%&\text{(Z, '价格/price', '上涨/rise', '', ''):-('', X, '遭受/suffer', Y,}\nonumber \\
%&\text{'袭击/attack'), isA(X, '国家/nation'), isA(Y, '自然灾害/dis-}\nonumber \\
%&\text{aster'), isA(Z, '金属/metal'), atLocation(Y,Z)}	\label{intro:logic_rule_1}
%\end{align}
%	\begin{equation}
%	\begin{split}
%	&\text{(X, '价格/price', '上涨/rise', '', ''):-('Y', '价格', '上涨/rise', '',}\\
%	&\text{''), IsA(X, '金属/mental'), IsA(Y, '产品/product'), Madeof(X,Y)}
%	\end{split}
%	\label{intro:logic_rule_2}
%	\end{equation}
\textit{First,} we try to use structures to represent the events in the cause part and effect part of the causality well. A simple and effective symbolic event representation is a structured form \textit{(Subject, Predicate, Object)} as used in \cite{Ding} and \cite{Ding2016}, also known as SPO. We extend the SPO form to \textit{(Modifier of Subj, Subj, Predicate, Modifier of Obj, Obj)} to capture richer event information. 
%Event (钢铁价格上涨/the price of steel rises) will be structured as (钢铁steel, 价格/price, 上涨/rise) rather than (价格/price, 上涨/rise,''), or (钢铁价格/the price of steel, 上涨/rise) that is not easy to generalize.
We call this event pair a rule instance, see (2) (following the convention of Prolog, we put the cause part in the head position).

(`铜/copper',`价格/price',`上涨/rise',`',`'):-(`',`智利/Chi le',`遭受/suffer',`地震/earthquake',`袭击/attack')\ \ (2)

%\begin{align}
%&\text{('铜/copper', '价格/price', '上涨/rise', '', ''):-('', '智利/Chile'}\nonumber \\
%&\text{, '遭受/suffer', '地震/earthquake','袭击/attack'}\label{intro:rule_instance_example}
%\end{align}
	%Also \citeauthor{zhao2017constructing} just use verbs and nouns to represent the event since the data they use is short news headlines which is hard to extract the complete SPO structure for each headline. Here we would like to adopt the structured form to represent the events like SPO. However informative causality event would give better understanding of the causality, so we expand the SPO form to \textit{(Compound Nouns Of Subj, Subj, Predicate, Compound Nouns Of Obj, Obj)} form which is abbreviated as NSPNO. NSPNO can capture more rich event information than SPO, meanwhile it can be degenerate to the SPO automatically, which means it is compatible with SPO. Event representation is the first key component of causality knowledge representation scheme, which is flexible by substituted it with other event representation scheme, such as NestIE \citeauthor{bhutani2016nested}.
\textit{Second,} using such a specific rule instance (2) to represent causal knowledge is less informative and lacks the ability to infer the unseen. Instead, we hope it's a general rule (3).

(Z,`价格/price',`上涨/rise',`',`'):-(`',X,`遭受/suffer',Y,
`袭击/attack'),isA(X,`国家/country'),isA(Y,`自然灾害/
disaster'),isA(Z,`金属/metal')\ \ (3)

%\begin{align}
%&\text{(Z, '价格/price', '上涨/rise', '', ''):-('', X, '遭受/suffer', Y,}\nonumber\\
%&\text{'袭击/attack'), IsA(X, '国家/nation'), IsA(Y, '自然灾害/dis-}\nonumber\\
%&\text{aster'), IsA(Z, '金属/metal')}	\label{intro:candidate_rule_example}
%\end{align}

\textit{Third,} reasoning using rule (3) may result in some errors. For example, we can get one inferred result: (`铁/steel',`价格/price',`上涨/rise',`',`'):-(`',`智利/Chile', `遭受/suffer', `地震/earthquake',`袭击/attack'). This is unreasonable because Chile is a large copper producer rather than a large steel producer. Therefore, we add some constraint relations, such as atLocation(Z,X), to exclude unreasonable inference.

%But the cause event and effect event in the rule instance are to specific, so we need to generalize it with a general form to represent a similar class of events. Inevitably, we would conceptualize some of the arguments(subject, object or the component nouns and so on) in the rule instance. Then, we can get the generalized rule instances as following, we call it candidate rule. argument conceptualization is the second key component of our causality knowledge representation scheme. This component makes our causality knowledge representation hierarchical, since we can generalize the rule instances into different concept granularities distributed in different candidate rules. with conceptualization, We can also alleviate the low coverage problem existed in specific rule instances, since concept itself represent lots of arguments in one class, so deriving one candidate means gaining lots of rule instances.  
%Here is a specific example, one rule instance \ref{notation:rule_instance} would be 
	
%After generalizing from rule instance, we find the generalized candidate rule is too general, even wrong. For example, we instantiate the candidate rule with $X_0$ to '乙醇' and $X_1$ to '燕麦'. This instantiation satisfies the candidate rule's requirement, but it is unreasonable since '燕麦' is seldom used to produce '乙醇'. So the generalized candidate rules further need some constraints. Here, we add some relations between arguments to candidate rule, then the candidate rules with relation constraints would be real rules. The relation constraints of (\ref{notation:candidate_rule_example}) should be $X0 \ madeof \ X1$. After adding the relation constraint, we will exclude the case of $X_0$ is '乙醇' and $X_1$ is '燕麦', since they don't satisfy the 'madeof' relation constraint. Relation constraint is the third key component of out causality knowledge scheme. So we finally formalize the causality knowledge , which is also called rule, with the following form:
\textit{Last,} after previous step, we assign a confidence value to enable it to reason with uncertainty. Finally, we get rule (1).

\textbf{\textit{To sum up}}, this kind of rule is very informative and friendly for machines to reason.
By the way, our causal knowledge representation scheme can be easily extended to the conjunction of multi-cause events by adding multiple cause events to the rule header. 


%\TD{mention: our representation can also represent the conjunction}
%\TD{explain how to do reasoning with uncertainty using Prolog}

%	Up to now, We elaborate all the procedure of how we represent the causality knowledge, generally from rule instance, to candidate rule, last to rule.
	
%	\textbf{Advantages.}	The advantages of above causality knowledge representation scheme can be summarized as following:
%	\begin{enumerate}
%		\item \textbf{Natural ability to reason.} Causality is particularly for reasoning.
%		\item \textbf{High coverage} rules with conceptualization can be instantiated into lots of rule instances, which highly improve the coverage of rule instances.
%		\item \textbf{Interpretable.} Symbolic form makes it interpretable.
%		\item \textbf{Hierarchical} rule conceptualized with different concept granularities make it capture different abstract levels of knowledge.
%		\item \textbf{Portability to Prolog.} causality knowledge representation in the form of first-order logic rule is very easy used in Prolog to do inference.
%	\end{enumerate} 
	
%	In this part, we elaborate on all the procedure of how we develop this causal knowledge representation scheme with logical form, such representation is interpretable, since it's symbolic, and is able to do reasoning work due to its portability to Prolog(shows later) and has high coverage about causality knowledge since the events is abstract.
	
\subsection{Causal Knowledge Acquisition}
\label{intro:Causal_Knowledge_Mining}
%imperfect/noisy(precision) and incomplete(recall)
After developing a machine-actionable causal knowledge representation scheme with the logic rule, we need to tackle the second challenge about how to automatically obtain a large number of rules.  
WWW (World Wide Web) is a very large treasure trove of knowledge, but most of the content is unstructured and noisy text, which challenges us to learn rules.
Rule learning has been studied extensively in Inductive Logic Programming (ILP) \cite{Quinlan1990,Muggleton1997}. However, the rule instances extracted from Web text are noisy and incomplete. That negative examples are mostly absent cannot make the closed-world assumption typically made by ILP systems. This paper presents a new ILP system, which adopts a bottom-up approach with two general stages:	

\textbf{1) From unstructured text to structured rule instances} Online text is massive but noisy. We first derive the sentences with causal relation through the designed patterns. Then, we use event extraction technology to extract the structured rule instances, such as above (2).
	
\textbf{2) From specific rule instances to general rules} With a large number of rule instances, we generalize them into rules, such as above (1), and balance generality and specificity:
	
\textbf{Generality} We try to represent given rule instances semantically with as few general rules as possible under the help of Probase\cite{Wu2012a}. For example, induce two rule instances (`corn/soybean',`price',`fall', `',`'):-(`corn/soybean', `yield',`rise',`',`') into the rule (X,`price',`fall',`',`') :- (X, `yield',`rise',`',`'),isA(X,`food').


\textbf{Specificity} Overgeneral rules may lead to errors. For example,
inducing two rule instances (`corn/soybean',`price', `fall',`',`'):-(`corn/soybean', `yield',`rise',`',`') into the rule (X,`price',`fall',`',`'):-(X,`yield',`rise',`',`'),isA(X,`thing') is unreasonable. Because when the machine instantiates `thing' into `air', the inferred events ``the yield or price of air rises" do not make sense.

Generalizing rule instances to rules can be seen as a semantic compression procedure, which accords with the MDL (Minimum Description Length) principle. Hypotheses (H) accords with rules and data (D) accords with rule instances. Thus, we propose a rule induction algorithm, which takes MDL as the evaluation criterion and regards this rule induction procedure as an optimization problem, see detail in \ref{sec:approach}. This procedure is similar to \cite{Cui2016} and \cite{Zhu}. Finally, each learned rule will be automatically assigned a confidence value by considering some heuristics.
	
%	we need a Chinese synonym dictionary which can help us do predicate generalization. we need a Chinese IS-A knowledge base as taxonomy which can help us do argument conceptualization. Also we need a commonsense knowledge base to add the relation constraints like AtLocation('智利/Chile','铜/copper') to the candidate rule to make it a rule.

%First of all, We need extract the cause and effect event span. Most of the approaches are based on elaborate patterns, We also follow the methods to extract the cause event span and effect span. for instance we '因为... 所以...' to catch the cause event span and effect event span. Alternatively, we can use a more sophisticated way to catch the causal relation such as \cite{b1} (Sendong Zhao 40), In this study, however, we focus more on the precision of extraction effect rather than the recall, also we think a more sophisticated method used on such already noisy text may lead to more errors.  
	
%	First of all, We parse the sentences and use the elaborate causal patterns to match cause and effect span, then extract the event structure according to the dependency relations. After this step, we can get rule instance composed of cause event and effect event.
	
%	Secondly, we further generalize the rule instances to candidate rules. we need a Chinese synonym dictionary which can help us do predicate generalization. we need a Chinese IS-A knowledge base as taxonomy which can help us do argument conceptualization. Also we need a commonsense knowledge base to add the relation constraints like AtLocation('智利/Chile','铜/copper') to the candidate rule to make it a rule.
	
%and common sense which can help ue find relation constraints. We decide to construct such knowledge base contain common sense, since we find that most relations are common sense. besides our data is Chinese, only ConceptNet5\cite{speer2013conceptnet} satisfies our need, which has the Chinese resource. But the quantity is far from our expectation. So we decide to build a new one with fusion of many existing knowledge bases. For example, Probase\cite{wu2012probase} which is special for taxonomy, and ConceptNet5\cite{speer2013conceptnet}, WebBrain\cite{chen2016webbrain}, Webchild\cite{tandon2014webchild} which are specially aimed at  common sense.
	
%Then, we would like to generalize these rule instances into candidate rules shown in the Figure \ref{fig:overview}'s middle part. This rule generalization component mainly contains two steps. The first step is predicate generalization and the second step is argument generalization. Predicate generalization is trying to normalize the similar predicates into the unified one. For example, "raise, rise, soar, increase, gain, enhance" have the same meaning, we need give a unified predicate to represent this group of predicates with the help of Cilin\footnote{ \url{ http://www.bigcilin.com/}}. Argument generalization step is trying to conceptualize the arguments in the rule instances which need to exploit the taxonomy in our built knowledge base. Since each candidate rule is derived by observing a cluster of similar rule instances, we need divide rule instances into several clusters. in each cluster, we generalize these rule instances to a candidate rule, and we will elaborate it in Section \ref{sec:approach}.         
	
%	Lastly, Since our rule learning system learn the rule in an incremental manner, when a rule is generated we need to check whether it is a existed one. if so, we merge it, else, we check whether it can be subsumed to a more general rule.
	
	%Last, after rule generalization, we carry on rule specialization via adding relation constraints into the candidate rules. Since the candidate rules are general, we exploit our built knowledge base to specialize the candidate rules into real rules. The relations of each candidate rule is derived by finding the mutual relations existed in this candidate rule's supported rule instances cluster.
	
	%So we need to specialize the candidate rules to real rules. we add some relation constraints to each candidate rule with the help of our built knowledge base. Since each rule has many supported rule instances, We just find the mutual relations existed in all rule instances. 
	
	%Above are main components of our proposed framework to mine causality knowledge.
	
\textbf{Contributions.}
In this paper, we present a full stack solution for causal knowledge representation, acquisition, and reasoning. More precisely, our contributions are as follows:
First, we design a novel and powerful causal knowledge representation scheme based on the logic rule with the ability of uncertainty reasoning.
Second, we propose a rule learning framework for obtaining rules from large unstructured text and experiments show that the rules leaned are reasonable and effective. 
Specifically, we learned 50000 rules and human's evaluation shows the top 10000 rules of the highest confidence reach 32.5\%(good), 39\%(fair), 28.5\%(bad). Last, we release the rules and the translated Chinese Probase and ConceptNet and provide an interactive demo to show the reasoning process and the application of futures price triggering. \TD{second and last repeat?}
	
%	The remainder of this paper is organized as follows. We define the necessary concepts and formulate the focal problem in Section \ref{sec:problem}. Our proposed approach is presented in Section \ref{sec:approach}. Section \ref{sec:experiment} reports the experimental observations followed by a brief review of related works in Section \ref{sec:related}. Section \ref{sec:conclusion} concludes the paper.
%	\section{Problem Definition}
%	\label{sec:problem}
%	
%	In this section, we will formally state the focal problem to be solved.
%	\begin{equation}
%	\begin{split}
%	&p_e('', X_1, no_e, o_e):-p_c(X_0, s_c, no_c, o_c)\\
%	&isA(X_0, c_1),isA(X_1, c_3)\\
%	&relation(X0,X1) 
%	\end{split}
%	\label{equ:rule_notation}
%	\end{equation}
	%The list of major symbols and notations in this paper is summarized in the following table.
	
	%\begin{table}[htbp]
	%	\caption{Table of notations }
	%	\begin{center}
	%		\begin{tabular}{|c|l|}
	%			\hline
	%			\textbf{Notation}&\textbf{Definition}\\
	%			\hline
	%			rule instance & structured causality proposition pair\\
	%			\hline
	%			$E_{c_i}$ & the instances of $i-th$ concept \\
	%			\hline
	%			\multirow{2}{*}{rule}
	%			& generalized rule instance with concept constraints \\
	%			&and relation constraints \\
	%			\hline
	%		\end{tabular}
	%		\label{tab1}
	%	\end{center}
%	\end{table}	
	
	%our goal:
	%	We try to make reasoning interpretable via symbol not neural network. 
	% 
%	\textbf{Problem Statement.} Mining the causality knowledge in the form of first-order logic rule \ref{equ:rule_notation} from large online free text.