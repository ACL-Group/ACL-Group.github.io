\section{Related Work}
\label{sec:related}
%We will review the related work from three perspectives: causal knowledge
%representation and reasoning, knowledge discovery, and some other related works.

\paragraph{Knowledge Representation and Reasoning}
%Different knowledge representations have different reasoning methods.
Neural-based knowledge representation, such as \cite{Bordes} and \cite{Li2016a}, which uses the `translation' method to embed knowledge graph into dense vectors, is effective for many downstream Natural Language Processing(NLP) works \cite{Wang2017}. However, it has problems of uninterpretability and weak reasoning ability.
In addition, there are many symbol-based knowledge representations.
For example,
% Semantic network provides a large semantic graph that describes kinds of knowledge, including causal knowledge. For example,
 ConceptNet \cite{Speer2016} uses the relation `/r/Cause' to represent causal knowledge, such as (`smoking', `/r/Causes',`cancer'). It suffers that such specific representation makes the knowledge less informative and less expressive. CausalNet\cite{Luo2016a} also represents the event with a word.
 FrameNet\cite{BakerCollinFandFillmoreCharlesJandLowe1997} uses `Causative\_of' to connect causal frames, which are too rough to clearly express causal knowledge.
 The semi-structured causal knowledge representation in ATOMIC \cite{sap2018atomic} is also unfriendly for machines to reason. 
%		second such kind of node representation with natural-language form is still hard for machine to understand. we human with background knowledge can easily know that triple ('smoking', '/r/Causes','cancer') mostly expresses ``if a person smoke often, it is more likely to get cancer'', But for machine, maybe it will think ``if a person smoke often, it is more likely to cure his cancer'', which implies that semantic completeness caused by that noun can represent the event accurately.
%	Recently, different from human-curated knowledge acquisition form like ConcepNet, CausalNet\cite{luo2016commonsense}, extracted from 1,6 billion web page, also suffers the same problem as ConceptNet.	
%FrameNet\cite{BakerCollinFandFillmoreCharlesJandLowe1997} 
%is based on a theory of meaning called Frame Semantics. It 
%consists of 1221 frames and 10 frame relations type. It uses `Causative\_of' to express causal knowledge, and only has 59 pairs of causal frames. For example, the `Causative\_of' relation exists between frame `Killing' and frame `Death'. However, the frame elements of frame, such as `Items', `Position', are too rough to clearly express causal knowledge.

%from the perspective of Constrain logic programming \cite{Jaffar1994}
%\cite{Zhao2017} extracts the causal event represented by verbs and nouns ,and
%generalize events to construct abstract causality network, and embed it to
%empower downstream application.\\
\paragraph{Rule Acquisition} 
Different knowledge representations have different knowledge acquisition methods.
Here, we mainly review how to acquire knowledge of logic rule. 
The Inductive Logic Programming(ILP)\cite{Quinlan1990,Bergadano1996,Muggleton1997} makes strong assumptions, such as high-quality training data, closed-world assumption, and so on, which are inappropriate to handle the extracted data from Web text.
Therefore, SHERLOCK system \cite{Schoenmackers2010} learns the first-order horn clauses in a top-down manner, which first identifies the classes(concepts in our rules) and relations(predicates in our events), enumerates all their combinations, and keep good ones as final rules. 
However, due to a large number of predicates and concepts in large web texts, as well as the complexity of the rule structure, their combination will be explosive in our case.
So, we propose a bottom-up framework based on MDL to learn rules.

%\subsection{Misc}
%\paragraph{Event-based Graph Construction}
%Event Extraction task has beed a long-standing problem in information Extraction which typically include two subtasks, event detecting and classification. In this paper, we mainly detect the events with a open form, which means we have no predefined event type constrains. many researchers use the parser to do event detection subtasks, for example, \cite{Huang2017} use the AMR parsing\cite{Wang2015e} or Semantic Role Labeling (SRL) to extract the event candidate, \cite{Ding2016} use the Open IE\cite{Fader2011} and dependency parsing to extract the structured events.

%Event-based Graph construction   
%\cite{Saleh2018} learns the narrative event chains from raw newswire text. Further
%\cite{Glavas2015} formally proposes the concept of event graphs where events are structured and concrete. Later, a hierarchical event network using event abstract duples (verbs and nouns) as nodes and causality to represent edges is proposed in \cite{Zhao2017}. \cite{Li} constructs the Narrative Event Evolutionary Graph and the graph neural network is used to predict the subsequent events. Our rules can also be seen as an abstract event graph, but it is a logical form that is easy to reason.

\paragraph{Financial Market Price Prediction} There are many works on financial market price forecasting using text information, mainly on stock prices. \cite{Ding} attempts to use structured event information to predict stock prices. 
Furthermore, \cite{Ding2016} enriches event representation with the knowledge base to improve prediction accuracy.
%deductive database \\
\section{Conclusion and Outlook}
\label{sec:conclusion}

In this paper, we design a novel and powerful causal knowledge representation scheme based on the logic rule with the ability of uncertainty reasoning and we propose a rule learning framework for obtaining rules from large unstructured text. The experiments show that the rules learned are reasonable and effective.
In the future, we would like to improve the main components in the rule learning framework, such as rule instance extraction, external knowledge bases, and rule induction. Besides, we will also explore other downstream applications based on this reasoning system, such as consumption intent prediction, stock price prediction, and reasoning-based information retrieval.




