\section{Related work}
\label{sec:related}

%In this section, we discuss the related work.
%In this paper, we solve the problem to generate question-answer pairs from a long text which is relevant to the methods of paragraph retrieval, answer extraction and question generation.

In this paper, we solve the problem to generate question-answer pairs from a long text with an aspect keyword.
The design of the naive baselines and our proposed joint model are related to the methods of paragraph retrieval, answer extraction and question generation.
In this section, we will discuss the related work of these methods respectively.

\subsection{Paragraph Retrieval}
For paragraph retrieval, there are the traditional retrieval methods, such as TF-IDF and BM25~\cite{robertson2009probabilistic}. 
In addition, the rapid development of neural ranking networks brings new solutions for retrieval, such as DRMM~\cite{guo2016deep}, KNRM~\cite{xiong2017end}, Co-PACRR~\cite{hui2018co}, and DUET~\cite{mitra2017learning}.
What's more, the pre-trained model BERT is also implemented for paragraph ranking~\cite{nogueira2019passage,qiao2019understanding}.

\subsection{Answer Extraction}
The methods to obtain answers can be divided into extractive methods and generative methods. 
The extractive methods usually use ranking model to select answer phrases from the candidates obtained by lexical features~\cite{witten2005kea,liu2011automatic,wang2016ptr,subramanian2017neural}.
Sequence labeling methods for NER such as BiLSTM+CRF are the simple and effective approaches to extract named entity and interesting phrases from the source document as the answers~\cite{lample2016neural}.
Pointer network is also chosen to extract answers from the source document~\cite{subramanian2017neural}.
The generative methods can both generate answers from vocabulary and the source text~\cite{meng2017deep,chen2018keyphrase,ye2018semi}.
The answers can also be generated by reading comprehension methods with the given document and questions.
In such area, BERT and the variants of BERT represent the state-of-the-art~\cite{devlin2018bert,lan2019albert}.
\subsection{Question Generation}
Question generation has been widely researched in recent years.
%Most question generation tasks generate the relevant question from the source document and an answer.
They can be divided into two types.
One is the traditional approach which uses hand-crafted rules to convert the input document into questions~\cite{heilman2010good,chali2015towards}.
The other is deep learning methods.
Du et al.~\shortcite{du2017learning} is the first to use sequence-to-sequence (seq2seq) model with attention to generate questions.
Zhou et al. and Subramanian et al.~\shortcite{zhou2017neural,subramanian2017neural} encoded the source document and the answer position to produce an answer-aware input representation, then it's fed to the decoder to generate an answer focused question.
Pre-trained seq2seq approaches are also used for question generation and achieve great success, such as MASS and UNILM~\cite{song2019mass,dong2019unified}.
In addition, the tree-to-sequence (tree2seq) model can also be chosen to generate questions which parses the document into a structure tree, using tree encoder and a attentive decoder with copy mechanism. Such generation approach was first used in NMT (Neural machine translation)~\cite{eriguchi2016tree}.
\subsection{QA Pairs Extraction and Generation}
Considering answer is unavailable in reality before question is generated, Subramanian et al. \shortcite{subramanian2017neural}, Krishna et al.~\shortcite{2019arXivGenerating} and  Lovenia et al. \shortcite{lovenia2018automatic} designed the two-stage strategy which first extracts answer spans from the input document and then generate questions relevant to the answers.
Wang et al.~\shortcite{wang2017joint} designed a joint model to generate the answers and questions which combined the A-gen model generating answers from questions and the Q-gen model generating questions from questions as the multi task. 

Inspired by the these methods, we designed two naive baselines which are the three-stage pipeline models and the filtering method remaining the relevant QA pairs after generation.
We also design a joint model to generate QA pairs from document and the aspect keyword based on UNILM.
Different from the previous methods which analyzed the individual quality of questions or answers, we also design an automatic metric to evaluate the conjunct QA pairs.

