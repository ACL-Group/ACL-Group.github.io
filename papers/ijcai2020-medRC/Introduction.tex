\section{Introduction}
% What is RC. Given sufficient data good, otherwise bad.
Relation classification (RC) is an indispensable problem in natural language processing (NLP). Given a sentence (e.g., \emph{Washington is the capital of the United States}) containing two target entities (e.g., \emph{Washington} and \emph{the United States}), an RC model aims to distinguish the semantic relation between the entities mentioned by the sentence (e.g., \emph{capital of}) among all candidate relation classes.

Conventional relation classification has been an extensively investigated task.
Recent approaches \cite{zeng-etal-2014-relation,RNNRC,vu-etal-2016-combining} are trained with large amount of human-annotated data and achieve satisfactory results.
%Recent approaches are substantially based on neural networks and
%deep learning \cite{zeng-etal-2014-relation,RNNRC,vu-etal-2016-combining},
%where the models are trained with large amount of human-annotated data
%and achieve satisfactory results.
%\KZ{What does this mean? OK}
However, it is often costly to obtain necessary amount of human labeled data,
without which a decrease in the performance of the RC models is inevitable.
% Distant supervised ways
Distant supervised  methods \cite{NYTdataset} have been adopted to enlarge annotation quantity by utilizing existing knowledge bases to perform auto-labeling on massive raw corpus. However, long-tail problems \cite{xiong-etal-2018-one,han-etal-2018-fewrel,ye-ling-2019-multi} and noise issues occur.
%The NYT-10 dataset \cite{NYTdataset} is a typical large dataset constructed under distant supervision.
%Although these approaches substantially augment labeled data,
%significant shortcomings remain: %\KZ{Elaborate a bit what you mean bylong-tail problem. OK}
%(1) Long-tail problems \cite{xiong-etal-2018-one,han-etal-2018-fewrel,ye-ling-2019-multi} exist in knowledge bases. While some particular relation classes contain a great proportion of instances, most classes consist of only tens of instances.  (2) Noise issues occur during auto-labeling, demanding for manual screening.

% Few-shot RC
%\KZ{The following three paras can be merged and shortened to make some space.
%Directly attack the problems in meta learning. Move some of the intro of
%few-shot learning and meta learning to related work.}
Few-shot relation classification is a particular RC task under minimum annotated data, where a model is required to classify a new incoming query instance given only few support instances (e.g., 1 or 5). An example is given in Table~\ref{FewShotRCExample}.

\begin{table}[t]
\centering
\small
\begin{tabular}{p{7.3cm}}
\hline
\textbf{Support Set} \\ \hline
\textbf{Class 1} \emph{~mother}: \\
\qquad \textbf{Instance 1}~\lbrack Emmy Acht\'e\rbrack$_{entity1}$ was the mother of the internationally famouse opera singers \lbrack Aino Ackt\'e\rbrack$_{entity2}$ and Irma Tervani. \\
%\qquad \textbf{Instance 2}~He was deposed in 922, and \lbrack Eadgifu\rbrack$_{entity1}$ sent their son, \lbrack Louis\rbrack$_{entity2}$ to safety in England. \\
%\qquad \textbf{Instance 3}~Jinnah and his wife \lbrack Rattanbai Petit\rbrack$_{entity1}$ had separated soon after their daughter, \lbrack Dina Wadia\rbrack$_{entity2}$ was born. \\
%\qquad \textbf{Instance 4}~Ariston had three other children by \lbrack Perictione\rbrack$_{entity1}$: Glaucon, \lbrack Adeimantus\rbrack$_{entity2}$, and Potone.\\
%\qquad \textbf{Instance 5}~She married (and murdered) \lbrack Polyctor\rbrack$_{entity2}$, son of Aegyptus and \lbrack Caliadne\rbrack$_{entity1}$. Apollodorus.\\
%\textbf{Class 2} \emph{~spouse}: ... \\
%\textbf{Class 3} \emph{~child}: ... \\
%\textbf{Class 4} \emph{~follows}: ... \\
... \\
\textbf{Class 5} \emph{~crosses}: ... \\ \hline
\textbf{Query Instance} \\ \hline
Dylan and \lbrack Caitlin\rbrack$_{entity1}$ brought up their three children, \lbrack Aeronwy\rbrack$_{entity2}$, Llewellyn and Colm. \\
\hline
\end{tabular}
\caption{\label{FewShotRCExample}
An example of 5 way 1 shot relation classification scenario from the FewRel validation set. The query instance is of Class 1: \emph{mother}. The support instances of Class 2-5 are omitted.
}
\end{table}

% Meta learning
Meta-learning is a major method for few-shot learning circumstances and is broadly studied in computer vision (CV) \cite{LakeHuman,Santoro2016,proto}.
Instead of training a neural network to learn a specific task, in meta-learning, the model is trained with a variety of similar but different tasks to gain the ability of quick adaptation to new tasks without meeting a ton of data.
One typical framework of meta-learning is to train an additional meta-learner %
%\KZ{Rephrase this: on the upper to guide the upgrading steps of the conventional learner on the lower OK}
which sets and adjusts the update rules for the conventional learner \cite{Andry2016,Finn2017,HN}. Another framework bases on metric learning and is aimed to learn the distance distribution among the relation classes \cite{Koch2015,Vinyals2016,proto}.
% Prototypical networks \cite{proto} is a typical and widely used metric learning based meta-learning framework.
% Recent works on meta-RC
%In recent years, meta-learning has been adopted in NLP to solve few-shot learning problems, including few-shot relation classification as a focus of this paper.
In recent years, meta-learning has been shown to be helpful in few-shot NLP tasks, including few-shot relation classification as a focus of this paper.
%\cite{gu-etal-2018-meta,han-etal-2018-fewrel,huang-etal-2018-natural,obamuyide-vlachos-2019-meta,ye-ling-2019-multi}.
%In this paper, we focus on meta-learning methods for few-shot relation classification task.
Han et al. \shortcite{han-etal-2018-fewrel} constructed the FewRel dataset, a few-shot relation classification dataset and applied distinct meta-learning frameworks intended for CV tasks on the FewRel dataset. Ye and Ling \shortcite{ye-ling-2019-multi}, Gao et al. \shortcite{hatt} and Gao et al. \shortcite{gao-etal-2019-fewrel} improved the models to cater for relation classification task and achieved better performance.
%In recent years, meta-learning has been adopted in NLP as a solution to multiple few-shot learning tasks. %\cite{gu-etal-2018-meta,han-etal-2018-fewrel,huang-etal-2018-natural,obamuyide-vlachos-2019-meta,ye-ling-2019-multi}.
%In this paper, we focus on meta-learning methods for few-shot relation classification task.
%\cite{han-etal-2018-fewrel} constructed the FewRel dataset, a few-shot relation classification dataset and first applied distinct meta-learning frameworks intended for CV tasks on the FewRel dataset. \cite{ye-ling-2019-multi,hatt,gao-etal-2019-fewrel} improved the models to cater for relation classification task and achieved better performances.
%Prototypical networks \cite{proto} with CNN encoder turned out to have the best performance. \cite{ye-ling-2019-multi} improved the framework of prototypical networks by interactively encoding the support and query instances at both local and instance level, and adding weights while calculating prototypes. \cite{gao-etal-2019-fewrel} adopted BERT \cite{devlin2018bert} in few-shot relation classification.

% What's different in our work
Admitting that meta-learning frameworks outperform conventional methods in few-shot RC, there is still room for further improvement.
%One disadvantage is that previous models concern much about
%\KZ{the outputs of the query instances OK}
%computations on query instances but lose sight of information within the support instances. Besides, the requirement for significant
%amount of human annotation is not \emph{really} eliminated in previous work.
%Although just few support instances are needed during testing,
%the training set still must be sufficiently large
%(e.g., the FewRel dataset \cite{han-etal-2018-fewrel}
%contains 700 instances per relation). Performance drops significantly when
%the training data size is restricted (e.g., tens of instances per relation).
First of all, the requirement for significant amount of human annotation is not \emph{really} eliminated in previous work. Although only few support instances are needed during testing, the training set still must be sufficiently large (e.g., the FewRel dataset \cite{han-etal-2018-fewrel} contains 700 instances per class). Performance drops significantly when the training data size is restricted (e.g., tens of instances per relation). Besides, previous meta-learning methods concern much about computations on query instances but lose sight of information within the support instances.

To address these weaknesses, we propose
%\KZ{Find another name?}
\emph{Meta-learning with Extra Support and Data} (MESDA) framework.
%Firstly, inspired by \cite{chen-2019-image}, we exploit underlying knowledge within support instances using a support classifier scheduled by a fast-slow learner (\secref{sec:cls}). Secondly, we improve the model performance by augmenting training data with open-source relation classification instances. This is quite helpful under conditions where few training data of the target domain is available (\secref{sec:data}).
Firstly, we improve the model performance by augmenting training data with open-source relation classification instances to instill cross-domain knowledge in the model (\secref{sec:data}).
%This is quite helpful under conditions where few training data of the target domain is available (\secref{sec:data}).
Secondly, inspired by Chen et al. \shortcite{chen-2019-image}, we exploit underlying knowledge within support instances using a support classifier scheduled by a fast-slow learner (\secref{sec:cls}).

Additionally, we propose our own dataset, TinyRel-CM dataset, a Chinese few-shot relation classification dataset in health domain with limited training data.
Previous few-shot RC dataset contains abundant training data, violating the original intention of ``few-shot'' (i.e., utilizing less amount of data).
%But in reality, if instances of some relation classes are so rare that few-shot RC is needed, it is often unrealistic to extract mass instances for other classes within the same domain and constructing a huge training set.
%Previous few-shot RC dataset contains abundant training data, which violates the original intention to use small amount of data.
%\KZ{Say why you create your own data. Must be the fewRel is not good enough?}
Our dataset contains 27 relation classes with 50 instances per class, and puts forward the challenge of few-shot relation classification under \emph{limited} training data. Experiments are conducted on both our proposed TinyRel-CM dataset and the FewRel dataset \cite{han-etal-2018-fewrel}. Experimental results show the strengths of our proposed framework.

 %that (1)
%%\KZ{How do you show it's a hard task? OK}
%The MSI framework with data augmentation achieves competitive results given sufficient training data. (2) Both MSI framework and the data augmentation method bring performance gains. (3) The MSI framework with data augmentation becomes more effective with less training data.

% Our contributions
In summary, our contributions include:
 (1) We propose TinyRel-CM dataset,
%a Chinese few-shot relation classification dataset in medical domain.
the {\bf second} and a more challenging dataset for few-shot relation classification task (\secref{dataset}).
%TinyRel-CM is more challenging than FewRel because it contains extremely limited amount of training data (see \secref{dataset}).
%(2) We find that extracting knowledge within support instances benefits the meta learning process.
%(2) Knowledge within support instances improves performance. We make use of this knowledge with a support classifier (\secref{results}).
(2) We utilize information within support instances using a support classifier and demonstrate the improvement (\secref{results}).
(3) We propose a data augmentation method, which is extremely useful when training data is very small (\secref{results}).
%(4) Our method achieves state-of-the-art performance in Chinese few-shot relation classification dataset in health domain.
(4) Our methods achieve state-of-the-art performance on our proposed TinyRel-CM dataset and competitive results on the FewRel dataset (\secref{results}).

%This is the {\bf second} dataset for few-shot relation classification task.
%TinyRel-CM is more challenging than FewRel because it contains extremely
%limited amount of training data (see \secref{dataset}).
%(2) We find that extracting knowledge within support instances benefits the meta learning process. To make use of knowledge within support instances, we facilitate the meta-learning framework with a support classifier (see \secref{results}).
%(3) We propose a data augmentation method. When training data is very small, it is useful to borrow knowledge from out-of-domain data (see \secref{results}).
%(4) Our methods achieve state-of-the-art performance
%on TinyRel-CM dataset and competitive results on FewRel \cite{han-etal-2018-fewrel} dataset (\secref{results}).
%\KZ{It seems that (3) can be combined into (2) because you said TinyRel-CM
%has much fewer training data earlier. So if u do well in TinyRel-CM, it means
%you are particularly good with small training data.}
