\section{Related Work}
%\subsection{Relation Classification}
%\KZ{Reduce this section}
%Relation classification task aims to categorize the semantic relation between two entities conveyed by a given sentence into a relation class.
%In recent years, deep learning has become a major method for relation classification.
%\cite{RecursiveNNRC} fed the recurrent neural network (RNN) with a syntactic tree to introduce attention mechanism in relation classification tasks. \cite{zeng-etal-2014-relation} utilized a convolutional deep neural network (DNN) during relation classification to extract lexical and sentence-level features. \cite{vu-etal-2016-combining} employed a voting scheme by aggregating a convolutional neural network with a recurrent neural network.
%
%Traditional relation classification models suffer from lack of data. To eliminate this deficiency, distant-supervised approaches are proposed, which take advantage of knowledge bases to supervise the auto-annotation on massive raw data to form large datasets. \cite{NYTdataset} constructed the NYT-10 dataset with Freebase \cite{Freebase} as the distant supervision knowledge base and sentences in the New York Times (2005-2007) as raw text. Distant-supervised methods suffer from long tail problems and excessive noise. \cite{zeng-etal-2015-distant} proposed piecewise convolutional neural networks (PCNNs) with instance-level attention to eliminate the negative effect caused by the wrongly labeled instances on NYT-10 dataset. \cite{liu-etal-2017-soft} further introduced soft label mechanism to automatically correct not only the wrongly labeled instances but also the original noise from the distant supervision knowledge base.

%Relation classification task aims to categorize the semantic relation between two entities conveyed by a given sentence into a relation class.
Few-shot relation classification is a newly-born task that requires models to do relation classification under merely a few support instances. Han et al. \shortcite{han-etal-2018-fewrel} proposed the FewRel dataset for few-shot relation classification and applied meta-learning methods intended for CV \cite{metanet,gnn,snail,proto} %with CNN and PCNN core
on the FewRel dataset.
Meta Networks \cite{metanet} implemented a high level meta learner based on the
conventional learner.
GNN \cite{gnn} regarded support and query instances as nodes in a graph.
SNAIL \cite{snail} aggregated attention into the meta learner.
Prototypical networks \cite{proto} assumed that each relation has a prototype and classifies a query instance into the relation of the closest prototype.
Prototypical networks \cite{proto} with CNN core turned out to have the best test accuracy among the reported results. ProtoHATT \cite{hatt} reinforced the prototypical networks with hybrid attention mechanism. MLMAN \cite{ye-ling-2019-multi} improved the prototypical networks by adding mutual information between support instances and query instances. Bert-Pair \cite{gao-etal-2019-fewrel} adopted BERT \cite{devlin2018bert} to conduct binary relation classifications between a query instance and each support instance and fine-tune on FewRel dataset. Baldini Soares et al. \shortcite{baldini-soares-etal-2019-matching} trained a BERT-like language model on huge open-source data with a Matching the Blanks task and applied the trained model on few-shot relation classification tasks.

Meta networks performs worst among all the methods \cite{han-etal-2018-fewrel} and
is time consuming (about 10 times slower than other methods with even better results).
%\KZ{How time-consuming? Better give some numbers. And then say that's why
%we didn't include it in our experiments.} % due to the complex training strategy.
Matching the Blanks is high-resource and not comparable to other methods. So we do not adopt these two methods as baselines.
%\subsection{Meta-learning}
% in CV... 组会的papers!
%Meta-learning aims to .... It is first proposed by [??] where the authors ...image classification... and is widely researched in computer vision. One category of meta-learning methods trains a meta-learner above the traditional learner. The meta-learner guides the learning steps of the traditional learner[?? ??]. Another category bases on metric learning, which aims to find the underlying distribution among all the classes[?? ??]. Prototypical networks[??] is a typical framework of metric-learning based meta-learning. It proposes the concept of prototype, which is a centroid among all the support vectors within the same class, and uses the prototypes to represent the vectors of each relation. In [??], prototypical network is adopted to handle NLP tasks such as few-shot relation classification.

%The methods of updating models in previous works rely on the output of the query instances to a great extent, which lose sight of the significant and precious information within the support instances.
Previous methods lose sight of the significant information within the support instances.
Chen et al. \shortcite{chen-2019-image} conducted image one-shot learning tasks with a image deformation sub net and trained a prototype classifier to classify the prototypes of the support instances to update the parameters within the image deformation sub net. Inspired by this work, we add a support classifier over each support instance during the training process. The support classifier helps with the update process of the parameters in both the support classifier and the encoder, and is scheduled with a fast-slow learner schema.


%\subsection{Datasets}
%Relation classification datasets have been released in past decades. Conventional ones include SemEval-2010 Task 8 dataset \cite{semeval8}, ACE 2003-2004 dataset \cite{ace}, TACRED dataset \cite{zhang-etal-2017-position} and NYT-10 dataset \cite{NYTdataset}. All these datasets encompass sufficient data to train a strong model.
Han et al. \shortcite{han-etal-2018-fewrel} released the first few-shot relation classification dataset, the FewRel dataset, which contains 100 relation classes with 700 instances per class. Although the dataset is intended for few-shot relation classification, the training data is still adequate.
% (the model meets 700 instances times 100 classes during training). % And the performance of models decrease if the training data decreases.
We propose TinyRel-CM dataset, which is a \emph{real} few-shot relation classification dataset with 27 classes and only 50 instances per class. The TinyRel-CM dataset is a much harder task than the previous dataset.
%few-shot relation classification tasks. %In our paper, we also propose a method that is capable of handling few training data.
