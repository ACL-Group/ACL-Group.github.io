\section{Introduction}

In Grammatical Error Correction (GEC) task, an errorful sentence is taken as the input and the error-corrected sentence corresponds to the target, where both sentences have the same meaning. With the boom of transformer-based pretrained models like BERT, XLNet and GPT2, various appoaches have been proposed for GEC task. Neural Machine Translation based methods take the advantage of the models developed for translation tasks and consider the errorful sentences and the error-free sentences as two different languanges. Parallel Iterative Edit Models~\cite{Awasthi_2019} have achieved the state-of-art performance by re-structuring the GEC task as a tagging task for each token and outputing the edits needed for the error correction. However, the performance of the existing models is limited by the size of the output edit space which brings the following issues:

\begin{itemize}
\item The output space cannot cover all the errors, nearly 30\% of the wrongly used collocations and idioms cannot be corrected because the right word does not exist in the edit space. Increasing the size of the edit space has a strong marginal effect on the coverage of errors, and can significately augment the training time and the prediction time.

\item The output space is hard to modify, in some cases, the use of the word are different depends on the subject of the text. The only way to make changes on the edit space so as to adapt to the change of the wording is to retrain the model with the custimized edit space, which takes long time since the model is only pretrained on corrected dataset and it is obliged to retrained on errorful datasets for a long period of time. 

\end{itemize}

In this paper, we present our architecture to deal with the previous issues.\footnote{Considering that the model requires high computing resources, we are going to open source our models and code after the double-blind period.}




We run the test 
and compare our approach to the state-of-the-art models

In summary, 


