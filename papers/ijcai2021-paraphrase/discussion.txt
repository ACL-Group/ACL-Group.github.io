review 1
1. Application 没有比较 -- 前面在xxx table已经比较过了，这里只是为了提出一个思路，paraphrase可以被用在translation任务上，前人没有做过相关的工作。
2. finetuning：finetuning也是需要并行数据的，没有并行数据的finetuning在DNPG里已经有过尝试，我们也已经比较了，且有较大的提升
3. share the same decoder：因为两个模型量级不一样，set2seq非常轻量级，翻译非常重，两者放在一起训练没有很好的效果

review 2
1. intuitively，我们用到了in-domain数据，2个BT没有用到
2. refer to review 1-1
3. lambda 篇幅不够，我们在最终版可以加进去 given the additional page可以放进去
4. 我们没有看到这篇，和我们的区别是xxxx，我们可以在related work里面加一段比较

review 3 -- nicely
1. we admit that the indixxx used in the framework is not new, intuitively put them together and use them in the paraphrasing is meaningful.
2. round-trip translation 和 back-translation的区别
3. 讨论一下几篇paper和我们的关系，写一段related work

review 4
1. refer to review 2-3
2. 我们不限制key word的number，可能有一些misunderstanding，我们的key words不是选出来的，去除掉所有的stopwords后所有的单词都是key words，这不是一个可以调整的参数
3. 这部分被accuracy cover了，accuracy需要和原句意思相同，如果原句符合common sense，那么accuracy也会要求生成的paraphrase也是make sense的
4. we will carefully read the paper and improve the language thourgh out

review 5 
回答一下就好
我们有translation的parallel data，但是没有paraphrase的parallel data，所以是distcance