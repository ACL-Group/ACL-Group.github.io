Hi everyone, today I would like to introduce a paper we contributed in IJCAI 2021: Automatically paraphrasing xxxxx

My speech includes the following five parts: introduction, xxxx, xxx,xxx

First of all, let me introduce what is paraphrase generation. Paraphrase is a pair of sentence with same meaning but different wording. As shown in the table below, one sentence on the left and one sentence on the right can form a pair of paraphrase. The task of paraphrase generation is to give a sentence and generate its paraphrase.

There is a natural contradiction for PG, that is how to balance the acc and the diver. If we only want to generate sentences with same meaning, we can simply copy the original sentence. If we only want to generate sentences with different wording, we can output a completely different sentence. Finding a balance between these two demands is the biggest difficulty for PG.
The balance is also reflected in the evaluation metric of PG:  in addition to calculating the Bleu scores of the output and target, we also punish the Bleu scores of output and source sentence.

There are four common datasets for PG, they are Quora, WikiAn, Twit and MSCOCO. Among them, quora is the most used dataset since it is specially proposed for paraphrase related tasks and has manual annotation. However, all these four datasets are all limited to specific domains. Quora and WikiAns only have questions, Twitter contains too much Internet-related expressions, and sentences in MSCOCO are all descriptions of pictures. We can't train a model that can be applied to any domain from these domain-specific datasets. So training without parallel in-domain data is an urgent need.
In this context, we propose a framework without any requirement for in-domain parallel data.

Before I introduce our framework, let's see a picture. Can anyone try to describe the picture?
Yes, the picture is about a man is sitting on a bench next to a bike.
Here comes another question: how do you come up with this description?

Let me guess: you see a man, a bike and a bench in the picture, and the man is sit on the bench. That is to say, you have four words in your mind: man,bike,bench and sit. You can use these four words to make sentences. The sentences may be different from each other, but they all describe the picture. That is to say, they are paraphrases of each other.

Let's go one step further, when you see this sentence: Young man sitting on a bench behind a personal bicycle. You have this picture in your mind. And when you want to generate paraphrase for this sentence, You'll try to describe the picture in different wordding. If we replace the piture with some words, we will get the framework on the right hand side. 
We first get a set of words from the original sentence, and expand the word set to a full sentence with a set2seq model.

For the word set constructor, we first remove the stop words from the original sentence, and then randomly replace the remaining words with their synonyms, so that the generated paraphrase can be expressed in more diverse ways.

For the set2seq model, we use a transformer without position encoding, so that the generated paraphrases have no word order restrictions.

However, such approach presents a problem. Here's an example, when we want to generate parpahrase for "Russia is bigger than Vatican", after stopwords removal and synonym replacement, we get the following wordset: Russia, Large and Vatican.
Here comes the problem: which country is bigger? Russia or Vatican? We loss such information in the wordset without sequential expression.

If we regard WS as an expression of the underlying semantics of the original sentence, we can use another expression of the underlying semantics to make up for this information loss. 
In our work, we translate the original sentence into another language and regard it as the second underlying semantic expression.
We get a wordset and a Chinese translation from the input sentence, all we need to do is to hybridly decode the set2seq model and the chinese 2 english translation model to get the output paraphrase.

Here is our paraphrsing framework. As we can see, the input sentence is "how can I make money online with free of cost?". We get the wordset and the Chinese Translation from the input sentence, and feed these expressions of underlying semantics into a hybrid decoder. In the hybrid decoder, set2seq model and Chinese 2 English model will give 2 scores for tokens in vocabulary, we sum them weighted to get the final score, and choose the output token.

In our experiment, we train the set2seq model with in-domain non-parallel data on a single GTX-2080 GPU for 3 hours, train the round-trip translation model with WMT17 parallel Chinese-English dataset on two GTX-2080 GPUs for 3 days, and train a set2seq common model with WMT17 English monolingual data on a single GTX-2080 GPU for 1.5 days.
Taking Quora as an example, we have 400k parallel training paris in the dataset, and we randomly extract one sentence from each training pair and put it into our non-parallel training set.
Set2seq common model is trained from a large cross domain dataset, we can use it when there is no in-domain monolingual corpus.

The result of our framework is shown in this table. It can be seen that our framework has a great advantage over the baseline methods. On the quora dataset, the iBLEU score of our method is 2.64 percentage points higher than the previous SOTA method, which is a significant improvement.

We apply our paraphrase generation framework as a data augmentation method on Nurual Machine Translation. 
In machine translation task, a training pair consists of two sentences with the same meaning in different languages. In this example, we generate paraphrases for the English sentence, and form the generated paraphrase and the Chinese sentence into new trainging pairs. We train the model with a dataset consists of original training pairs and augmented training paris.
As shown in the table on the right, model trained with augmented dataset recieved better performance.

The contribution of this paper can be summarized as follows:
1. We are the first to apply the set2seq model to the task of paraphrase generation by combining it with a round-trip translation model through a hybrid decoder. 
2. The framework proposed by us achieve state-of-the-art accuracies on four benchmark datasets compared with existing methods.
3. We apply our method to augment the training data of low-resource translation tasks and obtain significant improvement in translation quality.


