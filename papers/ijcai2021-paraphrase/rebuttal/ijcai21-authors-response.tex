% IJCAI-21 Author's response

% Template file with author's reponse

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai21-authors-response}


\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lipsum}
\urlstyle{same}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}


\begin{document}
Many thanks for the valuable comments! Our responses follow.
 
\section{Reviewer \#4}
\label{sec:review1}
\textbf{Q1: About set2seq-common.}
A: Our main approach is set2seq+BT, which targets majority of the cases
where in-domain non-parallel data for paraphrasing is available. 
set2seq-common is a variant that targets the special cases where in-domain
non-parallel data is not available or very limited, such as Twitter, or
NMT to low-resource languages. This is why set2seq-common+BT outperforms
others for Twitter but underperforms for other datasets in Table 2.\\
\textbf{Q2: Compare the proposed framework with other previous models in 
``Application''.}
A: The sole purpose of our application section is to demonstrate that 
one possible use of the paraphrases we generate is to augment training data for 
machine translation for low-resource languages, which is a novel idea. 
It's by no means our intention to compete with SOTA NMT models.\\
\textbf{Q3: Why not fine-tuning?}
A: For domains with abundant non-parallel data such as Quora or MSCOCO,
fine-tuning set2seq-common on the in-domain data will not be better than
training set2seq directly on the in-domain data. \textbf{We did a follow-up
experiments on this for all four datasets in Table 2 and found that ...}\\
\textbf{Q4: Using a shared decoder?}
A: Since set2seq is a lightweight model, and the machine translation (seq2seq) 
model is complex and heavy, sharing the decoder would make the combined model 
very expensive to train. We actually made some attempts like this before, but
did not get any better results, thus didn't include the idea in the paper.

\section{Reviewer \#10}
\label{sec:review2}
\textbf{Q1: Experiments in NMT is not good enough?}
A: Please refer to Reviewer \#4, Q2.\\
%A: As we described in Q2 of Review-1, the main purpose of the Application is to put forward an idea. There is no conflict between data augmentation through paraphrase generation and back-translation. The former generates paraphrase for one side in the training sentence pair, combining the paraphrase with the other side to get new training pairs, and the latter translates new monolingual sentences to get new training pairs.
\textbf{Q2: About lambda?}
A: Due to space limitations in the initial submission, 
we were not able to show all the experiments 
including those regarding varying lambda. We will definitely put the following
results in the final version given an additional page.
When $\lambda$ is close to 0, the result is similar to the reverse translation 
result. When $\lambda$ is between 0.4-0.8, the result is stable, and
iBLEU is above 14. As $\lambda$ goes to infinity, the result is slowly 
approaching that of set2seq.\\
\textbf{Q3: Difference with Set Transformer?}
A: Our set2seq model is essentially a light-weight transformer model without
positional encoding. The set transformer, on the other hand, is a transformer
without positional encoding, but with additional modifications adapted to
computer vision tasks in which input is not a sequence. 
The use of set2seq model on paraphrase is novel because permutations of 
words and their variants in the input sequence are analogous to its paraphrases.
This dicussion will go into the related work section.

\section{Reviewer \#29}
\label{sec:review3}
\textbf{Q1: Techniques proposed before...}
As the reviewer rightfully pointed out, our framework is indeed a combination
of existing techniques, which works well on paraphrase generation.
We thank the reviewer for kind reminder of the references and in the 
revised version we will definitely cite them and put them in perspective 
with our framework, given an additional page. 
One subtle difference between our DAE and the two 
papers mentioned by reviewer is our unique way of generating the noises
which is a set of words.\\
%A: This paper is mainly about ASR, We are very different from them in the way to construct the set. Besides, as the paper says, in autoregressive setup, the sound signal itself has position information. It is intuitive to remove position encoding. But in our paraphrase generation task, the order of input contains certain information, and the removal of position encoding is a trade-off between accuracy and diversity. Removal of position encoding is a very simple operation, but knowing what tasks need such an operation is innovative.\\
%\textbf{Q2: Seq2seq-based DAE ?}
%A: As we mentioned in ``Introduction'', the set2seq model is inspired by DAE. We specially designed a noise generation method for the paraphrase generation task. Converting the original input sequence into an input set is a bolder change, which is also based on the information provided by the back translation model. Such changes have not appeared in previous papers.\\
\textbf{Q2: Title of paper and back-translation.}
Thanks for pointing out the difference between
``back-translation'' and ``round-trip translation'', which we got confused.
We will duely change the title and all references to BT. 

\section{Reviewer \#43}
\label{sec:review4}
\textbf{Q1: Setting for lambda? }
A: Please see Reviewer \#10, Q2.\\
\textbf{Q2: Number of keywords? }
A: There are two ways to construct the keyword set (Sec 2.2). First is
is to remove all stopwords, which is a non-parametric method. The second
takes the top $k$ words with high IDF scores, which is the number of
keywords in question. We experimented with both (including varying $k$)
and conclude that removing all stopwords is the best approach overall
(see Sec. 3.5 Ablation study)\\ 
\textbf{Q3: Common sense in human evaluation?}
A: Whether the phrase makes commonsense is implicitly accounted for 
when scoring accuracy. If the original sentence makes common sense, 
the correct paraphrase must also conform to common sense.\\
\textbf{Q4: Writing needs improvement.}
A: We will carefully proofread the paper 
and improve the language through out.

\section{Reviewer \#97}
\label{sec:review5}
\textbf{Q1: Difference between three categories of paraphrasing methods.}
A: We consider methods that use parallel paraphrase data to be \textit{supervised methods}, methods that do not use parallel paraphrase data but some other form
of parallel data (such as translation pairs) to be \textit{distantly supervised methods}, and finally methods that do not use parallel data of any kind to 
be \textit{unsupervised methods}. Therefore our method belongs to
\textit{distantly supervised methods}. Yes, the ``back-translation'' model 
was trained on out-of-domain data.\\
\textbf{Q2: DNPG outperforms set2seq+BT?}
A: DNPG is a supervised method, which is listed only for reference. 
Our method outperforms the DNPG used with domain adaptation.\\
\textbf{Q3: About set2seq-common+BT.}
A: We train the set2seq-common+BT with the English monolingual data from WMT17 (see Section 3.3). This is a large cross-domain dataset, which makes up for
the lack of in-domain data for Twitter and benefits the translation task.
For other three datasets which have abundant in-domain data, training
the set2seq directly on the in-domain data is better than using 
the general-purpose WMT17 data.\\
\textbf{Q4: Problem of computational expensive?}
A: set2seq is a light-weight while the translation models are 
relatively heavy. Fortunately the translation model
only needs to be trained once. In comparison, Liu et al., ParaNMT and ParaBank
also require a translation model, which means their training
time will be similar to ours. CGMH and UPSA, however, only need to train
a language model using LSTM, so the training time is a few hours.
\end{document}

