@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}

@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}


@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{roberta,
  author    = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint    = {1907.11692},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{albert,
  author    = {Zhenzhong Lan and
               Mingda Chen and
               Sebastian Goodman and
               Kevin Gimpel and
               Piyush Sharma and
               Radu Soricut},
  title     = {{ALBERT:} {A} Lite {BERT} for Self-supervised Learning of Language
               Representations},
  journal   = {CoRR},
  volume    = {abs/1909.11942},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.11942},
  eprinttype = {arXiv},
  eprint    = {1909.11942},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11942.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{kd,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{pkd,
  author    = {Siqi Sun and
               Yu Cheng and
               Zhe Gan and
               Jingjing Liu},
  title     = {Patient Knowledge Distillation for {BERT} Model Compression},
  journal   = {CoRR},
  volume    = {abs/1908.09355},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09355},
  eprinttype = {arXiv},
  eprint    = {1908.09355},
  timestamp = {Fri, 04 Sep 2020 16:10:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09355.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{alpkd,
  author    = {Peyman Passban and
               Yimeng Wu and
               Mehdi Rezagholizadeh and
               Qun Liu},
  title     = {{ALP-KD:} Attention-Based Layer Projection for Knowledge Distillation},
  booktitle = {Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2021, Thirty-Third Conference on Innovative Applications of Artificial
               Intelligence, {IAAI} 2021, The Eleventh Symposium on Educational Advances
               in Artificial Intelligence, {EAAI} 2021, Virtual Event, February 2-9,
               2021},
  pages     = {13657--13665},
  publisher = {{AAAI} Press},
  year      = {2021},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/17610},
  timestamp = {Wed, 01 Sep 2021 15:30:54 +0200},
  biburl    = {https://dblp.org/rec/conf/aaai/PassbanWRL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ckd,
  author    = {Yimeng Wu and
               Peyman Passban and
               Mehdi Rezagholizadeh and
               Qun Liu},
  title     = {Why Skip If You Can Combine: {A} Simple Knowledge Distillation Technique
               for Intermediate Layers},
  journal   = {CoRR},
  volume    = {abs/2010.03034},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.03034},
  eprinttype = {arXiv},
  eprint    = {2010.03034},
  timestamp = {Wed, 01 Sep 2021 15:30:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-03034.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gaml,
    title = "{GAML}-{BERT}: Improving {BERT} Early Exiting by Gradient Aligned Mutual Learning",
    author = "Zhu, Wei  and
      Wang, Xiaoling  and
      Ni, Yuan  and
      Xie, Guotong",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.242",
    pages = "3033--3044",
    abstract = "In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT{'}s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT{'}s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.",
}

@article{layerdrop,
  author    = {Angela Fan and
               Edouard Grave and
               Armand Joulin},
  title     = {Reducing Transformer Depth on Demand with Structured Dropout},
  journal   = {CoRR},
  volume    = {abs/1909.11556},
  year      = {2020},
  url       = {http://arxiv.org/abs/1909.11556},
  eprinttype = {arXiv},
  eprint    = {1909.11556},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11556.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{theseus,
  author    = {Canwen Xu and
               Wangchunshu Zhou and
               Tao Ge and
               Furu Wei and
               Ming Zhou},
  title     = {BERT-of-Theseus: Compressing {BERT} by Progressive Module Replacing},
  journal   = {CoRR},
  volume    = {abs/2002.02925},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.02925},
  eprinttype = {arXiv},
  eprint    = {2002.02925},
  timestamp = {Wed, 19 Feb 2020 17:11:34 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-02925.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mrpc,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://aclanthology.org/I05-5002",
}

@article{cola,
  author    = {Alex Warstadt and
               Amanpreet Singh and
               Samuel R. Bowman},
  title     = {Neural Network Acceptability Judgments},
  journal   = {CoRR},
  volume    = {abs/1805.12471},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.12471},
  eprinttype = {arXiv},
  eprint    = {1805.12471},
  timestamp = {Mon, 13 Aug 2018 16:47:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-12471.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{stsb,
  author    = {Alexis Conneau and
               Douwe Kiela},
  title     = {SentEval: An Evaluation Toolkit for Universal Sentence Representations},
  journal   = {CoRR},
  volume    = {abs/1803.05449},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.05449},
  eprinttype = {arXiv},
  eprint    = {1803.05449},
  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-05449.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sst2,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}

@article{rte, title={Recognizing textual entailment: Rational, evaluation and approaches}, volume={15}, DOI={10.1017/S1351324909990209}, number={4}, journal={Natural Language Engineering}, publisher={Cambridge University Press}, author={DAGAN, IDO and DOLAN, BILL and MAGNINI, BERNARDO and ROTH, DAN}, year={2009}, pages={i-xvii}}

@inproceedings{attack,
  title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},
  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={119--126},
  year={2020}
}

@article{adamw,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Fixing Weight Decay Regularization in Adam},
  journal   = {CoRR},
  volume    = {abs/1711.05101},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05101},
  eprinttype = {arXiv},
  eprint    = {1711.05101},
  timestamp = {Mon, 13 Aug 2018 16:48:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-05101.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@article{cka,
  author    = {Simon Kornblith and
               Mohammad Norouzi and
               Honglak Lee and
               Geoffrey E. Hinton},
  title     = {Similarity of Neural Network Representations Revisited},
  journal   = {CoRR},
  volume    = {abs/1905.00414},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.00414},
  eprinttype = {arXiv},
  eprint    = {1905.00414},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00414.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{quantization,
  author    = {Yunchao Gong and
               Liu Liu and
               Ming Yang and
               Lubomir D. Bourdev},
  title     = {Compressing Deep Convolutional Networks using Vector Quantization},
  journal   = {CoRR},
  volume    = {abs/1412.6115},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6115},
  eprinttype = {arXiv},
  eprint    = {1412.6115},
  timestamp = {Mon, 13 Aug 2018 16:46:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GongLYB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pruning,
  author    = {Victor Sanh and
               Thomas Wolf and
               Alexander M. Rush},
  title     = {Movement Pruning: Adaptive Sparsity by Fine-Tuning},
  journal   = {CoRR},
  volume    = {abs/2005.07683},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.07683},
  eprinttype = {arXiv},
  eprint    = {2005.07683},
  timestamp = {Tue, 02 Jun 2020 12:48:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-07683.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{patience,
  author    = {Wangchunshu Zhou and
               Canwen Xu and
               Tao Ge and
               Julian J. McAuley and
               Ke Xu and
               Furu Wei},
  title     = {{BERT} Loses Patience: Fast and Robust Inference with Early Exit},
  journal   = {CoRR},
  volume    = {abs/2006.04152},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.04152},
  eprinttype = {arXiv},
  eprint    = {2006.04152},
  timestamp = {Wed, 18 Nov 2020 16:11:03 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-04152.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@article{byd,
  author    = {Canwen Xu and
               Wangchunshu Zhou and
               Tao Ge and
               Ke Xu and
               Julian J. McAuley and
               Furu Wei},
  title     = {Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of {BERT}
               Compression},
  journal   = {CoRR},
  volume    = {abs/2109.03228},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.03228},
  eprinttype = {arXiv},
  eprint    = {2109.03228},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-03228.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tinybert,
  author    = {Xiaoqi Jiao and
               Yichun Yin and
               Lifeng Shang and
               Xin Jiang and
               Xiao Chen and
               Linlin Li and
               Fang Wang and
               Qun Liu},
  title     = {TinyBERT: Distilling {BERT} for Natural Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1909.10351},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.10351},
  eprinttype = {arXiv},
  eprint    = {1909.10351},
  timestamp = {Wed, 01 Sep 2021 15:40:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-10351.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}