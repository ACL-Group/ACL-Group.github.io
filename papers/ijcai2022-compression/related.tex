\section{Related Work}
\subsubsection{Static Model Compression}
Knowledge distillation~(KD)~\cite{kd} aims to train a compact model to behave like the original teacher model. In the field of language model compression, a wide spectrum of KD objectives has been introduced. \citeauthor{pkd} proposed to 
align the intermediate representations of student model and teacher model through one-to-one layer mapping. Subsequent works~\cite{ckd,alpkd} found that such one-to-one mapping ignores 
important information from other layers and proposed different solutions for more sufficient knowledge transfer. In addition to KD, there are mainly two other lines of 
researches for model compression: quantization~\cite{quantization} and pruning~\cite{pruning}. The former reduces the bit width used to represent a number in a network while the latter removes less useful 
elements of a network in a structured or unstructured manner. Our work can be categorized as an enhanced solution for KD with both new architectural and learning designs.
\subsubsection{Dynamic Model Acceleration}
Dynamic acceleration~\cite{patience,gaml} aims to adaptively allocate computation resources to different instances in order to achieve inference speed-up. 
Common practice is to insert additional classifiers into each intermediate layer and jointly train them with other model parameters. Various early exiting strategies based on entropy or patience have been proposed to strike a balance 
between model precision and inference latency. Though delivering considerable speed-up, such methods need to store all layers, thereby leaving the memory consumption an open problem. Our framework produces models with the same amortized per-sample speed-up while largely decreasing the 
memory footprint.
