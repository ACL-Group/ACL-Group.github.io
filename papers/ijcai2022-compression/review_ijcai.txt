Reviewer #1
Questions
1. Please briefly summarize the main contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper propose a two-stage method to compress a multi-layer pre-trained transformer models, such as BERT-base.
2. What are the main strengths of the paper? Please focus on novelty, soundness, significance and impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility (as specified in our reproducibility guidelines).
The method achieves a competitive performance on five NLU tasks.
The method is somewhat novel.
3. What opportunities are there to improve the paper?
Conduct experiments on MNLI, QNLI and QQP to prove the effectiveness.
4. What pressing questions do you have for the authors in the rebuttal ? List (and number) only questions about specific issues here that 1) could directly influence your evaluation of the paper, and 2) do not require providing new results. Typical questions include requests to clarify or justify particular issues, or about important relationships to other works.
The results in this paper look good, but the experiments are limited to several few relatively small-scale datasets and lack several experiments with larger data sizes commonly in GLUE, such as MNLI, QNLI.
5. Overall assessment.
Borderline Reject. Marginally below the acceptance threshold. The paper has merits but there are key weaknesses. It would benefit from another revision. Can be rejected. But having it in the program would not be that bad. Please use sparingly.
6. Justify your score in a few lines. Please focus on novelty, soundness, significance, expected impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility.
Additional experiments on MNLI and QNLI are needed to demonstrate the generalizability of the method in this paper and a fair comparison with other methods.
7. Are the results in this paper easily reproducible ?
CREDIBLE: I believe that the obtained results can, in principle, be reproduced. Even though key resources (e.g., proofs, code, data) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.
8. Independent of your judgement of the quality of the work, are there any ethical concerns with regard to responsible research or potential negative societal impacts of this submission that must be considered by IJCAI-ECAI 2022 before the paper can be accepted? Papers with a yes here will undergo additional ethical screening by senior members of the program committee. In case of glaring violations of well accepted ethical principles, IJCAI-ECAI 2022 reserves the right to reject the submission. Please check our Ethics Policy in the Call for papers for more details.
No
Reviewer #2
Questions
1. Please briefly summarize the main contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper presents a compression framework for BERT model without considering the knowledge learned from pre-training. The main contribution is the proposed two-stage compression framework which includes implicit collaboration and explicit demonstration. Extensive experiments on the GLUE benchmark confirmed the effectiveness of the proposed framework in comparison to some baselines like BERT-PKD and TheseusBert. Some interesting analysis and insights on the compressed student network are also given.
2. What are the main strengths of the paper? Please focus on novelty, soundness, significance and impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility (as specified in our reproducibility guidelines).
The paper is well written and organized and the proposed method achieves good performances.
3. What opportunities are there to improve the paper?
In the proposed two-branch container, teacher and student layers are interleaved stochastically, it would be interesting to conclude some patterns from the randomly constructed branches, is it possible to extend to more-branches (e.g., three-branches)?

It tends to be more difficult for PLMs compression, when the depth gap between teacher and student networks becomes larger, e.g., 24 vs 6, have you considered your framework for this challenging setting?

On the experiments: in contrast to some strong baselines like MobileBERT, MiniLM and TinyBERT, the proposed method does not need compute-intensive KD for pre-training stage, so it is better to conduct more experiments for some deeper PLMs like 12-layers RoBERTa, and more experiments on challenging reading comprehension like SQuAD and RACE should also be included, and the full GLUE tasks should be considered.
4. What pressing questions do you have for the authors in the rebuttal ? List (and number) only questions about specific issues here that 1) could directly influence your evaluation of the paper, and 2) do not require providing new results. Typical questions include requests to clarify or justify particular issues, or about important relationships to other works.
1. The performance gap between LCD and BERT-of-Theseus tends to be small when the student network has more layers, please give more analysis for this observation.

2. The knowledge learned at pre-training stage plays a key role for improving OOD performances, it will be interesting to evaluate how the size of the augmented dataset at the demonstration stage affect the final performances on OOD dataset.
5. Overall assessment.
Weak accept. Useful. A good paper. The results and insights will benefit the field. I believe it should be accepted.
6. Justify your score in a few lines. Please focus on novelty, soundness, significance, expected impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility.
This paper is well-written, and the proposed two-stage compression framework is also interesting and effective. More experiments on deeper PLMs and challenging reading comprehension tasks can improve the quality of this work.
7. Are the results in this paper easily reproducible ?
CREDIBLE: I believe that the obtained results can, in principle, be reproduced. Even though key resources (e.g., proofs, code, data) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.
8. Independent of your judgement of the quality of the work, are there any ethical concerns with regard to responsible research or potential negative societal impacts of this submission that must be considered by IJCAI-ECAI 2022 before the paper can be accepted? Papers with a yes here will undergo additional ethical screening by senior members of the program committee. In case of glaring violations of well accepted ethical principles, IJCAI-ECAI 2022 reserves the right to reject the submission. Please check our Ethics Policy in the Call for papers for more details.
no
Reviewer #3
Questions
1. Please briefly summarize the main contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposes a two-stage compression framework that allows the student to learn from the teacher via implicit collaboration and explicit demonstration. The paper focuses on language models and empirical experiments on 5 datasets show that the proposed method is better than the comparison methods. The writing of the paper can be improved, some concepts are not easy to map to the detials.
2. What are the main strengths of the paper? Please focus on novelty, soundness, significance and impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility (as specified in our reproducibility guidelines).
1. The proposed method is better than the comparison methods and achieves state-of-the-art performances on 5 datasets.
2. The whole workflow of the method is clear.
3. The paper may give some impact on the area of knowledge distillation.
4. The thinking of the design is not clearly stated in the paper.
3. What opportunities are there to improve the paper?
1. The explanations for thinking are not comprehensive and clear. The author claims the method learns from collaboration and demonstration. WHAT is the collaboration and demonstration in the method. Not the details but the design or the logic that makes the author believe these steps reflect the concepts.

2. The description of modules and branches are not comprehensive, please give some illustration.

3. The literature review is not sufficient.
4. What pressing questions do you have for the authors in the rebuttal ? List (and number) only questions about specific issues here that 1) could directly influence your evaluation of the paper, and 2) do not require providing new results. Typical questions include requests to clarify or justify particular issues, or about important relationships to other works.
1. What's the metric presented in Table 1/2/3?
2. This paper focus on language model compression, what is the restriction of the method that limits its generality?
3. The motivation of probability scheduler is not very clear, what it used for and why it works?
4. What is a module exactly? The author should give an example.
5. From the section 2.1, is every layers in two branch inteactive with each other?
6. Is all the student models for the comparsion share the same structure?
7. What is the defintion of KD-free methods in this paper? Does it contain other DNN compression methods like pruning?
8.
5. Overall assessment.
Borderline Accept. Marginally above the acceptance threshold. Technically correct, but not particularly exciting or inspiring. Could be accepted more or less in its currrent form. Not a big loss if it is not included in the program. Please use sparingly.
6. Justify your score in a few lines. Please focus on novelty, soundness, significance, expected impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility.
This idea and contribution of this paper is clear. The novelty is minor, but the details and experiments is sufficient.
7. Are the results in this paper easily reproducible ?
CONVINCING : I am convinced that the obtained results can be reproduced, possibly with some effort. Key resources (e.g., proofs, code, data) are already available, will be made available upon acceptance, or good reasons as to why they are not (e.g., proprietary data or code) are reported in the paper. Key details (e.g., proofs, experimental setup) are sufficiently well described but their exact recovery may require some work.
8. Independent of your judgement of the quality of the work, are there any ethical concerns with regard to responsible research or potential negative societal impacts of this submission that must be considered by IJCAI-ECAI 2022 before the paper can be accepted? Papers with a yes here will undergo additional ethical screening by senior members of the program committee. In case of glaring violations of well accepted ethical principles, IJCAI-ECAI 2022 reserves the right to reject the submission. Please check our Ethics Policy in the Call for papers for more details.
NO
Reviewer #4
Questions
1. Please briefly summarize the main contributions of the paper in your own words. (Please do not include your evaluation of the paper here).
This paper proposes a method for language model compression that takes inspiration from and combines two existing techniques, namely distillation and structured dropout. The method involves two stages: a) it trains two branches that mix teacher and student layers stochastically with the task-specific objective and b) it trains the two branches and the full student branch with a distillation loss for the same task. Evaluation shows that this method outperforms methods with or without knowledge distillation even high compression rate is required.
2. What are the main strengths of the paper? Please focus on novelty, soundness, significance and impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility (as specified in our reproducibility guidelines).
The paper provides an interesting synthesis of two existing techniques for compressing language models. The combination is straightforward but the precise objectives that are proposed for making it effective are non-trivial and provide value for the community focusing on this area.

Exposition and analysis were clear for the most part. Certain design choices such as the L_intra and L_inter objectives were not sufficiently justified through ablation studies. The terminology regarding "collaboration" and "demonstration" was a bit hard to follow. I would suggest using terminology already established in the literature e.g. finetuning with structured dropout and distillation.

Reproducing the results might be difficult given the exact way the methods were trained is not sufficiently described (e.g. hyper-parameters), especially if the distillation data are not made publicly available.
3. What opportunities are there to improve the paper?
Evaluation should include more details to allow reproducibility in future work. What are the hyper-parameters used for the baselines and how they were decided?

It would be useful for the reader if the evaluation included a proper ablation study that justifies the objectives introduced in the model section. Which objective is contributing the most to the final performance and what is the behavior of the method in the limits?
4. What pressing questions do you have for the authors in the rebuttal ? List (and number) only questions about specific issues here that 1) could directly influence your evaluation of the paper, and 2) do not require providing new results. Typical questions include requests to clarify or justify particular issues, or about important relationships to other works.
The paper could benefit from discussion about the training cost involved for the proposed method.
How does it compare to the individual distillation and structure dropout methods? Is there additional resource budget required for training or it is about the same as the budget required for the individual alternatives?
5. Overall assessment.
Weak accept. Useful. A good paper. The results and insights will benefit the field. I believe it should be accepted.
6. Justify your score in a few lines. Please focus on novelty, soundness, significance, expected impact, relevance to AI, clarity of exposition, and credibility with regard to reproducibility.
Overall, this paper provides an interesting synthesis of two existing techniques which outperforms the individual ones in terms of speed-quality tradeoff. Analysis could benefit from discussion about training cost and impact of individual design choices to the final performance.
7. Are the results in this paper easily reproducible ?
CREDIBLE: I believe that the obtained results can, in principle, be reproduced. Even though key resources (e.g., proofs, code, data) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.
8. Independent of your judgement of the quality of the work, are there any ethical concerns with regard to responsible research or potential negative societal impacts of this submission that must be considered by IJCAI-ECAI 2022 before the paper can be accepted? Papers with a yes here will undergo additional ethical screening by senior members of the program committee. In case of glaring violations of well accepted ethical principles, IJCAI-ECAI 2022 reserves the right to reject the submission. Please check our Ethics Policy in the Call for papers for more details.
The only concerns I can think of are the ones derived from big LMs (unfair discrimination, perpetuating sterotypes, privacy concerns, bias, toxicity, etc).
