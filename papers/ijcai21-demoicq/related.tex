\section{Related Work}
\label{sec:related}

Our work is related to three research fields: spurious features, bias measurement and dataset filtering.
 
\textbf{Spurious features} analysis is hot in recent research. Since 
lots of work~\cite{sharma2018tackling,srinivasan2018simple,zellers2018swag} 
have shown that many models can even 
get good performance without observing the stem of  multiple choice questions.
More generally, some research~\cite{sanchez2018behavior} find the models suffer
from an insensitivity to certain small but semantically significant alterations, and are also
often influenced by simple statistical correlations between words and training labels. 
There are mainly 2 types of spurious features which have been observed: 
 unlexicalized and  lexicalized~\cite{bowman2015large}:
lexicalized feature mainly contains the indicator of n-gram tokens, cross-ngram tokens, 
and unlexicalized features involves word overlap, sentence length and BLUE score between 
premise and hypothesis. ~\citealp{naik2018stress} refined the lexicalized classification to Negation, Numerical Reasoning, 
Spelling Error. ~\citealp{mccoy2019right} refined the word overlap 
features to Lexical overlap, Subsequence and Constituent 
which also consider the syntactical structure overlap. ~\citealp{sanchez2018behavior} 
provided an extra lexicalized feature, unseen tokens. 
%Most of these research works for finding the specific cues or 
%provide accuracy baselines rather than evaluate the datasets and

\textbf{Bias measurement} means the the methods to get the extent of bias for cues. 
Quantities of work~\cite{clark2019don,he2019unlearn,yaghoobzadeh2019robust} 
try to encoding the cue feature implicitly with
option training or extracting the feature from the embeddings based on a certain label. 
%They learn a bias model given source of bias as
%input, and de-bias through logit re-weighting or logit ensembling. 
Other methods measure the bias by computing with statistical metrics. For example, conditional
 probability~\cite{yu2020reclor} for each word based on specific labels. Considering the frequency of 
 the tokens appear to shown the existence of cues. LMI~\cite{schuster2019towards} is 
 also used to evaluate cues and re-weight weights for models. However, they didn't show 
 how efficient of these methods to represent the cues. Though~\citealp{Marco2020acl} provided test data 
 augmentation method, their work can't assess the degree of bias for datasets.
 
\textbf{Dataset filtering} is one of the directions to get higher quality datasets by reducing artifacts. 
Variants of this filter approach have recently
been used to create datasets such as SWAG and Reclor 
iteratively perturbing dataset instances until a target 
model cannot fit the resulting dataset. Some methods~\cite{yaghoobzadeh2019robust} 
leave out the bias samples in the procedure of training epochs. \citealp{bras2020adversarial} 
investigate model-based reduction of dataset cues and design an algorithm with the iterative training. 
Though these methods are more efficient than human annotating and more general 
they heavily rely on the capacity of models. In reality, the data cues belongs to datasets. The 
model-dependent methods are not stable enough.




%or different statistical methods and then 
% reweighting the samples with these scores.



%Large scale datasets are fraught with give-away
%phrases (McCoy et al., 2019; Niven and Kao,
%2019). Crowd workers tend to adopt heuristics
%when creating examples, introducing bias in the
%dataset. In SNLI (Stanford Natural Language Inference)
%(Bowman et al., 2015), entailment based
%solely on the hypothesis forms a very strong baseline
%(Poliak et al., 2018; Gururangan et al., 2018).
%Similarly, as shown by Kaushik and Lipton
%(2018), reading comprehension models that rely
%only on the question (or only on the passage referred
%to by the question) perform exceedingly
%well on several popular datasets (Weston et al.,
%2016; Onishi et al., 2016; Hill et al., 2016). To
%address deficiencies in the SQuAD dataset (Jia
%For example, a model might assign a label of contradiction
%to any input containing the word ``not", since
%``not" often appears in the examples of contradiction
%in standard NLI training sets. The adversarial stress test balance the word ``not'' with 
%each label to test whether the model is just sensitive to this word or not. 
%These methods provide corresponding adversarial test data for each feature to 
%test the robustness of models for the known concrete spurious features, 
%like ``word overlap'', ``negation'', ``length mismatch'', ``antonyms'', 
%``spelling error'' and ``numerical reasoning'' in MNLI, 
%  However, it's also not generalized. For other different genres 
%of statistical cues, like word ``a'' exploited in COPA~\cite{roemmele2011choice}, 
%design other logic rules to generate stress test or 
%balance that by human~\cite{kavumbabalanced-copa}.
