\section{Conclusion}

We observe that models can select correctly without premise and 
pay little attention to the premise on the attention map. 
Inspired by a speculation that 
models can short circuit the premises on MCQs and become fragile, 
we propose two data augmentation methods 
\textit{crossover} and \textit{mutation}. 
Our experimental results show that, while the
proposed methods do not always improve
results on the original datasets,
they significantly and consistently increase the
accuracy on stress tests. 
They improve the model robustness and generalization capability. 
%\KZ{We also analyze the reason for this improvement 
%with fine-grained stress tests, choice-only tests and 
%case study.} 
We also confirm the reason for this improvement is the reduction 
of short-circuits with choice-only tests and case study.
We conclude that our data augmentation methods can 
encourage models to pay more 
attention to the premise of questions. 

%Our experiments verify the existence 
%of short circuit behavior in three fine-tuned strong models.
%We also find that crossover is better than choice-only and 
%human annotation as proxy test to detect short circuit for models. 
%In addition, we try different data augmentation methods and 
%recommend two operators, crossover and mutation, 
%to make models more robust.
