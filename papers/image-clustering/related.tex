\section{Related Work}
\label{sec:related}

\newcommand{\QS}[1]{\textcolor{magenta}{[Qingyu: #1]}}

In this section, we present the recent developments in classification or clustering
of images as well as document search results. In addition, we will also mention
some of the recent work on image context extraction and concept disambiguation
which are related to key components of our work.
% FIXME: rewrite this paragraph.

\subsection{Image Clustering}

We divide image clustering methods into three categories:
content-based, context-based and the combined approaches.

Content-based image clustering approaches \cite{FergusFPZ05,FanGL07,GaoFLS08}
rely on the visual signals from the images.
For example, Fu et al.\cite{Fu2011} gave a constraint propagation
framework for multi-model situations. They constructed multiple graphs,
one for each visual modalities
such as color histogram, SIFT descriptors \cite{Lowe99}, etc. The nodes are images
while the edges are similarities between the images by a particular visual modality.
Then a random walk process is employed on these graphs. During this process, the
walker has some probability of walking to the same node in other graphs. Finally,
they could obtain some propagated constraints, which could be used for
constrained clustering. And there are other content-based approaches.
Zhong et al.\cite{ZhongLL11} designed a deep learning architecture and algorithms.
Some of these techniques have been used in Content-Based Image Retrieval (CBIR)
systems \cite{Chang1984, Datta2008,ChenWK03,ChenWK05}. 

All of the above work uses low-level visual signals of images such colors,
gray scales, contrasts, patterns etc. These signals are often insufficient
to capture high level semantics of the images. This is evident from our
experiments on Fu's algorithm which heavily relies on basic visual signals.
There has been some development on high level visual object recognition
and semantic annotation \cite{Li09scene}, but even the state-of-the-art
techniques in this area suffer from low accuracy and unreliability.

With the difficulty in content based clustering,
some researchers turns to signals coming from the context of the images, 
such as file name, alternate text and surrounding text. 
Cai et al. made some progress in this respect.
They represented a web page segmentation algorithm named VIPS \cite{VIPS},
which works by rendering the web page visually and detecting
the important visual blocks in the page. And they subsequently proposed three
kinds of representations for images \cite{Cai2004b,Cai2004}:
visual feature based representation, textual feature based representation and
link graph based representation, and proposed a two-level clustering algorithm
which combined the latter two.

Jing et al. \cite{Jing2006} introduced a novel method to return clustered image
search result which is similar to the goal of this paper.
Instead of clustering on returned images directly, they first search the query
on normal web search engine, and cluster the titles and snippets from the search results.
They then construct a new query string to represent each of the cluster, and
send these query strings to the image search engine to get images for each
cluster. To construct the query string, they used an algorithm
proposed by Zeng\cite{Zeng2004}, which will be mentioned later.
There is no visual signals used in their algorithm, and the surrounding texts
are even not used too.

The main problem with the above approaches is that they model the image context
by bag-of-words or n-gram models. These are low-level signals which are again
inadequate for understanding the complex semantics of surrounding text. Methods
that rely on bag-of-words or n-grams can easily confuse noise with meaningful
signals. Our approach, on the other hand, leverages co-occurrence information on
high level concepts mined from Wikipedia which is a large, comprehensive knowledge
source, and most importantly, is able to disambiguate entities and concepts using
this knowledge. Hence, we are able to achieve better results than
these competitors.

Recently there are many attempts on combining visual features and textual features in
image clustering. Feng et al. \cite{Feng2004} used the surrounding text of images
and a visual-based classifier to build a co-training framework.
Gao et al.\cite{Gao2005} represented the relationship among low-level visual features,
images and the surrounding texts in a tripartite graph. What's more, Leuken et al.
\cite{LeukenPOZ09} investigated three methods for visual diversification of
image search results in their paper. Main challenge here is that there is a
semantic gap between visual signals and textual signals so there is no easy way
to combine the two kinds of similarity measures into one unifying measure. 
In a way, the framework proposed in this paper is a hybrid approach too except
we focus more on text context and only use visual cues as complementary information
to merge clusters that are left out due to lack of textual signals.

%The most popular image retrieval method is content-based image
%retrieval (CBIR), which organizes and classifies images
%by their visual content or visual signals only \cite{Datta2008}.
%There have been many important attempts on CBIR. Some of the pioneering work was done
%by Chang et al.\cite{Chang1984}. They introduced a
%picture indexing and abstraction approach for pictorial database retrieval.

%Since then, CBIR has been successfully commercialized in
%image search engines, such as VisualSEEk \cite{Smith1996}.
%\KZ{This para is too simple and i'm not sure what you want to say? The next para
%doesn't follow naturally.}

%More recently, CBIR has been successfully applied to image
%classification.
%Zhong et al.\cite{ZhongLL11} designed a deep learning architecture and
%algorithms. %The architecture is the same as the
%multi-layer physical structure of the human visual cortex, which is associated
%with many cognitive abilities for human beings. There are three stages in their
%algorithm: bilinear discriminant initialization, greedy layer-wise
%reconstruction, and global fine-turning. With this algorithm, they could
%differentiate various kinds of picture from each other,
%such as airplanes, motorbikes, faces, tall buildings and so on.
%\KZ{We say so much about Zhong but we didn't compare with him in the experiments.
%What's the point? Unless we can kill his idea directly with a strong argument.}
%Fu et al. \cite{Fu2011} gave a constraint propagation framework for multi-model
%situations. They constructed multiple graphs, one for each visual modalities
%such as color histogram, SIFT \KZ{cite here}, etc. The nodes are images
%while the edges are similarities between the images by a particular visual modality.
%Then a random walk process is employed on these graphs. During this process, the
%walker has some probability of walking to the same node in other graphs. Finally,
%they could obtain some propagated constraints, which could be used for
%constrained clustering.

%All of the above work uses low-level visual signals of images such colors,
%gray scales, contrasts, patterns etc. These signals are often insufficient
%to capture high level semantics of the images. This is evident from our
%experiments on Fu's algorithm which heavily relies on basic visual signals.

%\KZ{What about work on high level visual semantics like Li Feifei's work, etc.?
%The problem in this space is that high level object recognition in computer vision
%is still immature with relatively low accuracy (evidence?).}

%But for the results from search engine, we have some textual signals, such as the
%surrounding text of picture, the search keyword and the URL information.

%Because of the challenges in high level visual recognition, other researchers
%turns to signals coming from surrounding text of web images,
%often by combining textual signals with the visual signals.
%Cai et al. made some progress in this respect \cite{Cai2004b,Cai2004}.
%They represented a method to bridge the gap between
%visual and text information by a web page segmentation algorithm named
%VIPS\cite{VIPS} which works by rendering the web page visually and detecting
%the important visual blocks in the page. They subsequently proposed three
%kinds of representations for images: visual feature based representation,
%textual feature based representation and link graph based representation,
%and proposed a two-level clustering algorithm.

%Since
%they think this it's an open problem for extracting sematic information by
%visual feature of pictures, they only use textual feature and link graph to
%finish their two-level clustering algorithm.
%And there are even more researches:%, such as \cite{Feng2004,Gao2005}.

%Along the same line, Feng et al.\cite{Feng2004} used the surrounding text of images
%and a visual-based classifier to build a co-training framework.
%Gao et al.\cite{Gao2005} represented the relationship among low-level visual features,
%images and the surrounding texts in a tripartite graph.

%Jing et al.\cite{Jing2006} introduced a novel method to return clustered image
%search result which is similar to the goal of this paper.
%Instead of clustering on returned images directly, they first search the query
%on normal web search engine, and cluster the titles and snippets from the search results.
%They then construct a new query string to represent each of the cluster, and
%send these query strings to the image search engine to get images for each
%cluster. To construct the query string, they used an algorithm
%proposed by Zeng\cite{Zeng2004}, which will be mentioned later.
%There is no visual signals used in their algorithm, and the surrounding texts
%are even not used too. The major weak point of their work is, the result
%relies on the image search engine.

%?gHowever, in all these researches above using textual information, they just
%?gextracted the surrounding text of images, or used web page abstract text in\cite{Jing2006},
%?gand there is no further processing on the surrounding text as
%?gwell. This surrounding text
%?gshould has a lot of noises which prevent us from understanding the meaning of
%?gthe surrounding text.
% FIXME: too many "surrounding text"s here!

%In our work, we use \textit{Wikification} method to help us extract useful
%information from surrounding text. Some related work of \textit{Wikification}
%is mentioned in Section \ref{wikification}.
% TODO: Is there any other algorithms we used? More description about our
% algorithm needed here.

\subsection{Web Document Search Results Clustering}

Because our clustering algorithm focuses on text signals in this paper,
next we survey a few representative pieces of work on clustering web documents.
%Besides image search results clustering, there are also many researches about
%web document search results clustering. The primary difference between them is
%that there is no image information in web document search results clustering
%methods. That is to say, we cannot extract the low-level visual signals and
%surrounding texts in web document search results clustering.
The two main clustering techniques
used in search results clustering are information retrieval and machine
learning\cite{Leouski1996,ZamirE98,ZamirE99}.

The most relevant work to this paper is done by
Zeng et al.\cite{Zeng2004} who introduced a web search results clustering algorithm
based on machine learning. First, they extracted all possible phrases from the
contents by n-grams, and extracted features from each phrase. Then they used
a regression model learned from training data, and applied on the features to
get a \textit{salience score} for each phrase. The phrases with top score were taken
as \textit{salience phrase}. These \textit{salience phrase} are actually the
names of candidate clusters which are merged later.
Moreover, Hearst and Pedersen\cite{Hearst1996} presented a cluster-based document
browsing method in 1996. They used pairwise cosine similarity between document
vectors and the fractionation clustering algorithm in their Scatter/Gather
system. The fractionation algorithm needs the number of clusters as an input, and the
complexity is $O(kn)$, while $k$ is the number of clusters, and $n$
is the number of documents.

%\cite{vivisimo}

%\KZ{Summarize the above approaches and compare/connect with our approach?}

\subsection{Image Context Extraction}

Work specifically on image context extraction is limited.
Alcic and Conrad \cite{Alcic2010} measured some context extraction methods, including
N-terms-environment \cite{Coelho2004}, the Monash extractor\cite{Fauzi2009},
siblings extractor which simply selects the sibling text nodes of the image node
in the DOM tree, and the VIPS \cite{VIPS} algorithm which we mentioned earlier.
%Since it's hard to define the exact the context of a web image
%and it's a little overly
%restrictive for testing with a too strong criterion, they consider the result
%as a correct one when there is partially accordance between the output and the
%test set. \KZ{Don't understand the above sentence.}
They used an F-score which is defined as
$$F_{score} = 2 \cdot \frac{P \cdot R}{P + R}$$
to represent the final result, where $P$ is the precision and $R$ is the recall.
And they computed the standard deviation of the F-score to measure the stability
of all the algorithms.
It is interesting that their experiments concluded that extracting context
based on purely on the DOM structure performed better than visual-based approach with
VIPS being the worst performer. Since the sibling based extractor strikes a good
balance between accuracy and time cost, we adopt a context extractor
which is modified from the sibling extractor in our work.

\subsection{Entity Disambiguation}
\label{wikification}

%In our work, we used a \textit{Wikification} method as a
%\textit{Word Sense Disambiguation} (WSD) algorithm. As it's an open problem,
%there are a lot of previous works on WSD \cite{GuthrieGWA91,Li1998:wcd,ChungKML01,Stokoe2003:WSD,Fernandez-AmorosGSS10}.
There is large body of work on entity disambiguation. Due to space constraint,
we limit our discussion on work that specifically leverages the knowledge from
Wikipedia which is what we do in this paper.
Techniques in this space are collectively called ``wikification'', which links
terms in a text document to Wikipedia articles, effectively giving each term
an explicit label (the Wikipedia concept).

Mihalcea and Csomai \cite{MihalceaC07} are among the first to try
automatically linking terms in a document to Wikipedia concepts. Strube and
Ponzetto\cite{StrubeP06} propose three kinds of methods to measure the semantic
distance between two Wikipedia concepts, including path based, information
context based and text overlap based method, which help to decide the relation
between two Wikipedia concepts.
Kulkarni et al. \cite{kulkarni2009collective} propose a wikification method
taking use of two factors, which are the compatibility between a term and a
concept and the relatedness between two concepts. A hill-climbing approach
is used to combine these two factors to get an approximate result.
In Cucerzan \cite{cucerzan2007large} and Ferragina et al. \cite{ferragina2010tagme}'s
work, they build a model for each Wikipedia concept. When wikifying a document, 
they merge the models of all the terms' candidate Wikipedia concepts to get
an average model of the whole document. And then for each term, the candidate concept
having the best similarity with the average model is picked up to link that term.
However, merging all the concept models will generate a lot of noise.

Most of the above work uses bag-of-words model to represent Wikipedia concepts.
However, using bag-of-words (or even bag-of-terms) model can not capture the 
semantic information well because
1) words and phrases can both be ambiguous and 2) when words are combined into phrases, 
their original meaning changes. Therefore, in our work, we use co-occurrence
frequency between two concepts to measure their relatedness, which 
attacks avoids the above drawbacks. 
When wikifying a document, instead of comparing the concept model with
the document model, like many previous methods do, 
we calculate the best combination of concepts according to
the mutual co-occurrence frequency among them, and then assign concepts inside
the combination to the corresponding term, which avoid the noises in 
Cucerzan and Ferragina et al.'s work.

%Mline and Witten \cite{lui2011generation} applied a machine learning
%method in their work.

%While most of the Wikification methods are supervised,
%which needs a lot of training to find some good features, we employ
%an unsupervised method in our work. Our method depends only on the concept
%co-occurrences which can be mined from the Wikipedia corpus automatically.
%\KZ{ZY: summarize the above and give a verdict.}
%Different from the above work,
%we set a window and try to find the best combination of concepts in this window,
%instead of merging all the candidate concepts together.
