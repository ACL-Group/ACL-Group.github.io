\section{Conclusion}
\label{sec:conclude}
%DIF < We presented a distributed attention mechanism to modify existing CNN seq2seq model 
%DIF < and were able to train a model that produces 
%DIF < summaries without repetition that are fluent and coherent. 
We analyze two possible reasons behind the repetition problem in abstractive
summarization: (1) attending to the same location in source,
and (2) attending to similar but different sentences in source. 
In response, we present a section-aware attention mechanism (ATTF)
as well as a sentence-level backtracking decoder (SBD). 
\DIFdelbegin \DIFdel{Our }\DIFdelend %DIF > Compared ,we presented a distributed attention mechanism to modify existing CNN seq2seq model 
%DIF > and were able to train a model that produces 
%DIF > The proposed models are able to train a model that produces 
%DIF > summaries without repetition that are fluent and coherent. 
\DIFaddbegin \DIFadd{The proposed }\DIFaddend model is able 
to produce \DIFaddbegin \DIFadd{more }\DIFaddend fluent and coherent summaries with minimal repetitions.
\DIFdelbegin \DIFdel{Besides, }\DIFdelend %DIF > Besides, 
\DIFaddbegin \DIFadd{It means that }\DIFaddend the summaries generated by our model are more accurate and 
readable\DIFaddbegin \DIFadd{. This can help user quickly get the main information from large of textual data,
saving the reading time and improving reading efficiency.
As some other natural language generation (NLG) tasks based on seq2seq model with attention mechanism
are orthogonal to our proposed methods,
they can also be enhanced with redistributed attention and repetition reduction}\DIFaddend .
%We find that the basic CNN seq2seq model 
%still has some problems, such as generating repeated word sequence. 
%We also argue that ROUGE is not a perfect evaluation metric for the abstractive 
%summarization. Our future work will focus on these two aspects.

