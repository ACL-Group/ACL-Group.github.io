\section{Related Work}
\label{sec:related}

%DIF < Summarization is the task of condensing a piece of document into a shorter paragraph or a single sentence, while retaining correctness, fluency, consistency as well as the core idea of the original document. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < Generally, this task is tackled in two different ways: extractive and abstractive. Extractive approach \cite{Dorr2003HedgeTA,DBLP:journals/corr/NallapatiZZ16} takes sentences from the source text to compose a summary, while abstractive one \cite{RushCW15,SeeLM17,PaulusXS17} \textit{generates} sentences word by word after comprehending the source text as a whole. For abstractive method, the algorithm has a vocabulary from which it can freely choose words and phrases to compose sentences, while the extractive one is more similar to catching the key sentences of the source text. Extractive summarization can more easily produce acceptable summaries, as copying sentences from the source text guarantees correctness and consistency. However, extractive methods lacks creativity. More often than not, humans summarize documents in an abstractive manner, where they comprehend the whole document before selecting elements from their vocabulary to compose a short text that encapsulates the main idea and even the underlying intent from the source sentences. This is where a sense of expressiveness and elegance can be found.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < Many \cite{RushCW15,SeeLM17} choose to build an abstractive model using sequence-to-sequence model using recurrent neural networks and attention mechanism. In pursuit of speed and  parallelism, \cite{gehring2017convs2s} proposed a convolutional seqence-to-sequence model with Gated Linear Units \cite{DauphinFAG17}, attention mechanism to tackle a series of generation tasks. It achieves state-of-the-art accuracy in abstractive single-sentence summarization and is much faster than recurrent approaches.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < Repetition is a persistent problem in the task of summarization. Models are found to in some cases generate similar or the same words and phrases repeatedly, which causes grammatical incorrectness and redundancy. This reflects a insufficiency in the decoder's awareness of previous generations. To address this issue,  \cite{SeeLM17} use coverage to keep track of what has been summarized, which discourages repetition in an indirect manner, \cite{PaulusXS17} propose intra-decoder attention to avoid attending to the same parts in the source text by dynamically revising attention scores while decoding. \cite{PaulusXS17} also avoid repetition in test time by directly banning the generation of repeated trigrams in beam search. However, the weakness of this method is that the action of eliminating highly probable sentences with repeated trigrams disturbs the process of beam search, giving rise to grammatically incorrect sentences, whose probabilities are actually low in the model. 
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Repetition problems 
is tackled in broadly two aspects }\DIFdelend \DIFaddbegin \DIFadd{Abstractive Summarization
is one of the most challenging and interesting problems 
in the field of Natural Language Processing (NLP)
\mbox{%DIFAUXCMD
\cite{CareniniC08,PallottaDB09,SankarasubramaniamRG14,BingLLLGP15,RushCW15,LiHZ16,YaoWX17,MohamedO19,LierdeC19,NguyenCNN19}}\hspace{0pt}%DIFAUXCMD
.
It is a process of generating a concise and meaningful summary of text from multiple text resources 
such as news articals.
Great progress~
\mbox{%DIFAUXCMD
\cite{RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17,HardyV18,KourisAS19,LiuL19,ZhangWZ19,WangQW19}
}\hspace{0pt}%DIFAUXCMD
has been made recently on
neural-based abstractive summarization.
Repetition is a persistent problem in the task of 
neural-based summarization. 
It is tackled broadly in two directions }\DIFaddend in recent years. 

\DIFdelbegin \DIFdel{Chen and Bansal}%DIFDELCMD < \shortcite{P18-1063} %%%
\DIFdel{and Li}%DIFDELCMD < \shortcite{D18-1205,D18-1441} %%%
\DIFdel{construct hierarchical models to deal with summarization. Their approaches typically involve information selection}\DIFdelend \DIFaddbegin \DIFadd{One involves }{\em \DIFadd{information selection}} \DIFaddend or sentence
selection before generating summaries.
\DIFdelbegin \DIFdel{Among them, }\DIFdelend \DIFaddbegin \DIFadd{Chen~}\DIFaddend \cite{P18-1063} uses an extractor  
\DIFdelbegin \DIFdel{agent }\DIFdelend to select salient sentences or highlights \DIFdelbegin \DIFdel{, }\DIFdelend and then employs 
an abstractor network to \DIFdelbegin \DIFdel{compress }\DIFdelend \DIFaddbegin \DIFadd{rewrite }\DIFaddend these sentences.
\DIFdelbegin \DIFdel{In their model, repetition is naturally avoided as long as the source document is non-repetitive. But it does not aim to }\DIFdelend \DIFaddbegin \DIFadd{It can not }\DIFaddend solve repetition in \DIFdelbegin \DIFdel{sequence-to-sequence generation (source document to summary).
\mbox{%DIFAUXCMD
\cite{D18-1205,D18-1441} }\hspace{0pt}%DIFAUXCMD
generate sentence representations and then use them to produce sentences for summary. All of the above models are RNN-based. 
However, 
their methods cannot be transferred easily to CNN-based models because there is no natural CNN-based approach to generate a sentence from a sentence }\DIFdelend \DIFaddbegin \DIFadd{seq2seq model.
Tan~\mbox{%DIFAUXCMD
\cite{TanWX17} }\hspace{0pt}%DIFAUXCMD
and Li~\mbox{%DIFAUXCMD
\cite{D18-1205,D18-1441} }\hspace{0pt}%DIFAUXCMD
encode
sentence using word vectors
and predicts words from sentence vector in sequential order 
whereas CNN-based models are naturally parallelized. 
While transferring RNN-based model to CNN model, 
the kernel size and the number of 
convolutional layers can not be easily determined when
converting between sentence and word }\DIFaddend vector. 
Therefore, we do not \DIFdelbegin \DIFdel{consider the above models as our baselines}\DIFdelend \DIFaddbegin \DIFadd{compare our models to those models}\DIFaddend . 

\DIFdelbegin \DIFdel{See}%DIFDELCMD < \shortcite{SeeLM17}%%%
\DIFdel{, Paulus}%DIFDELCMD < \shortcite{PaulusXS17} %%%
\DIFdel{and Fan}%DIFDELCMD < \shortcite{FanGA18} %%%
\DIFdel{choose to adopt sequence-to-sequence approach, where source document and
summary are treated as two long sequences, without the idea of separate sentences.
\mbox{%DIFAUXCMD
\cite{SeeLM17} }\hspace{0pt}%DIFAUXCMD
integrate coverage }\DIFdelend \DIFaddbegin \DIFadd{The other direction is to improve the 
}{\em \DIFadd{memory of previously generated words}}\DIFadd{.
Suzuki~\mbox{%DIFAUXCMD
\cite{SuzukiN17} }\hspace{0pt}%DIFAUXCMD
and Lin~\mbox{%DIFAUXCMD
\cite{LinSMS18} 
}\hspace{0pt}%DIFAUXCMD
deal with word repetition in single-sentence summaries, 
while we primarily deal with multi-sentence summaries with 
sentence-level repetition. 
%DIF > There is almost no word repetition in CNN-based model.
There is almost no word repetition in multi-sentence summaries.
Jiang~\mbox{%DIFAUXCMD
\cite{JiangB18} }\hspace{0pt}%DIFAUXCMD
adds a new decoder without attention mechanism.
In CNN-based model, attention mechanism is necessary to connect encoder 
and decoder.
Thus, our model also is not compared with the above models. 
The following models can be transferred to CNN seq2seq model and
are used as our baselines.
See~\mbox{%DIFAUXCMD
\cite{SeeLM17} }\hspace{0pt}%DIFAUXCMD
integrates coverage mechanism}\DIFaddend , 
which keeps track of what \DIFdelbegin \DIFdel{has }\DIFdelend \DIFaddbegin \DIFadd{have }\DIFaddend been summarized, as a feature that helps 
redistribute the attention \DIFdelbegin \DIFdel{score }\DIFdelend \DIFaddbegin \DIFadd{scores }\DIFaddend in an indirect manner\DIFaddbegin \DIFadd{,
}\DIFaddend in order to discourage repetition. 
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{PaulusXS17} }\hspace{0pt}%DIFAUXCMD
propose }\DIFdelend \DIFaddbegin \DIFadd{Tan~\mbox{%DIFAUXCMD
\cite{TanWX17} }\hspace{0pt}%DIFAUXCMD
uses distraction attention
\mbox{%DIFAUXCMD
\cite{ChenZLWJ16}}\hspace{0pt}%DIFAUXCMD
, which is identical to coverage mechanism. 
Gehrmann~\mbox{%DIFAUXCMD
\cite{GehrmannDR18} }\hspace{0pt}%DIFAUXCMD
adds coverage penalty to loss function
which increases whenever the decoder directs more than 1.0 of total attention
towards a word in encoder.
This penalty indirectly revises attention distribution and results in
the reduction of repetition.
}{\DIFadd{\c{C}}}\DIFadd{elikyilmaz~\mbox{%DIFAUXCMD
\cite{elikyilmazBHC18} }\hspace{0pt}%DIFAUXCMD
uses semantic cohesion loss,
which is the cosine similarity between two consecutive sentences, as part of
the loss that helps reduce repetition.
Paulus~\mbox{%DIFAUXCMD
\cite{PaulusXS17} }\hspace{0pt}%DIFAUXCMD
proposes }\DIFaddend intra-temporal attention \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{NallapatiZSGX16} }\hspace{0pt}%DIFAUXCMD
}\DIFaddend and 
intra-decoder attention \DIFdelbegin \DIFdel{that dynamically revises }\DIFdelend \DIFaddbegin \DIFadd{which dynamically revises the }\DIFaddend attention distribution while decoding. 
\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{PaulusXS17} }\hspace{0pt}%DIFAUXCMD
also avoid repetition in }\DIFdelend \DIFaddbegin \DIFadd{It also avoids repetition at }\DIFaddend test time by directly banning the generation of 
repeated trigrams in beam search. 
\DIFdelbegin \DIFdel{These two models are RNN-based. }\DIFdelend %DIF > These two models are RNN-based. 
\DIFaddbegin \DIFadd{Fan~}\DIFaddend \cite{FanGA18} borrows the idea from \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{PaulusXS17} }\hspace{0pt}%DIFAUXCMD
and build a RNN-based model. 
We transfer methods from these models to form our baselines, as introduced in }%DIFDELCMD < \secref{sec:basic}%%%
\DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{Paulus~\mbox{%DIFAUXCMD
\cite{PaulusXS17} }\hspace{0pt}%DIFAUXCMD
and 
builds a CNN-based model. 
}\DIFaddend 

Our model \DIFdelbegin \DIFdel{also adopts a sequence-to-sequence approach and }\DIFdelend deals with the attention in \DIFaddbegin \DIFadd{both }\DIFaddend encoders and decoders. 
Different from the previous methods, 
our \textit{attention filter mechanism} does not 
\DIFdelbegin \DIFdel{deal with }\DIFdelend \DIFaddbegin \DIFadd{treat }\DIFaddend the attention history \DIFdelbegin \DIFdel{collectively. Instead,  
our attention filter is section-aware. 
In \mbox{%DIFAUXCMD
\cite{SeeLM17,gehring2017convs2s}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{as a whole data structure,  
but divides it into sections (}\figref{fig:model_main}\DIFadd{). 
%DIF > \KZ{I actually think it might be better to draw a diagram
%DIF > to show this in approach.} 
Previously}\DIFaddend , the distribution curve of accumulated attention scores 
for each token in the source document \DIFdelbegin \DIFdel{grows flatwith critical information washed away }\DIFdelend \DIFaddbegin \DIFadd{tends to be flat, 
which means critical information is washed out }\DIFaddend during decoding.
Our method \DIFdelbegin \DIFdel{, on the other hand, amplifies most }\DIFdelend \DIFaddbegin \DIFadd{emphasizes }\DIFaddend previously attended sections 
so that \DIFaddbegin \DIFadd{important }\DIFaddend information is retained\DIFdelbegin \DIFdel{(}%DIFDELCMD < \secref{sec:attf}%%%
\DIFdel{).
Also, given }\DIFdelend \DIFaddbegin \DIFadd{.
}

\DIFadd{Given }\DIFaddend our observation that repetitive sentences in the source \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{are
}\DIFaddend another cause for repetition in summary, 
which cannot be directly resolved by manipulating attention \DIFaddbegin \DIFadd{values}\DIFaddend , 
we introduce \DIFdelbegin \DIFdel{our }\DIFdelend \textit{sentence-level backtracking decoder}. 
Unlike \DIFaddbegin \DIFadd{Paulus }\DIFaddend \cite{PaulusXS17}, 
we do not ban repetitive \textit{trigrams} in test time. 
\DIFdelbegin \DIFdel{Instead, our }\DIFdelend %DIF > Instead, 
\DIFaddbegin \DIFadd{Our }\DIFaddend decoder regenerates a sentence that is similar to \DIFdelbegin \DIFdel{any }\DIFdelend previously generated ones.
With the two \DIFdelbegin \DIFdel{methods}\DIFdelend \DIFaddbegin \DIFadd{modules}\DIFaddend , our model is capable of generating summaries with \DIFaddbegin \DIFadd{a
}\DIFaddend natural level of repetition while retaining fluency and consistency.
 %DIF < plz let me know if revised or reused
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD <  %%%
\DIFdelend