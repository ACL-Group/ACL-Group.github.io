\section{Introduction}
\label{sec:intro}

Abstractive summarization is the task of creating a short, accurate,
informative and fluent summary from a longer text document.
It attempts to reproduce the semantics and topics of original text
by paraphrasing. 
Recently, sequence to sequence
models~\cite{RushCW15,ChopraAR16,NallapatiZSGX16,SeeLM17,PaulusXS17}
have made great progress on abstractive summarization.
Recent study~\cite{bai2018empirical} suggests that, 
without additional, complicated structures or features,
convolutional sequence to sequence 
(CNN seq2seq) models~\cite{gehring2017convs2s,FanGA18,LiuLZ18} 
are more effective and can be trained much faster due to 
its intrinsic parallel nature 
compared to recurrent sequence-to-sequence seq2seq (RNN seq2seq) models.
Furthermore, unlike RNN-based models, 
the CNN-based models have more stable gradients 
because of their backpropagation paths~\cite{bai2018empirical, LiuLZ18}. 
Thus, we take CNN seq2seq models as the target model to improve on and
compare with
in this paper.

Unfortunately, just like RNN-based models, CNN-based models also produce
summaries with substantial repeated word sequences which impacts the reading efficiency.
\tabref{tab:example} illustrates one 
test case from the CNN/Daily Mail summarization dataset. 
In this case, the basic CNN model produces two 
identical sentences (italized). 
Unlike machine translation or paraphrasing in which the output words
and input words are almost one-to-one aligned, the output of summarization
is ``compressed'' from the input document. Naturally, every sentence or 
word sequence in the summary corresponds to one or more places in the source
document. If there were two identical word sequences in the summary,
they might be looking at and summarizing the same ``spots'' in the source document.
This is evident from the attention map for the three sentences generated by 
basic CNN model, shown in \figref{fig:attn_map}. 
The 1st and 3rd sentences attend to
the same location in the source (red boxes), while the second
sentence attends to another separate location in the source (green box). 
The two attention maps in the red boxes are very similar.

\begin{table}[th!]
\begin{center}
\scriptsize
\begin{tabular}{|l|}%{|p{7cm}|rl|}
\hline \bf Source document \\
\hline manchester city are rivalling manchester united and arsenal for valenciennes \\
       teenage defender dayot upamecano . the 16-year-old almost joined united in \\
	   the january transfer window only for him to opt to stay in france for a few \\
	   more months . centre-back umecano has played for france at u16 and u17 \\
	   level . monaco , inter milan and paris stgermain had also expressed interest. \\
	   fourth-placed city face aston villa at the etihad stadium on saturday . \\
\hline \bf Reference summary \\
\hline dayot upamecano was close to signing for manchester united in january . \\
       the 16-year-old, however , opted to stay in france with valenciennes . \\
	   centre-back upamecano has played for france at u16 and u17 level . \\
	   arsenal are also interested in the defender as man city join chase . \\
\hline \bf Basic CNN model (CNN) \\
\hline \textit{manchester city face aston villa at the etihad stadium on saturday .}\\
       the 16-year-old almost joined united in the january transfer window . \\
	   \textit{manchester city face aston villa at the etihad stadium on saturday .}\\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:example} Summary generated by the basic CNN model}
\end{table}


\begin{figure}[th!]
\centering
\subfigure[Attention distribution]{
\includegraphics[width=0.8\linewidth]{map}
}
\quad
\subfigure[Green box]{
\includegraphics[width=0.44\linewidth]{map_1}
}
\quad
\subfigure[Red box]{
\includegraphics[width=0.44\linewidth]{map_2}
}
\caption{Attention for example on the basic CNN model in \tabref{tab:example}}
\label{fig:attn_map}
\end{figure}

Driven by this intuition, a few efforts have been made on ``remembering''
what has been focused on before at decoding. 
For example, 
Paulus~\cite{PaulusXS17} and 
Fan~\cite{FanGA18} use intra-temporal 
attention~\cite{NallapatiZSGX16} as well as intra-decoder attention to avoid
attending to the same parts in the source by 
revising attention scores while decoding. 
See~\cite{SeeLM17} and Gehrmann~\cite{GehrmannDR18}
respectively propose coverage mechanism and coverage penalty,
which records the sum of attention distributions of all previously generated words 
in a different way to track the summarized information.  
While these approaches discourage repetition to some extent,
they do so in an indirect manner. That is, they do not 
make use of the attention information in source directly.
Consequently, they may still generate repeated phrases, 
especially in long sentences (shown in the first 5 summaries of
\tabref{tab:strong_methods}).


\begin{table}[th!]
\begin{center}
\scriptsize
\begin{tabular}{|l|}%{|p{7cm}|rl|}

\hline \bf Intra-temporal attention (ITA) \\
\hline \textit{manchester city face aston villa at the etihad stadium on saturday .} \\
	   \textit{manchester city face aston villa at the etihad stadium on saturday .}\\
\hline \bf Intra-temporal $+$ Intra-decoder (ITDA) \\
\hline \textit{manchester city are rivalling manchester united and arsenal }for valenciennes\\
       teenage . manchester city face aston villa at the etihad stadium on saturday . \\
	   \textit{manchester city are rivalling manchester united and arsenal }. \\
\hline \bf Coverage model (COV) \\
\hline \textit{manchester city face aston villa at the etihad stadium on saturday .} \\
       manchester city are rivalling manchester united and arsenal for valenciennes . \\
	   \textit{manchester city face aston villa at the etihad stadium on saturday .}\\
\hline \bf Coverage penalty (COVP)\\
\hline \textit{manchester city face aston villa at the etihad stadium on saturday .}\\
       \textit{manchester city face aston villa at the etihad stadium on saturday .}\\
	   manchester city are rivalling manchester united and arsenal .\\
\hline \bf Semantic cohesion loss (SCL) \\
\hline \textit{manchester city are rivalling manchester united and arsenal for } defender dayot upamecano . \\
       \textit{manchester city are rivalling for} valenciennes teenage. \\
\hline \bf Trigram decoder (TRI) \\
\hline \underline{defender dayot upamecano} has played for \underline{france} at unk and unk level .\\ 
       manchester city face aston villa at the etihad stadium on saturday . \\
\hline \bf Ours (Attention Filter + Backtracking decoder) \\
\hline manchester city face aston villa at the etihad stadium on saturday . \\
       the 16-year-old almost joined united in the january transfer window . \\
	   manchester city are rivalling manchester united and arsenal for teenage defender daypot upamecano .\\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:strong_methods} Generated summaries of the source in \tabref{tab:example}}
\end{table}

In this paper, we propose an attention filter mechanism that directly 
redistributes the attention from each word in the output summary to the source. 
It does so by computing the \textbf{parts of interest} (POIs) in the source
per segment in the summary
and then minimizing the attention scores of
words in these POIs that have already been attended to by the preceding 
segments during decoding. 
A segment means a sentence or clause delimited by punctuation,
which carries syntactic and semantic information. 
This is very simple but effective way to split the summaries. 
Since punctuations 
play an important role in written language to organize 
the grammatical structures and to clarify the meaning of sentences~\cite{briscoe1996,Kim19,LiWE19}
Specifically, we calculate the attention in terms of segments, larger semantic units than tokens, 
which intuitively helps with emphasis of attention and POIs in source.
Different segments in summary thus do not attend to the same semantic spots
of source, and repetition is reduced. 
This is different from previous approaches
that all do not exactly pinpoint these parts in the source,
which we believe is critical in reducing repetition. 

Despite the above effort, there are cases, where similar sentences 
exist in the same source document:
\begin{example}
\label{ex:repeatsrc}
\small{``..the standout fixture in the league on saturday sees leaders 
	   chelsea welcome manchester united ... chelsea midfielder oriol romeu, 
\textbf{currently on loan at stuttgart}, ... romeu is 
\textbf{currently on a season-long loan at bundesliga side stuttgart.}''} 
\end{example}

In this case, even if the decoder attends to different POIs of 
source document as it produces words, repetition may still result.  
At different timesteps,
the decoder may attend 
to sentences that are similar in different positions.
There are two potential solution to this.
One is semantic cohesion loss~\cite{elikyilmazBHC18}
which takes the cosine similarity between two consecutively generated sentences
as part of the loss function. However, it may attend to the same POI
and generate similar sentences (SCL row in \tabref{tab:strong_methods}).  
The other is a trigram decoder~\cite{PaulusXS17} 
which directly forbids repetition of previously generated trigrams at test time. 
While this simple but crude method avoids the repeat of any kind
completely, 
it ignores the fact that \textit{some amount of repetition} may exist
in natural summaries.  
On the other hand, the meddling of the beam search
~\footnote{Beam search is an iterative algorithm widely used to decode
summaries in neural-based abstractive summarization.
~\cite{beamsearch}}
at run-time causes another problem: 
it tends to generate sentences that are logically incorrect. 
In \tabref{tab:strong_methods} (TRI row), the defender dayot didn't
really play for France, according to the source.
That is, the subject and object are mismatched.
So in this paper, we instead introduce a {\em sentence-level} backtracking decoder
that prohibits the repeat of the same sentence at test time.
Compared with trigram decoder, {\em sentence-level} backtracking decoder
does not impact
the process of beam search in the middle of a sentence, 
which can reduce factual errors and grammatical erros.
Our summary produced for the example is shown in last summary of 
\tabref{tab:strong_methods}.

Reducing repetition in abstractive summarization provides high-quality summaries for users and improves their reading efficiency.
We expect that other natural language generation (NLG) tasks with repetition problem can be enhanced with our proposed approach. 
Our contributions are summarized as follows:
\begin{enumerate}
\item We identify the repetition problem in abstractive summaries generated
by CNN seq2seq models and find the reasons behind the repetition problem.
\item We propose new metrics to evaluate this problem: Repeatedness and Readability.
\item We propose an effective approach that redistributes attention scores 
during training time, and prevents repetition by sentence backtracking
at test time to reduce repetition in CNN seq2seq model.
\item Our approach
outperforms the state-of-the-art (SOTA) CNN-based baselines 
substantially by all evaluation metrics, including ROUGE scores, 
repeatedness, correlation and readability.
\end{enumerate}

Next, we discuss the related work (\secref{sec:related}),
followed by 
our approach based on convolutional sequence to sequence 
model (\secref{sec:approach})
and the evaluation of our approach (\secref{sec:eval}).
