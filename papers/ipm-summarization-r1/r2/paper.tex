\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Journal of Information Processing and Management}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\usepackage{soul}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

\usepackage{helvet}
\usepackage{courier}
\usepackage{color}
\usepackage{amsmath,amsfonts,amssymb,amsthm,amsopn}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{array}
\usepackage{multicol}
\usepackage{threeparttable}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{multirow}
\usepackage{subfigure}
\theoremstyle{definition}
\newtheorem{example}{Example}

\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\cut}[1]{}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

%\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
%\newcommand{\YZ}[1]{\textcolor{red}{Yizhu: #1}}

\begin{document}

\begin{frontmatter}

\title{Reducing Repetition in Convolutional Abstractive Summarization}

%% Group authors per affiliation:
\author[mymainaddress]{Yizhu Liu}
\ead{liuyizhu@sjtu.edu.cn}
\author[mymainaddress]{Xinyue Chen} 
\ead{sherryicss@gmail.com}
\author[mysecondaryaddress]{Xusheng Luo} 
\ead{lxs140564@alibaba.com}
\author[mymainaddress]{Kenny Q. Zhu\corref{mycorrespondingauthor}}
\ead{kzhu@cs.sjtu.edu.cn}
\cortext[mycorrespondingauthor]{Corresponding author}
\address[mymainaddress]{Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China}
\address[mysecondaryaddress]{Search and Recommendation Team, Alibaba Group,
Hangzhou, China}

\begin{abstract}
Convolutional sequence to sequence (CNN seq2seq) models
have met great success in abstractive summarization. 
%However, their outputs often contain repetitive word sequences and logical
%inconsistencies, limiting the practicality of their application.
%It is important for CNN seq2seq model to get the ability to 
%generate summaries without repetition in abstractive summarization. 
However, repetition is a persistent problem in the task of 
CNN seq2seq abstractive summarization. 
In this paper, we identify the repetition problem in abstractive summarization and find the reasons behind the repetition problem.
%based on CNN seq2seq model and attention mechanism.
%The repetition is caused by decoders attending to the same locations in source document and attending to similar (but different) sentences in the source. 
We propose to reduce the repetition in summaries by 
Attention Filter mechanism (ATTF) and Sentence-level Backtracking Decoder (SBD),
which dynamically redistributes attention over the input sequence as the output sentences are generated. 
The ATTF can record previously attended locations in the source document directly and prevent decoder from attending to these locations. The SBD prevents the decoder from generating similar sentences more than once via backtracking at test.
The proposed model outperforms the baselines 
%{\bf consistently} 
in terms of ROUGE score, repeatedness, correlation, and readability. 
The results show that this approach 
generates high-quality summaries with minimal repetition,
and makes the reading experience better.
\end{abstract}

\begin{keyword}
Abstractive summarization;
Repetition reduction;
Convolutional sequence-to-sequence model; Attention mechanism 
\end{keyword}

\end{frontmatter}
\linenumbers
\input{intro}
\input{related}
\input{approach}
\input{eval}
\input{conclude}

\section*{References}

\bibliography{mybibfile}
\end{document}
