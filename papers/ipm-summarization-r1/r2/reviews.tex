============================================================================ 
ACL 2019 Reviews for Submission #1161
============================================================================ 

Title: Reducing Repetition in Convolutional Abstractive Summarization
Authors: Yizhu Liu, Xinyue Chen, Xusheng Luo and Kenny Zhu

============================================================================
                            META-REVIEW
============================================================================ 

Comments: The paper introduces attention filtering and backtracking decoder in Convolutional Neural Networks-based sequence-to-sequence architecture to reduce repetition in abstractive summaries of documents. During the discussion, we all find the model to be interesting. However, most of us were concerned about the lack of human evaluation and the details of the re-implementation of See 2017 and Paulus 2017. From the paper, we could not answer why the performance of COV and ITDA differs from the original implementations (why the original numbers were not reported?). It is an interesting model to learn about, but the lack of details about reimplementation of SOTA models and human evaluation really undermine their work. Overall, we felt that authors need to work on these important issues before the paper should be accepted.
============================================================================
                            REVIEWER #1
============================================================================

What is this paper about, what contributions does it make, what are the main strengths and weaknesses?
---------------------------------------------------------------------------
The paper addresses the problem of repetitions in outputs by neural text generation systems. In particular, this issue is tackled for the abstractive summarisation task with a Convolutional Neural Networks (CNNs) -based sequence-to-sequence architecture. The work proposes two mechanisms to deal with repetitions: attention filtering and backtracking decoder. 


strengths

- The paper shows that a backtracking decoder with Seq-CNNs reduces repetition and still has better time performance than RNNs based models.
- The improvements by the backtracking decoder alone are close to those obtained by the attention coverage mechanism.

weakness

- The best proposed models improve ROUGE scores w.r.t. CNN-based re-implementations of existing mechanisms, e.g. COV, ITA, etc.; however, scores are not better than those reported on respective RNN versions. For instance, (See et al. 2017) and (Paulus et al, ) achieve R1=39.53/R2=17.28  and R1=41.16/R2=15.82 on the same data-set. The authors did not mention this. Is it the case that the proposed mechanisms (e.g. COV, ITA, etc.) work for RNNs and do not work for CNNs ?

- Proposed attention mechanism. The exposition about how the attention re-weighting works is not clear, for instance Eq. 9 (and hence Eq. 10) is not well explained. Also, the book-keeping of sections' boundaries and attention times/tokens per boundary seems rather involved. Would a hierarchical representation of sentences result in a simpler model? Also, the fact of restricting out parts of the input from subsequent attentions has been explored in the work of (Tan et al. 2017). This work is not mentioned in the paper, how the proposed attention filtering differs from this work. It is not clearly explained the motivation for a segmented attention mechanism and its key difference with 'temporal attention'.

- Scalability of the backtracking search (SBD) method in generation of longer (>4 sentences) texts. How big the number of explored candidate hypothesis will be up-to a repetition point.

- Evaluation. The 'correctness' concept seems to strict to mark a sentence as a binary correct/incorrect judgement. Still the proportion of completely correct sentences seems quite high for some systems. Also, the definition of correctness does not seem to encompass salience, e.g. an irrelevant sentence from the summary is marked as correct.
---------------------------------------------------------------------------


Reasons to accept
---------------------------------------------------------------------------
- Improving decoder search mechanism is interesting research line of work
---------------------------------------------------------------------------


Reasons to reject
---------------------------------------------------------------------------
- Weak comparison with existing works and novelty of the proposed approach.
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                  Overall Recommendation: 3

Missing References
---------------------------------------------------------------------------
Abstractive document summarization with a graph-based attentional neural mode. (Tan et al. 2017)
---------------------------------------------------------------------------



============================================================================
                            REVIEWER #2
============================================================================

What is this paper about, what contributions does it make, what are the main strengths and weaknesses?
---------------------------------------------------------------------------
This paper describes an approach to better abstractive summaries through minimizing repetitions in generated summaries.
The approach includes 1) the addition of an attention filter to a typical seq2seq summarizer, and 2) the use of backtracking to improve on beam search decoding.
The approach is benchmarked against several state-of-the-art abstractive summarization systems and performs better in ROUGE, while reducing the occurrences of text repetition.

Strengths:
1. The proposed idea is simple and is an effective enhancement to abstractive summarization systems.
2. Experimental results are convincing, and show good performance against the state-of-the-art.
3. The paper is well-written, and provides insights into the workings of the attention filer, as well as runtimes of the proposed system.

Weaknesses:
1. The paper did not delve into the interplay of the attention filter and the backtracking decoder.
2. The lack of assessment on other aspects of the generated summaries (e.g. readability)
---------------------------------------------------------------------------


Reasons to accept
---------------------------------------------------------------------------
The proposal is a simple, yet effective extension of a typical seq2seq summarizer. This nudges the state-of-the-art forward, and the community will benefit from this evolution.
This is also a well-written paper which is a pleasure to read.
---------------------------------------------------------------------------


Reasons to reject
---------------------------------------------------------------------------
While it makes strides in ROUGE scores, it is not clear how readable the generated summaries are.
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                  Overall Recommendation: 4

Questions and Suggestions for the Author(s)
---------------------------------------------------------------------------
To think about:

1. Between repeatedness and redundancy, it is not clear what you are trying to show with these two different, but subtly similar metrics

2. Looking at Tables 4 + 5, it is clear that ATTF reduces repetition less effectively than SBD. Linking this to the premise of your paper (i.e. the reduction of repetition helps improve summarization), it will be useful to shed some insights of hypothesis on why ATTF (alone) is more effective than SBD (in Table 4). 

3. It is not clear to me how correctness is scored even though it was described in L523 to L542. It feel that there is a high level of subjectivity in ensuring "logical consistency with the document", but the high Kappa indicates that your annotators have no problems doing this. It would be helpful if you could share your annotation guidelines as part of an Appendix to the paper.


Editing:

1. Coloring used for diagrams - Think about how you can make them more discernible when printed in B-W.

2. Missing legend item for purple boxes in Figure 2

3. L545  - Accuracy seems to be a proxy for ROUGE in your paper, but yet there are statements that indicate otherwise (L205). Not sure if this is a typo, but please make clear how you are evaluating accuracy, if not with ROUGE.
---------------------------------------------------------------------------



============================================================================
                            REVIEWER #3
============================================================================

What is this paper about, what contributions does it make, what are the main strengths and weaknesses?
---------------------------------------------------------------------------
Summary and contributions
An approach  reduce the repetition in output summaries by dynamically redistributing attention over the input sequence as the output sentences are generated in CNN sequence model. The contributions are: (1) identify the repetition problem in abstractive summaries generated by CNN seq2seq models and propose new metrics (2) propose an effective approach that redistributes attention scores during training time, and guards against repetition by sentence backtracking at runtime to reduce repetition in CNN seq2seq model.

Strength:
The paper is well written and the analysis of the model architecture to better understand the impact of modeling decisions

The problem that the authors want to solve is valuable.The proposed method also works as well. From the metric of ROUGE, it outperforms other models.

Weakness:
For abstractive summarization, the human evaluation is always recommended.
---------------------------------------------------------------------------


Reasons to accept
---------------------------------------------------------------------------
It is an effective method to handle the redundancy problem in CNN s2s model and the results seem promising.
---------------------------------------------------------------------------


Reasons to reject
---------------------------------------------------------------------------
Not clear how good the summary is since we know better ROUGE does not mean better summary.
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                  Overall Recommendation: 3

Questions and Suggestions for the Author(s)
---------------------------------------------------------------------------
One question is how the author defines the repetition? What's the difference between redundancy? 
Actually we allow repetition in summary in some terms. What we really do not want is to express the similar meaning in the summary.
---------------------------------------------------------------------------
