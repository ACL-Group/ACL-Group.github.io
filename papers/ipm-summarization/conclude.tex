\section{Conclusion}
\label{sec:conclude}
%We presented a distributed attention mechanism to modify existing CNN seq2seq model 
%and were able to train a model that produces 
%summaries without repetition that are fluent and coherent. 
We analyze two possible reasons behind the repetition problem in abstractive
summarization: (1) attending to the same location in source,
and (2) attending to similar but different sentences in source. 
In response, we present a section-aware attention mechanism (ATTF)
as well as a sentence-level backtracking decoder (SBD). Our model is able 
to produce fluent and coherent summaries with minimal repetitions.
Besides, the summaries generated by our model are more accurate and 
readable. 
%We find that the basic CNN seq2seq model 
%still has some problems, such as generating repeated word sequence. 
%We also argue that ROUGE is not a perfect evaluation metric for the abstractive 
%summarization. Our future work will focus on these two aspects.

