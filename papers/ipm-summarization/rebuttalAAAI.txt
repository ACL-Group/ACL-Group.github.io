General:
Thank you all for the precious comments and suggestions.

G1 'RNN':
We explain the reason for choosing vanilla CNN seq2seq model 
as our basic model in Baselines section (page 4).
Gehring et al.(2017) show that the vanilla CNN seq2seq model 
is fast and enjoys the best accuracy among 
the other vanilla seq2seq models 
such as RNN seq2seq model and LSTM seq2seq model(Gehring et al., 2017).
Compared with vanilla CNN seq2seq model (R-1 34.33/R-2 14.25/R-L 35.68), 
the ROUGE scores of the vanilla LSTM seq2seq model (R-1 31.33/R-2 11.81/R-L 21.83)
shown in (See et al., 2017) are actually worse. 

In this paper, we argue that the quality of a summary cannot be evaluated by
ROUGE score alone.
For example, 
    source: justin timberlake and jessica biel, welcome to parenthood. 
	        the celebrity couple announced the arrival of their son,
			silas randall timberlake, ...
			the couple announced the pregnancy in january, ...
			it is the first baby for both .
    reference summary: timberlake and jessica biel welcome son silas 
                   	   randall timberlake. the couple announced the 
					   pregnancy in january .
    CNN+COV (See 2017): timberlake and jessica biel announced 
	                    the pregnancy in january. 
                        the couple announced the pregnancy in january. 
    CNN+ATTF_SBD (our): the couple announced the arrival of their son, 
	                silas randall timberlake. the couple announced the 
					pregnancy in january. it is the first baby for both.

The R-2 scores of above example: CNN+COV 0.60, CNN+ATTF_SBD 0.52.
CNN+ATTF_SBD obviously produces a better, logically more consistent summary despite 
a lower ROUGE score.  Readability (Human-evaluation) and Repeatedness score, 
in our opinion, are important complementary metrics to ROUGE score.  
Our evaluation mainly compares the effectiveness of different repetition reduction techniques,
such as ATTF, SBD, COV, etc, in terms of all three metrics above. If these methods were
applied on top of more advanced models such as PtGen (effectively it is CNN with mechanism that cannot reduce repetition), 
the room for improvement on the ROUGE score will be
very limited because the advanced models already achieves very high ROUGE scores for any abstractive
summarization techniques (ROUGE is not very good at evaluating abstractive summarization anyway). 
Consequently, the differences in ROUGE scores by these repetition reduction techniques will 
indistinguishable. Hence, in this work, we choose to implement these methods (Table 3) on top of vanilla CNN
models.

Review 1:
Q1:
Refer to G1.

Q2 'no evaluation':
The Readability score (page 5) in Table 5 is human-evaluation, 
which determines whether the sentences in summaries
are free of grammatical and factual errors, 
and are logically consistent with source document.
As shown in Tabe 5, our model can generate 
more logically consistent and factual summaries.

Q3 'segment':
The current segment, which is a sentence or clause delimited 
by punctuation, represents syntactic and semantic information of sentences. 
It is very simple but effective. Since punctuations 
play an important role in written language to organize 
the grammatical structures and to clarify the meaning of sentences 
(Jones, 1996;Briscoe et al., 1997;Kim et al., 2019;Li et al., 2019).

Q4 'citation':
We will proofread the paper in the final version.

Review 2:
Q1:
Refer to G1.

Q2 'using LSTM':
Since most models using LSTM require some sort of attention,
and our approach aims at fixing incorrect attention distribution, 
we can reasonably deduce that these models will benefit from our techniques as well.

Review 3:
Q1 'ATTF section': 
we will explain it as follows:
    source: 1)justin timberlake and jessica biel, 
	        2)welcome to parenthood. 
	        3)the celebrity couple announced the arrival of their son,
            4)...
			5)the couple announced the pregnancy in january,
            6)...
			7)it is the first baby for both .
    Basic CNN: 1)the couple announced the the arrival of their son. 
               2)the couple announced the pregnancy in january. 
               3)the couple announced the pregnancy in january. 
    ATTF (our): 1)the couple announced the arrival of their son. 
	            2)the couple announced the pregnancy in january. 
				3)it is the first baby for both.

The segments in example are separated by punctuation.
For basic CNN model, the 2nd and 3rd sentence repeatedly attend to 
the 5th segment in source.
After applying ATTF model, 
the attention score of 3rd and 5th segment in source are penalized 
during generating words in 3rd sentence of ATTF.
The last sentence of the summary generated by ATTF attend to 7th segment in source.

Q2 'significance':
The differences between ROUGE scores in Table 4 are statistically significant.
We have done the significance test on ROUGE score in Table 4
but it was excluded in the paper due to space constraint. 
Because the ROUGE scores of generated summaries do not follow
normal distribution, we take Kruskal-Wallis test (Albert, 2017) as our 
significance test. We use Kruskal-Wallis test(Albert, 2017) as significance test. 
All p-values are less than 0.05: 3.41e-32(R-1), 2.12e-45(R-2), 0.01(R-L). 
We will add these into the final version.
