General:
Thank you all for the precious comments and suggestions.

G1 'RNN':
We explain the reason for choosing CNN-based model 
as our basic model in Baseline section (page 4).
Gehring et al.(2017) show that the vanilla CNN seq2seq model 
is fast and enjoys the best accuracy among 
the other vanilla seq2seq models 
such as RNN seq2seq model and LSTM seq2seq model(Gehring et al., 2017).
Compared with vanilla CNN seq2seq model (R-1 34.33/R-2 14.25/R-L 35.68), 
the ROUGE scores of the vanilla LSTM seq2seq model (R-1 31.33/R-2 11.81/R-L 21.83)
shown in (See et al., 2017) are actually worse. 

PtGen (See et al., 2017) is an extension of the vanilla LSTM seq2seq model.
The improvement in ROUGE mainly comes from the copy mechanism,
which is orthogonal to repetition reduction. 
@@@@@@ copy orthogonal to repeat reduction? 
This kind of structure s orthogonal to our attention filters and we expect them just
as well on our model if applied (Gehring et al., 2017; Fan et al., 2017).
However, we did not implement the repetition reduction methods on top of seq2seq models 
with other structural tricks, such as copy mechanism, 
because these tricks employed in the seq2seq models
may interact with the repetition reduction methods and complicate the analysis.
For example, in (See et al., 2017), the R-2 score of 
vanilla LSTM seq2seq model with copy mechanism is 15.66.
After adding repetition reduction method (called COV in our paper), 
it becomes 17.28 and rises 1.62, which is increased by 10%.
As shown in Table 4, the R-2 score of our proposed model is 15.82.
Compared with vanilla CNN seq2seq model, it rises 1.57, which is increased by 11%.
This shows that the fluctation of experimental results on vanilla seq2seq model 
will be more obvious and can help analyze.
Thus, we take vanilla seq2seq model as basic model 
to evaluate the effectiveness of repetition reduction techniques.

We aim at reducing repetition in abstractive summarization to improve the quality
of generated summaries, which is not necessarily reflected in the ROUGE. 
Due to variable nature of abstractive summarization, ROUGE is a common summarization 
evaluation metric but not everything. Human-evaluation and Repeatedness score, 
in our opinion, are important complementary metrics to ROUGE score. 
For example, 
    source: justin timberlake and jessica biel, welcome to parenthood. 
	        the celebrity couple announced the arrival of their son,
			silas randall timberlake, ...
			the couple announced the pregnancy in january, ...
			it is the first baby for both .
    reference summary: timberlake and jessica biel welcome son silas 
                   	   randall timberlake. the couple announced the 
					   pregnancy in january .
    (See et al., 2017): timberlake and jessica biel announced 
	                    the pregnancy in january. 
                        the couple announced the pregnancy in january. 
    ATTF_SBD (our): the couple announced the arrival of their son, 
	                silas randall timberlake. the couple announced the 
					pregnancy in january. it is the first baby for both.

The R-2 score of above example: COV 0.60, ATTF_SBD 0.52.
The summary generated by our proposed model are non-repetitive and more 
logically consistent and factual. 
This shows that ROUGE score is not optimal summarization evaluation methods.
Repeatedness score and Readability score is also important.
As shown in Table 5, our proposed models are 
more readable and closer to the natural repetition level.

Review 1:
Q1:
Refer to G1.

Q2 'no evaluation':
The Readability score (page 5) in Table 5 is human-evaluation, 
which determines whether the sentences in summaries
are free of grammatical and factual errors, 
and are logically consistent with source document.
As shown in Tabe 5, our model can generate 
more logically consistent and factual summaries.

Q3 'segment':
The current segment, which is a sentence or clause delimited 
by punctuation, represents syntactic and semantic information of sentences. 
It is very simple but effective. Since punctuations 
play an important role in written language to organize 
the grammatical structures and to clarify the meaning of sentences 
(Jones, 1996;Briscoe et al., 1997;Kim et al., 2019;Li et al., 2019).

Q4 'citation':
We will proofread the paper in the final version.

Review 2:
Q1:
Refer to G1.

Q2 'using LSTM':
Since most models using LSTM require some sort of attention,
and our approach aims at fixing incorrect attention distribution, 
we can reasonably deduce that these models will benefit from our techniques as well.

Review 3:
Q1 'ATTF section': 
we will explain it as follows:
    source: 1)justin timberlake and jessica biel, 
	        2)welcome to parenthood. 
	        3)the celebrity couple announced the arrival of their son,
            4)...
			5)the couple announced the pregnancy in january,
            6)...
			7)it is the first baby for both .
    Basic CNN: 1)the couple announced the the arrival of their son. 
               2)the couple announced the pregnancy in january. 
               3)the couple announced the pregnancy in january. 
    ATTF (our): 1)the couple announced the arrival of their son. 
	            2)the couple announced the pregnancy in january. 
				3)it is the first baby for both.
The text in example is separeted by punctuation.
For basic CNN model, the 2nd and 3rd segment repetitively attend to 
the first segment of part 5) in source.
After applying ATTF model, 
the attention score of 3rd and 5th sentence in source are penalized 
during generating words in 3rd sentence of ATTF.
The last sentence of the summary generated by ATTF attend to 7th sentence in source.

Q2 'significance':
The differences between ROUGE scores in Table 4 are statistically significant.
We have done the significance test on ROUGE score in Table 4
but it was excluded in the paper due to space constraint. 
Because the ROUGE scores of generated summaries do not follow
normal distribution, we take Kruskal-Wallis test (Albert, 2017) as our 
significance test. We use Kruskal-Wallis test(Albert, 2017) as significance test. 
All p-values are less than 0.05: 3.41e-32(R-1), 2.12e-45(R-2), 0.01(R-L). 
We will add these into the final version.
