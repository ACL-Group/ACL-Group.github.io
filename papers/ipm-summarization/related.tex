\section{Related Work}
\label{sec:related}

%Summarization is the task of condensing a piece of document into a shorter paragraph or a single sentence, while retaining correctness, fluency, consistency as well as the core idea of the original document. 

%Generally, this task is tackled in two different ways: extractive and abstractive. Extractive approach \cite{Dorr2003HedgeTA,DBLP:journals/corr/NallapatiZZ16} takes sentences from the source text to compose a summary, while abstractive one \cite{RushCW15,SeeLM17,PaulusXS17} \textit{generates} sentences word by word after comprehending the source text as a whole. For abstractive method, the algorithm has a vocabulary from which it can freely choose words and phrases to compose sentences, while the extractive one is more similar to catching the key sentences of the source text. Extractive summarization can more easily produce acceptable summaries, as copying sentences from the source text guarantees correctness and consistency. However, extractive methods lacks creativity. More often than not, humans summarize documents in an abstractive manner, where they comprehend the whole document before selecting elements from their vocabulary to compose a short text that encapsulates the main idea and even the underlying intent from the source sentences. This is where a sense of expressiveness and elegance can be found.

%Many \cite{RushCW15,SeeLM17} choose to build an abstractive model using sequence-to-sequence model using recurrent neural networks and attention mechanism. In pursuit of speed and  parallelism, \cite{gehring2017convs2s} proposed a convolutional seqence-to-sequence model with Gated Linear Units \cite{DauphinFAG17}, attention mechanism to tackle a series of generation tasks. It achieves state-of-the-art accuracy in abstractive single-sentence summarization and is much faster than recurrent approaches.

%Repetition is a persistent problem in the task of summarization. Models are found to in some cases generate similar or the same words and phrases repeatedly, which causes grammatical incorrectness and redundancy. This reflects a insufficiency in the decoder's awareness of previous generations. To address this issue,  \cite{SeeLM17} use coverage to keep track of what has been summarized, which discourages repetition in an indirect manner, \cite{PaulusXS17} propose intra-decoder attention to avoid attending to the same parts in the source text by dynamically revising attention scores while decoding. \cite{PaulusXS17} also avoid repetition in test time by directly banning the generation of repeated trigrams in beam search. However, the weakness of this method is that the action of eliminating highly probable sentences with repeated trigrams disturbs the process of beam search, giving rise to grammatically incorrect sentences, whose probabilities are actually low in the model. 

Repetition problems is tackled in broadly two aspects in recent years. 

Chen and Bansal\shortcite{P18-1063} and Li\shortcite{D18-1205,D18-1441} construct hierarchical models to deal with summarization. Their approaches typically involve information selection or sentence selection before generating summaries. Among them, \cite{P18-1063} uses an extractor agent to select salient sentences or highlights, and then employs an abstractor network to compress these sentences. In their model, repetition is naturally avoided as long as the source document is non-repetitive. But it does not aim to solve repetition in sequence-to-sequence generation (source document to summary). \cite{D18-1205,D18-1441} generate sentence representations and then use them to produce sentences for summary. All of the above models are RNN-based. However, their methods cannot be transferred easily to CNN-based models because there is no natural CNN-based approach to generate a sentence from a sentence vector. Therefore, we do not consider the above models as our baselines.

See\shortcite{SeeLM17}, Paulus\shortcite{PaulusXS17} and Fan\shortcite{FanGA18} choose to adopt sequence-to-sequence approach, where source document and summary are treated as two long sequences, without the idea of separate sentences. \cite{SeeLM17} integrate coverage, which keeps track of what has been summarized, as a feature that helps redistribute the attention score in an indirect manner in order to discourage repetition. \cite{PaulusXS17} propose intra-temporal attention and intra-decoder attention that dynamically revises attention distribution while decoding. \cite{PaulusXS17} also avoid repetition in test time by directly banning the generation of repeated trigrams in beam search. These two models are RNN-based. \cite{FanGA18} borrows the idea from \cite{PaulusXS17} and build a RNN-based model. We transfer methods from these models to form our baselines, as introduced in \secref{sec:basic}.

Our model also adopts a sequence-to-sequence approach and deals with the attention in encoders and decoders. Different from the previous methods, our \textit{attention filter mechanism} does not deal with the attention history collectively. Instead, our attention filter is section-aware. In \cite{SeeLM17,gehring2017convs2s}, the distribution curve of accumulated attention scores for each token in the source document grows flat with critical information washed away during decoding. Our method, on the other hand, amplifies most previously attended sections so that information is retained (\secref{sec:attf}). Also, given our observation that repetitive sentences in the source is another cause for repetition in summary, which cannot be directly resolved by manipulating attention, we introduce our \textit{sentence-level backtracking decoder}. Unlike \cite{PaulusXS17}, we do not ban repetitive \textit{trigrams} in test time. Instead, our decoder regenerates a sentence that is similar to any previously generated ones.   With the two methods, our model is capable of generating summaries with natural level of repetition while retaining fluency and consistency.
%plz let me know if revised or reused



