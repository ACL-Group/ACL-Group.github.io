\section{Introduction}
\label{sec:intro}

Set-valued data, especially transactional data sources are valuable in many data mining and data analysis tasks~\cite{agrawal1994fast,cui2002probabilistic,adar2007we}. 
For example, retail companies may want to know what items are top sellers, or whether there is an association between the purchase of the items.
We may summarize the applications on set-valued data into two main categories: 
one is {\em statistical analysis}; the other is {\em mining of association rules} between items.
In many cases, analysis tasks are {\em outsourced} to external companies, or simply {\em published} to the general masses.

Publishing set-valued data, can pose significant privacy risks. Set-valued transactions can be divided into two classifications according to whether including privacy information: {\em non-sensitive} and {\em sensitive}. Privacy is in general associated with the sensitive items. Table \ref{tab:orig-sample} shows an example of retail transactions in which each record (row) represents a set of items purchased by an individual. {\em condom} is sensitive. An individual's privacy is breached if he can be {\em re-identified}, or associated with a record in the data which contains  sensitive items.
Past research has shown that such breach is possible through {\em linking attacks} \cite{FungWCY10:Survey,samarati1998}, and sensitive information may be inferred from
combinations of non-sensitive items. This is out the scope of this paper.

\begin{table*}[th]
\centering
\caption{A Retail Dataset and Anonymization Results\label{tab:sample}}{
\subtable[Original Dataset]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, beer, {\em condom} \\ \hline
2 & coffee, fruits  \\ \hline
3 & beer, {\em condom}  \\ \hline
4& coffee, fruits  \\ \hline
5& flour, {\em condom}\\ \hline
6& bread, coffee  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:orig-sample}
}
\subtable[Global Suppression]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, beer, \sout{\em condom} \\ \hline
2 & coffee, fruits  \\ \hline
3 & beer, \sout{\em condom}  \\ \hline
4& coffee, fruits  \\ \hline
5& flour, \sout{\em condom}\\ \hline
6& bread, coffee  \\ \hline
7& fruits, \sout{\em condom}  \\ \hline
\end{tabular}
\label{tab:sample2}
}
\subtable[TDControl]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, \sout{{\em beer}}, \sout{{\em condom}} \\ \hline
2 & coffee, fruits  \\ \hline
3 & \sout{{\em beer}}, \sout{{\em condom}}  \\ \hline
4& coffee, fruits  \\ \hline
5& \sout{{\em flour}}, \sout{{\em condom}} \\ \hline
6& bread, coffee  \\ \hline
7& fruits,  condom  \\ \hline
\end{tabular}
\label{tab:sample5}
}

\subtable[Approach 1: Distribution]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, beer, \sout{{\em condom}} \\ \hline
2 & coffee, fruits  \\ \hline
3 & beer, {\em condom}  \\ \hline
4&coffee, fruits  \\ \hline
5& \sout{flour}, {\em condom} \\ \hline
6& bread, coffee  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sample3}
}
\subtable[Approach 2: Mine]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, beer, \sout{{\em condom}} \\ \hline
2 & coffee, fruits  \\ \hline
3 & beer, {\em condom}  \\ \hline
4&coffee, fruits  \\ \hline
5& flour, \sout{{\em condom}}\\ \hline
6& bread, coffee  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sample4}
}
}
\end{table*}

The privacy model is called $\rho$-uncertainty, where
no sensitive association rules can be inferred with a confidence higher than
$\rho$ \cite{Cao:2010:rho}. The parameter $\rho$ is an input to the model,
which determines the amount of privacy protection the user requires. 
A lower $\rho$ means stronger protection while a higher $\rho$ means weaker
protection. 
%
One popular approach is called ``global suppression'' in which once an occurrence of an item $t$ is determined to be removed from one record, all occurrences of $t$ are removed from the whole dataset, and another approach is called 
``TDControl'' \cite{Cao:2010:rho} which disables sensitive rules with confidence 
larger than a given $\rho$. We instead opt to {\em partially} suppress the data set so only {\em some} occurrences of item $t$ are deleted. Table \ref{tab:sample} shows the example dataset
and three anonymized datasets produced by popular and our approaches.
The orginal dataset is not safe because sensitive rules such as
$\text{beer} \rightarrow condom$ and $\text{flour} \rightarrow condom$
can be inferred with confidence 100\%, which is greater than our threshold $\rho=50\%$.
Table \ref{tab:sample2} is the anonymized dataset where all the occurrences of the sensitive item {\em condom} are deleted due to global suppression. Table \ref{tab:sample5} is the result of TDControl approach, $\text{beer} \rightarrow condom$ and $\text{flour} \rightarrow condom$ have been deleted by $\rho$-suppression, when $\rho=70\%$.
Table \ref{tab:sample3} shows the result of our first
approach, which is optimized to preserve data distribution, so different
items ({\em condom} and flour) are deleted to make the dataset safe.
Table \ref{tab:sample4} is the result of our second approach, which is optimized
to preserve important data association, so only
two occurrences of the item {\em condom} are deleted for safety. Notice here that our method doesn't take consider duplicated items in a transaction,
because even if an item appears multiple times in a record, it cannot affect number of co-occurrences
between this item and other types of items, neither does it change 
the confidence of rules related to this item. 

%To the best of our knowledge, the partial suppression technique
%has not been studied in the context of set-valued data anonymization before.
We choose to solve the set-valued data anonymization problem by partial
suppression because global suppression tends to
delete more items than necessary,
and the removal of all occurrences of the same item not only changes the
data distribution significantly but also makes mining association rules
about the deleted items impossible.
%
The problem of anonymization by suppression is very
challenging \cite{atallah99:disclosure,Xu:2008:ATD}, exactly because, (i) the number of possible inferences from
a given dataset is exponential, and (ii) the size of the search space, i.e.
the number of ways to suppress the data is also exponential to the number of
data items. We therefore propose two heuristics in this paper to anonymize
input data, giving rise to the two kinds of output in \tabref{tab:sample}.

The main contributions of this paper are as follows.
\begin{enumerate}
%\item To the best of our knowledge, we are the first to propose an
%    effective \emph{partial} suppression framework for anonymizing
%    set-valued data (Section \ref{sec:prob} and
%    \ref{sec:algo}).
\item We propose an effective \emph{partial} suppression framework for anonymizing set-valued data (Section \nameref{sec:prob} and \nameref{sec:algo}).
\item We adopt a ``pay-as-you-go'' approach based on divide-and-conquer,
    which can be adapted to achieve both space-time and quality-time
    trade-offs (Section \nameref{sec:algo} and \nameref{sec:eval}). Our two heuristics
    can be adapted to either preserving data distribution or retaining useful
    association in the data (Section \nameref{sec:algo}).
\item We show by the experiments, in which our algorithm outperforms
    the peers in preserving the data distribution or in retaining mineable useful association rules while reducing the item deletions by large margins (Section \nameref{sec:eval}).
\end{enumerate}
