\section{Related Work}
\label{sec:related}
The field of intercultural research has received significant cross-fertilization from many
academic disciplines suggested by~\cite{triandis1994culture}. For instance,~\cite{weber1998cross}
demonstrates that apparent differences in risk preference exist in different cultures.

EL(Entity Linking) is a hot topic in these years.
The improvement of EL technique enhances a big range of issues such as name transliteration
\cite{lin2016leveraging}.
Solutions for EL problem
usually have to trade off between precision and recall. In addition to traditional local and global
models, recently~\cite{gruetze2016coheel} proposes an efficient
linking method that uses a random walk strategy to combine a precision-oriented and a recall-oriented
classifier. Probabilistic approach making use of an effective graphical model\cite{ganea2016probabilistic}
is also a good solution for EL. Comparing with these state-of-art models, our method is much simpler but
achieve a reasonable performance in our task.

Cognitive linguistic studies~\cite{kovecses2006language}
have shown that equivalent terms in different languages may have very different meanings.
This phenomenon proves to hold between English and Chinese
\cite{chen2007chinese,tavassoli1999temporal,krifka199511}.
In computational linguistics, discovery of relationships across languages is an emerging topic.
There are generally two research directions: graph-based knowledge network or
distribution-based vector representation.

BabelNet~\cite{Navigli:2012dn} and Yago3~\cite{mahdisoltani2014yago3} are representatives
of graph-based knowledge network, with an ambition
to construct a unified multilingual knowledge base. They
integrate resources such as WordNet and Wikipedia to achieve
this goal. The knowledge base thus built can be used to
calculate relatedness across languages. However, both of them
rely on existing structured resources to create the networks,
which limit their scale and extendability.

For distributional models, the predominant approach to
represent the semantics of words is word embedding.
The embeddings are usually trained
using co-occurrence matrix,
matrix factorization~\cite{lebret2013word,levy2014neural,li2015word} or
neural network~\cite{Mikolov2013distributed}.
Traditionally, these vectors are trained on monolingual corpus and
the vector spaces of different languages are not directly comparable
with each other. To solve this, some researchers try to train
unified representations from multilingual
corpus~\cite{Klementiev:2012uk,hermann2014multilingual,Vulic:2015to}
or construct a mapping between the vector spaces of
different languages~\cite{Mikolov:2013tp}.
These vectors are then evaluated
in tasks such as bilingual lexicon induction or cross-lingual
word sense disambiguation, and have shown to achieve
state-of-art performance.

Our task is similar to bilingual lexicon induction,
though we want to detect differences for named entities instead of
finding similar words. Tomas Mikolov~\cite{Mikolov:2013tp}
shows the potential to detect errors
in bilingual dictionary with Word2Vec and linear transformation among
different vector spaces. In this paper, we implement their idea
(linear transformation) and compare with ideas of ours.
