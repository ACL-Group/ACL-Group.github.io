\section{Extraction information from image URLs}
Usually, the URL of image may contain some useful information. This kind of information is always strongly related with the picture, so it's reliable.

Extracting information from URL seems easy: the only thing to do is split the URL into words, and check for available phrases from the extracted words. One problem is that how to check for useful phrases. The other problem is, there are some randomly generated strings, while mostly of them are hexadecimal strings, e.g. ``66ccff'' or ``7E145F45''.

To extract useful phrases, we used wikification. It will parse the whole sentence into several wiki concepts with their correct sense, and the Wikipedia concepts is easy to be combined with other parts in our algorithm.

Ant to deal with random strings, we labeled a dataset with about 3000 samples, and used liblinear to learn a model to tell the random strings from meaningful strings. Each item in the dataset is a ``word'' which only contains letters and digits. For example, in URL ``http://www.example.com/0EX-CoJ9/aXwjfwh0I/Fazer_2007.jpg'', we have these words: ``0EX'', ``CoJ9'', ``aXwjfwh0I'', ``Fazer'' and ``2007''. Notice that the domain and the file extension is ignored. For each word, we will parse all 3-gram in it, each as ``CoJ'' and ``oJ9'' in ``CoJ9'', and ``Faz'', ``aze'' and ``zer'' in ``Fazer''. Each 3-gram corresponds to a dimension in the vector of this word. In the example below, ``0EX'',  ``CoJ9'' and ``aXwjfwh0I'' are meaningless words, while ``Fazer'' and ``2007'' are meaningful. Then we will learn a model to classify these two kinds of words, and abandon meaningless words directly.
