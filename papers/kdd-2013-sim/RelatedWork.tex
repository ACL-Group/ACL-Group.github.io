\section{Related Work}
\label{sec:related}

Contrary to the semantic relatedness which represents the more general relationships such as part-whole and the co-occurrence, semantic
similarity measures the degree of taxonomic likeness between concepts and considers relations such as hyperonymy and synonymy. In this section,
we only discuss previous work on semantic similarity, while most of them can be adapted or generalized to deal with semantic relatedness. To
compute the semantic similarity between terms, existing efforts mainly
follow two approaches: The first approach calculates the semantic
similarity based on some distance in a preexisting thesauri, taxonomy or encyclopedia, such as WordNet. The second approach computes similarity
by the terms' context in large text corpora (such as the search snippets and web documents) and such similarities are derived from
distributional properties of words or n-grams in the corpora.
%{\color{red} Clearly, all of term co-occurrence based methods also
%belong to the corpus-based approach.}

\subsection{Knowledge-based Approach}

Most methods in this direction use a taxonomy such as WordNet, which is
a tree hierarchy, as the knowledge base to compute
the similarity between terms.
%One is the concept likeness based approach regarding the relationships in
%WordNet and the other is the the graph-based learning approach built on
%the isA-relationship structure of WordNet.
The most straightforward way to calculating similarity between two
terms on the WordNet is to find the length of the shortest path
connecting the two terms in the taxonomy graph\cite{Rada:1989}.
This path-length based approach is very simple, but has a low accuracy
because: i) it relies on the notion that all links in the taxonomy represent
a uniform distance; ii) it ignores the amount of information hidden in the concept nodes.

More advanced approaches \cite{Resnik:1995, Jiang:1997, Lin:1998, Seco:2004, Snchez:2011} compute the similarity between $t_1$ and $t_2$ by the
information content of these terms with respect to the taxonomy structure. The pioneer work by Resnik \cite{Resnik:1995} suggests that the
similarity measure is the information content of the least common ancestor node of the two terms in the taxonomy tree.
%between two concepts $c_{1}$ and $c_{2}$ in the taxonomy as the maximum of the information content of all concepts \emph{C} that
%subsume both $c_{1}$ and $c_{2}$, and then it takes the maximum of the similarity between any concepts that the terms belong to as the final
%semantic similarity.
To compute the information content of a term, it requires a large text corpus
to obtain the occurrences of the term.
A limitation of this method is that the similarities between all children of
a concept are identical, regardless of their individual information content.
The most recent information content based approach \cite{Snchez:2011} calculates
the information content of term $t$ by the ratio of
the number of hypernyms of $t$ divided by the number of all descendants of $t$ in WordNet. 
%Pedersen etc. adapted the above measures to the SNOMED-$CT^(R)$ ontology of medical concepts. Meanwhile, they also derived a context vector measure based on medical corpora that can be used as a measure of semantic relatedness.
Some of the above measures have been adapted to the biomedical field by incorporating domain information extracted from clinical data or from medical ontologies (such as MeSH or SNOMED-$CT^{(R)}$)\cite{Pedersen:2007, Batet:2011}.
 
%This method yields a Pearson correlation coefficient of up to
%0.87 on the M\&C benchmark database.

%The first work applying information theory to semantic similarity
%computation was proposed by Resnik \cite{Resnik:1995}. It states concept
%similarity depends on the amount of shared information
%between two concepts. That is, given two concepts $c_1$ and $c_2$, the algorithm first finds the most specific common ancestor subsuming both
%concepts (denoted as the Least Common Subsumer: LCS) by exploiting a background ontology. Secondly, it uses the information theoretic evaluation
%function to compute the IC (Information Context) value of LCS, and it computes the $IC$ of a concept by $-log(p(c))$, where $p(c)$ indicates the
%probability of encountering \emph{c} or any of its taxonomical hyponyms in the given corpus. The popular corpus include the SemCor texts distributed
%with WordNet 3.0, Brown corpus and British National Corpus. In this algorithm, there is a problem that any pair of concepts with the same LCS will
%result in exactly the same semantic similarity. Thus, to tackle this problem, researchers have extended the Resnik's work by considering the IC of
%each of the evaluated concepts. For example, Jiang and Conrath proposed to quantify the length of the taxonomical links as the difference between
%the IC of a concept and its subsumer\cite{Jiang:1997}. Instead of subtracting the IC of their LCS from the sum of the
%IC of each concept as the similarity between two concepts, Lin adopted the ratio between the amount of the IC of their LCS and the sum of the IC of
%each concept to measure their similarity.
%
%In the analysis of corpora-based IC algorithms mentioned above, researchers found that to accurately compute the information therapy, the contents of corpora should be adequate regarding the ontology scope and big enough. However, large and general purpose corpora such as Brown corpus and British National Corpus may be only suitable for WordNet. Therefore, some authors proposed computing IC from an ontology in an
%intrinsic manner\cite{Seco:2004} without the corpus. Comparing corpora-based IC computation models, the intrinsic IC-based models take a function of the
%number of hyponyms in a taxonomy as the appearance probability of a concept (and the amount of information it provides). For example, Seco et al. first proposed to compute the base IC calculations on the number
%of concept hyponyms \cite{Seco:2004}. S$\acute{a}$nchez et. al. proposed an advanced intrinsic IC computation algorithm \cite{Snchez:2011}, which combines the information of the number of leaves in all hyponyms of the concept $c$ and the number of taxonomical subsumers that the concept $c$ belongs to.

%Considering the graph-based learning approach built on the ontology structure of
%WordNet, the representative works are below.
Other researchers attempted to apply graph learning algorithms
on term similarity computation.
Given two terms $t_1$ and $t_2$, Alvarez and Lim \cite{Alvarez:2007}
build a rooted weighted graph called $Gsim$,
using the terms hypernyms, other relations, and descriptive
glosses from WordNet, and then calculate the similarity score
by selecting the minimal distance between any two hypernyms $c_1$ and
$c_2$ of $t_1$ and $t_2$ respectively, by random walk.
%The authors claimed that this method achieves
%a pearson correlation coefficient of 0.913
%on the M\&C benchmark data set\cite{Miller:1998}.
Agirre et. al. subsequently proposed a WordNet-based personalized PageRank
algorithm \cite{Soroa:2009, Agirre:2010}. It first computes
the personalized PageRank of each word and aggregates into
a probability distribution for each synset. Similarity is then defined by
the cosine between two distributions.
%any two discrete probability distributions are by
%encoding them as vectors and computing the cosine similarity between the vectors.
%In addition, authors trained a SVM to combine three basic
%methods including bag-of-words, context-window-based method and the WN30g-based method \cite{Agirre:2009}. In this case, the highest pearson
%correlation coefficient is up to 0.93 on the M\&C benchmark data set.

The above knowledge based approaches depend heavily
on the completeness of the underlying taxonomy and
the external corpora.
%To provide reliable results at a conceptual level, it requires the
%knowledge source must be as complete as possible, namely,
%it should include most of the specializations of each concept covered in the corpus.
However, the popular taxonomy like WordNet does not have the adequate coverage
as it cannot keep up with the development of new terms and phrases everyday.
%especially for the Web data. Meanwhile, to avoid data sparseness in
%the information content computation, it requires contents of corpora should
%be adequate and big enough regarding the ontology scope.
%Similarly, popular corpora such as the SemCor\cite{semcor} texts
%distributed with WordNet 3.0, Brown corpus\cite{Brown} or
%British National Corpus\cite{BNC} are mostly designed for WordNet,
%and have limited coverage on terms beyond that.

The framework proposed in this paper is also knowledge based, but is
more scalable and effective,
because i) the knowledge we use was acquired from the entire Web; and
ii) the clustering algorithm detects the senses of the input terms
and the max-max similarity function effectively picks the senses that are most
suitable given the pair of terms. The above methods cannot be easily adapted
to use Probase because it is a general network, not a tree structure.

\subsection{Corpus-based Approach}

%Regarding the distributional context based similarity evaluation approaches, main works are below.
%Sahami et al. measured semantic similarity
%between two queries using search snippets\cite{Sahami:2006}. For each query, the proposed approach first collects snippets from a search engine
%and represents each snippet as a TF-IDF weighted term vector, and then it defines the semantic similarity between two queries by the inner product
%between the corresponding centroid vectors.
In this space, Chen et. al. proposed a double-checking model
using text snippets returned by a Web search engine
to compute semantic similarity between words \cite{Chen:2006}.
The proposed method uses the occurrences of terms $X$ and $Y$
in their search snippets to evaluate the semantic similarity.
Recently, Bollegala et.al. proposed a new measure using
page counts and snippets from Web search \cite{Bollegala:2011}.
%To compute the similarity between $X$ and $Y$,
%it first collects the snippets from a web search engine by
%querying ``\emph{X} AND \emph{Y}'', and then extracts lexical patterns that combine \emph{X} and \emph{Y} from snippets. Then it uses
%frequencies of 200 lexical patterns in snippets and four co-occurrence measures: Dice coefficient, overlap coefficient, Jaccard coefficient and
%pointwise mutual information to create the feature vectors. Finally, it trains a two-class support vector machine using automatically selected
%synonymous and non-synonymous word pairs from WordNet. This method reports a Pearson correlation coefficient of 0.837 with Miller-Charles
%ratings.
%However, the above methods heavily depends on the search engine's
%ranking algorithms.
The search engine based methods are more time-consuming because
i) snippets and search results must be obtained online;
ii) it requires parsing of the returned text by the patterns.

Radinsky et. al. proposed
a new model, Temporal Semantic Analysis (TSA)
\cite{Kira:2011}, which captures the temporal information of corpus.
TSA uses a more refined representation, where each concept is no
longer scalar, but is instead represented as time series over a
corpus of temporally-ordered documents. This method can improve the
pearson correlation coefficient, but it requires massive historical data.
%which leads to more time-consuming in the handling of texts.

Most corpus based methods are more suitable for the semantic
relatedness not for the semantic similarity because they
make heavy use of the co-occurrence context in the representation
of terms or in similarity functions.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
