\section{Experimental Results}
\label{sec:eval}
We conduct two experiments to evaluate the performance of our system 
in modeling the association process. The first experiment is 
conducted on the well-known benchmark data set Word-Similarity-353 
(a.k.a. WS-353) collection \cite{FinkelsteinGMRSWR02}, 
comprising 353 word pairs, 
for which humans have provided relatedness scores. The result shows that 
the system can measure the semantic relatedness between two terms effectively, 
outperforming the the similarity achieved by normalized Google distance
by significant margin. 
%JY{STSS}
The second experiment shows that the association path given by our system 
can be used to imporove the performance of traditional short text semantic similarity 
algorithm.
% The second experiment shows that the relatedness 
% measure given by our system can be used to improve the performance of 
% recommendation system, especially in solving the problem of ``cold start''. 
% The details of these two experiments are discussed next.

\subsection{Evaluation on WS-353}
The original WS-353 dataset is in English and has to be 
translated into Chinese for the experiment. 
To do that, we first use Google Translate to translate the terms 
automatically, then manually check and refine the translation result. A
number of English terms have multiple Chinese translations. We try to give
the best ones based on the semantics implied by the word pair. 
There are four pairs for which we can't find their proper Chinese counterparts,
and one pair that appears twice in the Word-Similarity-353 collection. 
After removing these five pairs from the data, we finally obtain our
test data with 348 pairs of terms. For convenience, we will still refer to the
data as WS-353. Our HMM model outputs a probability score for 
a given word pair and this probability score is typically small compared to
the human ratings in the original data due to multiplication. For example,
input pairs ``quilt'' and ``sheet'' yields a score of 0.01, which is 
already a fairly large score in our model. Due to the incompatibility of our
probability scores and the ground truth scores, we transform all results on
WS-353 as well as the original ground truth into rankings. That is, each
word pair receives a rank from 1 to 348. Word pairs with the same score or
probability yields the same ranking (tied) and the next lower pair 
are ranked down accordingly.
we compute the cosine similarity between the computed rankings by different
methods and the ground truth rank. 
%The rank similarity is measured through cosine similarity. Specifically, each word pair would be one dimension in the rank vector, and the weight each dimension has is its rank value.  

Our baseline algorithm is simply computing the the sentence-level
co-occurrence between two terms within the three data sources. We do
not compute document-level co-occurrence because the resulting co-occurrence
matrix is too large to store in memory.
Our next reasonable comparison is normalized Google distance (NGD) 
\cite{Cilibrasi:Google}:
\begin{equation}
\label{eq:ngd}
NGD(t_1, t_2) = 
\frac{\max(\log f(t_1), \log f(t_2)) - \log f(t_1, t_2)}
{\log M - \min(\log f(t_1), \log f(t_2))}
\end{equation}
%and also compute the rank similarity between the rank given by Google distance algorithm and the ground truth rank.
where $M$ is the total number of pages indexed by Google. 
NGD essentially represents the document level co-occurrence on an extremely
large corpus, which is all the pages of the entire web crawled by Google.
For our HMM model, we give three results with different number of hops.
HMM (1 hop) means no intermediate node between the terms, i.e., 
direct connection; HMM (up to 2 hop) means zero or one intermediate node 
in between, etc.
The results are shown in Table \ref{Similarity353}. 
The HMM model outperforms the baseline and the NGD by a relatively large
margin. When more hops are allowed, the performance increases. This result
reaffirms our initial position that computing through latent connections
gives superior results than direct co-occurrence and that the traversing
through the concept association network can overcome the sparsity of 
direct co-occurrence in our input data source. 
\tabref{tab:20} shows the first 20 word pairs which didn't have significantly
large direct co-occurrences in WS-353, as well as their top association paths
on the network. Note, the terms were translated from Chinese into English and
may not carry exact same meaning.
%table
\begin{table}[th]
\centering
\caption{\label{Similarity353}WS-353 Evaluation Results}
\small
\begin{tabular}{|l|c|}
\hline
Similarity Methods & Rank Similarity \\
\hline\hline
Baseline (direct co-occur)& 62.31\%\\ \hline
NGD & 81.99\%\\
\hline
HMM (1 hop) & 88.61\%\\
\hline
HMM (up to 2 hop) & 88.94\%\\
\hline
HMM (up to 3 hops) & {\bf 89.15\%}\\
\hline
\end{tabular}
\end{table}

\begin{table*}[th]
\caption{First 20 Word Pairs from WS-353 without Direct Connections and Their Association Paths}
\label{tab:20}
\small
\centering
\begin{tabular}{|c|l|} \hline 
love, sex&	(sex$\rightarrow$human sex behavior$\rightarrow$love) (sex$\rightarrow$sexual intercourse$\rightarrow$love)	(sex$\rightarrow$male$\rightarrow$love) \\ \hline
plane, car & (car$\rightarrow$engine$\rightarrow$plane) (car$\rightarrow$drive$\rightarrow$plane) \\ \hline
professor, doctor & (doctor$\rightarrow$dean$\rightarrow$professor) (doctor$\rightarrow$medical doctor$\rightarrow$professor) \\ \hline 
stock, phone & (phone$\rightarrow$company name$\rightarrow$stock)	(phone$\rightarrow$company$\rightarrow$stock) \\ \hline	
stock, jaguar& (jaguar$\rightarrow$company$\rightarrow$stock)	(jaguar$\rightarrow$purchase$\rightarrow$stock) \\ \hline
stock, egg& (egg$\rightarrow$poultry$\rightarrow$stock)	(egg$\rightarrow$feed$\rightarrow$stock) \\ \hline
stock, live& (stock$\rightarrow$man$\rightarrow$live)	(stock$\rightarrow$sacrifice$\rightarrow$live) (stock$\rightarrow$animal$\rightarrow$live) \\ \hline
stock, life& (stock$\rightarrow$man$\rightarrow$life)	(stock$\rightarrow$sacrifice$\rightarrow$life)	(stock$\rightarrow$animal$\rightarrow$life) \\ \hline
professor, cucumber& (cucumber$\rightarrow$anti cancer$\rightarrow$professor)	(cucumber$\rightarrow$breed$\rightarrow$professor) \\ \hline
holy, sex& (sex$\rightarrow$semantics$\rightarrow$holy)	(sex$\rightarrow$pronoun$\rightarrow$holy)\\ \hline	
space, chemistry&(space$\rightarrow$physics$\rightarrow$chemistry)	(chemistry$\rightarrow$substance$\rightarrow$space) (space$\rightarrow$substance$\rightarrow$chemistry)\\ \hline
drink, car& (drink$\rightarrow$manufacturing$\rightarrow$car)	(drink$\rightarrow$company$\rightarrow$car) (drink$\rightarrow$produce$\rightarrow$car)\\ \hline
drink, ear& (ear$\rightarrow$pet$\rightarrow$drink)	(ear$\rightarrow$hearing$\rightarrow$taste$\rightarrow$drink)\\ \hline
drink, mouth& (drink$\rightarrow$soda$\rightarrow$mouth)	(drink$\rightarrow$vita$\rightarrow$mouth)	(drink$\rightarrow$food$\rightarrow$mouth)\\ \hline
drink, mother& (drink$\rightarrow$drinking water$\rightarrow$mother)	(drink$\rightarrow$instant noodle$\rightarrow$mother)	(drink$\rightarrow$like$\rightarrow$mother)\\ \hline
journey, voyage& (journey$\rightarrow$mile$\rightarrow$voyage)	(voyage$\rightarrow$mile$\rightarrow$journey)	(journey$\rightarrow$sail$\rightarrow$voyage)\\ \hline
boy, lad & (lad$\rightarrow$soft hair$\rightarrow$boy)	(lad$\rightarrow$lass$\rightarrow$boy)	(boy$\rightarrow$lass$\rightarrow$lad)\\ \hline
asylum, madhouse & (madhouse$\rightarrow$detain$\rightarrow$asylum)	(madhouse$\rightarrow$madman$\rightarrow$trump$\rightarrow$asylum)	(madhouse$\rightarrow$madman$\rightarrow$hospital$\rightarrow$asylum)\\ \hline
furnace, stove& (furnace$\rightarrow$wind box$\rightarrow$stove)	(stove$\rightarrow$wind box$\rightarrow$furnace)	(furnace$\rightarrow$wind blow$\rightarrow$stove)\\ \hline
brother, monk & (brother$\rightarrow$sworn$\rightarrow$monk)	(brother$\rightarrow$master and apprentice$\rightarrow$monk)	(brother$\rightarrow$sworn$\rightarrow$monk)\\ \hline
%
%love	sex &	(sex-人类sex行为-love) (sex-sex交-love)	(sex-男sex-love) \\ \hline
%飞机	汽车 & (汽车-发动机-飞机) (汽车-驾驶-飞机) \\ \hline
%教授	医生	& (医生-院长-教授) (医生-医学博士-教授) \\ \hline 
%股票	电话	& (电话-公司名称-股票)	(电话-公司-股票) \\ \hline	
%股票	光盘	& (光盘-出版-股票)	(光盘-价格-股票) \\ \hline
%股票	美洲虎	& (美洲虎-公司-股票)	(美洲虎-采购-股票) \\ \hline
%家畜	蛋	& (蛋-家禽-家畜)	(蛋-饲料-家畜) \\ \hline
%家畜	生存	& (家畜-人-生存)	(家畜-牺牲-生存) (家畜-野兽-生存) \\ \hline
%家畜	生命	& (家畜-人-生命)	(家畜-牺牲-生命)	(家畜-野兽-生命) \\ \hline
%教授	黄瓜	& (黄瓜-主要-教授)	(黄瓜-抗癌-教授)	(黄瓜-育种-教授) \\ \hline
%神圣	性	& (性-语意-神圣)	(性-代词-神圣)\\ \hline	
%空间	化学	&(空间-物理学-化学)	(化学-物质-空间) (空间-物质-化学)\\ \hline
%饮料	汽车	& (饮料-制造业-汽车)	(饮料-公司-汽车) (饮料-生产-汽车)\\ \hline
%饮料	耳朵	& (耳朵-宠物-饮料)	(耳朵-喜乐蒂-喜乐-饮料)	(耳朵-听觉-味觉-饮料)\\ \hline
%饮料	嘴巴	& (饮料-汽水-嘴巴)	(饮料-维他-嘴巴)	(饮料-食物-嘴巴)\\ \hline
%饮料	妈妈	& (饮料-白开水-妈妈)	(饮料-即食面-妈妈)	(饮料-喜欢-妈妈)\\ \hline
%旅程	航程	& (旅程-英里-航程)	(航程-英里-旅程)	(旅程-航行-航程)\\ \hline
%男孩	小伙子	& (小伙子-毛头-男孩)	(小伙子-小姑娘-男孩)	(男孩-小姑娘-小伙子)\\ \hline
%收容所	疯人院	& (疯人院-关押-收容所)	(疯人院-疯人-露宿者-收容所)	(疯人院-疯人-病院-收容所)\\ \hline
%熔炉	炉灶	& (熔炉-风箱-炉灶)	(炉灶-风箱-熔炉)	(熔炉-鼓风-炉灶)\\ \hline
%
%工具	实施	1.4190726802E-6	(工具-编制-实施)	1.4066609087999998E-6	(实施-编制-工具)	1.092124481E-6	(实施-部队-工具)\\ \hline
%兄弟	和尚	3.7306796076000004E-6	(兄弟-结义-和尚)	2.2019646528E-6	(兄弟-师徒-和尚)	2.0137836576E-6	(兄弟-结拜-和尚)\\ \hline
%起重机	实施	3.2141848466E-6	(起重机-工作-实施)	2.2641192139999997E-6	(起重机-作业-实施)	1.8153859643999998E-6	(起重机-企业-实施)\\ \hline
%小伙子	兄弟	1.00741918716E-5	(小伙子-姑娘-兄弟)	2.8641204775E-6	(小伙子-兄-兄弟)	2.7457304832E-6	(小伙子-嫂嫂-兄弟)\\ \hline
%旅程	汽车	2.1451506440000003E-6	(旅程-公共汽车-汽车)	1.3300576047E-6	(旅程-驾驶-汽车)	1.1457233015999998E-6	(汽车-车主-旅程)\\ \hline
%和尚	神谕	8.16985065316E-5	(神谕-道镜-和尚)	3.4740640303200004E-5	(神谕-寺庙-和尚)	1.6435173041599998E-5	(神谕-孝谦天皇-和尚)\\ \hline
%墓地	林地	8.0052212874E-6	(墓地-耕地-林地)	5.4009455840999994E-6	(林地-荒山-墓地)	4.3412088412E-6	(林地-园地-墓地)\\ \hline
%海滨	林地	8.7116375036E-6	(海滨-公顷-林地)	6.5569462704E-6	(海滨-耕地-林地)	4.5663960384000006E-6	(海滨-平方公里-林地)\\ \hline
%和尚	奴隶	2.8999054368E-6	(奴隶-寺庙-和尚)	2.0699733708E-6	(和尚-家当-奴隶)	1.0276367736E-6	(奴隶-主人-和尚)\\ \hline
%小伙子	巫师	3.226094284E-6	(小伙子-英俊-巫师)	2.6705465128E-6	(巫师-英俊-小伙子)	1.4955953985E-6	(巫师-帚-小伙子)\\ \hline
%和弦	微笑	7.863787112E-6	(微笑-托卡塔曲-和弦)	5.768911923600001E-6	(微笑-旋律-和弦)	5.145956173800001E-6	(微笑-夜曲-和弦)\\ \hline
%玻璃	魔术师	1.016766845E-6	(魔术师-镜子-玻璃)	7.01799266E-7	(玻璃-镜子-魔术师)	3.9336849977899046E-8	(魔术师-魔术-镜子-玻璃)\\ \hline
%中午	绳子	2.7945714384E-6	(中午-犯人-绳子)	1.8756678324E-6	(中午-骗人布-绳子)	1.5156777159E-6	(绳子-犯人-中午)\\ \hline
%公鸡	航程	1.5135858834800001E-5	(公鸡-千克-航程)	8.6687647174E-6	(公鸡-公斤-航程)	1.620201456E-6	(公鸡-战斗机-航程)\\ \hline
%金钱	操作	6.0005043008E-6	(操作-储存-金钱)	4.216566172E-6	(操作-用户-金钱)	3.1655575070000004E-6	(操作-关机-金钱)\\ \hline
%老虎	生物	1.4866042455600001E-5	(老虎-食物链-生物)	1.37672589952E-5	(老虎-豹-生物)	1.0119540230100001E-5	(老虎-华南虎-生物)\\ \hline
%老虎	动物群	1.03498679711E-5	(动物群-大熊猫-老虎)	8.18563515E-6	(动物群-动物-老虎)	7.930105678E-6	(动物群-猫科-老虎)\\ \hline
%老虎	动物园	0.00456669	(动物园-老虎)	0.00443854	(老虎-动物园)	2.42475434847E-5	(动物园-狮-老虎)\\ \hline
%心理学	医生	2.61551872284E-5	(医生-心理学家-心理学)	6.7021269642E-6	(医生-医患关系-心理学)	6.6885353672E-6	(心理学-心理学家-医生)\\ \hline
%行星	空间	2.14462706328E-5	(空间-太阳系-行星)	1.13866651656E-5	(行星-太阳系-空间)	1.086941859E-5	(空间-宇宙-行星)\\ \hline
%行星	天文学家	0.01267385	(天文学家-行星)	0.00876213	(行星-天文学家)	2.031698206611E-4	(天文学家-太阳系-行星)\\ \hline
%先例	例子	7.223150978E-6	(先例-援引-例子)	3.2300039928E-6	(先例-特例-例子)	1.9731447E-6	(先例-成功-例子)\\ \hline
%先例	集合	2.0780231256E-6	(先例-拼装-集合)	6.460032904E-7	(先例-特例-集合)	6.25753245E-7	(先例-约束-集合)\\ \hline
%先例	群组	3.68262804E-7	(先例-录音-群组)	7.41700826116068E-8	(先例-不成文-时光网-群组)	6.258146823555534E-8	(先例-汉语-通话记录-群组)\\ \hline
%杯	作品	1.168832667E-5	(杯-圣杯-作品)	1.02824827794E-5	(杯-瓷杯-作品)	7.3106576298E-6	(杯-茶具-作品)\\ \hline
%杯	物质	4.8808980558E-6	(杯-液体-物质)	3.116887112E-6	(杯-圣杯-物质)	2.7082742203999995E-6	(杯-粒子列表-物质)\\ \hline
%杯子	液体	0.00466362	(杯子-液体)	6.51483782256E-5	(杯子-杯壁-液体)	5.9868159867E-5	(杯子-马克杯-液体)\\ \hline
%美洲虎	猫	3.8353292445599994E-5	(猫-猫科-美洲虎)	3.4800092876E-5	(美洲虎-猫科-猫)	2.2343582034399998E-5	(美洲虎-大猫-猫)\\ \hline
%能量	秘书	8.770069511999999E-7	(秘书-战士-能量)	6.158792068000001E-7	(秘书-初级-能量)	8.753111773876839E-8	(秘书-处长-供给-能量)\\ \hline
%秘书	参议院	3.27154619141E-5	(秘书-约法会议-参议院)	1.28813414642E-5	(秘书-议长-参议院)	7.000601858200001E-6	(秘书-民意代表-参议院)\\ \hline
%武器	秘密	7.019451504000001E-6	(秘密-军事-武器)	6.2721442875E-6	(秘密-盔甲-武器)	4.323890461800001E-6	(秘密-投掷-武器)\\ \hline
%调查	努力	1.581330377E-6	(努力-结果-调查)	1.0591079525E-6	(调查-结果-努力)	4.0703377000000003E-7	(调查-满意度-努力)\\ \hline
%火星	水	1.8957075639599996E-5	(水-地球-火星)	1.7169143826800002E-5	(水-月球-火星)	1.2406308663000002E-5	(水-地表-火星)\\ \hline
%发现	空间	2.1525284737E-6	(空间-存在-发现)	2.0461491804E-6	(空间-宇宙-发现)	1.9482200749E-6	(空间-观测-发现)\\ \hline
%水	渗漏	7.9402126404E-6	(水-地下水-渗漏)	6.025122942E-6	(水-雨水-渗漏)	3.0314398420000002E-6	(水-蒸发-渗漏)\\ \hline
%标志	休息	1.9951885306E-6	(休息-愿意-标志)	1.4010895898999999E-6	(休息-设置-标志)	1.4009248589999999E-6	(休息-状态-标志)\\ \hline
%星期三	新闻	1.05699878116E-5	(星期三-晚间新闻-新闻)	9.0862547248E-6	(星期三-节目-新闻)	7.675504992E-6	(星期三-无线电视晚间新闻-新闻)\\ \hline
%计算机	新闻	3.2971434777E-6	(新闻-信息-计算机)	3.2099327226E-6	(新闻-互联网-计算机)	2.8662868682000004E-6	(新闻-国际-计算机)\\ \hline
%领土	表面	8.1336103616E-6	(表面-边界-领土)	7.3759033985E-6	(表面-毫米-领土)	4.9826770956E-6	(表面-温度-领土)\\ \hline
%大气	风景	3.2524317104000003E-6	(风景-气候-大气)	3.1822139945E-6	(大气-气候-风景)	2.4327108438E-6	(风景-自然-大气)\\ \hline
%总统	奖牌	0.00123713	(奖牌-总统)	4.89359382414E-5	(奖牌-勋章-总统)	2.06021792445E-5	(奖牌-美国总统-总统)
%战争	军队	0.00635082	(战争-军队)	6.2217E-4	(军队-战争)	2.7427158312499998E-5	(战争-卡尔冯克劳塞维茨-军队)
%记录	数字	9.1679E-4	(记录-数字)	4.7564545414000006E-6	(记录-数据-数字)	4.0717318218E-6	(记录-映射-数字)
%皮肤	眼睛	0.01224816	(眼睛-皮肤)	0.00303234	(皮肤-眼睛)	1.7624401925E-4	(眼睛-眼霜-皮肤)
%日本	美国	0.01322583	(日本-美国)	0.01076551	(美国-日本)	1.118380773657E-4	(日本-加拿大-美国)
%戏剧	历史	6.4447E-4	(历史-戏剧)	3.1005255680000002E-6	(历史-题材-戏剧)	2.1387273195E-6	(历史-悲剧-戏剧)
%志愿者	口号	0.00100855	(志愿者-口号)	5.1594E-4	(口号-志愿者)	1.33280218323E-5	(志愿者-第29届奥林匹克运动会组织委员会-口号)
%偏见	认知	0.00304312	(偏见-认知)	3.2149106464799995E-5	(偏见-刻板印象-认知)	2.02553263575E-5	(偏见-主观-认知)
%装饰	勇气	1.82869358496E-5	(勇气-小兽-装饰)	8.232995219679738E-7	(勇气-小兽-故宫太和殿-装饰)	3.4722029901277043E-7	(勇气-小兽-中国宫殿-装饰)
%世纪	年	0.00316141	(世纪-年)	2.3757489043600002E-5	(世纪-公历-年)	1.81548394542E-5	(世纪-公元-年)
%世纪	国家	0.00108391	(世纪-国家)	2.4140052895E-5	(世纪-中国-国家)	1.02846504762E-5	(世纪-世界-国家)
%延迟	种族主义	6.264709232E-7	(种族主义-妨碍-延迟)	3.6880639E-7	(种族主义-卫报-延迟)	4.2436397598616555E-8	(种族主义-种族歧视-引信-延迟)
%延迟	新闻	1.6140648408E-6	(新闻-晚间新闻-延迟)	1.346804254E-6	(延迟-现场直播-新闻)	1.1535154465000001E-6	(新闻-发布-延迟)
%部长	党派	0.00137608	(党派-部长)	1.0784021173980001E-4	(党派-内阁-部长)	6.51114271568E-5	(党派-议会-部长)
%和平	计划	6.3799E-4	(和平-计划)	2.5390301057E-6	(和平-国家-计划)	2.0416917682999997E-6	(和平-裁军-计划)
%少数民族	和平	3.2545970015999998E-6	(和平-朝鲜-少数民族)	3.2448419716E-6	(和平-团结-少数民族)	2.5954289025000004E-6	(和平-中华民族-少数民族)
%尝试	和平	1.867325424E-6	(尝试-努力-和平)	1.062182915E-6	(尝试-希望-和平)	6.524973268934638E-8	(尝试-阿斯兰萨拉-O.R.B.-和平)
%政府	危机	0.00338045	(危机-政府)	8.66486727729E-5	(危机-危机处理-政府)	5.01885322498E-5	(危机-危机管理-政府)
%部署	出发	6.6615E-4	(出发-部署)	7.8700129224E-6	(出发-部队-部署)	4.18990698E-6	(出发-航空母舰战斗群-部署)
%部署	撤离	0.00139263	(撤离-部署)	3.47574574355E-5	(撤离-嘉手纳空军基地-部署)	2.91892075216E-5	(撤离-部队-部署)
%能源	危机	9.9152987508E-6	(危机-能源危机-能源)	5.1785175288E-6	(能源-能源危机-危机)	4.4430870032E-6	(危机-短缺-能源)
%公告	新闻	4.2775978986E-5	(公告-发布-新闻)	9.1997017506E-6	(公告-登载-新闻)	7.509037325E-6	(公告-刊登-新闻)
%公告	努力	7.766625682E-7	(公告-停止-努力)	6.19570015E-7	(公告-全体-努力)	5.710341650000001E-7	(公告-加强-努力)
%中风	医院	5.6591E-4	(中风-医院)	2.84391535099E-5	(中风-康复-医院)	2.8329984815899998E-5	(中风-患者-医院)
%残疾	死亡	0.0013161	(残疾-死亡)	4.9098477076E-5	(残疾-离休干部-死亡)	3.0969056148400004E-5	(残疾-老年-死亡)
%受害者	紧急	9.194745483000001E-6	(紧急-救护-受害者)	5.592197787E-6	(紧急-外援-受害者)	3.1900228437E-6	(紧急-亲属-受害者)
%治疗	康复	0.00705844	(康复-治疗)	0.00195377	(治疗-康复)	1.70238386333E-4	(康复-患者-治疗)
%杂志	协会	8.2266E-4	(杂志-协会)	6.1162E-4	(协会-杂志)	2.8563627302399997E-5	(杂志-理事-协会)
%医生	员工	8.576780013000001E-7	(医生-业绩-员工)	8.248360959999999E-7	(医生-看病-员工)	7.90282335E-7	(医生-工作-员工)
%医生	责任	1.0803904500000002E-6	(医生-车祸-责任)	9.255469027000001E-7	(医生-职责-责任)	8.778723702000001E-7	(医生-医疗卫生-责任)
%责任	保险	9.6865E-4	(保险-责任)	6.2722808864E-5	(保险-保险人-责任)	3.93467691084E-5	(保险-被保险人-责任)
%学校	中心	0.00133733	(中心-学校)	6.5772E-4	(学校-中心)	5.9052158698E-5	(中心-示范-学校)
%理由	高血压	1.1654986256100001E-5	(理由-牙痛-高血压)	3.0374962059000003E-6	(理由-饮酒-高血压)	1.9845384928371033E-6	(理由-摘录-全国中草药汇编-高血压)
%理由	标准	1.42329080153E-5	(理由-判断-标准)	9.6548082554E-6	(理由-采纳-标准)	7.036500975000001E-6	(理由-提出-标准)
%哈佛	耶鲁	0.02389277	(哈佛-耶鲁)	0.01192047	(耶鲁-哈佛)	2.772898566657E-4	(哈佛-哈佛大学-耶鲁)
%医院	基础设施	5.1659E-4	(医院-基础设施)	1.374539233E-5	(医院-医疗卫生-基础设施)	8.3364587152E-6	(医院-现代化-基础设施)
%死亡	吵闹	0.00113999	(吵闹-死亡)	3.0065711579799998E-5	(吵闹-叔伯兄弟-死亡)	2.8047362944E-5	(吵闹-儿媳-死亡)
%死亡	犯人	0.00200429	(犯人-死亡)	5.84360898672E-5	(犯人-监狱-死亡)	4.00793443744E-5	(犯人-自杀-死亡)
%律师	证据	0.00703681	(律师-证据)	7.2839E-4	(证据-律师)	3.054545351722E-4	(律师-被告-证据)
%生命	死亡	0.00169357	(生命-死亡)	2.53616718776E-5	(生命-自杀-死亡)	1.47487035443E-5	(生命-车祸-死亡)
%生活	长期	0.00135904	(长期-生活)	1.65384992188E-5	(长期-精神食粮-生活)	7.176879728699999E-6	(长期-灵性-生活)
%词语	相似	3.22582124128E-4	(相似-词汇-词语)	2.770569231569515E-5	(相似-词汇-汉语词典-词语)	2.0469397706E-5	(相似-方言-词语)
%董事会	建议	6.0484E-4	(建议-董事会)	1.5128921354999998E-5	(建议-审议-董事会)	1.06267259395E-5	(建议-议案-董事会)
%州长	面试	8.6805753554E-6	(面试-参选人-州长)	7.5932217305999994E-6	(州长-参选人-面试)	2.3462293079999997E-6	(面试-初选-州长)
%欧佩克	国家	0.00555556	(欧佩克-国家)	1.3122570997120003E-4	(欧佩克-石油输出国组织-国家)	1.105761799816E-4	(欧佩克-石油-国家)
%和平	气氛	2.7216766775E-6	(气氛-缓和-和平)	2.5442773496E-6	(气氛-华彩-和平)	2.4898223568E-6	(气氛-紧张-和平)
%领土	公里	0.00173984	(领土-公里)	7.5751E-4	(公里-领土)	2.9749694591260005E-4	(领土-平方公里-公里)
%旅行	活动	6.3417E-4	(旅行-活动)	1.06303273969E-5	(旅行-休閒-活动)	8.537009352899999E-6	(旅行-露营-活动)
%竞争	价格	0.00220281	(竞争-价格)	9.1163E-4	(价格-竞争)	2.56833130113E-5	(竞争-降价-价格)
%消费者	信心	0.00442478	(信心-消费者)	1.2162835263999999E-5	(信心-认知-消费者)	1.2024591549E-5	(信心-承诺-消费者)
%消费品	能量	2.20034597082E-5	(消费品-消耗-能量)	1.57750978025E-5	(消费品-供给-能量)	4.9356977814E-6	(消费品-自养生物-能量)
%问题	机场	1.5286776856E-6	(机场-建设-问题)	1.26211596E-6	(机场-发展-问题)	1.0899029154E-6	(机场-QoS-问题)
%汽车	航班	4.0140723246E-6	(航班-公共交通-汽车)	3.7261928039999997E-6	(汽车-公共交通-航班)	3.0429269764E-6	(汽车-乘客-航班)
%信用卡	卡	0.2038835	(卡-信用卡)	0.008215645517156	(卡-发卡银行-信用卡)	0.0066574199445215	(卡-磁性卡-信用卡)
%信用	信息	0.00112824	(信用-信息)	3.7059725412300004E-5	(信用-信用信息-信息)	1.8774956263799997E-5	(信用-企业信用-信息)
%酒店	预订	0.02368191	(预订-酒店)	0.00179463	(酒店-预订)	7.76274960468E-4	(预订-酒店预订-酒店)
%杂货店	钱	3.06155788468E-5	(杂货店-纸币-钱)	2.220640625E-5	(杂货店-零钱-钱)	1.4146921899499998E-5	(杂货店-钱庄-钱)
%登记	安排	4.9384948025E-6	(安排-手续-登记)	2.3016550829999997E-6	(安排-规定-登记)	1.8156331398E-6	(安排-机构-登记)
%安排	住宿	0.00343278	(安排-住宿)	2.26730588832E-5	(安排-就餐-住宿)	1.98843716595E-5	(安排-日程-住宿)
%月份	酒店	1.34588257158E-5	(月份-旺季-酒店)	4.9121873164E-6	(月份-自理-酒店)	3.5004496634E-6	(月份-学费-酒店)
%类型	种类	0.00231078	(类型-种类)	5.2265E-4	(种类-类型)	7.065501355400001E-6	(类型-植被-种类)
%到达	酒店	0.00153118	(到达-酒店)	1.9233414487E-5	(到达-旅游-酒店)	1.7874554055E-5	(到达-景区-酒店)
%床	衣柜	4.5691316183859996E-4	(床-小床-衣柜)	3.2675576070579997E-4	(床-床铺-衣柜)	1.006151124264E-4	(床-床垫-衣柜)
%衣柜	衣服	0.00621808	(衣柜-衣服)	0.00409277	(衣服-衣柜)	6.43387302558E-5	(衣柜-衣架-衣服)
%情况	结论	9.1282E-4	(结论-情况)	7.7465E-4	(情况-结论)	3.142700547E-6	(结论-调查-情况)
%情况	隔离	6.415034513999999E-6	(隔离-审查制度-情况)	2.9451216257999997E-6	(情况-审查制度-隔离)	2.3827536872E-6	(隔离-监测-情况)
%公正	利益	0.00113001	(利益-公正)	2.8548545385599995E-5	(利益-客观性-公正)	2.0214994132E-5	(利益-个人利益-公正)
%方向	组合	9.2864981628E-6	(组合-风水-方向)	4.92753888E-6	(组合-财运-方向)	3.5700298966E-6	(组合-石碑护身符-方向)
%街道	地点	2.4766777200000004E-6	(地点-平面道路交叉-街道)	2.0650511797999997E-6	(地点-胡同-街道)	1.8498236388E-6	(地点-中华人民共和国-街道)
%街道	大道	1.0	(街道-大道)	5.249223936880001E-5	(街道-行政区划-大道)	4.22139363346E-5	(街道-先进街道-大道)
%街道	街区	0.00214133	(街区-街道)	7.1019E-4	(街道-街区)	1.4623028651999998E-5	(街区-步行街-街道)
%街头	儿童	4.6475736313E-5	(街头-涂鸦-儿童)	3.2252415104799996E-5	(街头-行乞-儿童)	2.8546118636E-5	(街头-贫穷-儿童)
%列表	靠近	6.006285445929899E-9	(靠近-置换-顺序-列表)	3.517540768310712E-9	(靠近-夏季-秋季-列表)	3.302184872177472E-9	(靠近-夏季-春季-列表)
%列表	类别	1.8996106470000002E-6	(类别-按钮-列表)	1.5285611392E-6	(类别-子类-列表)	1.3965657019999998E-6	(类别-窗口-列表)
%电池	电话	1.7744574386E-5	(电池-电量状态-电话)	1.62825352542E-5	(电池-干电池-电话)	7.2812054444E-6	(电池-话筒-电话)
%产量	上升	0.00170829	(上升-产量)	1.5010042472989998E-4	(上升-下降-产量)	2.70432577062E-5	(上升-总产量-产量)
%基准	指数	0.00367158	(指数-基准)	0.00164067	(基准-指数)	8.120702289129999E-5	(指数-成份股-基准)
%媒体	贸易	8.827925444999999E-7	(媒体-垄断-贸易)	4.379750312E-7	(贸易-全球-媒体)	4.119629448E-7	(贸易-外国-媒体)
%媒体	收获	1.0426930315999998E-6	(收获-刊登-媒体)	7.600937313E-7	(收获-人民日报-媒体)	6.795613404000001E-7	(收获-发行量-媒体)
%股息	支付	0.00661858	(股息-支付)	0.00325425	(支付-股息)	1.206310370265E-4	(股息-利息-支付)
%股息	运算	6.549611068000001E-6	(股息-公式-运算)	1.7525135252E-6	(股息-计算-运算)	1.7288257376E-6	(股息-信号-运算)
%运算	计算	0.00206876	(运算-计算)	0.00181277	(计算-运算)	5.91222413375E-5	(运算-乘法-计算)
%货币	市场	0.00734049	(货币-市场)	0.00447901	(市场-货币)	1.083127666586E-4	(货币-纸币-市场)
%欧佩克	石油	0.06296296	(欧佩克-石油)	0.00243719	(石油-欧佩克)	0.001656727069868	(欧佩克-石油输出国组织-石油)
%石油	股票	1.5383154015E-5	(石油-道琼斯工业平均指数-股票)	3.4045713399999998E-6	(股票-道琼斯工业平均指数-石油)	2.8709230572E-6	(石油-上涨-股票)
%公告	产量	7.9043839884E-6	(公告-变动-产量)	6.0981681023999995E-6	(公告-国家商标局-产量)	5.5016831400000004E-6	(公告-均价-产量)
%公告	警告	1.67657802649E-5	(公告-改正-警告)	1.4118702760000002E-5	(公告-逾期-警告)	1.16777893795E-5	(公告-规定-警告)
%利润	警告	9.8005062648E-6	(警告-撤销党内职务-利润)	7.3445463345999995E-6	(警告-留党察看-利润)	2.294857842E-6	(警告-开除党籍-利润)
%利润	亏损	0.02067386	(亏损-利润)	0.01270471	(利润-亏损)	1.1212263136799999E-4	(亏损-亏损额-利润)
%美元	日元	0.00849392	(日元-美元)	0.00167511	(美元-日元)	4.890379251274E-4	(日元-售价-美元)
%美元	美金	1.0	(美元-美金)	3.1918627939670006E-4	(美元-利用外资-美金)	2.93696694215E-4	(美元-进出口总额-美金)
%美元	利润	0.00485138	(利润-美元)	0.00206255	(美元-利润)	6.20620001616E-5	(利润-亏损-美元)
%美元	亏损	0.00488496	(亏损-美元)	0.00127627	(美元-亏损)	1.0029675092679999E-4	(亏损-利润-美元)
%计算机	软件	0.00381444	(软件-计算机)	0.00295119	(计算机-软件)	2.5369035199200004E-5	(软件-硬体-计算机)
%网络	硬件	0.00149286	(硬件-网络)	4.7578250455E-5	(硬件-软件-网络)	3.10521698072E-5	(硬件-驱动程序-网络)
%手机	设备	1.0498245672000001E-5	(手机-蓝牙-设备)	6.4792407080999995E-6	(手机-用户-设备)	5.648776155600001E-6	(手机-终端-设备)
%设备	制造商	0.00156051	(制造商-设备)	0.0012826	(设备-制造商)	1.0964243950499999E-5	(制造商-通信设备-设备)
%奢侈品	汽车	8.9303E-4	(奢侈品-汽车)	1.526553567E-5	(奢侈品-必需品-汽车)	7.6922525463E-6	(奢侈品-梅赛德斯宾士-汽车)
%报告	收益	5.555101786E-6	(报告-投资者-收益)	2.2080934064E-6	(报告-季度-收益)	2.1385882913999996E-6	(报告-确认-收益)
%投资者	盈利	0.00763891	(盈利-投资者)	0.002463	(投资者-盈利)	1.188599307818E-4	(盈利-亏损-投资者)
%液体	水	0.01105845	(水-液体)	5.293698281613E-4	(水-冷却剂-液体)	3.0647687586159996E-4	(水-固体-液体)
%棒球	赛季	0.00123014	(棒球-赛季)	2.0505105877749997E-4	(棒球-效力-赛季)	2.049162422328E-4	(棒球-联赛-赛季)
%游戏	胜利	7.1917E-4	(胜利-游戏)	4.7539748296E-5	(胜利-玩家-游戏)	1.6435710407500002E-5	(胜利-战斗-游戏)
%比赛	球队	0.00793316	(比赛-球队)	0.0079225	(球队-比赛)	1.61694126469E-4	(比赛-赛季-球队)
%马拉松	冲刺	0.00159886	(冲刺-马拉松)	0.00106692	(马拉松-冲刺)	3.630341862E-5	(马拉松-柏林马拉松-冲刺)
%游戏	系列	0.00398853	(系列-游戏)	0.00271455	(游戏-系列)	1.8784615586799998E-5	(系列-三国无双-游戏)
%比赛	失利	0.00428745	(失利-比赛)	1.092607574082E-4	(失利-决赛-比赛)	1.0436473478859998E-4	(失利-比分-比赛)
%海鲜	海洋	1.64991723654E-5	(海洋-贝类-海鲜)	1.3210158823000001E-5	(海洋-海洋文化-海鲜)	9.90853177E-6	(海洋-海洋生物学-海鲜)
%海鲜	食物	0.00721056	(海鲜-食物)	0.00165474	(食物-海鲜)	8.4329275672E-5	(海鲜-烹调-食物)
%海鲜	龙虾	0.00979783	(龙虾-海鲜)	0.00263891	(海鲜-龙虾)	1.189227292778E-4	(龙虾-鲍鱼-海鲜)
%龙虾	食物	0.00213896	(龙虾-食物)	7.064784108480001E-5	(龙虾-海鲜-食物)	3.9081974600499995E-5	(龙虾-美味-食物)
%龙虾	葡萄酒	1.2441280530400001E-5	(龙虾-金枪鱼-葡萄酒)	8.8359578454E-6	(龙虾-红酒-葡萄酒)	6.9718827975000004E-6	(龙虾-啤酒-葡萄酒)
%食物	烹饪	0.00778253	(烹饪-食物)	0.00621474	(食物-烹饪)	5.743362412502E-4	(烹饪-食谱-食物)
%视频	档案	0.00206029	(档案-视频)	3.4542024496E-5	(档案-照片档案-视频)	2.6469072232499997E-5	(档案-全宗-视频)
%开始	年	0.00205262	(开始-年)	5.2073E-4	(年-开始)	8.0382273375E-6	(开始-旬-年)
%开始	比赛	0.00139431	(开始-比赛)	9.005584975E-6	(开始-球队-比赛)	8.9344596072E-6	(开始-结束-比赛)
%比赛	回合	0.00988621	(回合-比赛)	9.5846E-4	(比赛-回合)	2.443428046566E-4	(回合-决赛-比赛)
%拳击	回合	0.00304411	(拳击-回合)	0.00156098	(回合-拳击)	1.7373714358139998E-4	(拳击-击倒-回合)
%冠军	联赛	0.02082708	(冠军-联赛)	0.0080767	(联赛-冠军)	3.3932160883649995E-4	(冠军-赛季-联赛)
%战斗	击败	4.6167013048E-6	(击败-回合-战斗)	2.8415374099999998E-6	(击败-战役-战斗)	2.8061349470000004E-6	(击败-敌人-战斗)
%线路	保险	7.221201036E-7	(线路-状态-保险)	5.730755325000001E-7	(线路-补偿-保险)	5.309146944E-7	(线路-位置-保险)
%白天	夏天	0.00198965	(夏天-白天)	9.092611313809999E-5	(夏天-冬天-白天)	1.0419876636E-5	(夏天-晚上-白天)
%夏季	干旱	0.00879167	(干旱-夏季)	0.00327129	(夏季-干旱)	3.18926458855E-4	(干旱-冬季-夏季)
%夏季	自然	5.2541E-4	(夏季-自然)	7.343621425799999E-6	(夏季-避暑-自然)	7.1988861402000005E-6	(夏季-凉爽-自然)
%白天	黎明	6.7102E-4	(白天-黎明)	6.921963294239999E-5	(白天-晚上-黎明)	2.7592392648000003E-5	(黎明-晚上-白天)
%自然	环境	0.00151636	(环境-自然)	8.7147138924E-6	(环境-生物-自然)	7.2977603061E-6	(环境-人与自然-自然)
%环境	生态	0.00259932	(生态-环境)	5.5517E-4	(环境-生态)	9.607260891199999E-6	(生态-生态系统-环境)
%自然	人	0.00401188	(人-自然)	0.00290221	(自然-人)	1.8069986316E-5	(人-造化-自然)
%男人	女人	0.03580166	(女人-男人)	0.00614153	(男人-女人)	1.328347072107E-4	(女人-因为女人-男人)
%男人	管理者	1.723511184E-6	(管理者-自己-男人)	9.380570031999999E-7	(管理者-个人-男人)	7.831617081000001E-7	(管理者-角色-男人)
%谋杀	凶杀	9.3025E-4	(凶杀-谋杀)	3.53078772234E-5	(凶杀-凶手-谋杀)	7.296840464500001E-6	(凶杀-强奸-谋杀)
%肥皂剧	歌剧	2.893055019E-5	(肥皂剧-戏剧-歌剧)	2.8193739109800004E-5	(肥皂剧-剧院-歌剧)	1.9779112776E-5	(肥皂剧-百老汇-歌剧)
%歌剧	表演	5.6022E-4	(歌剧-表演)	2.6573781348599995E-5	(歌剧-戏剧-表演)	1.5392851089E-5	(歌剧-音乐会-表演)
%生活	教训	1.0058005805400002E-5	(教训-汲取-生活)	6.5526855554E-6	(教训-重蹈覆辙-生活)	5.6727827058E-6	(教训-惨痛-生活)
%焦点	生活	1.65751955057E-5	(焦点-聚焦-生活)	1.14639444578E-5	(焦点-折射-生活)	8.9989926822E-6	(焦点-反差-生活)
%生产	工作人员	1.4564718502400002E-5	(工作人员-企业-生产)	1.0347094706E-5	(工作人员-人员-生产)	8.7423938976E-6	(工作人员-部门-生产)
%电视	电影	0.00240437	(电视-电影)	0.00143477	(电影-电视)	1.4225682744000001E-5	(电视-Soundtrack-电影)
%情人	争吵	6.8521E-4	(情人-争吵)	2.0675654536E-6	(情人-伴侣-争吵)	2.0397073725E-6	(情人-误会-争吵)
%观众	连载	0.00140154	(连载-观众)	3.7643025336E-5	(连载-花与梦-观众)	3.30181984378E-5	(连载-少年漫画-观众)
%可能性	女孩	9.466228464000001E-7	(女孩-如果-可能性)	7.929980429E-7	(女孩-暗示-可能性)	7.220720556E-7	(女孩-妊娠-可能性)
%人口	发展	0.00209263	(人口-发展)	2.5167438888E-5	(人口-经济发展-发展)	2.3582871196800002E-5	(人口-面积-发展)
%道德	重要性	8.2618E-4	(道德-重要性)	7.662784219199999E-6	(道德-强调-重要性)	6.535509225E-6	(道德-道德修养-重要性)
%道德	婚姻	1.21916067442E-5	(道德-孝敬父母-婚姻)	9.727675325199999E-6	(道德-礼仪-婚姻)	6.1670675232E-6	(道德-不道德-婚姻)
%墨西哥	巴西	0.01072815	(墨西哥-巴西)	0.00620764	(巴西-墨西哥)	1.348835154579E-4	(墨西哥-阿根廷-巴西)
%性别	平等	0.00596359	(性别-平等)	4.344484122833E-4	(性别-社会性别-平等)	1.862959889118E-4	(性别-性别歧视主义-平等)
%改变	态度	0.00656153	(态度-改变)	8.0194E-4	(改变-态度)	1.9185666474809998E-4	(态度-态度的改变-改变)
%家庭	规划	1.6402891694E-6	(家庭-建设-规划)	1.5523793691000003E-6	(家庭-发展-规划)	1.5447876612000002E-6	(家庭-经济发展-规划)
%歌剧	工业	2.664442142E-7	(工业-俄罗斯-歌剧)	8.648411291377862E-8	(歌剧-理查德瓦格纳-李斯特-工业)	5.8291657775729406E-8	(歌剧-理查德瓦格纳-增长-工业)
%糖	方法	8.5106870282E-6	(糖-食用-方法)	5.9343528572E-6	(糖-发酵(生物化学)-方法)	5.632589663199999E-6	(糖-方糖-方法)
%实践	机构	1.4607902925E-5	(实践-教育-机构)	5.5306070487E-6	(机构-教育-实践)	3.883620293E-6	(实践-改革-机构)
%部门	文化	8.8801E-4	(部门-文化)	3.13585818952E-5	(部门-主管-文化)	8.026810706700001E-6	(部门-人民政府-文化)
%问题	挑战	0.0018351	(挑战-问题)	3.152325372823E-4	(挑战-面临-问题)	2.3516191688400003E-5	(挑战-国家经济安全-问题)
%规模	突出	9.8941557E-7	(规模-特色-突出)	9.444279550000001E-7	(规模-优势-突出)	8.371711116000001E-7	(突出-效益-规模)
%国家	公民	0.00372634	(公民-国家)	9.5219E-4	(国家-公民)	2.24908439424E-5	(公民-宪法-国家)
%行星	人类	0.00107082	(行星-人类)	3.0997275588E-5	(行星-太阳系-人类)	2.6437575810800002E-5	(行星-地球-人类)
%开发	发行	0.00129527	(发行-开发)	1.52530279755E-5	(发行-中国-开发)	1.3481052629999998E-5	(发行-论文-开发)
%经验	音乐	4.229803355699999E-6	(经验-借鉴-音乐)	4.0488717204000005E-6	(经验-总结-音乐)	3.3762006364E-6	(经验-累积-音乐)
%音乐	项目	6.0692567265E-6	(音乐-主持人-项目)	3.6712911968000003E-6	(项目-主持人-音乐)	2.3449008924E-6	(音乐-签约-项目)
%玻璃	金属	0.00154039	(玻璃-金属)	9.1745E-4	(金属-玻璃)	1.008618912E-5	(玻璃-熔化-金属)
%铝	金属	0.00303567	(铝-金属)	3.2619955186800004E-5	(铝-延展性-金属)	3.0958155265599996E-5	(铝-氧化膜-金属)
%机会	信誉	5.121564824686895E-8	(信誉-泥沼-随随便便-机会)	4.530433409015884E-8	(信誉-趋势科技-威胁-机会)	3.400582408050663E-8	(信誉-企业-竞争优势-机会)
%展览	纪念品	0.00131519	(纪念品-展览)	1.75876268055E-5	(纪念品-世界博览会-展览)	1.1657014812800001E-5	(纪念品-手工艺-展览)
%音乐会	演奏家	0.00515729	(音乐会-演奏家)	0.00488725	(演奏家-音乐会)	6.2801483316E-5	(演奏家-演奏-音乐会)
%摇滚	爵士	0.00235408	(摇滚-爵士)	6.44074447588E-5	(摇滚-爵士摇滚-爵士)	4.2716141024299994E-5	(摇滚-爵士乐-爵士)
%博物馆	剧院	0.00614557	(剧院-博物馆)	0.00142258	(博物馆-剧院)	3.3834103128000004E-5	(剧院-美术馆-博物馆)
%观察	建筑	7.3371783996E-6	(观察-望远镜-建筑)	3.9681648566E-6	(观察-观察者-建筑)	2.37980153E-6	(观察-形态-建筑)
%空间	世界	6.7295868023999996E-6	(空间-具体事物-世界)	6.4941732458E-6	(空间-平行宇宙-世界)	5.434904913E-6	(空间-物质-世界)
%维护	世界	1.39410153896E-5	(维护-世界和平-世界)	6.4315354272E-6	(维护-秩序-世界)	2.8884578712E-6	(维护-中国-世界)
%入场	门票	0.01137656	(入场-门票)	1.0289975200919999E-4	(入场-购票-门票)	9.56014266594E-5	(入场-2005年爱知世博会-门票)
%阵雨	雷阵雨	0.01681957	(雷阵雨-阵雨)	0.00703829	(阵雨-雷阵雨)	1.5856814248499997E-4	(雷阵雨-液态降水-阵雨)
%阵雨	洪水	2.1784311289919998E-4	(阵雨-暴雨-洪水)	1.26421061151E-4	(阵雨-冰雹-洪水)	1.094679194154E-4	(阵雨-大雨-洪水)
%天气	预报	0.01951317	(预报-天气)	0.00459123	(天气-预报)	4.450986009088E-4	(预报-天气预报-天气)
%灾难	区域	2.9859794551E-6	(区域-发生-灾难)	1.3703184E-6	(区域-干旱-灾难)	1.1571931335E-6	(区域-生态环境-灾难)
%州长	办公室	8.4720835208E-6	(州长-书记-办公室)	7.493911075E-6	(州长-人民政府-办公室)	6.3599009408E-6	(州长-党委-办公室)
%建筑	世纪	0.0017162	(世纪-建筑)	1.93583518566E-5	(世纪-公元-建筑)	1.81389002805E-5	(世纪-80年代-建筑)
\end{tabular}
\end{table*}

\subsection{Evaluation on Short Text Semantic Similarity}
% \subsection{Evaluation on Recommendation System}
In this section, we discuss in detail how we evaluate our association network's
effectiveness in short text semantic similarity (STSS).
First we will introduce the data set we use in the experiment. Then we 
present a baseline algorithm 
(i.e., latent semantic analysis)
commonly used in short text semantic similarity
and a corresponding refined algorithm leveraging 
the concept association network. 
Comparison shows that our refined algorithm consistently outperform 
their baseline counterparts.
% In this section, we discuss in detail how we evaluate our association network's
% effectiveness in recommendation system. 
% First we will introduce the data set we use in the experiment. Then we 
% present two baseline algorithms 
% (i.e., content-based and collaborative filtering)
% commonly used in recommendation systems
% and two corresponding refined algorithms leveraging 
% the concept association network. 
% Comparison shows that our refined algorithms consistently outperform 
% their baseline counterparts, and have great advantage 
% when training data is small.

\subsubsection{Data Preparation}
\label{sec:preprocess}
%
We use a benchmark dataset for short text semantic algorithms, 
STSS-131 \cite{o2013new}. It contains 131 pairs of short texts with 
10-20 words in length. 
The semantic similarity rating for each pair of short texts is calculated 
as the average of the human ratings for the pair (0.00-4.00). 
We normalize the huaman ratings to between 
0.00 and 1.00. Then we calculate the Pearson score between our results 
and the human ratings to do the evaluation.  

% We randomly download 1000 users' transaction histories from 
% JD Mall \cite{Jingdong}, one of the largest Chinese online mall. 
% JD sells millions of products which are organized in a 4-level hierarchical
% categories.
% Each transaction history contains the name of the product, 
% e.g., ``TP-LINK WR340G+ 54M wireless router'', 
% and the category label of this product, e.g., 
% ``Office and Computers>Network Products>Routers>TP-LINK>WR340G+.'' 
% %
% Because we only have limited transaction histories, which makes the 
% data extremely sparse, it is almost impossible for us to recommend a 
% concrete product name to the user. Therefore we choose to categorize 
% each purchase to its corresponding ``level-3 category label'', 
% for the previous example it would be ``Routers'', i.e., 
% instead of recommending a concrete product to the user, we recommend 
% a level-3 product category which fits the user's need and 
% preference. We argue that the ``level-3 category label'' already 
% captures sufficient information to build a recommendation system. 
% After this data preprocessing, we have 635 items in our data, 
% such as ``Routers'', ``high-heeled shoes'', ``umbrellas'', etc.

% Next we split each user's transaction history into two parts, 
% using the first part to learn the user's characteristics, 
% and the second part as the ground truth to test the performance of the 
% recommendation. We will vary the ratio between these two parts to illustrate
% the effect of training size on the effectiveness of the recommendation. 
% For each user, suppose all his purchased item in the second part of 
% history forms a set $A$, we will offer the top-$k$ recommended items 
% for this user, where $k$ is equal to the size of $A$, 
% and we denote the set of recommended items as $B$, therefore $|A|=|B|$. 
% To measure the quality of our recommended items, we use F1 score 
% as our metric, which is computed by averaging the F1 scores of 
% the recommendation results for all users. For each user, the F1 
% score is computed as $|A\cap B|/|A|$ which is equal to
% the precision and recall in this experiment.

% \subsubsection{Baseline Recommendation Algorithms}
%JY{To check the paper cited}
\subsubsection{Baseline Short Text Similarity Algorithm}
We implement a fundamental STSS measure, Latent Semantic 
Analysis (LSA), with the help of a semantic similarity toolkit, 
SEMILAR \cite{rus2013semilar}. 
% We implement two heuristic-based recommendation algorithms as 
% the baseline algorithms. One of them is the famous collaborative filtering 
% algorithm proposed by Paul Resnick et al \cite{ResnickISBR94}. 
% The other one is the content-based algorithm 
% as described in an authoritative survey in the literature of 
% recommendation system \cite{AdomaviciusT05}.
%JY{Language and term to sentence}
LSA is a modified vector-space model in which the semantic space has its 
dimensions reduced by 
Singular Value Decomposition \cite{deerwester1990indexing}.
We use the semantic space provided by SEMILAR (300 factors).
The resulting dimensions represent generalized 
semantic information rather than specific terms, so it does not require highly 
populated long vectors. This makes it potentially useful for short texts.

%JY{Normalized to 0.0 to 1.0 and Pearson result}

% In the collaborative filtering method, we implement the exact 
% same algorithm proposed by Resnick et al., except that we use 
% implicit ratings rather than explicit ratings to measure a 
% user's preference of a certain item, for it helps improve the 
% performance of the baseline algorithm. We will elaborate the difference 
% between implicit and explicit ratings later.

% In the content-base method, we treat the concatenation of all
% product names belonging to a certain category as this item's 
% (notice that each item here is a category) {\em content} and 
% use the content to extract keywords as this item's features. 
% The feature weights are computed through term frequency/inverse document 
% frequency (TF-IDF) measure \cite{Salton89}, and the feature vector built 
% can be seen as this item's profile, which will be used to measure 
% the similarity between two items. In content-based method we also 
% use implicit ratings for the same purpose as in the collaborative 
% filtering method.
% We will denote the baseline content-based algorithm as BCB
% (baseline content-based) and the baseline collaborative filtering 
% algorithm as BCF (baseline collaborative filtering) for the convenience 
% of discussion.

% Explicit ratings are entered by a user directly 
% (e.g., 1 to 5 stars on JD.com), while implicit ratings are inferred 
% from other user behaviors\cite{HerlockerKTR04}. The explicit ratings in 
% JD.com is not appropriate for our construction of recommendation system 
% as we define each ``level-3 category label'' as an item, 
% because it can only represent a user's degree of satisfaction for
% a concrete product rather than this user's preference of the corresponding 
% item (category). We argue that a more appropriate rating should be inferred 
% from the user's purchase history. 
% So we define a user's implicit ratings for a certain category as the number of
% times this user has purchased products of this category
% divided by the total number of purchases. To verify that this way of define
% user preference is more appropriate, we compare the results of recommendation
% using the explicit and the implicit ratings.


% \tabref{Ratings} clears indicates that implicit ratings does a better job.

% %table
% \begin{table}[ht]
% \centering
% \caption{\label{Ratings}Implicit Ratings versus Explicit Ratings}
% \small
% \begin{tabular}{|c|c|c|}
% \hline
%  & Implicit Ratings & Explicit Ratings\\
% \hline
% BCB F1 score & {\bf 9.16\%} & 5.88\%\\
% \hline
% BCF F1 score & {\bf 17.1\%} & 11.3\%\\
% \hline
% \end{tabular}
% \end{table}

% \subsubsection{Refined Recommendation Algorithms}
\subsubsection{Refined Short Text Similarity Algorithm}
We propose a refined alogrithms to improve the performance of LSA, 
which we will denote as RLSA (refined latent semantic analysis).
% We propose two refined algorithms to improve the performance of BCB and BCF 
% respectively, which we will denote as RCB(refined content-based) and 
% RCF(refined collaborative filtering). 
%We will show how we design our RCB and RCF algorithms 
%in the following paragraphs.

Usually, short texts contain less than 20 words, which means they don't 
have enough words to represent their meaning. 
To enrich the information of the short texts, we generate latent nodes from 
our association network and take them as additional informaiton.

In the short text expansion process, 
we first get all words from the text except stop words. Then, we search the 
words in our association network pairwise and get paths between two 
words. We choose latent nodes from the path as the additional information, 
if the path is not longer than two hops. We limite the path within 
two hops is beacuse the path with longer hops means these two words have 
little relatedness and latent nodes on the path will be noises to the short text. 

%JY{Example}
Since LSA ignore the position information of the words in the short text 
and treat it as a ``bag of words''. We can directly expand the original short text 
by add in the additional information we found and get a ``enriched bag of words''. 
After the expansion, we do the LSA again on the enriched short text. 
The result shows that we improve the Pearson score by expanding the short text 
with useful information.

% Our RLSA has fellowing advantages.

% The additional information we generated is highly related to the original short 
% text. 

% In content-based methods, after every item's profile is built, 
% the similarity between two items is measured in order to recommend to 
% a user such items that are similar to what he has already purchased. 
% And in collaborative filtering methods, the similarity between two users 
% is measured to discover the ``neighbors'' of a user. 
% \footnote{People who share similar taste and preference of 
% products with a certain user are viewed as this user's neighbors.} 
% In the following, we define the general problem of measuring 
% the similarity between two entities where an entity can be either a user
% or an item. 

% Suppose we have $m$ entities and $n$ features globally in our system. 
% Each entity is described by several features, with a weight measuring the 
% significance of a feature in describing this entity. 
% We denote the significance weight feature $a$ have in describing entity $i$ 
% as $w_a^i$. Each feature then can be represented by a unit vector 
% in the $n$-dimensional vector space. We denote the unit vector for feature 
% $a$ as $\vec{f_a}$ and the vector describing entity $i$ as $\vec{e_i}$, 
% and $\vec{e_i}$ can be represented as the linear combination 
% of $\vec{f_1}$, $\vec{f_2}$ , ... , $\vec{f_n}$.

% \begin{displaymath}
% \vec{e_i} = \sum_{a=1}^n(w_a^i\vec{f_a})
% \end{displaymath}
% The similarity between entities $i$ and $j$ is computed as
% the cosine similarity $cos(\vec{e_i},\vec{e_j})$:
% \begin{eqnarray*}
% &&\cos(\vec{e_i},\vec{e_j}) = \frac {\vec{e_i}\vec{e_j}}
% {|\vec{e_i}||\vec{e_j}|} \\
% &=&\frac
% {\sum_{a=1}^n(w_a^i\vec{f_a}) \cdot \sum_{b=1}^n(w_b^j\vec{f_b})}
% {|\sum_{a=1}^n(w_a^i\vec{f_a})||\sum_{b=1}^n(w_b^j\vec{f_b})|}\\
% &=&\frac
% {\sum_{a=1}^n\sum_{b=1}^n(w_a^iw_b^j\vec{f_a}\vec{f_b})}
% {\sqrt{\sum_{a=1}^n\sum_{b=1}^n(w_a^iw_b^i\vec{f_a}\vec{f_b})}\sqrt{\sum_{a=1}^n\sum_{b=1}^n(w_a^jw_b^j\vec{f_a}\vec{f_b})}}
% \end{eqnarray*}
% In content-based methods, each item can be seen as an entity described above, 
% and each keyword can be seen as a feature. To measure two items' similarity, 
% traditional methods like BCB typically assume that $\vec{f_a}\vec{f_b}=0$ 
% if $a$ and $b$ are two different features. 
% Then $cos(\vec{e_i},\vec{e_j})$ is rewritten as 
% \begin{equation*}
% \cos(\vec{e_i},\vec{e_j}) = \frac {\vec{e_i}\vec{e_j}} {|\vec{e_i}||\vec{e_j}|}
% =\frac
% {\sum_{a=1}^n(w_a^iw_a^j\vec{f_a}^2)}
% {\sqrt{\sum_{a=1}^n(w_a^iw_a^i\vec{f_a}^2)}\sqrt{\sum_{a=1}^n(w_a^jw_a^j\vec{f_a}^2)}}
% \end{equation*}
% The assumption that two different features vectors are orthogonal 
% simply ignores the relatedness between two features, 
% which may be inappropriate when measuring two items' similarity. 
% For example, the item ``antivirus software'' and the item 
% ``laptop bag'' may share no features(keywords) at all. 
% So methods like BCB will just regard these two items as absolutely different, 
% even if ``antivirus software'' is likely to have ``software'' 
% as its feature while ``laptop bag'' probably has feature ``laptop'', 
% which is quite related to the feature ``software''.

Our RCB algorithm has the following advantage. 
Since the HMM model can measure the relatedness strength of two concepts, 
the inner product $\vec{f_a}\vec{f_b}$ can be assigned a proper value 
corresponding to the relatedness strength of feature $a$ and feature $b$, 
so it is no longer zero even if the two features are different. 
For two strongly connected features $\vec{f_a}$ and $\vec{f_b}$ 
like ``laptop'' and ``software'', we assign a relative higher value to 
$\vec{f_a}\vec{f_b}$. Thus the relatedness between ``laptop'' and 
``software'' is captured. Further, the latent relatedness information 
of ``antivirus software'' and ``laptop bag'' is also captured. 
As a result, we are able to rationally 
recommend ``antivirus software'' for the users who has purchased 
``laptop bag'' and recommend ``laptop bag'' for the users who has purchased 
``antivirus software''. Such recommendation may never be possible when 
algorithms like BCB are used. Similar examples are quite rich in 
recommendation applications and as a result our RCB algorithm outperforms 
BCB constantly, especially when the number of features extracted from 
each item are relatively low.

Similar improvements are made in RCF. In collaborative filtering methods, 
each user is treated as an entity, and each item can be seen as a feature. 
%Similarly, to measure two users' similarity, traditional methods like BCF 
%also assume that $\vec{f_a}\vec{f_b}=0$ if features $a$ and $b$ are 
%orthogonal.  $cos(\vec{e_i},\vec{e_j})$ is also be rewritten as above.
Suppose two users share no purchased products, but one of them have 
bought a lot of baby milk powder and the other quite a number of diapers. 
Traditional algorithms like BCF would decide the two user to have 
widely different preference, which is not the case because both of them 
obviously have babys at home.
Like RCB, our RCF algorithm is able to assign a value to 
$\vec{f_a}\vec{f_b}$, which can make the two users neighbors. 
Our RCF also outperforms BCF consistently, and the advantage is 
especially large when less data is available for learning 
the users' characteristic. 
Therefore the RCF algorithm is particularly useful in solving 
the common problem of ``cold start'' in recommendation systems.

\subsubsection{Evaluation Results}
In this section, we show the evaluation results about the 
two algorithms discussed above, by the Pearson score mentioned in 
\secref{sec:preprocess} as the metric. 
The ``Feature Num'' in \tabref{CB} specify the number of features 
we extract from each item, and The ``Learn Size'' in \tabref{CF} 
specify how much data we use to learn the users' characteristics, 
with 1.0 denoting all available data is used. The ``RCB Advantage'' in 
\tabref{CB} is defined as 
\[\frac{\mbox{RCB F1-score} - \mbox{BCB F1-score}}{\mbox{BCB F1-score}},
\]
which measures how much advantage RCB have over BCB. Similarly, 
the ``RCF Advantage'' in \ref{CF} is defined as 
\[\frac{\mbox{RCF F1-score} - \mbox{BCF F1-score}}{\mbox{BCF F1-score}}.
\] 

For content-based methods, we can see from Table \ref{CB} that 
RCB outperforms BCB across the board, especially when the number of features 
extracted from each item is small.

%table
\begin{table}[ht]
\centering
\caption{\label{CB}CB Methods Evaluation Results}
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
{\bf Feature Num} & 5 & 10 & 15 & 20 & 25\\
\hline
BCB F1 score& 6.72\% & 7.68\% & 8.09\% & 8.56\% & 8.72\%\\
\hline
RCB F1 score& 8.06\% & 8.77\% & 9.01\% & 9.32\% & 9.4\%\\
\hline
{\em RCB Advantage} & 19.9\% & 14.2\% & 11.4\% & 8.88\% & 7.80\%\\
\hline\hline
{\bf Feature Num} & 30 & 35 & 40 & 45 & 50\\
\hline
BCB F1 score& 8.78\% & 8.87\% & 8.97\% & 9.08\% & 9.16\%\\
\hline
RCB F1 score& 9.47\% & 9.52\% & 9.61\% & 9.63\% & 9.63\%\\
\hline
{\em RCB Advantage} & 7.86\% & 7.33\% & 7.13\% & 6.05\% & 5.13\%\\
\hline
\end{tabular}
\end{table}

For collaborative filtering methods, we can see from Table \ref{CF} 
that our RCF outperforms BCF constantly, and the advantage is especially large 
in the ``cold start'' stage, i.e. when {\em learn size} is small.
\tabref{tab:rec} shows some recommendations our RCF algorithm makes
to users who have bought only a couple of items.
 
%table
\begin{table}[ht]
\centering
\caption{\label{CF}CF Methods Evaluation Results}
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
{\bf Learn Size} & 0.1 & 0.2 & 0.3 & 0.4 & 0.5\\
\hline
BCF F1 score & 6.50\% & 9.53\% & 11.0\% & 12.2\% & 12.9\%\\
\hline
RCF F1 score & 7.66\% & 10.7\% & 11.9\% & 13.1\% & 13.5\%\\
\hline
{\em RCF Advantage} & 17.8\% & 12.3\% & 8.05\% & 7.50\% & 5.23\%\\
\hline \hline
{\bf Learn Size} & 0.6 & 0.7 & 0.8 & 0.9 & 1.0\\
\hline
BCF F1 score& 13.5\% & 14.1\% & 15.1\% & 16.5\% & 17.1\%\\
\hline
RCF F1 score& 14.1\% & 14.6\% & 15.5\% & 16.7\% & 17.2\%\\
\hline
{\em RCF Advantage} & 4.27\% & 3.55\% & 2.54\% & 1.26\% & 0.99\%\\
\hline
\end{tabular}
\end{table}

%\begin{table*}[ht]
%\centering
%\caption{\label{tab:rec}Examples of RCF Results}
%\small
%\begin{tabular}{|l|l|l|}
%\hline 
% & User Bought & We Recommend Correctly \\ \hline  \hline
%1 & speaker, main board & casing, power supply \\ \hline   
%2 & cdrom drive, CPU & phone, car accessories, monitor, belt, casing, ... \\ \hline 
%3 & scanner, webcam & monitor, USD flash drive, cell phone \\ \hline  
%4 & graphics card, hard drive & cdrom drive, power socket, digital products, memory, CPU, ...  \\ \hline
%\end{tabular}
%\end{table*}

\begin{table*}[ht]
\centering
\caption{\label{tab:rec}Examples of RCF Results}
\small
\begin{tabular}{|l|l|l|}
\hline 
 & User Bought & We Recommend Correctly \\ \hline  \hline
1 & Telephones, vacuum cleaners, home theater, &
	Quilts, memory cards, automotive supplies, monitor, \\
& purifiers, flat screen TV & mouse, phone, mouse pads \\ \hline
2 & Gaming devices, specialty, candy, & \\
 & ginseng, coffee, meat, biscuits, snacks, & Mobile hard disk, shampoo, cleaners, headphones \\
& drinks, outdoor clothing & \\ \hline
3 & Tablet, DVD player & Routers, juicer, U disk \\ \hline 
4 & Tents, mobile hard drives, cables, & Speaker, CPU, mouse, power supply,  \\
 & routers, radiators, home theater, digital cameras, & optical drive, keyboard, chassis \\
 & memory cards, outdoor equipment & \\ \hline
5 & Automotive supplies, hair dryer & U disk, memory card \\ \hline
6 & Fan, creative gifts, mobile phone sets, & CPU, network card, power supply, cables, digital cameras, \\
& routers, plastic cups, gold and silver, U disk &  memory cards, motherboards, radiator, hard drives, mobile phones \\ \hline
7 & Cleanser, monitors, eye care, mat, &Speakers, skin, memory, CPU, power supply,\\
 & washing machines, card, mouse, mask, & fan, drive, chassis, motherboard, hard drive \\
 & cream, notebook accessories, toner &  \\ \hline
8& Whiteboard, kitchen utensils, humidifiers, scissors&	Monitors, chassis, hard drives, power supplies\\ \hline
\end{tabular}
\end{table*}
