\section{Related Work}
\label{sec:related}
In this section, we introduce a number of studies for relatedness measuring.
Some researchers use Wikipedia to solve the relatedness measuring problem.
\cite{ZhangX:2010,Ito:2008,Gabrilovich:2007}
Zhang et al. \cite{ZhangX:2010}propose a method using a generalized maximum flow
to measure implicit relations between two Wikipedia concepts, and argue that 
their method is able to measure the strength of a relation appropriately by using 
three important factors: distance, connectivity, and co-citation.
They also argue that another important aspect of their method 
is mining elucidatory objects, that is, objects constituting a relation, 
which can help people understand relations between concepts.
Ito et al. \cite{Ito:2008}propose an approach that leverages global statistical information of the whole Wikipedia to compute semantic relatedness among concepts (disambiguated terms) by analyzing co-occurrences of link pairs in all Wikipedia articles. They argue that In Wikipedia, an article represents a concept and a link to another article represents a semantic relation between these two concepts. So the co-occurrence of a link pair indicates the relatedness of a concept pair. Then they design algorithms based the link information in Wikipedia to solve the accuracy and
scalability problems existing in previous methods.
The problem with these two methods described above is that they just use link information in Wikipedia, and the links is much more sparse compared with TB and SL used in our MindDrifter. In addition to incorporating more knowledge information, another advantage of our MindDrifter lies in that we simulate the real mind drifting process based on the transition of context, which was never considered in these two methods described above.
Other kind of relatedness measuring is also searched. For example, Gabrilovich et al. \cite{Gabrilovich:2007} focus on computing semantic relatedness
of natural language texts. They propose a method that represents the meaning
of texts as a vector in a high-dimensional space of concepts derived from Wikipedia,
and then use conventional metrics like cosine similarity to 
assess the relatedness of two text vectors.

Besides Wikipedia, other corpora are also used in corpus-based methods.\cite{Chen:2006,Bollegala:2011}
Chen et al. \cite{Chen:2006} propose a web search with double checking
model to explore the web as a live corpus for the associations' mining. 
Bollegala et al. \cite{Bollegala:2011}propose an empirical method to estimate semantic similarity using page counts and text snippets retrieved from a web search
engine for two words. The optimal combination of different relatedness information they mined from the web is learned using support vector machines.

Apart from corpus-based methods, the knowledge-based methods  
are also explored in this field.
Resnik \cite{Resnik:1995}presents a new measure of semantic similarity
in an is-a taxonomy, based on the notion of information content.
The author argue that one of the advantage his method have over edge counting 
method is that it is not sensitive to the problem of varying link distances. 
In addition, by combining a taxonomic structure with empirical
probability estimates, it provides a way of adapting
a static knowledge structure to multiple contexts.
Agirre et al. \cite{Agirre:2009} presents and compares WordNet based
and distributional similarity approaches and then propose a combination of 
these two approaches.
Morris et al. \cite{Morris:1991} argue that in text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning.
These lexical chains are a direct result of units of text being ``about the
same thing,'' and finding text structure involves finding units of text that are about the same thing. Then they a method to compute the chains and use it to know the structure of the text.
Strube et al. \cite{Strube:2006}investigate the use of Wikipedia for computing
semantic relatedness measures and shows that Wikipedia provides a suitable
encyclopedic knowledge base for extracting semantic information.

Alvarez et al. \cite{Alvarez:2007} present a novel algorithm for scoring the semantic similarity(SSA) between words. Given two input words $w_1$ and $w_2$, SSA exploits their corresponding concepts, relationships, and descriptive glosses available in WordNet in order to build a rooted weighted graph. The output score is calculated by exploring the concepts present in the rooted weighted graph and selecting the minimal distance between any two concepts $c_1$ and $c_2$ of 
$w_1$ and $w_2$ respectively.
Li et al. \cite{Bollegala:2011}propose a lightweight and effective approach for measuring
semantic similarity using a large scale semantic network automatically
acquired from billions of web documents, which can accurately compute
the semantic similarity between terms with multi-word expressions(MWEs) and ambiguity.
Pirro et al. \cite{Pirro:2009} present a method exploiting some notions of the feature-based theory of similarity and translates it into the information theoretic domain, which leverages the notion of Information Content (IC). In particular, the proposed metric exploits the notion of intrinsic IC which quantifies IC values by scrutinizing how concepts are arranged in an ontological structure.
