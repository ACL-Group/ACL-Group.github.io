\section{Conclusion}
\label{sec:conclude}

In this paper, we present a novel hybrid framework which combines web mining 
and audio signal processing for recognizing auditory scene. This framework
is unsupervised in the sense that no manual labeling of the audio training
data is needed. Instead of training audio scene data directly, like most
existing work does, we train GMMs on primitive audible events which are
downloaded from online sound search engines. Then the framework leverages
large text corpus of online TV and movie transcripts to mine statistical
models between a scene and its constituent events. Experiments for
10-scene classification showed promising results of 42\% accuracy which is
higher than the baseline and on par with state-of-the-art methods 
reported at recent
IEEE AASP scene recognition challenge.
%
%Our system does not need manually labeled training data. Instead, we analyzed the drama scripts which can easily collected from Internet. At the same time, we used some knowledge base and sound search engine to build our audible events set, and downloaded and labeled audio clips automatically. When we analyzing the text data, we also built a probabilistic model between scenes and events. Then we trained GMM models for audible events. Using these models, we can infer scenes from an audio clip. In our experiments, our system can achieve $37\%$ accuracy in a 10-scene classification work.
%
%TODO(kenny): do we need to write future work here?

%\begin{itemize}
%\item Enlarge text corpus to get more information.
%\item Make the quality of training data higher, including audio clips and text corpus.
%\item Use more relations found by Stanford NLP.
%\item Besides using only events to recognize scene, take some global features in testing clips into consideration for recognition.
%\item Use some other model like universal background model (UBM) to reduce the impact of noise.
%\end{itemize}
