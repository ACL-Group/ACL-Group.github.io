\section{Preliminary Results}
\label{sec:eval}

In this section, we present some preliminary results of the system from three
experiments.  The first two experiments test the precision and recall 
of the two main functions, namely title recognition and list extraction,
respectively.
%The third one tests the time efficiency and scalability of the whole system.
In the last experiment, we performed large-scale extraction 
on a massive distributed computing platform.
%The first two experiments are done on a PC with 3GB RAM and 
%2.7GHz Dual-Core CPU. 

%\subsection{Title Recognition}
%We test the performance of the CRF model in this section.
For title recognition, we build a benchmark with 2000 random web page titles,
all of which contains at least one number.
50 of these are ``top-$k$ like'' and are treated as ground truth.
%The classifier returns 60 titles, 46 of which are 
%true positives. 
The precision of the classifier is 76.7\%, 
while the recall is 92\%.
The high recall ensures that most of the real ``top-$k$'' pages can pass
through this stage.
%The 14 false positives are mostly due to misunderstanding of 
%some regular phrases, such as ``six pixels of separation'' 
%and ``Fifa 10 cheat codes''.
%We can filter these errors in the following stages.

%\subsection{List Extraction}
%In this section, we test precision and recall of extraction algorithm 
%which includes the candidate picker and ``top-$k$'' ranker.
%which are realized by the candidate picker and top-$k$ ranker.
In the second experiment, the input is the DOM representation of 
100 correct ``top-$k$'' pages
as well as the correct title analysis result (number $k$ and concept set). 
%As we discussed in Section\ref{sec:picker}, we provide two algorithms:
%{\bf Default} and {\bf +Pattern}. 
\cut{%%%%%%%%%%%%%%%%%%%%%%%%%
The result is shown in Table \ref{tab:listRes}.

\begin{table}
\centering
\begin{tabular}{|c||c|c|} 
\hline
Algo & Precision & Recall\\\hline
Default & 94.4\% & 85.0\% \\
Def+Patt & 97.4\% & 75.0\% \\
\hline
\end{tabular}
\caption{Results for List Extraction}
\label{tab:listRes}
\end{table}
}%%%%%%%%%%%%%%%%%%%%%%%%%
Both algorithms obtain very high precision, 
with {\em Def+Patt} at 97.4\%. 
In terms of recall, {\em Default} is better at 85\% 
since {\em Def+Patt} uses stricter patterns.

%\subsection{Big Data}
%One of the purpose for our system is to extract the
%``top-$k$ lists'' from the whole web.
In the last experiment, we apply the framework on 
1/10 of a high-frequency web snapshot from Bing,
which are about 160 million.
\cut{%%%%%%%%%%%%%%
According to the estimated sum of ``top-$k$'' pages,
there should be in total 224,000 ``top-$k$'' pages (ground truth)
in the input data. The detailed result is shown in Table \ref{tab:cosmosRes}.

\begin{table}
\centering
\begin{tabular}{|c||c|c|c|c|}
\hline
Algo & Total & Precision & True Positive & Recall\\\hline
Default & 256,231 & 66.0\% & 169,112 & 75.5\% \\
Def+Patt& 142,886 & 90.4\% & 129,169 & 57.7\% \\
\hline
\end{tabular}
\caption{Results for Big Data}
\label{tab:cosmosRes}
\end{table}

From Table \ref{tab:cosmosRes}, 
}%%%%%%%%%%%%%%%%%%
Algorithm {\em Def+Patt}
achieves 90.4\% precision and 57.7\% recall.
We measured the recall by taking a smaller sample of the web corpus, 
manually checking the pages returned after the title recognition.
%though the recall is lower than {\em Default}. 
%Nevertheless,
%{\em Def+Patt} still manages to obtain a large number of ``top-$k$'' lists.
If applied to the whole web corpus,
{\em Def+Patt} is expected to harness over 1.4 million
``top-$k$ list'' with over 90\% precision.

\cut{%%%%%%%%%%%%%%%%%%%
%\subsection{The Sum of Top-K Pages }
%\label{sec:topKSum}
When calculating the recall of the overall system, 
we must estimate the total number of ``top-$k$'' pages.
Given that the recall of the title classifier is 92\%, we use it to 
classify 1.6 million pages (about 1/1000 of total pages in the web), 
and obtain 5,994 pages.
%By manually checking these pages, 
2,061 of them are real ``top-$k$'' pages. 
%Considering 8\% is missed by the classifier, 
Therefore total number of ``top-$k$'' pages in the sample is 
$2,061/0.92\approx2,240$, 
and the total number of ``top-$k$'' pages in the whole web 
is approximately 2.2 million.
Assuming there are 1.6 billion web pages in total, 
the proportion of ``top-$k$'' pages is
approximately 
%$2,240,000 \div 1,600,000,000\approx0.0014=$
1.4\textperthousand.
}%%%%%%%%%%%%%%%%%%%%%

