\section{Introduction}
\label{sec:intro}

Commonsense causal reasoning, a central challenge in artificial intelligence,
has been actively studied by both linguists and computer scientists.
It aims at understanding the general causal dependency
between common events or actions.
Such understanding essentially amounts to measuring the {\em plausibility} of
one event statistically leading to another.

In this paper, we focus on commonsense causal reasoning between short texts,
which is crucial to many natural language processing applications,
such as text understanding, question answering, etc.
To further illustrate commonsense causal reasoning problem,
we present a question from Choice of Plausible Alternatives (COPA)
evaluation~\cite{roemmele2011choice}
which consists of one thousand multiple-choice questions requiring
commonsense causal reasoning to answer correctly.
Specifically, each question is composed of a premise and two
alternatives, where the task is to select the more plausible
alternative as a cause (or effect) of the premise.

\begin{example}
\label{ex:copa}
\noindent
\begin{itemize}
\item[] Premise: \emph{I knocked on my neighbor's door.} What
happened as an effect?
\item[] Alternative 1: \emph{My neighbor invited me in.}
\item[] Alternative 2: \emph{My neighbor left her house.}
\end{itemize}
\end{example}

This example shows that a key challenge is
harvesting causality knowledge that the action of
{\em knocking} is more likely to cause that of {\em invitation}
than that of {\em leaving}.

Existing work on harvesting causality knowledge has been conducted in
two directions.
First direction, pursuing high {\em precision} of
causality knowledge, usually requires expensive manual efforts.
For example, ConceptNet~\cite{HavasiSALAM10} leverages human efforts
to encode causal events as common sense knowledge.
Khoo et al.~\shortcite{khoo2000extracting} hand-crafted
lexical syntactic patterns from the dependency tree to recognize
causal knowledge.  Rink et al.~\shortcite{rink2010learning}
automatically generated such patterns encoded with
lexical, syntactic and semantic information to extract
causality knowledge, but the approach requires initial training data,
which determines the quality and quantity of the generated patterns.
Such iterative approach also tends to bring in ill-formed patterns
and unreliable results.
Other approaches reported in~\cite{gordon2012copa} build on deeper
lexical syntactic analysis of sentences,
to identify knocking and inviting in our example as
\emph{events}, and determine whether causality between two events hold.
However, knowledge acquired by these approaches,
based on human and in-depth analysis, inherently lack coverage.

Second direction, harvesting causality from large
text corpus with a data-driven approach, seeks to overcome
the limitation in breadth of the first direction.
The best known approach~\cite{gordon2011commonsense} here,
outperforming the approaches in the first direction~\cite{gordon2012copa},
leverage personal stories as a source of information about causality and use
Pointwise Mutual Information (PMI) statistics \cite{church1990word}
between words in the premise and alternative, to identify the pairs with
high correlation. More specifically, under this framework,
words $A$ and $B$ are considered causal, if $A$ is frequently co-located
with and succeeded by $B$ in text.
In our example, while we expect the words \emph{knock} and \emph{invite}
to co-occur frequently in narrative text, which indicates a potential causality;
the words \emph{door} and \emph{house} are also observed frequently together.
Misidentifying both as causality may incorrectly give the second
alternative as the result.
Thus implicit causality from lexical co-occurrence alone is noisy.
Therefore, current data-driven approaches may address the coverage
limitation of causality acquisition, but suffer from low precision in return.

In contrast, our goal is to pursue both coverage and precision
in modeling causality. Combining in-depth lexical syntactic analysis with
personal stories is not an option because given the limited availability
of such data,
the amount of extractable precise causal knowledge would be much smaller.
To pursue coverage, we propose a data-driven approach of harvesting
a comprehensive \emph{term-based causality network} from a large web corpus.
Such network would encode tens of thousands of unique terms,
and tens of millions of causality evidences, much larger scale than other
existing causal knowledge bases (more comparisons in \secref{sec:causalnet}).

To pursue precision, we leverage explicit causal indicators
(e.g., cause, because), to prune substantial non-causal co-occurrences and
introduce separate cause and effect roles to every term in our
causality knowledge.

With the differentiated cause and effect roles of causalities encoded in text,
our causality network carries more reasonable and
directional {\em causal co-occurrences}, e.g., from the corpus,
$knock$ causes $invite$ $m$ times, while $invite$ causes
$knock$ $n$ times. Furthermore, we distinguish sufficiency causality (i.e.,
$A$ is the sufficient condition to cause $B$) from necessity causality
(i.e., $A$ is the necessary cause of $B$).
These refined representations of terms and their causal relations
give rise to new ways of computing causality strength between
terms and between texts,
thus yields better results in commonsense causal reasoning.

In sum, this paper makes the following contributions:
\begin{itemize}
\item We harvest a term-based causality co-occurrences network from large
web text by leveraging causal cues (see \secref{sec:network});
\item We develop a new statistical metric that captures causal strength
between any two pieces of text (see \secref{sec:causalstrength}
and \secref{sec:reasoning});

\item Our proposed framework achieves state-of-the-art accuracy of $70.2\%$
on the difficult COPA task, outperforming all existing methods by subtantial margins.
Further evaluation on causality detection between phrases also
demonstrate the advantage of the proposed framework (see \secref{sec:eval}).
\end{itemize}
