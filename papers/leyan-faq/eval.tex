\section{Experiments}
\label{sec:eval}
In this section, we first introduce the metric to evaluate each module of the pipelines and the entire QA pairs.
We then talk about the human baseline and experimental setup in this paper.
%We then discuss the result of each module to select the best combination for the pipeline method and the filtering method.
%Finally, we will discuss the results of pipeline methods and filtering methods, and the performance of the aspect.

\subsection{Experimental Setup}
We use BERT tokenizer to do the tokenization for BERT retrieval, BiLSTM-CRF and UNILM, and use Stanford CoreNLP~\cite{manning-EtAl:2014:P14-5} to do the tokenization in other models.
The BERT retrieval model is fine tuned with the learning rate of 3e-6 and 3 epochs.
For LSTM-CRF model, we use BiLSTM with hidden state size of 128.
The Adam optimizer is adapted with the learning rate of 1e-2 and the model is trained for 300 epochs.
For pointer network, we follow the parameters set as Subramanian et al's~\shortcite{subramanian2017neural}.
The Seq2seq model for question generation follows the parameters set as Zhao et al~\shortcite{zhao2018paragraph}. We add additional 1-layer LSTM encoder and self-gated layer to encode aspect. The hidden size of LSTM and self-gated layer is the same as the hidden size which is used for encoding paragraph.
The UNILM model for question generation is fine tuned with the learning rate of 5e-5 and 8 epochs with half-precision training.