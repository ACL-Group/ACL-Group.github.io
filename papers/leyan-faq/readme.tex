\section{System Guide}
\label{sec:readme}

Here we give a simple introduction about how to use our codes.

\subsection{Paragraph Retrieval}
\subsubsection{BERT Fine-tuning}
BERT is a pre-trained model and we only need to fine tune it for classification.
The data is under the path ``data/data\_with\_aspect''. 
When training a fine-tuned BERT, you should specify the path of the data in ``bert/ranking.sh'' and run ``bash bert/ranking.sh''.
\subsubsection{TF-IDF, BM25 and BERT}
We implemented the retrieval methods of TF-IDF, BM25 and BERT in the folder ``paragraph\_retrieval/Retrieval''.
Run ``python retrieval\_evaluation.py'' and you can get the retrieval results.
\subsection{Answer Extraction}
\subsubsection{NER}
In "prepro.py", "merge\_raw" function will create the training and test files. Since NER model is pretrained, we don't need to train this model. NER model is from spacy. Just run "python main.py --desc [model name] [--n\_epoch [n\_epoch]] --extract" to test model on test set. Detailed setting can be found in "config.py". You can find more details in readme file.
\subsubsection{BiLSTM-CRF}
For data preprocess, it uses BERT tokenizer to do the tokenization.
First, you need to run ``python make\_NER\_data\_with\_hint.py'' to obtain the tokenized data and transform the answer spans into "BIO" labels.
Then, you should run ``python run\_bilstm\_crf.py  --data\_dir [data path]  --output\_dir [model save path] --do\_train  --use\_sent --max\_seq\_length [max sequence length] --train\_batch\_size [batch size for train]'' to train the model and run ``python ner/run\_bilstm\_crf.py \
    --data\_dir [data path] --output\_dir [model save path]     --do\_test     --eval\_all\_checkpoints     --use\_sent'' to evaluate the model.
\subsubsection{Pointer Network}
First, you need to download glove word embedding into the specific directory.
Then, run ``python build\_data.py'' to obtain the preprecessed data files.
You can train the model by running ``python train.py'' and evaluate the model ny running ``python evaluation.py''.

\subsection{Question Generation}
\subsubsection{Seq2Seq}
The source data is stored in "squad\_hint" directory. First you need to run "process\_data.py" in "data" directory. You may need to download some pretrained word embedding in this step. After data process, you need to specific parameters in "config.py". For example "use\_aspect" determined whether to use aspect in the model. Set "train" as "True" to train the model and "False" to test model on test set. You can find more details in readme file.

\subsubsection{UNILM}
UNILM is a pretrained language model. Here we just use it without changing the model structure. The source data is stored in ``src/qg\_data/raw" directory. In ``data\_process.py", ``process\_data\_hint" and ``process\_data\_no\_hint" will generated the training data with/without aspect. ``process\_data\_test\_annoted" and "process\_data\_test\_annoted\_no\_hint" will generated the testing data with/without aspect. The others are the same as the original unilm model's. Except two differences: we train our model with half-precision(which is much faster) and we train our model in 8 epochs(The result is almost the same compare with 10 epochs).You can find more details in readme file.
\subsection{End-to-end}
\subsubsection{Pipeline Method}
You can run ``python evaluation\_for\_annotation.py'' or ``python evaluation\_for\_annotation2.py'' to get the pipeline results.
There are several parameters need to set before running. If ``do\_decode'' is true, you can generate the QA pairs, and if it's set as false, you can obtain the evaluation results.
Different individual modules can be set in parameter list ``retrieval\_model'', ``an\_model'' and ``qg\_model''.

\subsubsection{Filtering Method}
For the filtering model with retrieval module, you can only run ``python filter.py --qa\_path [path of QA generated by pipeline] --filter\_path [path to store the QAs after filtering]'' after pipeline.
For the filtering model without retrieval module, you should first run ``python filter\_baseline.py --qa\_path [path to store the generated QAs]'' to obtain the QA pairs for all paragraphs, then run ``python filter\_and\_eval.py --qa\_path [path to store the generated QAs] --filter\_path [path to store the QAs after filtering]'' to remove the irrelevant QAs and evaluate the results.
You can find more details in readme file.