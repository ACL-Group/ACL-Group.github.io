\section{Results and Analysis}
\label{sec:res}

In this section, we will first discuss the result of each module to select the best combination for the pipeline method and the filtering method.
We then discuss the results of pipeline methods and filtering methods qualitatively and quantitatively.

\subsection{Results of Individual Module from Pipelines}
The pipeline method and filtering method are designed as the QA pairs generation frameworks which take the state-of-the-art of different areas into consideration.
We have separately introduced the methods of each module in Section \ref{sec:method}.
The comparison of the results for each module is listed in Table \ref{tab:retrieval}, \ref{tab:answer}, \ref{tab:QG}.

\subsubsection{Paragraph Retrieval}
From Table~\ref{tab:retrieval}, TF-IDF and BM25 have similar performance.
Compared to human performance, they have lower F1 scores but higher MRR scores, which represents the traditional retrieval methods are still good at ranking.
BERT is much better than the former two traditional methods and outperforms the human baseline. 
Here we just use the base model of BERT due to the limit of the GPU resource. 

%All retrieval methods are better than the human baseline, which represents machines are better at large-scale information retrieval.

\begin{table}[th]
\scriptsize
\centering
\begin{tabular}{ccc}
\toprule[1.5pt]
\textbf{} & \textbf{F1@K} & \textbf{MRR@K} \\ 
\midrule
\textbf{TF-IDF} & 54.94 & 57.22 \\ 
\textbf{BM25} & 55.86 & 58.45 \\ 
$\text{\textbf{BERT}}_{\text{\textbf{base}}}$ & \textbf{76.50} & \textbf{73.48} \\ 
\midrule
\textbf{Human Baseline} & 63.83 & 46.87\\
\bottomrule[1.5pt]
\end{tabular}
\caption{\label{tab:retrieval} The results of paragraph retrieval module.}
\end{table}


\subsubsection{Answer Extraction}

From Table~\ref{tab:answer}, the NER model has a high recall and low precision. Because the largest proportion of the answers in our dataset are entities. We keep all ner results of the NER model, so the precision is low. BiLSTM-CRF gets the maximum recall but also performance bad on precision. We think the reason is in the training part we only use the sentences with answers. This may make the model predict answer tags for every sentence. The pointer network gets the highest F1 score. But the recall is the lowest. Because it's a generative model and it's difficult for it to generate the same answer in the ground truth.

None of the three methods has higher precision than human but they outperforms on recall. 
Human can easily hit the interesting phrases in paragraphs but hard to cover all of the ground truth.
This also shows that it is difficult for us to find suitable features for extracting answers but there is still large room for precision improvement.

\begin{table}[th]
\scriptsize
\centering
\begin{tabular}{cccc}
\toprule[1.5pt]
\textbf{} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} \\ 
\midrule
\textbf{NER} & 20.63 & 14.58 & 35.26 \\ 
\textbf{BiLSTM-CRF} & 31.17 & 22.50 & \textbf{50.72} \\ 
\textbf{Ptr} & \textbf{32.61} & \textbf{32.03} & 33.21 \\ 
\midrule
\textbf{Human Baseline} & 25.82 & 35.75 & 28.42\\
\bottomrule[1.5pt]
\end{tabular}
\caption{\label{tab:answer} The results of answer extraction module. \textbf{NER} is the entity tagging model. \textbf{BiLSTM-CRF} is the combination of BiLSTM and CRF. \textbf{Ptr} is the pointer network.}
\end{table}

%\begin{table}[th]
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%\textbf{} & \textbf{BLEU-1} & \textbf{BLEU-2} & \textbf{BLEU-3} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE-L} \\ \hline
%\textbf{Seq2seq} & 43.93 & 28.24 & 20.17 & 14.92 & 19.97 & 43.84 \\ \hline
%\textbf{UNILM} & 49.50 & 34.58 & 26.30 & 20.65 & 24.37 & 49.36 \\ \hline
%\textbf{Seq2Seq+Aspect} & 45.32 & 29.09 & 20.77 & 15.27 & 20.40 & 43.58 \\ \hline
%\textbf{UNILM+Aspect} & 51.42 & 36.57 & 28.12 & 22.27 & 25.63 & 50.85 \\ \hline
%\end{tabular}
%\caption{\label{tab:QG} The results of question generation module.}
%\end{table}

\subsubsection{Question Generation}

From Table~\ref{tab:QG}, we find seq2seq with aspect and UNILM with aspect both have better performance than the original model, which means the aspect can help the question generation model to generate questions closer to the targets.
UNILM is a very strong model whose performance is much better than seq2seq and similar to human's.
\begin{table}[th]
\scriptsize
\centering
\begin{tabular}{cccc}
\toprule[1.5pt]
\textbf{} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{ROUGE-L} \\ 
\midrule
\textbf{Seq2seq} & 14.92 & 19.97 & 43.84 \\ 
$\text{\textbf{Seq2seq}}_{\text{\textbf{Aspect}}}$ & 15.27 & 20.40 & 43.58 \\ 
\midrule
\textbf{UNILM} & 20.65 & 24.37 & 49.36 \\ 
$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$ & \textbf{22.27} & \textbf{25.63} & \textbf{50.85} \\ 
\midrule
\textbf{Human Baseline} & 20.22 & 26.44 & 47.72\\
\bottomrule[1.5pt]
\end{tabular}
\caption{\label{tab:QG} The results of question generation module. \textbf{Seq2Seq} is the seq2seq model with gated self-attention and copy mechanism. $\text{\textbf{Seq2seq}}_{\text{\textbf{Aspect}}}$ is the seq2seq model with aspect. $\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$ is the UNILM model with aspect.}
\end{table}

\subsection{Results of End-to-end QA Pairs Generation}
\label{sec:end2end}
In this paper, pipeline and filtering methods are chosen for end-to-end QA pairs generation task.
%We will discuss the results qualitatively and quantitatively here.
In Table \ref{tab:allres}, we display the results of these two methods.
Since BERT is much better than the other two methods, we just use BERT in retrieval step. 
In filtering experiment, we only use UNILM as the generation model for the same reason.
%Due to the limit of space, we 
%The discussion is as follows.


\begin{table*}[h]
\scriptsize
\centering
\begin{tabular}{cccccccccc}
\toprule[1.5pt]
 & \multicolumn{3}{c}{\textbf{J-BLEU4}} & \multicolumn{3}{c}{\textbf{J-METEOR}} & \multicolumn{3}{c}{\textbf{J-ROUGE}} \\ 
 & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\ 
 \midrule
 \textbf{BERT+BiLSTM-CRF+Seq2seq} &  0.15  &  0.62  &  0.24  &  1.58  &  5.50  &  2.46  &  2.02  &  6.96  &  3.13 \\
 \textbf{BERT+BiLSTM-CRF+}$\text{\textbf{Seq2seq}}_{\text{\textbf{Aspect}}}$ & 0.19  &  0.71  &  0.31  &  1.62  &  5.62  &  2.51  &  2.04  &  6.98  &  3.16 \\
\textbf{BERT+Ptr+Seq2seq} & 0.65  &  0.50  &  0.57  &  3.94  &  4.15  &  4.04  &  4.59  &  4.72  &  4.66  \\ 
\textbf{BERT+Ptr+}$\text{\textbf{Seq2seq}}_{\text{\textbf{Aspect}}}$  & 0.52  &  0.45  &  0.48  &  3.84  &  4.06  &  3.95  &  4.55  &  4.79  &  4.67   \\ 
\textbf{BERT+Ptr+UNILM} & 1.38  &  1.37  &  1.37  &  4.79  &  4.89  &  4.84  &  6.67  &  7.04  &  6.85 
  \\ 
\textbf{BERT+Ptr+}$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$ & 1.49  &  \textbf{1.41}  &  1.45  &  5.02  &  5.17  &  \textbf{5.10}  &  7.06  &  7.51  &  \textbf{7.28}  \\ 
\midrule
\textbf{Ptr+UNILM+Filtering} & 0.72   &  1.26  &  0.92   & 2.78 &  \textbf{5.23}  & 3.63  & 3.71 &   \textbf{7.79} &  5.02  \\ \midrule
%\textbf{Ptr+}$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$\textbf{+Filtering}& 1.10 & 1.62  &  1.31   & 2.77  &  \textbf{5.39} &  3.66 &  3.93  &  7.87   & 5.25  \\ \hline
\textbf{BERT+Ptr+UNILM+Filtering} & 2.06 & 1.12 & 1.45 & 6.36 &  3.76  & 4.73  & 8.32 & 5.19  & 6.39  \\ 
\textbf{BERT+Ptr+}$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$\textbf{+Filtering} & \textbf{2.23}  &  1.12  &  \textbf{1.49}  &  \textbf{6.73}  &  3.79  &  4.85  &  \textbf{8.74}  &  5.28  &  6.58   \\ 
\bottomrule[1.5pt]
\end{tabular}
\caption{\label{tab:allres} The result of pipeline framework in first four columns and filtering framework in last four columns.}
\end{table*}

\subsubsection{Different Combination of Pipeline Method}
The results in the first four rows show that BiLSTM-CRF performs worse than the pointer network in the pipeline method, which also follows the results of answer extraction. 
%Therefore, we just use pointer network in the UNILM pipelines and from the results, we can see the combination of BERT, pointer network and UNILM with the aspect is the best. Each of them also has the best performance in individual module evaluation.
Therefore, we just use pointer network in the UNILM pipelines.
Among all the pipeline results, the combination of \textbf{BERT+Ptr+}$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$ which has the best performance in individual module evaluations is undoubtedly the best.

\subsubsection{Using Filtering with Retrieval}
If we use filtering at the end of the pipeline method, the precision is better but the recall is lower. 
Because the filter can remove the QA pairs which is irrelevant to the aspect. 
However, the filter also removes some relevant pairs which leads to a lower recall.
%Overall, the filtering method can get better performance on F1 score and precision but worse performance on recall.

\subsubsection{Using Filtering without Retrieval}
%If we don't use retrieval in the first step, the final performance is the worst in precision.
%Without retrieval the pipeline model will generate QA pairs for each paragraph, so the recall is lower than filtering with retrieval.
Without retrieval the pipeline model will generate QA pairs for each paragraph.
The filter here is not strong enough to remove all the irrelevant QA pairs, so the final performance of this method is the worst in precision.
Instead, its recalls have been greatly improved for the same reason.
In most metrics, this method has the best performance on recall.

%\begin{table*}[th]
%\small
%\centering
%\begin{tabular}{cccccccccc}
%\hline
% & \multicolumn{3}{c}{\textbf{J-BLEU}} & \multicolumn{3}{c}{\textbf{J-METEOR}} & \multicolumn{3}{c}{\textbf{J-ROUGE}} \\ 
% & \textbf{F1} & \textbf{Precison} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} \\ \hline\hline
%\textbf{BERT+Ptr+Seq2seq} & 0.83 & 0.90 & 0.76 &  &  &  & 6.85 & 6.91 & 6.79 \\ 
%\textbf{BERT+Ptr+}$\text{\textbf{Seq2seq}}_{\text{\textbf{Aspect}}}$  & 0.79 & 0.83 & 0.75 &  &  &  & 6.91 & 6.96 & 6.85 \\ 
%\textbf{BERT+Ptr+UNILM} & 2.23 & 2.22 & 2.23 &  &  &  & 9.80 & 9.67 & 9.93 \\ 
%\textbf{BERT+Ptr+}$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$ & 2.30 & 2.35 & 2.25 &  &  &  & \textbf{10.00} & 9.92 & \textbf{10.08} \\ \hline
%\textbf{Ptr+UNILM+Filtering} & 0.92 & 0.72 & 1.26 &  &  &  & 5.02 & 3.71 & 7.79 \\ 
%\textbf{Ptr+}$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$\textbf{+Filtering}& 1.31 & 1.10 & 1.62 &  &  &  & 5.25 & 3.93 & 7.87 \\ \hline
%\textbf{BERT+Ptr+UNILM+Filtering} & 2.18 & 2.77 & 1.80 &  &  &  & 9.31 & 11.45 & 7.85 \\ 
%\textbf{BERT+Ptr+}$\text{\textbf{UNILM}}_{\text{\textbf{Aspect}}}$\textbf{+Filtering} & 2.32 & 3.14 & 1.84 &  &  &  & 9.39 & \textbf{11.99} & 7.71 \\ \hline
%\end{tabular}
%\end{table*}

