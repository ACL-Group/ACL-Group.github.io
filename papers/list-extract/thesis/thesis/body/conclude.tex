\chapter{Conclusion}
\label{sec:conclusion}
 %
%In this paper, we first make some definition for our problem.
%A ``top-$k$'' list is a list of $k$ instances of a particular topic or concept. A ``top-$k$'' page is a page that contains solely one ``top-$k$'' list, while a ``top-$k$ like'' title is a title that is likely to be of a ``top-$k$'' page.
%
%In general, our task is to extract ``top-$k$'' lists from web pages. In our implementation, we divide it into three subtasks.


In this paper, we define a novel list extraction problem,
in which, we aim at recognizing, extracting and understanding ``top-$k$'' lists from web pages.
This problem is distinctive from other data mining task, because compared to other structured data,
``top-$k$'' lists are clearer, easier to understand and more interesting for readers.
Besides the advantages above, ``top-$k$'' lists are of great importance in knowledge discovery and fact answering for the reason that there are millions of ``top-$k$'' lists around on the web. With the massive knowledge stored in those lists, we can enhance the instance space of Probase. As another application, we are able to build a search engine for ``top-$k$'' lists as an effective fact answering machine.

As a solution to this ``top-$k$'' problem, we present our system.
The system mainly performs three tasks. First is to recognize ``top-$k$ like'' title, which is done by the title classifier. In the classifier, we use a CRF-based model, trained from 6000 labeled titles, to identify whether a title is ``top-$k$ like'' and obtain necessary information.
Second is to extract ``top-$k$'' lists from the page body. To do this, we collect candidate lists based on tag path and the number ``k'', and select the best one as the result according to Equation \ref{equ:fScore}. Third is to process and understand the content of ``top-$k$'' lists, including inferring the inner structure and conceptualize each column list. This is done by the content processor.

%The main mechanism of the system is to first identify a ``top-$k$'' page by recognizing its title,
%then to extract the ``top-$k$'' list based on structural features(tag path) and the meaning of list content(Probase).
%Our evaluation shows that the system performs well in both accuracy and efficiency,
%it is able to scale to large web corpus and obtain high-quality ``top-$k$'' lists,
%which is important in knowledge discovery and fact answering.

From working on this project, there are many benefits. First, since the our project is an all-around job, involving technologies in natural language processing, machine learning, data mining, knowledge discovery and so on, we obtain a lot of knowledge of these field by studying relative papers. Second, we use some tools in our project, including CRF++, Stanford Parser and HTML Parser, which familiarizes us the usage and performance of these tools and enables us to make some optimizations. We also gain large amount experience in classifier design, web page analysis, distributed system deployment as well as the coding. So far, our work have drawn attentions from interactional academic authorities: the demo paper\cite{ZZX2012KDD} ``A System for Extracting Top-K Lists from the Web'' has been accepted by KDD2012. And now we are working on the full size paper aiming at VLDB or other international conferences. You are very welcomed to cite our work in your research.

In the future, we can continue our work on the following aspects. First, although the current experimental result is already satisfactory, it still has potential to be better. The goal is to boost the recall to 70\% on the premise of not reducing the precision.
One idea is to replace current ranker with a probabilistic model (e.g., Naive Bayes Model), we can consider $F\textnormal{-}Score$, $V\textnormal{-}Score$, number $k$, tag paths as well as other visual signals as features for this model. Therefore this model can be compatible with the previous ranker and gain better precision score.
Second we can continue the experiment and apply our system on the whole web snapshot from Bing. With the result data, we can analyze the characters of ``top-$k$'' lists and the people's behavior on writing ``top-$k$'' lists,
for example, the distribution of the number $k$ and the column number, most popular ``top-$k$'' concepts and so on.
We can build a database of all the ``top-$k$'' lists extracted, and open an online search interface, i.e., we can build a search engine of ``top-$k$'' lists. Third, we can further improve the title classifier so that it can extract more information besides the number $k$, such as the criteria the the time and location and the head concept (currently we just extract all the concepts in the title).
Finally, we can focus on the applications of the system.

%\section{Conclusion and Future Work}
%We presented an algorithm to automatically extract top-k lists from
%the web. What distinguishes this work from previous list extraction
%algorithms is that we are not concerned with extracting all the list objects
%from a page assuming all the repetitive segments in the page are valid objects,
%regardless of whether they come from the same list. While this is a common
%scenario for deep web mining, we focus on identifying one main list in
%a page and extract it only if it has exactly $k$ items. This is the case
%of ``Top K'' like web page and the information herein is very valuable for
%concept instantiation and fact answering. While the idea of using
%tag paths to identify repeative HTML elements is not new,
%we have introduced several new technique to adapt to the top-k list extraction
%problem. For future work of this paper, we are interested in
%two directions. One direction is to how to retrieve as many top-k pages as
%possible from the web. This task is related to suitable search
%query generation, e.g. generation of key words can match top-k page titles.
%The other direction is refine the returned list items to retrieve a concise
%and accurate name for the instance or entity if available.

