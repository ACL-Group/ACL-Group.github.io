\chapter{Introduction}
\label{sec:intro}
The world wide web is by far the largest source of information today.
Much of that information contains structured data such as tables and lists
which are very valuable for knowledge discovery and data mining.
This structured data is valuable not only because of the relational
values it contains, but also because it is relatively easier to unlock
information from data with some regular patterns than free text which
makes up most of the web content. However, when encoded in HTML,
structured data becomes {\em semi-structured}.
And because HTML is designed for rendering in a browser, different
HTML code segments can give the same visual effect at least to
the human eye. As a result,
HTML coding is much less stringent than XML, and
inconsistencies and errors are abundant in HTML documents.
All these pose significant challenges in the extraction of structured
data from the web \cite{Weninger10:UnexpectedList}.

In this paper, we focus on list data in web pages. In particular, we are
interested in extracting from a kind of web pages which present
a list of $k$ instances of a topic or a concept. Examples of such topic include
``20 Most Influential Scientists Alive Today'',
``Ten Hollywood Classics You Shouldn't Miss'',
``50 Tallest Persons in the World''.
We call these pages ``top-$k$'' pages.
Figure \ref{fig:topscientists}
shows some features of a typical ``top-$k$'' page \cite{InfluentialScientists}.
Figure \ref{fig:topscientists}.(a) gives a snapshot of the partial page
and Figure \ref{fig:topscientists}.(b-d) are some page segments.
Generally, the title (Figure \ref{fig:topscientists}.(a)) of a ``top-$k$'' page
will contain a number $k$ indicating the list size (20),
as well as the head word/phrase (scientist) which best describe entities in
the list. Figure \ref{fig:topscientists}.(c) shows one instance
of ``top-$k$'' list, which contains not only  the instance name
(Timothy J. Berners-Lee), but also optionally some additional information like
a picture, a textual description and a link to a relevant wikipedia page,
which can be treated as the attributes of the instance.
It is also possible that there are other unwanted lists in a ``top-$k$'' page
such as Figure \ref{fig:topscientists}.(d) which
should be filtered out.

\begin{figure}[th]
        \centering
        \epsfig{file=./pics/page5_detail.eps,width=0.9\columnwidth}
        \caption{Snapshot of a typical  ``top-k'' page
	\cite{InfluentialScientists} and its page segments}
        \label{fig:topscientists}
\end{figure}

Our system is designed to extract ``top-$k$'' lists from web pages.
Basically, it performs three tasks:
\begin{enumerate}
\item \textit{Recognize ``top-$k$'' page}:
By parsing and analyzing the title of the web pages, we can identify ``top-k'' pages.
Furthermore, we can get information such as the list size $k$ and the head concept,
which is necessary in next steps.

\item \textit{Extract ``top-$k$'' list}:
For those recognized ``top-k'' pages, our system is able to extract a list of $k$ items
as the main list from the page body. If the extracted list items contain additional information,
our system can also extract them as attribute columns.

\item \textit{Process and Understand list content}:
Since our final goal is to understand the content of ``top-k'' lists we get,
we need to process the extraction result. For each column(main ``top-$k$'' list or attribute column),
we will conceptualize the content and generate a key concept to describe it.
\end{enumerate}

The input of the system is any HTML web page
and the output would be the extracted ``top-$k$'' list of the page, if any.
Table \ref{tab:sampleoutput} shows the sample output from the page shown in
Figure \ref{fig:topscientists}.
\footnote{The actual output is stored in XML format and
includes additional information.}.

\begin{table}
\centering
\caption{Sample extraction output of ``20 Most Influential Scientists Alive Today'' \cite{InfluentialScientists}}
\begin{tabular}{|l|l|l|l|l|} \hline
{\bf Index} & {\bf Name} & {\bf Image} & {\bf Description} & {\bf Wiki. Link} \\ \hline
1 & {\em Timothy J. ...} & tim-berners-lee... &     who invented the World Wide Web... & [link]\\
2 & {\em Noam Chomsky} & noam\_chomsky.jpg &        who, though a linguist and... & [link]\\
3 & {\em Richard Dawkins} & richard\_dawkins.jpg &  whose use of evolutionary biology... & [link] \\
4 & {\em Persi Diaconis} & persi\_diaconis.jpg &    who in merging the mathematical... & [link] \\
5 & {\em Jane Goodall} & jane\_goodall.jpg &        whose work on primates has led... & [link] \\
6 & {\em Alan Guth} & alan\_guth.jpg &              whose idea of inflationary... & [link] \\
7 & {\em Stephen Hawking} & stephen\_hawking.jpg &  whose work on the nature of... & [link] \\
8 & {\em Donald Knuth} & donald\_knuth.jpg &        whose work on the theory of... & [link] \\
9 & {\em Lynn Margulis} & alan\_guth.jpg &          whose ideas about symbiogenesis... & [link] \\
10 & {\em Gordon Moore} & gordon\_moore.jpg &       who as founder of Intel merged... & [link] \\
11 & {\em Roger Penrose} & roger\_penrose.jpg &     who has broken new ground not... & [link] \\
12 & {\em Allan Sandage} & allan\_sandage.jpg &     who continued the work of the... & [link] \\
13 & {\em Frederick Sanger} & frederick\_sanger.jpg & whose research first revealed... & [link] \\
14 & {\em Charles Townes} & charles\_townes.jpg &   who invented the laser, which ... & [link] \\
15 & {\em Craig Venter} & craig\_venter.jpg &       whose completion of the Human... & [link] \\
16 & {\em James Watson} & james\_watson.jpg &       whose codiscovery with Francis... & [link] \\
17 & {\em Steven Weinberg} & steven\_weinberg.jpg & whose work on unifying the... & [link] \\
18 & {\em Andrew Wiles} & andrew\_wiles.jpg &       who in resolving the 300-year ... & [link] \\
19 & {\em Edward O. Wilson} & edward\_wilson.jpg &  whose work on sociobiology has ... & [link] \\
20 & {\em Edward Witten} & edward\_witten.jpg &     whose work on the mathematical... & [link] \\
\hline
\end{tabular}

\label{tab:sampleoutput}
\end{table}


There were many previous attempts to extract lists or tables from the web.
None of them targets the ``top-$k$'' list extraction that is studied in
this work. In fact, most of the methods are based on either very specific
list-related tags \cite{googlesets,webtables08}
such as {\tt <ul>}, {\tt <li>} and {\tt <table>}
or the similarity between DOM trees
\cite{LiuGZ03:MDR,MiaoTHSM09:TagPathClustering} and ignore
the visual aspect of HTML documents. These approaches are likely to be
brittle because of the dynamic and inconsistent nature of web
pages. More recently, several groups
have attempted to utilize visual information in HTML in
information extraction. Most notably, Ventex
\cite{GatterbauerBHKP2007:Towards} and HyLiEn \cite{FumarolaWBMH11:List}
were designed to correlate the rendered visual model or features
with the corresponding DOM structure and achieved remarkable improvements
in performance. We will discuss them further in \ref{sec:related}
However, these techniques undiscriminatingly extract {\em all}
elements of {\em all} lists or tables from a web page, therefore the objective
is different from that of this work which is to extract {\em one} specific
list from a page while purging all other lists (e.g. (d) in
Figure \ref{fig:topscientists}) as noise. The latter poses
different challenges such as distinguishing ambiguous list boundaries
and identifying unwanted lists.

We target ``top-$k$'' list data for information extraction for the following reasons.
%Besides, compared with normal web tables and lists, ``top-$k$'' lists are more important for the following reasons:
First, there are {\em large} amount of ``top-$k$'' lists around on the web.
We estimate that the total number in Bing's corpus is around 2.24 million (1.4\textperthousand~
of total),
and our system can effectively extract up to 75.5\% of them. The scale of this
data is larger than any manually or automatically extracted lists in
the past.

Second, list data is generally {\em cleaner} than other forms of web data.
Free text contains a lot of variation and ambiguity and is known to be hard
to understand and extract. General tables, even though structured,
can have many different forms (such as row span and column span) and styles,
and many of them are not meaningful if we don't
know the schema of the table or the meaning of the headers.
Lists, on the other hand, have relatively simpler structures and are easier to
identify. What's more, ``top-$k$'' lists, with their unique semantics, are even
cleaner than ordinary lists.
%on the web. General tables and lists extracted from internet
%contain a lot of noises such as advertisements and comments,
%which are often useless for further analysis; while our system can filter those unwanted data
%and extract ``top-$k$'' list with up to 90\% precision(see Section
%\ref{sec:eval}).

Third, ``top-$k$'' lists are relatively {\em easier} to understand.
``top-$k$'' list pages share a general style: the page title contains a number and
the topic or the main concept of the list.
Each list item can be considered as an instance of the page title.
The number of items should be equal to the number mentioned in the title.
Besides the name of the instance, each list item
may contain additional attributes of the instance.

Finally, ``top-$k$'' lists have {\em interesting} semantics.
The fact that the list items
are called ``top XXX'' means that these items are more important, popular or
meaningful than an arbitrary list. What's more, people are always fascinated
about the rankings of things. Information of this sort is likely to find a
large audience.

%Since the input of our system are independent,
%we can easily deploy it onto some Map/Reduce system,
%in order to process massive web pages.
We deployed our prototype system on a distributed computing platform
and performed extraction on up to 1/10 of a high frequency web snapshot
crawled by Bing. Our preliminary results showed that the system achieved
90.4\% precision and 57.7\% recall. And that equivalent to the correct
extraction of 129,169 lists from a total of 160 million randomly selected web pages.

The work described in this paper is an important step in our bigger effort of
automatic constructing a universal knowledge base
that includes large number of known concepts and their instances.
To that end, we have already built one of the largest open-domain taxonomy
called Probase \cite{WuLWZ12:Probase}
which consists of 2.8 million concepts and many more instances.
The ``top-$k$'' lists we extracted from the web can be an important source for Probase and enrich its instance space.
Also, with our system,
we are able to build an effective fact answer engine \cite{YinTL11:Facto}.
With such an engine, we can answer queries such as
``Who are the 10 tallest persons in the world'', or ``What are 50 best-selling
books in 2010'' directly, instead of referring the users to a set of ranked
pages like all search engines do today.

The paper is organized as follows.
 \ref{sec:related} will talk about
some state-of-the-art technologies on automatic extraction of structured data in web.
 \ref{sec:prelim} will focus on DOM Model, Probase, CRF and other techniques that are key to our system.
 \ref{sec:algo} will discuss in detail the framework of our system.
 \ref{sec:eval} will introduce the preliminary experiments and show the evaluation results of our system.
In  \ref{sec:eval}, we will introduce our web interface and present a plan for demonstration.
At last, we will conclude future work and contributions in  \ref{sec:conclusion},
and make an acknowledgement in  \ref{sec:ack}.
%Then we will discuss in detail the framework of our system
%(Chapter \ref{sec:algo}) and give the evaluation results
%(Chapter \ref{sec:eval}), and present a plan for demonstration (Chapter
%\ref{sec:demo}).
%And finally, we will summarize our contributions and conclude the paper in Chapter
