Thanks to the reviewer for your insightful comments.

* This paper primarily focuses on the curation of audio-text data. While we acknowledge the importance of the audio-text model architecture, our main objective is to compare our data-curation method, which utilizes a tag-based captioning model, with previous visual-based data-curation approaches. We argue that visual-based method might involve unwanted noises and our experiments suggest that the current data is of higher quality for downstream audio-text tasks.
  * Regarding the architecture, we adopt the current mainstream one since the architecture is not the focus of this paper. We think the comparison of using different datasets while adopting the same architecture is sufficient to validate the advantage of our data curation approach. Adding more architectures for comparison is beneficial but is not the key to data quality evaluation.
* It is worth noting that this work was contemporary with or earlier than several CLAP works in 2022. Therefore, audio-text data curation and CLAP pre-training were not as prevalent during the time we wrote this paper. Additionally, in comparison to existing CLAP works, our paper introduces a novel data curation approach and explores the transferability of the pre-trained model across a wider range of downstream tasks such as retrieval, generation, and classification.
* Actually, prior to proposing BLAT, we conducted a preliminary attempt to generate audio-text pairs using tags and templates, similar to your suggestion. However, the results fell significantly behind existing approaches, therefore we chose not to include them in our paper. Nevertheless, we acknowledge the value of this preliminary attempt and plan to include it in our results section to provide a more comprehensive comparison.