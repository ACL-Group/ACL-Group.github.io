\section{Task 1: Mining cross-cultural differences of named entities}
\label{sec:mcdne}

%We first explain how we obtain the ground truth from human annotators, then present several baseline methods to this problem, and finally 
%discuss our experiment results in detail.
\textbf{Task definition:}
	This task is to discover and quantify cross-cultural differences of concerns towards named entities.  
	Specifically, the input to this task is a list of 700 named entities of interest, two monolingual social media corpora; the output is the scores for the 700 entities indicating the cross-cultural differences of the concerns towards them between two corpora. 
	We map the labels to numerical scores from 1 to 5 and use the average scores from the annotators as the ground truth {scores}. 
%Thus, 

\subsection{Ground Truth Scores}
\label{sec:mcdne_truth}
\citet{harris1954distributional} states that the meaning of 
words is evidenced by the contexts they occur with. 
Likewise, we assume that the cultural properties of an entity 
can be captured by the terms they always co-occur with in a large social media corpus. 
Thus, for each named entity, we present human annotators with two lists of 20 most co-occurred terms with it in Twitter and Weibo corpus respectively. 
We select 700 named entities for annotators to label, which
are the most frequently mentioned both in Twitter and Weibo. 
Annotators are instructed to rate the topic-relatedness between the 
two word lists using one of following labels: ``very different'', 
``different'', ``hard to say'',  ``similar'' and 
``very similar''. We utilize this way for efficiency and avoiding subjectivity. 
As the word lists presented come from social media messages, the social and cultural elements are already embedded in their chance of occurrence.
\footnote{All four annotators are native Chinese speakers 
but have excellent command of English. Two of them lived in the US extensively. 
Annotators are educated with many 
selected examples and thus have shared understanding of the
labels. The inter-annotator agreement is 0.672 by Cohen's kappa coefficient, 
suggesting substantial correlation.} 



 
\begin{table*}[th!]
	\scriptsize
	\centering
	
	\caption{{\small Selected culturally different named entities, with Twitter and Weibo's trending topics manually summarized}\vspace{-15pt}}
	\begin{tabular}{L{1.5cm} L{5cm} L{8cm}}
		\textbf{Entity} & \textbf{Twitter topics} & \textbf{Weibo topics}
		\\ \hline
		Maldives & coup, president Nasheed quit, political crisis & holiday, travel, honeymoon, paradise, beach \\ \hline
		Nagoya & tour, concert, travel, attractive, Osaka & Mayor Takashi Kawamura, Nanjing Massacre, denial of history\\  \hline
				Quebec & Conservative Party, Liberal Party, politicians, prime minister, power failure & travel, autumn, maples, study abroad, immigration, independence   \\ \hline
				Philippines & gunman attack, police, quake, tsunami & South China Sea, sovereignty dispute, confrontation, protest  \\ \hline
		Yao Ming & NBA, Chinese, good player, Asian  & patriotism, collective values, Jeremy Lin, Liu Xiang, Chinese Law maker, gold medal superstar   \\ \hline USC & college football, baseball, Stanford, Alabama, win, lose & top destination for overseas education, 
Chinese student murdered, scholars, economics, Sino American politics \\ \hline
	\end{tabular}
\vspace{-10pt}
	\label{tab:mcdne_res_4}
\end{table*}
%\vspace{-10pt}
\subsection{Baseline and Our Methods} 
We propose eight benchmark methods for this novel task. 
The first three are \emph{distribution}-based, while the next two 
are \emph{transformation}-based. 
The last three, namely MultiCCA,
MultiCluster and Duong are three popular bilingual word representation models for general use.   
Distribution-based methods compare lists of surrounding
English and Chinese terms, denoted as $L_E$ and $L_C$, 
by computing cross-lingual relatedness between two lists, 
though different baselines differ in the
selection of words and the way of computing similarities.
Transformation-based methods compute the vector representation 
in English and Chinese corpus respectively, and
then train a transformation.
Bilingual word representations based methods use the existing state-of-the-art models and then compute the similarities between two bilingual word vectors as $clsim$.
% of words, known $L_E$ and $L_C$. The differences of these methods are the selecting method of terms and %the computation method of two word lists.  ii) the second type of baseline methods are first obtain the %comparable vectorial representation of the English title and Chinese title of the given entity, and then just %calculate the similarity between two comparable vectors.
\begin{itemize}[-]
%	\renewcommand\labelitemi{--} 
 	\item \textbf{BL-JS}: \textit{Bilingual Lexicon Jaccard Similarity}~~
 	%	The $L_E$ and $L_C$ of both BL-JS and WN-WUP  are the same as the lists that annotators judge.
 	BL-JS uses the bilingual lexicon to translate $L_E$  to a Chinese word list 
 	$L_E^*$ as a medium and then calculates the Jaccard Similarity between 
 	$L_E^*$ and $L_C$ as $J_{EC}$. Similarly, we can compute $J_{CE}$. 
 	Finally, we compute $\frac{J_{EC}+J_{CE}}{2}$ as the cross-cultural similarity 
 	of this given named entity.
 	
 	\item \textbf{WN-WUP}:	\textit{WordNet Wu-Palmer Similarity}~~ Instead of using 
 	the bilingual lexicon and Jaccard Similarity, WN-WUP uses Open Multilingual 
 	Wordnet~\cite{wang2013building,bond2013linking} to calculate the average 
 	similarity of two lists of words from different languages.
 	
 	\item \textbf{EM-JS}: \textit {Word Embedding based Jaccard Similarity}~~ EM-JS is 
 	very similar to BL-JS, except that its $L_E$ and $L_C$ are generated by 
 	ranking the similarities between the name of entities and all English words 
 	and Chinese words respectively. 
 	
 	\item \textbf{LTrans}: \textit {Linear Transformation}~~
 	We follow the steps in Mikolov et al.~\shortcite{Mikolov:2013tp} 
 	to train a transformation matrix between \textit{EnVec} and \textit{CnVec}, 
 	using 3000 translation pairs with confidence of 1.0 in the bilingual lexicon. 
 	Given a named entity, this solution simply calculates cosine similarity 
 	between the vector of its English name and the \textit{transformed} vector 
 	of its Chinese name. 
 	
 	\item \textbf{BLex}: \textit {Bilingual Lexicon Space}~~
 	This baseline is similar to \textit{SocVec} but it does not 
 	utilize any social word vocabularies and  uses bilingual lexicon entries as pivots instead.
 	
 	\item	\textbf{{MultiCCA}}
 	\cite{ammar2016massively} This method takes two mono-lingual word 
embeddings and a bilingual lexicon as input and develop a bilingual word 
representations.  We use both the Microsoft bilingual lexicon (BL)
and the bilingual social lexicon (BSL) we constructed as the bilingual lexicon
to compare their effectiveness. Dimensionality is tuned from 
$\{50,100,150,200\}$ in all methods.
 	\item 	\textbf{{MultiCluster}} \cite{ammar2016massively} 
 	This method requires re-training the bilingual word embeddings from the two mono-lingual corpora with a bilingual lexicon. We also use our BSL as 
an additional test (MultiCluster-BSL). 
 	\item	\textbf{{Duong} }
 	\cite{duong2016learning}
 	Similar to MutltiCluster, this method retrains the embeddings from 
mono-lingual corpora with an EM style training algorithm. 

\item \textbf{Our SocVec-based method}  With our constructed \textit{\socvec},~given a named entity with its English and Chinese names, we can simply compute the similarity between their \textit{SocVec}s as its cross-cultural difference score. 
\footnote{Note that our method is more efficient because it requires no re-training on original corpora. Thus, we do not have to obtain new bilingual word embeddings by re-training on the corpora, every time when we have a updated BL or BSL, which is very helpful for efficient examining different BSLs. }

%Our method is only based on monolingual word embeddings and B(S)L, and thus does not need re-training on the corpora at all. 
\end{itemize}

\subsection{Experimental Results}

For qualitative evaluation, \tabref{tab:mcdne_res_4} shows some of 
the most culturally different entities mined by our method. 
The hot and trending topics on Twitter and Weibo are 
manually summarized to help explain the cultural difference. 
The perception of these entities diverges widely between English and
Chinese social networks, thus suggesting
significant cross-cultural differences.
\footnote{Admittedly, some cultural differences are time-specific and platform-specific.
However, we argue that the two microblog platforms are among the most popular ones in their languages, and therefore the common cultural differences and the temporal variations of cultural differences can be valuable and beneficial for social studies as well.
%\footnote{Separating temporal influences from the social media corpora can be another interesting future research topic about our proposed task.}
}

\begin{table}[t]
	\scriptsize
	\centering
	\caption{{\small Comparison of Different Methods}}
	\begin{tabular}{c|c|c|c}
		
		\textbf{Method} & \textbf{Spearman} & \textbf{Pearson}  & \textbf{MAP} \\ \hline
		BL-JS& 0.276 & 0.265 & 0.644   \\ 
		WN-WUP  & 0.335 & 0.349 & 0.677 \\ 
		EM-JS & 0.221 & 0.210  & 0.571\\ 
		LTrans& 0.366 & 0.385  & 0.644  \\
		BLex& 0.596 & 0.595  & 0.765 \\ \hline 
MultiCCA-BL(dim=100)&0.325&0.343&0.651\\  
MultiCCA-BSL(dim=150)&0.357&0.376&0.671\\ 
MultiCluster-BL(dim=100)&0.365&0.388&0.693\\ 
MultiCluster-BSL(dim=100)&0.391&0.425&0.713\\ 
Duong-BL(dim=100)&0.618&0.627&0.785\\ 
Duong-BSL(dim=100)&0.625&0.631&0.791 \\ \hline 
		SocVec:opn& 0.668 & 0.662   & \textbf{0.834} \\ 
		SocVec:all& \textbf{0.676} & \textbf{0.671}  & \textbf{0.834}\\ 
		SocVec:noun & 0.564 & 0.562 & 0.756 \\ 
		SocVec:verb & 0.615 & 0.618 & 0.779 \\ 
		SocVec:adj. & 0.636 & 0.639 & 0.800 \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_1}
\end{table}
\begin{table}[h]
	\centering
	\scriptsize
	\caption{{\small Evaluation of Different Similarity Functions \vspace{-10pt}}}
	\label{tab:mcdne_res_2}
	\begin{tabular}{l|c|c|c}
		\textbf{Similarity} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\ \hline
		PCorr. & 0.631 & 0.625 & 0.806\\ 
		L1 + M & 0.666 & 0.656 & 0.824 \\  
		Cos & \textbf{0.676} & 0.669 & \textbf{0.834} \\ 
		L2 + E & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
\end{table}

\begin{table}[th]
	\centering
	\scriptsize
	\caption{{\small Evaluation of Different Pseudo-word Generators}\vspace{-10pt}}
	\begin{tabular}{c|c|c|c}
		\textbf{Generator} & \textbf{Spearman} & \textbf{Pearson}   & \textbf{MAP} \\  \hline
		Max. & 0.413 & 0.401 & 0.726\\ 
		Avg. & 0.667 & 0.625 & 0.831\\ 
		W.Avg. & 0.671 & 0.660 & 0.832 \\  
		Top & \textbf{0.676} & \textbf{0.671} & \textbf{0.834} \\ \hline
	\end{tabular}
	\label{tab:mcdne_res_3}
\end{table}

In~\tabref{tab:mcdne_res_1}, we evaluate the benchmark methods and our approach with three metrics: Spearman and 
Pearson correlation on the ranking problem, and Mean Average Precision (MAP) on the classification problem. 
The \textit{BSL} of \textit{SocVec:opn} uses only OpinionFinder as English socio-linguistic vocabulary, while \textit{SocVec:all} uses the union of Emapth and OpinionFinder vocabularies.\footnote{Having tuned the  parameters, we use the best parameters for the \textit{SocVec} methods: 5-word context window and 
150 dimensions used in training monolingual word vectors,
cosine similarity as the \textit{sim} function within the 
\textit{\socvec}~space, and ``\textit{Top}'' as the pseudo-word generator.} 

\textbf{Lexicon Ablation Test} To show the effectiveness of social-linguistic vocabulary versus other type
of words as the bridge between the two cultures, we also compare the
results using sets of nouns, verbs and adjectives within the 
same \textit{SocVec} framework.
All vocabularies under comparison are of similar sizes 
(around 5000), which also indicates that the improvement of our method 
is not just the result of sparsity.
Results show that \textit{SocVec} models, and in particular, the \textit{SocVec} model using the social words as cross-lingual media, performs the best. 

\textbf{Similarity Options} We also evaluate the effectiveness of four different similarity options in 
\textit{\socvec}, namely, Pearson Correlation Coefficient 
(\textit{PCorr}.), L1-normalized Manhattan distance (\textit{L1+M}), 
Cosine Similarity (\textit{Cos}) and  L2-normalized Euclidean distance (\textit{L2+E}).
%It is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking.
From~\tabref{tab:mcdne_res_2}, we conclude that among these four options, \textit{Cos} and \textit{L2+E} perform the best. 
%Although it is mathematically proved that \textit{L2+E} is identical to \textit{Cosine} in ranking, we can find that 

\textbf{Pseudo-word Generator} 
\tabref{tab:mcdne_res_3} shows effect of using four pseudo-word generator functions, from which we can infer that ``\textit{Top}'' generator function performs best for 
it reduces the noise brought by the less probable translation pairs. 