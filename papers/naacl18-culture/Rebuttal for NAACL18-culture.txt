Response to Review #1: 

-- We assume that the paper (Garimella, 2016) you mentioned is “Quantifying Controversy on Social Media” (arXiv:1507.05224). They propose a framework for quantifying the controversy of topics on Twitter with both textual content and social network structure. We do agree that the two tasks have a similar goal: they both want to quantify the extent of the “divergence” of the opinion towards the same “concepts” in social media.  

However, we argue that the two tasks have significant differences as follows: 
(a) Concept of interest: our task considers “named entities”, while (Garimella, 2016) focuses on “topics” such as #beefban. 
(b) Divergence of interest: (Garimella, 2016) quantifies the extent of controversy of a given topic within a single language/culture. However, we quantify the divergence of opinions of entities in different cultures & languages, even when the entity is controversial in both cultures. 
(c) Required data: the input to (Garimella, 2016) contains social network structure and user interactions info (commenting, retweeting, mentioning and endorsement), but we only require textual content.


We will clarify these points accordingly in the revised version. 


-- We annotate data for popular entities and slang terms because it requires less background knowledge of annotaters. Also, our preliminary investigation show that less popular terms are less likely to produce cultural differences. We will also show such results in the revised version.


Response to Review #2:


-- Multilingual Extension: we argue that the resources for new languages are not expensive to acquire:


(a)  public social media corpus for a new language can also be obtained in the link we provided (filtering the language option in the meta-data).


(b)  public bilingual lexicons for a new language pair can be constructed/obtained from online translator or wiktionary.


(c)  social lexicons for new languages can be manually constructed by filtering translation results of English social lexicons. (usually several thousands words)


(d) human annotation ground truth for other languages can be obtained by directly following our methods.


--  We will release a detailed documentation along with our code & data about the implementation.


Response to Review #3:


- given a word, “confidence” is the probability distribution on its multiple translation obtained by Bing Translator.


- slang and ordinary terms are identified by a slang lexicon consisting of single words from footnote 16


- the size of L_E/C used in first three baseline methods is 20, which is the same in annotating; we will clarify these details about them more clearly in the revised version.


- correlation is computed between ground truth averaged scores (line 366~368) and computed cultural difference scores from different methods; classification problem convert averaged score as label by setting 3.0 as threshold. 


We will fix the writing issues accordingly, and illustrate the evaluation details with more intuitive examples!


General Response to Reviewers:


Thank you so much for your insightful comments and suggestions! We will revise the paper according to the comments carefully. 
 
 
