Response to Review #1: 


- We assume that the paper (Garimella, 2016) you mentioned is “Quantifying Controversy on Social Media” (arXiv:1507.05224). They propose a framework for quantifying the controversy of topics on Twitter with both textual content and social network structure.  We do agree that the two tasks have a similar goal: they both want to quantify the “divergence” of the opinion towards the same “concepts” in social media.  However, there are some significant differences between them: 
(a) Concepts of interest: our task considers “named entities”, while (Garimella, 2016) focuses on “topics” such as #beefban. 
(b) Divergences of interest: (Garimella, 2016) quantifies the controversy of a given topic within a single language/culture. However, we quantify the divergence of opinions of named entities in different cultures&languages, which is orthogonal to mono-cultural controversy. In other words, entities with controversy in both cultures may or may not have cross-cultural differences.
(c) Required data: the input to (Garimella, 2016) contains social network structures and user interactions info (commenting, retweeting, mentioning or endorsement), but we only require cheap textual content.
 
- We annotate data for popular entities and slang terms because it requires less background knowledge of annotators. Also, our preliminary investigation shows that unpopular terms are less likely to produce cross-cultural differences. 








Response to Review #2:


- Multilingual Extension: public social media corpora and bilingual lexicons for most new languages can be obtained with our mentioned approach; new social lexicons can be constructed by filtering translations of English social lexicons (usually several thousand words);  ground truth for new languages can be obtained directly following the paper. 
 
- We will release a detailed documentation of our implementation.


- Thank you very much for the comments on writing and minor typos! 






Response to Review #3:


- Given a word, “confidence” is the probability distribution on its multiple translations obtained by Bing Translator.


- Slang and ordinary terms are identified by a slang lexicon constructed from the link in Footnote 16


- The size of L_E/C used in first three baseline methods is 20, which is the same in annotation; We will clarify such details in a detailed documentation. Thanks!


- Correlation is computed between ground truth averaged scores (line 366~368) and computed cultural difference scores from different methods; classification problem converts averaged score as a label, by setting 3.0 as the threshold. 


- To evaluate the 19 results in Table 6, we have 3,800 pairs of word lists in total to be compared and scored, which is rather expensive if humans are employed. The proposed automatic evaluation metric, however, minimizes this effort.


- A is the ground truth word sets mentioned in line 583~590. For the example slang terms (line 589~590), A={foolish, stubborn, rude, impetuous}; B is a similar list produced by each method, e.g., B={imbecile, brainless, scumbag, imposter}. We will add more such illustrative examples in the evaluation section!


- Multilingual Extension: please see the response to review #2.






General Response to Reviewers:


Thank you so much for your insightful suggestions! We will revise the paper according to them carefully.