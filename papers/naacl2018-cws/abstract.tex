\begin{abstract}
  Chinese word segmentation (CWS) based on open source corpus faces dramatic performance drop when dealing with domain text, especially for a domain with lots of terms and variant writing style, such as the medical domain. However, building domain-specific CWS requires extremely high annotation cost. In this paper, we propose Adaptive Multi-Task Transfer Learning for CWS by exploiting domain-invariant knowledge from high resource to low resource domains. Experiments on three datasets from medical domain and three open source datasets\footnote{Datasets information is discussed in Sec. \ref{sec:datasets}\label{fn:1}} show that our model achieve persistent higher performance than single-task CWS and several transfer learning baselines, especially when there is a large
disparity between source and target domains.
\end{abstract}
