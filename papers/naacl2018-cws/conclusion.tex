\section{Conclusion}

In this paper, we propose \textit{Adaptive Multi-Task Transfer Learning} framework and three model instances with different settings. 15 experiments between medical datasets and open source datasets show that: (1) \textit{Adaptive Multi-Task Transfer Learning} outperforms multi-task learning all the way; (2) \textit{Adaptive Multi-Task Transfer Learning} outperforms all baselines when the \textit{Disparity} between target and source dataset is high. For future work, we plan to study the transferability between different tasks for Chinese NLP and cross-lingual NLP tasks.
