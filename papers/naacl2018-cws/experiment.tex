\section{Experiment}

In this section, we evaluate our proposed models on real-world medical Chinese word segmentation tasks, where annotated data is scarce and domain-drift is significant with open source annotated data. We conduct extensive experiments and discuss the result in detail. We also conduct an Ablation test.

% especially in the medical domain where annotated data is scarce and domain-drift is significant with open source annotated data, \textit{e}.\textit{g}. SIGHAN2005, which is discussed in Section \ref{sec:intro}. We first conduct 6 experiments between three medical corpus from different sources, including Electric Medical Record (EMR) which is totally professional and forum data which contains consumer language. We also conduct 9 experiments between medical domain and open source datasets (news and blog).

\subsection{Datasets}\label{sec:datasets}

\begin{table}[th]
\small
\centering
\caption{Statistics of number of sentences for open source corpus.}\label{table:open_source}
\begin{tabular}{lccc}
\hline
Type & \#Train & \#Dev & \#Test\\
\hline
PKU & 70498 & 8369 & 1945\\
MSR & 173850 & 19453 & 3985 \\
WEIBO & 38086 & 3834 & 16673 \\
\hline
\end{tabular}
\end{table}

\begin{table}[th]
\small
\centering
\caption{Statistics of number of sentences for medical corpus.}\label{table:medical}
\begin{tabular}{lccc}
\hline
Type & \#Train & \#Dev & \#Test\\
\hline
Cardiology(EMR) & 5636 & 1658 & 1658\\
Respiratory(EMR) & 5191 & 1661& 1549\\
Forum & 4863 & 1412 & 1474\\
\hline
Sum & 15690 & 4731 & 4691\\
\hline
\end{tabular}
\end{table}

\noindent \textbf{Open-Source} We utilize three open source CWS datasets, respectively are PKU and MSR from SIGHAN2005 Bakeoff\footnote{http://sighan.cs.uchicago.edu/bakeoff2005/} and WEIBO from ~\cite{qiu2016overview}. The information of the datasets is shown in Table \ref{table:open_source}.

\noindent \textbf{Medical} We collected three datasets of medical CWS data for our experiment and future research. The first two datasets are electric medical records (EMR) from different departments. The third dataset is medical forum data from \textit{Good Doctor Online}\footnote{http://www.haodf.com}, which is a Chinese forum for medical consult. The information of the datasets is shown in Table \ref{table:medical}.

\subsection{Disparity Study}

\textit{Transfer Learning} aim to improve the performance of low-resource domain task by exploiting the annotated data form high-resource domain, thus the \textit{Disparity} between different tasks is a leading factor to influence the \textit{transferability} between different domains with different methods.

In this paper, we used $\mathcal{X}^2$ test ~\cite{DBLP:conf/emnlp/KilgarriffR98} to quantify the \textit{Disparity} between three medical corpus. If the size of corpus 1 and corpus 2 are $N_1$, $N_2$ and word $w$ has observed frequencies $o_{w,1}$, $o_{w,2}$, then expected value $e_{w,1} = \frac{N_1 \times (o_{w,1}+o_{w,2})}{N_1+N_2}$, and likewise for $e_{w,2}$, then 

\begin{equation}
\mathcal{X}^2 = \sum \frac{(o-e)^2}{e}
\end{equation}

$\mathcal{X}^2$ test shows that \textit{Disparity} between forum dataset and two EMR datasets are close, both far larger than the \textit{Disparity} between two EMR datasets, as shown in Table \ref{table:x2test}.

Due to the fact that $\mathcal{X}^2$ test doesn't permit comparison between corpus of different sizes ~\cite{DBLP:conf/emnlp/KilgarriffR98}, we propose a simple \textit{agreement} test, using the size of the intersection between the most common $n$ tokens (bi-gram) to quantify the \textit{disparity} between medical corpus and open source corpus.  We set $n$ to 500.

\begin{table}[th]
\small
\centering
\caption{Result of $\mathcal{X}^2$ test between medical datasets, the larger the higher disparity.}\label{table:x2test}
\begin{adjustbox}{width=7.7cm}
\begin{tabular}{lccc}
\hline
Dataset & Cardiology & Respiratory & Forum\\
\hline
Cardiology & 0 & 0.069 & 0.126\\
Respiratory & 0.069 & 0 & 0.122\\
Forum & 0.126 & 0.122 & 0\\
\hline
\end{tabular}
\end{adjustbox}
\end{table}

\begin{table}[th]
\centering
\small
\caption{Result of \textit{agreement} test between medical datasets and open source datasets, the smaller the higher disparity.}\label{table:simpletest}
\begin{tabular}{lccc}
\hline
Dataset & Cardiology & Respiratory & Forum\\
\hline
PKU & 25 & 27 & 76\\
MSR & 23 & 25 & 80\\
WEIBO & 54 & 50 & 135\\
\hline
\end{tabular}
\end{table}

% \KZ{Give the detailed formula of the $X^2$ test and show the results in a table.}

% \GX{Since test based on frequency ($\mathcal{X}^2$) is sensitive to corpus size, so I use two test methods, two tables are given}

\textit{Agreement} test shows that the \textit{Disparity} between PKU/MSR and two EMR datasets are close, both far larger than the \textit{Disparity} between PKU/MSR and forum dataset. WEIBO dataset is more similar with medical datasets than PKU and MSR.

\begin{table}[t!]
\small
\centering
\caption{Performance (F1-score) of Single-task model compared with state-of-art CWS.}\label{table:single-robust}
\begin{adjustbox}{width=7.7cm}
\begin{tabular}{lcccccc}
% \hline
% Dataset & Single-task & ~\cite{DBLP:journals/corr/CaiZ16}\\
% \hline
% Cardiology & 81.10 & 80.1\\
% Respiratory & 81.33 & 81.5\\
% Forum & 75.62 & 73.0\\
% PKU & 95.45 & 95.5\\
% \hline
\hline
Models & Cardiology & Respiratory & Forum\\
\hline
Single-task & 81.10 & 81.33 & 75.62\\
~\cite{DBLP:journals/corr/CaiZ16} & 80.1 & 81.5 & 73.0\\
~\cite{DBLP:conf/acl/ZhangZF16} & 82.46 & 81.74 & 77.14 \\
% ~\cite{DBLP:conf/acl/CaiZZXWH17}  & ? & ? & ? \\
\hline
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Training}

The training phrase aims to optimize the model parameters $\theta^{(a)}$ and $\theta^{(b)}$ by minimizing the objective function defined in Eq. \eqref{eq:objective}. We use Adam ~\cite{DBLP:journals/corr/KingmaB14} with mini-batch. Each batch contains sentences from both domains. The hyper-parameter setting is discussed later.

\subsection{Single-task Performance}\label{sec:single-robust}

Before introducing our experiments on proposed \textit{Adaptive Multi-Task Transfer Learning}, we first evaluate the effectiveness of the single-task model (Bi-LSTM-CRF), which is our base model. 
We compare the model with the two state-of-art on Chinese word segmentation, proposed by ~\citet{DBLP:journals/corr/CaiZ16} and ~\citet{DBLP:conf/acl/ZhangZF16} respectively. We run experiments on our datasets with their code released on github\footnote{https://github.com/jcyk/CWS}$^{,}$\footnote{https://github.com/SUTDNLP/NNTransitionSegmentor}. The results show that the performance of single-task model and state-of-art are close, as shown in Table \ref{table:single-robust}, which indicates the single-task model is a strong baseline for our advanced models. 

\subsection{Experiment Settings}

The dimension of character embedding and the LSTM hidden state dimension are 50. The batch size is 30. We evaluate our \textit{Adaptive Multi-Task Transfer Learning} for totally 15 transfer learning tasks. For each task, we take all of source training data and 10\% of target training data. Hyper-parameters are determined by tuning against the development set.

\subsection{Baselines}

Several baseline methods are compared.
\begin{itemize}
\item \textbf{Single-task} uses target domain data only, as discussed in Section \ref{sec:single-task}.
\item \textbf{INIT} loads parameters of model trained on source domain data and then fine-tune the model on target domain data.
\item \textbf{Multi-Task} shares parameter for both source and target domain, the model is trained simultaneously.
\end{itemize}

\noindent Our implementation of \textbf{INIT} follows ~\citet{DBLP:journals/corr/MouMYLXZJ16}, and the implementation of \textbf{Multi-Task} follows the models we proposed in Sec. \ref{sec:multi-cws} by removing $\mathcal{J}_\textnormal{Adap.}$, annotating \textit{Model w/o $\mathcal{J}_\textnormal{Adap.}$} in Table \ref{table:experiment} and \ref{table:experiment2}.

\begin{table*}[t!]
\small
\centering
\caption{F1-score of 6 cross domain multi-task learning CWS tasks. R, C, F, P stand for \textit{Respiratory}, \textit{Cardiology}, \textit{Forum}, \textit{PKU} respectively. \textit{Model without Adaptive} are Multi-Task Learning with different setting according to our models. }\label{table:experiment}
% \begin{adjustbox}{width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Method} & \multicolumn{6}{c|}{Cross Medical}\\ \cline{2-7} & R$\rightarrow$C & F$\rightarrow$C & C$\rightarrow$R & F$\rightarrow$R & C$\rightarrow$F & R$\rightarrow$F\\
\hline
\hline
\multicolumn{7}{|l|}{Baselines} \\
\hline
Single-task & 81.10 & 81.10 & 81.33 & 81.33  & 75.62 & 75.62\\
\hline
INIT & \underline{90.62} & 87.19 & \underline{88.88} & 85.56 & 79.41 & 78.53\\
\hline
Model-\RN{2} w/o $\mathcal{J}_\textnormal{Adap.}$ & 86.71 & 85.27 & 85.34 & 83.40 & 77.62 & 78.34\\
\hline
Model-\RN{3} w/o $\mathcal{J}_\textnormal{Adap.}$. & 84.39 &  83.59 & 83.80 & 83.27 & 77.18 & 77.38\\
\hline
\hline
\multicolumn{7}{|l|}{Adaptive Multi-Task Transfer Learning-KL} \\
\hline
Model-\RN{1} & 86.94 & 86.70 & 85.64 & \textbf{85.57} & 78.35 & 78.46\\
\hline
Model-\RN{2} & 87.73 & 87.05 & 86.65 & \textbf{\underline{86.51}} & \textbf{79.44} & \textbf{78.92}\\
\hline
Model-\RN{3} & 86.66 & 86.53 & 85.86 & 85.39 & 78.67 & \textbf{78.72}\\
\hline
\hline
\multicolumn{7}{|l|}{Adaptive Multi-Task Transfer Learning-MMD} \\
\hline
Model-\RN{1} & 85.96 & 85.43 & 85.45 & \textbf{85.58} & 77.85 & 78.16\\
\hline
Model-\RN{2} & 87.55 & \textbf{\underline{87.24}} & 86.27 & \textbf{86.40} & \textbf{79.45} & \textbf{78.57}\\
\hline
Model-\RN{3} & 86.30 & 85.49 & 85.13 & 85.19 & 77.05 & 77.23\\
\hline
\hline
\multicolumn{7}{|l|}{Adaptive Multi-Task Transfer Learning-CMD} \\
\hline
Model-\RN{1} & 86.17 & 86.03 & 85.58 & \textbf{85.83} & 78.61 & 78.39\\
\hline
Model-\RN{2} & 87.49 & 86.95 & 86.79 & \textbf{86.29} & \textbf{\underline{79.52}} & \textbf{\underline{79.08}}\\
\hline
Model-\RN{3} & 86.54 & 86.36 & 85.68 & \textbf{86.05} & 78.23 & \textbf{78.63}\\
\hline
\end{tabular}
% \end{adjustbox}
\end{table*}

\begin{table*}[t!]
\small
\centering
\caption{F1-score of 9 multi-task learning CWS tasks between open source datasets and medical datasets. R, C, F, P, M, W stand for \textit{Respiratory}, \textit{Cardiology}, \textit{Forum}, \textit{PKU}, \textit{MSR}, \textit{WEIBO} respectively. \textit{Model without Adaptive} are Multi-Task Learning with different setting according to our models.}\label{table:experiment2}
% \begin{adjustbox}{width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Method} & \multicolumn{9}{c|}{Open Source - Medical} \\ \cline{2-10} & P$\rightarrow$C & M$\rightarrow$C & W$\rightarrow$C & P$\rightarrow$R & M$\rightarrow$R & W$\rightarrow$R & P$\rightarrow$F & M$\rightarrow$F & W$\rightarrow$F \\
\hline
\hline
\multicolumn{10}{|l|}{Baselines} \\
\hline
Single-task & 81.10 & 81.10 & 81.10 & 81.33 & 81.33 & 81.33 & 75.62 & 75.62 & 75.62 \\
\hline
INIT & 86.20 & 84.32 & \underline{87.72} & 84.05 & 82.83 & \underline{86.56} & \underline{82.54} & \underline{81.78} & \underline{84.37}\\
\hline
Model-\RN{2} w/o $\mathcal{J}_\textnormal{Adap.}$ & 85.63 & 85.84 & 86.14 & 84.17 & 85.42 & 86.09 & 78.60 & 78.80 & 78.32\\
\hline
Model-\RN{3} w/o $\mathcal{J}_\textnormal{Adap.}$ & 84.43 & 86.19 & 85.61 & 84.38 & 85.02 & 85.79 & 77.61 & 77.87 & 78.38\\
\hline
\hline
\multicolumn{10}{|l|}{Adaptive Multi-Task Transfer Learning-KL} \\
\hline
Model-\RN{1} & \textbf{86.30} & \textbf{86.60} & 86.64 & \textbf{85.66} & \textbf{85.44} & 85.69 & 78.55 & 78.21 & 78.11\\
\hline
Model-\RN{2} & \textbf{87.01} & \textbf{86.20} & 86.94 & \textbf{85.88} & \textbf{85.61} & 85.96 & 78.82 & 78.69 & 79.37 \\
\hline
Model-\RN{3} & \textbf{86.56} & \textbf{86.25} & 87.29 & \textbf{85.30}  & \textbf{85.60} & 85.52 & 78.20 & 77.45 & 78.56 \\
\hline
\hline
\multicolumn{10}{|l|}{Adaptive Multi-Task Transfer Learning-MMD} \\
\hline
Model-\RN{1} & 85.82 & \textbf{86.62} & 86.47 & \textbf{85.26} & \textbf{85.48} & 85.87 & 77.69 & 78.26 & 79.01 \\
\hline
Model-\RN{2} & \textbf{86.77} & \textbf{86.34} & 86.82 & \textbf{85.98} & \textbf{\underline{86.17}} & 85.86 & 79.04 & 79.21 & 78.80 \\
\hline
Model-\RN{3} & 85.89 & 85.68 & 86.59 & \textbf{85.05} & 85.27 & 85.64 & 78.37 & 78.30 & 78.39\\
\hline
\hline
\multicolumn{10}{|l|}{Adaptive Multi-Task Transfer Learning-CMD} \\
\hline
Model-\RN{1} & \textbf{86.52} & 85.93 & 86.39 & \textbf{85.71} & 85.36 & 85.97 & 78.66 & 78.29 & 78.49\\
\hline
Model-\RN{2} & \textbf{\underline{87.21}} & \textbf{\underline{86.92}} & 86.83 & \textbf{85.83} & \textbf{85.82} & 86.24 & 78.82 & 79.01 & 78.90 \\
\hline
Model-\RN{3} & \textbf{86.54} & 85.99 & 86.64 & \textbf{\underline{86.12}} & \textbf{85.66} & 85.63 & 78.73 & 78.15 & 78.71 \\
\hline
\end{tabular}
% \end{adjustbox}
\end{table*}

\subsection{Hyper-parameter}

In \textit{Adaptive Multi-Task Transfer Learning}, we have two hyper-parameters $\alpha$ and $\beta$, which controls the weight of $\mathcal{J}_{\textnormal{Adap.}}$ and $\mathcal{J}_{L_2}$. Our experiments show that $\alpha \in [0.3, 0.7]$ and $\beta \in [0.2, 0.3]$ works best.


\subsection{Result and Discussion}

% \GX{The result and discussion may be re-wrote after I fill Table 8. But the overall result is expected. In Table 7 and Table 8, we can both see that the relative performance of our proposal and baseline increases when Disparity increases, for similar domains, INIT performs well but our performs well when similarity goes down. \textbf{Another finding!!!} in Table 7, focus on target domain as C, I find that the performance delta of INIT between source R and F is (90.62-87.17), however, for our variants, the largest is (86.30-85.53)(Moldl-\RN{3}-KL), the finding is the same to target domain as R and F. I think it is because our proposal do extracts domain-invariant knowledge, thus the performance is less sensitive to source domain, but INIT is fully dependent on the similarity between.}

Table \ref{table:experiment} show the performance of 6 cross medical CWS experiments, Table \ref{table:experiment2} show the performance of 9 experiments between open source datasets and medical datasets. 
\textbf{Bold} indicates scores that outperforms all baselines. \underline{Underline} indicates the highest score for each task.
We first discuss the result from several general aspects:

(1) All transfer learning methods outperforms strong baseline of single-task method (discussed in Section \ref{sec:single-robust}). Especially, our models outperforms from 2\% to 6\% than single-task baseline.

(2) The \textit{Adaptive} part of our model, $\mathcal{J}_\textnormal{Adap.}$, is proven to be promising. First,  Model-\RN{1}, which is a parallel training without sharing parameters and leveraging pretrained optimized initialization, outperforms single-task baseline by 4\% on average. Second, $\mathcal{J}_\textnormal{Adap.}$ improves the performance by 1\% on average for both Model-\RN{2} and Model-\RN{3}. It shows that the $\mathcal{J}_\textnormal{Adap.}$ do capture domain-invariant knowledge apart from the shared parameters. 

(3) Within the three models we proposed, Model-\RN{2} performs best, outperforms other two on 40/45 experiment instances. Model-\RN{1} and Model-\RN{3} are equal in match. We argue that it is because the missing of shared parameter of Model-\RN{1} and the possible noise encoded by the specific layer of Model-\RN{3}.

(4) For the three statistic distance measures we test in experiment, the overall performance is close. Compared with MMD and CMD, KL gains a more stable improvement on all experiments. However, CMD performs better to hit more best scores than KL and MMD.

Next, we analyze the result from a special aspect, the \textit{Disparity} between source and target datasets:

(1) In Table \ref{table:experiment}, INIT outperforms all other baselines and our approaches in task R $\rightarrow$ C and C $\rightarrow$ R, but downperforms our approaches in the others. We argue that the effectiveness of INIT on task between domain R and C result from the low \textit{Disparity} between the two domains. As shown in Table \ref{table:x2test}.

(2) We first refer to Table \ref{table:simpletest}. We can simply categorize the \textit{Disparity} of 9 combinations into 4 levels. P $\rightarrow$ C, P $\rightarrow$ R, M $\rightarrow$ C and M $\rightarrow$ R indicate high \textit{Disparity}, W $\rightarrow$ C, W $\rightarrow$ R indicate low \textit{Disparity}, P $\rightarrow$ F, M $\rightarrow$ F indicate low \textit{similarity}, W $\rightarrow$ F indicates high \textit{similarity}. Then we can find that, in 4 tasks of high \textit{Disparity}, our approach outperforms all baselines. When \textit{Disparity} goes down to the second level, our approach underperforms INIT but only with gap of 0.4\%. However, when \textit{Disparity} continuously goes down to the third and forth level, INIT outperforms our approach by 3-4\%.
% (2) The \textit{Adaptive} part of our model, $\mathcal{J}_{\textnormal{MMD}}$ is proven to be promising. First, Model-\RN{1}, which is a parallel training without sharing parameters, outperforms single-task baseline by 4\% on average. It even outperforms Model-\RN{2} w/o Adap on 5/9 experiments. Second, for Model-\RN{2} and Model-\RN{3} , the \textit{Adaptive} version outperforms the \textit{w/o Adap.} version on 16/18 experiments, the last two has close performance with gap 0.13\% and 0.15\% respectively. It shows that The \textit{Adaptive} part adapts domain-invariant knowledge to each domain thus boosting the performance. Third, Model-\RN{3} underperforms  Model-\RN{2} and outperforms Model-\RN{1} all the way. We argue that the specific layers which may encode noise into the model account for the former experiment finding, and the use of shared parameters accounts for the latter finding.

% (3) The experiment result of Model-\RN{2} with \textit{Adaption} and INIT is interesting. Model-\RN{2} outperforms INIT on 5/9 tasks. We find that 5 tasks come from data with larger domain drift.For example, the domain drift between medical forum data and medical EMR data, \textit{e.g.} F $\rightarrow$ C and F $\rightarrow$ R, is larger than the domain drift between medical EMR data, R $\rightarrow$ C and C $\rightarrow$ R. We argue that the INIT model is more likely to achieve better optimal for low domain shift transfer learning because it is trained sequentially and it is much easier for a single task training and provides a better global initialization for target domain. But for large domain shift task, the initialization can be a worse one. Meanwhile, Model-\RN{2} is trained simultaneously for source and target domain, the model can learn domain-invariant knowledge through shared parameters and MMD. When domain-drift is small, the random initialization of Model-\RN{2} may account for the loss. However, when domain-drift is large, Model-\RN{2} performs best.

\subsection{Ablation Test}

To investigate the effectiveness of different components in our \textit{Adaptive Multi-Task Transfer Learning} framework, we do ablation test based on Model-\RN{2} on task (P $\rightarrow$ R) with $\mathcal{J}_\textnormal{Adap.}$ calculated by MMD. Results are reported in Table \ref{table:ablation}. \textit{Model-\RN{2} w/o shared Bi-LSTM} uses domain-specific Bi-LSTM, while \textit{Model-\RN{2} w/o specific embedding} uses shared embedding for both domains. 

Results show that the choice of statistic distance measure weights least in the components, since the performance of different measures are close. The test verifies our choice of \textit{shared Bi-LSTM} and \textit{specific embedding}, since their significance is clear.


\begin{table}[th]
\small
\centering
\caption{Comparisons of different settings of our method.}
\begin{adjustbox}{width=7.7cm}
\begin{tabular}{|c|c|c|}
\hline
Settings & F1-score & $\delta$ \\
\hline
\textbf{Model-\RN{2} + $\mathcal{J}_\textnormal{Adap.}$-MMD} & 85.98 & 0\\
Model-\RN{2} + $\mathcal{J}_\textnormal{Adap.}$-KL & 85.88 & -0.10 \\
Model-\RN{2} + $\mathcal{J}_\textnormal{Adap.}$-CMD & 85.83 & -0.15 \\
Model-\RN{2} w/o $\mathcal{J}_\textnormal{Adap.}$ & 84.17 & -1.49 \\
Model-\RN{2} w/o shared Bi-LSTM & 85.26 & -0.40 \\
Model-\RN{2} w/o specific embedding & 82.09 & -3.57 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table}\label{table:ablation}


