\section{Introduction} \label{sec:intro}

Chinese word segmentation (CWS) is a fundamental task for Chinese natural language processing (NLP). Most state-of-art methods are based on statistical supervised learning and neural networks. They all rely heavily on human-annotated data, which is a time-consuming and expensive work. Specially, for domain CWS, \textit{e}.\textit{g}. medical area, the annotation expense is even higher because only domain experts are qualified for the work. 

Moreover, CWS tools based on open source datasets, \textit{e}.\textit{g}. SIGHAN2005\footnote{http://sighan.cs.uchicago.edu/bakeoff2005/}, face a significance performance drop when dealing with domain text. The ambiguity caused by domain terms and writing style makes it extremely difficult to train a universal CWS tool. As shown in Table \ref{table:1}, given a medical term “高铁血红蛋白血症” (methemoglobinemia), Chinese medical experts tend to annotate it as “高/铁/血红蛋白/血症”, which holds the correct definition, an anemia caused by hemoglobin with ``high iron'' (in Chinese, means iron with valence of three), corresponding to the morphology of ``Methemoglobinemia''. ``PKU'' stands for a model trained on PKU's People's Daily corpus, we can see that after segmentation the word ``铁血'' (jagged) is treated as a word, which is totally wrong semantically. Also, another popular Chinese CWS tool Jieba \footnote{https://github.com/fxsjy/jieba}
mistakenly puts the characters ``高'' and ``铁'' together, which stands for the high-speed bullet train in China.

\begin{table}[t!]
\centering
\begin{adjustbox}{width=7.7cm}
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf CWS tool} & \multicolumn{4}{c|}{\bf 高铁血红蛋白血症} \\
\hline
\multirow{2}{*}{PKU} & 高 & 铁血 & 红蛋白 & 血症 \\ & high & jagged & albumen & anemia\\
\hline
\multirow{2}{*}{Jieba} & 高铁 & \multicolumn{2}{c|}{血红蛋白}  & 血症 \\ & train & \multicolumn{2}{c|}{hemoglobin} & anemia\\
\hline
\multirow{2}{*}{Medical} & 高 & 铁 & 血红蛋白  & 血症 \\ & high & iron & hemoglobin & anemia\\
\hline
\end{tabular}
\end{adjustbox}
\caption{Medical CWS ambiguity with CWS tools. PKU stands for a model trained on PKU dataset. Jieba \footnote{https://github.com/fxsjy/jieba} is another
popular CWS tool.}\label{table:1}
\end{table}

In summary, domain specific CWS task poses significant challenges because:
\begin{enumerate}
\item Tools built on open source annotated corpus works bad on domain specific CWS.
\item Domain annotated data is scarce and annotating domain specific data costs expensively.
\item Leaving open source annotated data behind is a waste of resources.
\end{enumerate}

Recently, efforts have been made to exploit open source (high resource) data to improve the performance of domain specific (low resource) tasks and decrease the amount of domain annotated data ~\cite{DBLP:journals/corr/YangSC17, DBLP:journals/corr/PengD16a,DBLP:journals/corr/MouMYLXZJ16}.
% For example, ~\citet{DBLP:journals/corr/PengD16a} proposed a multi-task architecture, treating shared layers as \textit{transferable} between different domains or tasks, considering domain-specific layers as \textit{un-transferable}. 
% However, domain-specific layers can also share domain-invariant knowledge which is \textit{transferable}. For instance, consider a multi-task training task for CWS and part-of-speech tagging, the task-specific layers can share some knowledge intuitively, because the boundaries of CWS and POS are often the same. 

In this paper, we further develop multi-task learning ~\cite{Caruana1997,DBLP:journals/corr/PengD16a} and propose a novel framework, named \textit{Adaptive Multi-Task Transfer Learning}. Inspired by the success of \textit{Domain Adaptation}~\cite{Saenko:2010:AVC:1888089.1888106, DBLP:journals/corr/TzengHZSD14, DBLP:journals/corr/Long015}, we propose to minimize distribution distance of hidden representation between source and target domain, thus make the hidden representations \textit{adapt} to each other and obtain domain-invariant features. Finally, we annotated 3 medical datasets from different medical departments and medical forum, together with 3 open source datasets$^{\ref{fn:1}}$, and do extensive experiments.

The contribution of this paper can be summarized as follows:

\begin{itemize}
\item We propose a novel framework, \textit{Adaptive Multi-Task Transfer Learning}, for Chinese word segmentation.
\item To the best of our knowledge, we are the first to analyze the performance of transfer learning methods against the amount of disparity between 
target/source domains.
\item Our framework outperforms strong baselines especially when there is substantial \textit{disparity}. 
% \KZ{Heterogeneity is not the right word. 
% Disparity or inconsistency is. You should fix this everywhere.}
\item We open source 3 medical CWS datasets from different sources, which can be used for further study.
\end{itemize}
