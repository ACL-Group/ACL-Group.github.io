\section{Related Work}

\textbf{Chinese word segmentation} \quad CWS is a preliminary step for Chinese natural language processing. It has long been treated as a sequence tagging problem since ~\cite{xue2003chinese}. Supervised learning methods are used, including maximum entropy ~\cite{low2005maximum}, conditional random fields ~\cite{lafferty2001conditional,Peng:2004:CSN:1220355.1220436,DBLP:conf/paclic/ZhaoHLL06}. These methods depend heavily on hand-crafted features. Recently, neural networks have been for CWS tasks. ~\citet{DBLP:conf/emnlp/ZhengCX13} first introduced the neural network architecture to CWS task. Later, different variants of RNN and score functions are developed to improve the performance~\cite{DBLP:conf/acl/PeiGC14,DBLP:conf/emnlp/ChenQZLH15,DBLP:conf/acl/ChenQZH15,DBLP:journals/corr/CaiZ16,DBLP:conf/acl/CaiZZXWH17}.
 % ~\citet{DBLP:conf/emnlp/ChenQZLH15} adopted the long short-term memory(LSTM) to keep long dependency and avoided the limit of window size of local context. ~\citet{DBLP:conf/acl/ChenQZH15} proposed to employ a gated recursive neural network to incorporate the complicated combinations of the context characters. ~\citet{DBLP:journals/corr/CaiZ16} employed a factory to produce word representation given governed characters and proposed sentence-level likelihood evaluation system for CWS. ~\cite{DBLP:conf/acl/CaiZZXWH17} proposed a greedy neural word segmenter with balanced word and character embedding.
 Besides, joint CWS with part-of-speech tagging was proven to improve both tasks~\cite{DBLP:journals/corr/ChenQH16a,DBLP:conf/ijcai/ChenQH17}. Also, the heterogeneous annotating problem was discussed~\cite{DBLP:conf/emnlp/QiuZH13,DBLP:conf/acl/ChenSQH17}.

\noindent \textbf{Transfer Learning} \quad Transfer learning distills knowledge from source domain to help target domain achieve a higher performance~\cite{Pan:2010:STL:1850483.1850545}. In feature-based models, many transfer approached have been studied, including instance transfer~\cite{DBLP:conf/acl/JiangZ07,DBLP:conf/icml/LiaoXC05}, feature representation transfer~\cite{DBLP:conf/nips/ArgyriouEP06,DBLP:conf/nips/ArgyriouMPY07}, parameter transfer\cite{DBLP:conf/icml/LawrenceP04,DBLP:conf/nips/BonillaCW07} and relation knowledge transfer\cite{DBLP:conf/aaai/MihalkovaHM07,Mihalkova09transferlearning}. However, there's little study on transfer learning for neural networks. ~\cite{DBLP:journals/corr/MouMYLXZJ16} used intuitive methods (INIT, MULT) to study the transferability of neural networks on NLP applications. ~\citet{DBLP:journals/corr/PengD16a} proposed to use domain mask and linear projection upon multi-task learning~\cite{DBLP:journals/corr/Long015a}.