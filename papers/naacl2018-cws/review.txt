
Review #1
Appropriateness (1-3):	3
Comments on Appropriateness
This submission is marginally appropriate / inappropriate for its track because the paper follows the NAACL HLT 2018 Format style guidelines.
NLP Tasks / Applications Description
Description of the task / application:
Strengths:

Weaknesses:

Impact of NLP Tasks / Applications (1-4):	1
Description of Method
Description of the method: Adaptive Multi-Task Transfer Learning for Chinese word segmentation task by exploiting domain-invariant knowledge from high resource to low resource domains.
Strengths: Good results are obtained by the proposed methods when there is a large disparity between source and target domains.

Weaknesses: Comparison to the conventional methods is not enough.

Impact of Methods (1-4):	3
Description of Theoretical / Algorithmic Results
What are the results?
Strengths of the method or results?

Weaknesses of the method or results?

Impact of Theoretical / Algorithmic Results (1-4):	1
Empirical Results: Hypotheses
It seems to be reasonable to minimize distribution distance of hidden representation between source and target domain.
Empirical Results: Method
The performance of transfer learning methods against the amount of disparity between target/source domains are analyzed.
Good results are obtained by the proposed methods when there is a large disparity between source and target domains although comparison to the conventional methods is not enough.

Impact of Empirical Results (1-4):	3
Description of Data / Resources
Description of the data / resource: 3 medical Chinese word segmentation data sets.
Strengths:

Weaknesses: The database used in the experiments are not available.

Impact of Data / Resources (1-4):	3
Description of Software / Systems
Description of the software / system:
Strengths:

Weaknesses:

Impact of Software / System (1-4):	1
Description of Evaluation Methods / Metrics
Description of evaluation method / metric:
Strengths:

Weaknesses:

Impact of Evaluation Method / Metric (1-4):	1
Description of Other Contributions
Description of contribution:
Strengths:

Weaknesses:

Impact of Other Contribution (1-4):	1
  Contributions Summary
Contribution 1:
Contribution 2:

Contribution 3:

Originality (1-5):	3
Soundness/Correctness (1-5):	3
Substance (1-5):	3
Replicability (1-5):	2
Handling of Data / Resources:	Yes
Handling of Human Participants:	N/A
    Discussion of the Handling of Data, Resources and Participants
Handling of data / resources: 3 medical Chinese word segmentation data sets.
Handling of human participants:

Meaningful Comparison (1-5):	4
Related Work: ACL Guidelines:	Yes
Discussion of Related Work
Strengths:
Weaknesses:

The empirical results using the shared embedding proposed by Ruder 2017 should be shown and compared to that using the proposed domain specific embedding.

The MULT method proposed by Mou et al.(2016) should be compared to the Multi-Task method proposed in the submitted paper.

Missing references:

Comments on adherence to the ACL guidelines for authors:

Readability (1-5):	4
NAACL Guidelines:	Yes
Discussion of Readability, Style and Format
Comments on structure:
Comments on clarity and writing:

Comments on the use of the NAACL style and format guidelines:

ACL Guidelines:	Yes
Reviewer Confidence (1-5):	4
Presentation Format:	Poster
Questions for Authors
Section 1: The paper says "We open source 3 medical CWS datasets". However, they do not seem to be available yet. Where can we get the datasets? It should be described in the paper.
Section 2.1:

(1) The definition of character embedding is missing. It should be described in more detail.

(2) Why not using word or character n-gram embedding?

Section 3.4: The objective function described in Section 3.4 and the idea minimizing the mean negative log likelihood seem to be reasonable. The idea is similar to that of MIRA(Margin Infused Relaxed Algorithm).

Section 3.5.2:

(1) The definition of the domain specific embedding is missing. It should be described in more detail.

(2) The empirical results using the shared embedding proposed by Ruder 2017 should be shown and compared to that using the proposed domain specific embedding.

Section 3.5.3: The direct sum is used in the equation (18). How about biasing one of the domain specific information and the shared information?

Section 4.5: The detail of the development set is missing. How to construct the development set? The size of it also should be described.

Section 4.6:

(1) Why not using all of the available open databases? The available databases should be used to get better performance on the target domain data. The INIT may be the best way in that case in terms of the results shown in the submitted paper.

(2) The MULT method proposed by Mou et al.(2016) should be compared to the Multi-Task method proposed in the submitted paper.

Section 4.7: Were the best hyper-parameters determined by using the development set? It is unclear.

Section 4.8: The significance test should be done for Table 7-9.

Section 4.9: There is a misprint in the line 708 : "...reported in Table 4.9" -> "...reported in Table 9"




Review #2
Appropriateness (1-3):	3
Comments on Appropriateness
N/A
NLP Tasks / Applications Description
N/A
Impact of NLP Tasks / Applications (1-4):	1
Description of Method
Description of the method: Adaptive Multi-Task Transfer Learning: The authors propose to include an additional term into the loss function of a neural multi-task model. This term, which measures the similarity of the hidden representations produced by two encoder, forces encoders for two different domains to produce more similar output.
Strengths: (1) It is an intuitive and straightforward approach.

Weaknesses: (1) Simple pretraining+fine-tuning outperforms the new method for similar domains; (2) it is unclear how this compares to sharing embedding layers

Impact of Methods (1-4):	3
Description of Theoretical / Algorithmic Results
N/A
Impact of Theoretical / Algorithmic Results (1-4):	1
Empirical Results: Hypotheses
Hypotheses? Adaptive Multi-Task Transfer Learning performs better than the baselines (pretraining, single-task model, multi-task model w/o new method) for Chinese word segmentation in specialized domains.
Empirical Results: Method
What is the method for testing the hypothesis/es? A lot of experiments with different divergence measures, etc.
Strengths of the method and results? Hypothesis is confirmed for some domain pairs.

Weaknesses of the method and results? Hypothesis is not confirmed for other domain pairs.

Impact of Empirical Results (1-4):	3
Description of Data / Resources
Description of the data / resource: Chinese segmentation datasets for 3 medical domains.
Strengths: Interesting datasets, since there are apparently important differences in segmentation compared to other domains.

Weaknesses: The datasets are relatively small. This is kind of the point in this work; however, it limits their usability for other applications.

Impact of Data / Resources (1-4):	3
Description of Software / Systems
N/A
Impact of Software / System (1-4):	1
Description of Evaluation Methods / Metrics
N/A
Impact of Evaluation Method / Metric (1-4):	1
Description of Other Contributions
N/A
Impact of Other Contribution (1-4):	1
  Contributions Summary
Contribution 1: A new method for domain-adaptation based on multi-task models.
Contribution 2: An evaluation of the effectiveness of the new method.

Contribution 3: 3 datasets for Chinese word segmentation, medical domain.

Originality (1-5):	3
Soundness/Correctness (1-5):	3
Substance (1-5):	4
Replicability (1-5):	4
Handling of Data / Resources:	Yes
Handling of Human Participants:	N/A
    Discussion of the Handling of Data, Resources and Participants
N/A
Meaningful Comparison (1-5):	2
Related Work: ACL Guidelines:	Yes
Discussion of Related Work
Strengths: (1) The baseline model was cited correctly; (2) related work from two areas (Chinese word segmentation; transfer learning) was discussed
Weaknesses: In particular one sentence seemed a bit odd to me: "there's little study on transfer learning for neural networks". This ignores a lot of previous work, i.e., Bingel and Søgaard (2017), Rei (2017), etc. In the same way, "to the best of our knowledge, we are the first to analyze the performance of transfer learning methods against the amount of disparity between target/source domains". This might be okay if it was a bit less general, but in the current form, it ignores work like Johnson et al. (2017) in MT or Kann et al. (2017) in morphology.

Missing references: see above (not exhaustive!)

Readability (1-5):	4
NAACL Guidelines:	Yes
Discussion of Readability, Style and Format
Comments on structure: The structure seems good to me.
Comments on clarity and writing: The write-up can (and should!) be improved a lot. There are many spelling and grammar mistakes, i.e., line 88, line 160, line 289 (!), etc., etc.

ACL Guidelines:	Yes
Reviewer Confidence (1-5):	3
Presentation Format:	Oral
Questions for Authors
This paper describes a multi-task approach for domain adaptation. It is well structured and the general idea seems easily applicable also to similar problems.
However, the form of the write-up and the related work (see above) need some improvement.

Besides that, in my opinion, the presented experiments are problematic: First, even though the authors claim to be comparing to a multi-task baseline, the multi-task baseline corresponds to their models without the adaptive loss. However, they claim themselves that usually in multi-task setups embedding layers are shared. Since my intuition would be that the adaptive loss forces the embeddings for both domains to be more similar, to compare to this baseline is crucial. (There is one comparison in the ablation study. However, the datasets used there have high disparity and the performance of INIT is low. This should be tried for cases where INIT performs well.) Second, I am not sure if I understood right that only 10% of the available in-domain data were used for training? If yes, why so? It makes me think that maybe the method did not perform well for the full setting. If this is the case, it should at least be discussed.

Smaller comments: - The authors could extend the bi-LSTM section a little bit. - The authors should add a small explication on how/by who the new datasets have been annotated. - It seems to me that there might be multiple mistakes in Definition 3.1.



Review #3
Appropriateness (1-3):	3
Comments on Appropriateness
This submission is marginally appropriate / inappropriate for its track because:
NLP Tasks / Applications Description
Description of the task / application: The paper identifies a drop in performance with Chinese Word Segmentation systems resulting from a change in domain and proposes a domain-adaptive technique via multi-task transfer learning.
Strengths:

Weaknesses:

Impact of NLP Tasks / Applications (1-4):	2
Description of Method
Description of the method: Inclusion of an "Adaptive" term in the loss expression.
Strengths:

Weaknesses: The practical implementation of this is not clear.

Impact of Methods (1-4):	2
Description of Theoretical / Algorithmic Results
What are the results?
Strengths of the method or results?

Weaknesses of the method or results?

Impact of Theoretical / Algorithmic Results (1-4):	1
Empirical Results: Hypotheses
This is the problem. There is no explicit hypothesis or motivation for why the specific technique explored in the paper. Why should state outputs from the two domains be similar given the disparity of text contents.
Empirical Results: Method
What is the method for testing the hypothesis/es?
Strengths of the method and results?

Weaknesses of the method and results?

Impact of Empirical Results (1-4):	2
Description of Data / Resources
Description of the data / resource:
Strengths:

Weaknesses:

Impact of Data / Resources (1-4):	1
Description of Software / Systems
Description of the software / system: As far as I can see, there is no software to be released.
Strengths:

Weaknesses:

Impact of Software / System (1-4):	1
Description of Evaluation Methods / Metrics
Description of evaluation method / metric:
Strengths:

Weaknesses:

Impact of Evaluation Method / Metric (1-4):	1
Description of Other Contributions
Description of contribution:
Strengths:

Weaknesses:

Impact of Other Contribution (1-4):	1
  Contributions Summary
Contribution 1:
Contribution 2:

Contribution 3:

Originality (1-5):	3
Soundness/Correctness (1-5):	3
Substance (1-5):	2
Replicability (1-5):	3
Handling of Data / Resources:	Yes
Handling of Human Participants:	N/A
    Discussion of the Handling of Data, Resources and Participants
Handling of data / resources:
Handling of human participants:

Meaningful Comparison (1-5):	4
Related Work: ACL Guidelines:	Yes
Discussion of Related Work
Strengths:
Weaknesses:

Missing references:

Comments on adherence to the ACL guidelines for authors:

Readability (1-5):	2
NAACL Guidelines:	No
Discussion of Readability, Style and Format
Comments on structure:
Comments on clarity and writing: The paper carries many grammatical and fluency errors throughout. This is especially apparent on the first page, including throughout the abstract. In general, the writing is not to a high enough level, especially on the first page. There is also a quote of a blog on line 382.

Comments on the use of the NAACL style and format guidelines:

ACL Guidelines:	Yes
Reviewer Confidence (1-5):	3
Presentation Format:	Poster
