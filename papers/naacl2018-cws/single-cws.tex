\section{Single-Task Chinese word segmentation}\label{sec:single-task}

In this section, we briefly formulate the Chinese word segmentation task and introduce our base model, Bi-LSTM-CRF ~\cite{DBLP:journals/corr/HuangXY15}.

\subsection{Problem Formulation}

Chinese word segmentation is often treated as a sequence tagging problem on character level. BIES tagging scheme is broadly accepted by annotators, each character in sentence is labeled as one of $\mathcal{L} = \{B, I, E, S\}$, indicating begin, inside, end of a word, and a word consisting of a single character. 

Given a sequence with $n$ characters $X = \{x_1, \ldots, x_n \}$, the aim of the CWS task is to find a mapping from $X$ to $Y^\ast = \{y_1^\ast, \ldots, y_n^\ast\}$:

\small
\begin{equation} \label{eq:1}
Y^\ast = \mathop{\arg\max}_{Y \in \mathcal{L}^n} p(Y | X)
\end{equation}
\normalsize
where $\mathcal{L} = \{B, I, E, S\}$

The general architecture of neural CWS contains: (1) a character embedding layer; (2) an encoder automatically extracts feature and (3) a decoder inferences tag from the feature.

In this paper, we utilize a widely-used model as the base if our framework, which consists of a bi-directional long short-term memory neural network (BiLSTM) as encoder and a conditional random fields ~\cite{lafferty2001conditional} as decoder.

\subsection{Encoder}

In neural network models, an encoder is usually adopted to automatically extract feature instead of human-crafted feature engineering.\\

\noindent \textbf{Bi-LSTM} \quad LSTM is a popular variant of RNN in order to alleviate the vanishing gradient problem ~\cite{Bengio:1994:LLD:2325857.2328340,Hochreiter:1997}. In addition to considering \textit{past} information from left, Bidirectional LSTM also captures \textit{future} information from the right of the token.

% \noindent \textbf{LSTM} \quad Recurrent neural networks (RNN) are capable of capturing contextual information over arbitrary length of sequence. However, it is unable to encode the long-term dependency due to the vanishing gradient problem ~\cite{Bengio:1994:LLD:2325857.2328340}. Long short-term memory network is a 
% popular variant, it introduces a memory cell to preserve previous states and gate mechanism to control the updates of hidden states and memory cell ~\cite{Hochreiter:1997}. Mathematically, for a LSTM with parameters $\theta_a$ and sequence $X = \{x_1, \ldots, x_n\}$, the LSTM recurrently updates hidden states $h_t = LSTM(x_{t-1}, h_{t-1}, \theta_a)$ at timestep $t$.\\

% \noindent \textbf{{BiLSTM}} \quad In order to leverage information both side of the sequence, bi-directional LSTM was introduced with both forward and backward directions:
% \begin{equation}
% % \setlength{\abovedisplayskip}{3pt}
% % \setlength{\belowdisplayskip}{3pt}
% \begin{aligned}
% \overrightarrow{h_t} &= \textnormal{LSTM}(x_t, \overrightarrow{h_{t-1}}, \theta_a) \\
% \overleftarrow{h_t} &=  \textnormal{LSTM}(\overrightarrow{h_t}, \overleftarrow{h_{t-1}}, \theta_b)  \\
% h_t  &= \overrightarrow{h_t} \oplus \overleftarrow{h_t}
% \end{aligned}
% \end{equation}
% \noindent where $\overrightarrow{h_t}$ and $\overleftarrow{h_t}$ are the hidden states at timestep $t$ for the forward and backward LSTM respectively; $\oplus$ is concatenation operation; $\theta_a$ and $\theta_b$ denotes the parameters of the LSTMs.

\subsection{Decoder}

% After the encoder extracts feature from the sequence, the goal of CWS is to inference a sequence of labels $Y$ from the feature. At this stage, 
We deploy a conditional random fields layer as decoder. Specifically, $p(Y | X)$ in Eq. \eqref{eq:1} could be formulated as
\begin{equation}
\small
p(Y | X) = \frac{\exp(\Phi(X, Y))}{\sum_{Y^{'} \in \mathcal{L}^n}\exp(\Phi(X, Y^{'}))}
\end{equation}
\noindent Here, $\Phi(\cdot)$ is a potential function, considering
only the dependency between two consecutive labels:
\begin{equation}
\small
% \setlength{\belowdisplayskip}{0pt}
\Phi(X, Y) = \sum_{j=1}^{n}\phi(X, i, y_i, y_{i-1})
\end{equation}
\begin{equation}
% \setlength{\abovedisplayskip}{0pt}
\phi(X, i, y_i, y_{i-1}) = s(X, i)_{y_i} + t_{y_{i}y_{i-1}}
\end{equation}
\noindent where $s(X, i) \in \mathbb{R}^{|\mathcal{L}|}$ is a function that measure the score of the $i_{th}$ character for each label in $\mathcal{L} = \{B, I, E, S\}$, and $t \in \mathbb{R}^{|\mathcal{L}|\times|\mathcal{L}|}$ denotes the transition score between labels. More formally:
\begin{equation}
\small
s(X, i) = \mathbf{W}^\top h_i + \mathbf{b} 
\end{equation}
\noindent where $h_i$ is the hidden state of the $i^{th}$ character after BiLSTM; $\mathbf{W} \in \mathbb{R}^{d_h \times |\mathcal{L}|}$ and $\mathbf{b} \in \mathbb{R}^{|\mathcal{L}|}$ are all parameters in the model.
