\section{Approach}
\label{sec:approach}
We first present the small-data text style transfer~(ST$^2$) framework, which adapts meta-learning scheme to effectively exploit relevant knowledge from other style pairs, then introduce a newly constructed literature translation dataset covering a board range of fine-grained personal writing styles.
\begin{algorithm}\small
	\caption{ST$^2$}
	\label{alg:maml}
	\KwIn{a set of $N$ style pairs, $\{(s_{t,src}, s_{t,tgt}), \ldots \}$, where $t = 1, \ldots, N$, step sizes $\alpha, \beta$}
	\KwOut{transfer function $f_{\theta}: (x, s_{src}) \mapsto y$, where $s$ is the source style, $x$ is the original sentence, $y$ is the transferred sentence in target style}
	\While{not done}{
		\ForEach{style pair $(s_{t,src}, s_{t,tgt})$}{
			Initialize sub learner with $\theta_t = \theta$\;
			\For{step in 1, \ldots, K}{
				Sample batch data from support set of $t$\;
				Update transfer function $f_{\theta}$ using\\$\qquad\theta_t = \theta_t - \alpha \nabla_{\theta_t}\mathcal{L}_{t}(f_{\theta_t})$\;
			}
			Sample batch data from query set of $t$\;
			Evaluate $\mathcal{L}_t(f_{\theta_t})$\;
		}
		Update meta-learner with $\theta = \theta - \beta\nabla_{\theta}\sum_{t=1}^N \mathcal{L}_t(f_{\theta_t})$\;
	}
\end{algorithm}
\label{sec:st2}
%\KZ{I find the notation $s_{t,1}$ and $s_{t,2}$ a bit confusing and misleading.
%Why 1 and 2? Why not use $s_{in}$ and $s_{out}$. 
%Also you used $s$ to mean the source style
%in the output. So $s$ without a subscript becomes the source style?}

\subsection{Small Data Text Style Transfer}
In contrast to traditional single style pair transfer, in our application, 
the sub-tasks contain different pairs of styles to be transferred. 
The meta-learner contains the transfer function 
$f_{\theta}: (x, s_{src})\mapsto x'$, which takes a sentence $x$ with its 
style label $s_{src}$, and outputs a sentence $x'$ in the target style with 
similar content. This transfer function is shared by all pairs of styles 
in the meta-training phase. In practice, the transfer function $f_\theta$ can be parameterized by any existing single style pair neural transfer model. In addition, for base models which include adversarial functions for style disentanglement, the updates for the adversarial parameters are also included in the updates of meta-learner. Since the data size for each task with a 
single pair of styles is assumed to be small, 
the goal of ST$^2$ is to transfer knowledge from other style pairs for a better initialization in the fine-tuning phase of a specific sub-task. 
The multi-task style transfer via meta-learning (ST$^2$) algorithm is 
described in Algorithm \ref{alg:maml}. 

\subsection{Literature Translation~(LT) Dataset Construction}
\label{sec:lt}
%Current researches on text style transfer generally require large-scale datasets for training, thus they are not able to be applied to personal writing styles. One reason is that personal writing styles are relatively difficult to learn compared to more discriminative styles. Furthermore, sources of data reflecting personal writing styles are quite limited. 

We leverage literature translations by different translators as a new challenging text style transfer task. Because there are multiple versions of translation from the same source and it is possible to align these comparable sentences to construct ground-truth references, they are well-suited for style transfer. Moreover, in addition to a popular book translated by several other translators, 
a translator may have other written works, which can be used as 
a non-parallel training corpus as in standard style transfer setting.
\begin{table*}[!th]
	\renewcommand\tabcolsep{3.0pt} % 调整表格列间的长度
	\scriptsize
	\centering
	\begin{tabular}{c|cccccccccccc}
		\hline
		\multirow{2}{*}{\textbf{Statistics}}  & \multirow{2}{*}{\shortstack{Alban\\Kraisheime}}  & \multirow{2}{*}{\shortstack{Isabel\\F.Hapgood}} & \multirow{2}{*}{\shortstack{Andrew\\R. MacAndrew}} &\multirow{2}{*}{\shortstack{Richard\\Pevear}} &\multirow{2}{*}{\shortstack{David\\Hawkes}} &\multirow{2}{*}{\shortstack{Yang\\Xianyi}}
		&\multirow{2}{*}{\shortstack{John\\E. Woods}} &\multirow{2}{*}{\shortstack{H. T.\\Lowe-Porter}} &\multirow{2}{*}{\shortstack{Ian C.\\Johnston}} & \multirow{2}{*}{\shortstack{Robert\\Fagles}} &\multirow{2}{*}{\shortstack{Julie\\Rose}}
		&\multirow{2}{*}{\shortstack{Michael\\R. Katz}}\\
		& & & & & & & & & & & &\\
		\hline
		Vocab Size & 16,810 & 17,205 & 11,814 & 14,831 &15,121 &11,436 &21,869 &20,819 &11,168 &13,521 &18,020 &13,908\\
		Average Length &20.9   &20.4  &19.5 &20.7 &18.6 &15.9 &25.1 &26.7 &13.5 &26.6 &20.0 &19.0\\
		\# of Adjectives &11,798   &11,187   &10,050 &11,364 &9,622 &7,581 &17,521 &19,187 &8,236 &15,246 &11,255 &11,283\\
		\# of Adverbs &9,462   &8,325  &14,848  &13,580 &10,622 &9,555 &14,667 &17,369 &7,901 &13,991 &9,362 &13,506\\
		\# of Conjunctions &22,649  &20,673 &16,640  &19,675 &17,086 &13,762 &25,837 &26,789 &13,916 &27,801 &20,112 &16,650\\
		Flesch Readability &68.7 &66.7 &71.1 &67.4 &73.0 &79.6 &61.3 &59.6 &79.8 &70.2 &70.6 &66.4\\
		Dale-Chall Readability &6.2 &6.3 &5.2 &5.7 &5.4 &4.9 &6.5 &6.7 &5.2 &6.1 &5.8 &5.9 \\
		\hline
	\end{tabular}
	\caption{Linguistic statistics of each writer. The higher the Flesch readability, the easier it is to read. The Dale-Chall readability score indicates the grade level required to understand the text. See \url{https://en.wikipedia.org/wiki/Dale-Chall_readability_formula} for full description.}\label{tb:stats}
\end{table*}
We align sentences for each style pair using the algorithm provided by \citet{chen2019align} for testing. The sentence pairs are extracted from the common translated work for each writer pair. The test data has 1k sentences for each writer. While it is difficult to characterize different writing styles using discrete representation, we report some statistics regarding each writer's translation. For example, Ian C. Johnston tends to use more concise expressions(13.4 v.s. 26.6 tokens per sentence) than Robert Fagles even they possess similar vocabulary(11168 v.s. 13521) when translating \textit{The Iliad}. See table \ref{tb:stats} for more detailed statistics.

%\KZ{The following notation was a bit confusing so I tried to simplify it.}
We collect a set of writers (from $1$ through $n$) with unknown writing styles 
$\{s_1, \ldots, s_n\}$, each writer having his/her own set of written works $W_i$. In order to have a test set with ground-truth references, we used translated works from non-English sources\footnote{Obtained from \texttt{http://gen.lib.rus.ec/}.}, so that each writer in our set has at least one translated work that is from the same source as another writer. Namely, for each writing style $s_i$ in the set, there exists another style $s_j$ and $\exists\ w_m \in W_i$ 
and $\exists\ w_n \in W_j$ such that $src(w_m) = src(w_n)$. In this dataset, each writer has approximately 10k non-parallel sentences for training.


%\begin{table}[th]
%	\small
%	\centering
%	\begin{tabular}{c|cc}
%		\hline
%		\textbf{Writer} & \textbf{Vocab Size} & \textbf{Avg.SentLen} \\
%		\hline
%		Alban Kraisheime &16,810  &20.9  \\
%		Isabel F. Hapgood &17,205  &20.4  \\
%		Andrew R. MacAndrew &11,814  &19.5  \\
%		Richard Pevear &14,831  &20.7  \\
%		David Hawkes &15,121  &18.6  \\
%		Yang Xianyi &11,436  &15.9  \\
%		John E. Woods &21,869 &25.1  \\
%		H. T. Lowe-Porter &20,819 &26.7 \\
%		Ian C. Johnston &11,168 &13.5 \\
%		Robert Fagles &13,521 &26.6 \\
%		Isabel F. Hapgood &17,205 &20.5 \\
%		Julie Rose &18,020 &20.0 \\
%		Michael R. Katz &13,908 &19.0 \\
%		\hline
%	\end{tabular}
%	\caption{Statistics regarding the writing style of each writer.}\label{tb:translations}
%\end{table}

