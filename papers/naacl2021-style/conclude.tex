\section{Conclusion}
\label{sec:conclude}

We extend the concept of text style to general writing styles 
with limited training data for style transfer. To tackle this new challenging problem, we propose a multi-task style transfer (ST$^2$) framework, which is the first of its kind to apply meta-learning to small-data text style transfer. We use the literature translation dataset and the grouped standard dataset to evaluate the state-of-the-art models and our proposed framework. Unlike previous state-of-the-art models that are resource-demanding to impart rich knowledge into the networks, ST$^2$ is able to effectively utilize off-domain information to improve both language fluency and style transfer accuracy in a way that conventional 
pretrained language models fall short.

%Since baseline models might not be able to learn an effective language models from small datasets, which is a possible reason for their bad performances, we further eliminate this bias by pretraining the base models using data from all tasks. From the results, we ascertain that the enhancement of meta-learning framework is substantial.
