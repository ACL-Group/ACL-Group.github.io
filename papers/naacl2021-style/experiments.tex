\section{Experiments}
\label{sec:eval}
%To incorporate a more diverse range of styles, we gather two collections of datasets for our experiments. The first one is previously shown LT, and the second one is a grouped standard dataset comprised of several existing style transfer tasks.
%
%We apply our ST$^2$ framework on several state-of-the-art models on these two datasets, and verify the effectiveness of ST$^2$ on few-shot style transfer scheme. By comparing our framework with the pretrained base models, we further demonstrate that ST$^2$ is able to improve the performance in terms of content preservation, language fluency and style transfer accuracy by incorporating relevant knowledge from other style transfer tasks.

\begin{table}[th]\footnotesize
	\centering
	\begin{tabular}{cc}
		\hline
		\textbf{Dataset} & \textbf{Style} \\
		\hline
		Yelp & (health) positive/negative \\
		Amazon & (musical instrument) positive/negative \\
		GYAFC & (relations )formal/informal \\
		Wikipedia & standard/simple \\
		Bible & standard/easy \\
		Britannica & standard/simple \\
		Shakespeare & original/modern \\
		\hline
	\end{tabular}
	\caption{Grouped standard dataset.}\label{tb:data2}
\end{table}


\subsection{Setup}
We use the Literature Translation Datasets (LT) (see \secref{sec:lt}) 
and a grouped standard dataset (GSD) (see \tabref{tb:data2}) 
~\citep{li2018delete,Sudhakar2020,rao-tetreault-2018-dear} in the following
experiments. For all datasets listed in \tabref{tb:data2}, we use 10k sentences for training and 1k sentences for testing.

We use the following evaluation metrics:
\begin{itemize}
	\setlength{\itemsep}{0pt}
	\setlength{\parsep}{0pt}
	\setlength{\parskip}{0pt}
	\item \textbf{BLEU-3.}~We report the BLEU-3 score~\citep{papineni2002bleu} between references and model outputs. 
	\item \textbf{Perplexity (PPL).}~We use a Kneser-Ney bigram language model as measurement of fluency~\citep{kneser1995improved}. The language models are trained in the target domain for each style pair before reduction.
	\item \textbf{Transfer Accuracy (ACC).}~We pretrain a RoBERTa~\citep{liu2019roberta} classifier for each style pair. It achieves test accuracy of 85.0\% on LT and 83.9\% on GSD on average.
	\item \textbf{Overall Performance.}~The geometric mean(\textbf{G3}) and harmonic mean(\textbf{H3}) of BLEU-3, $\frac{1}{\log{PPL}}$ and ACC.
	\item \textbf{Human Evaluation~(HE).}~For each model with each transfer direction, we randomly sample 25 sentences for human evaluation. Each annotator(two native English speakers) is asked to assess the overall transfer effectiveness of each output sentence at a 4-point scale by jointly considering content preservation, transfer strength and language fluency, given the source sentence and the target style. For LT collection, we additionally provide annotators with writer-specific statistics as auxiliary information.
	The final score for each model is calculated as the average score given by the annotators. The kappa inter-judge agreement is 0.769.
\end{itemize}
%\KZ{Consider trimming down.}


%When ground-truth sentences are available in the test set~(all tasks in LT, GYAFC and Wikipedia in GSD), we calculate the BLEU scores between generated sentences and ground-truth sentences. When they are missing, we calculate self-BLEU scores based on the original sentences.

%\subsubsection*{Perplexity (PPL)}
%We use a Kneser-Ney bigram language model as measurement of fluency~\citep{kneser1995improved}. The language models are trained in the target domain for each style pair before reduction.


%\subsubsection*{Transfer Accuracy (ACC)}
%We pretrain a RoBERTa~\citep{liu2019roberta} classifier for each style pair. It achieves test accuracy of 85.0\% on LT and 83.9\% on GSD on average.
%It achieves test accuracy of 80.0\% $\sim$ 97.0\% on LT(85.0\% on average) and 73.8\% $\sim$ 98.5\%(83.9\% on average) on GSD.

%\KZ{Can we do some human eval for transfer accuracy? Not for all the datasets but for those that human can identify? But if those that human can easily identify has good automatic accuracy scores, then not much point. I think maybe you want to show the detailed transfer accuracies for each datasets, cos some of the numbers are not that high, like 67.8.}

%\begin{itemize}
%
%\end{itemize}

We adopt the following as our base models~(\textbf{CrossAlign}~\citep{shen2017style},  \textbf{VAE}~\citep{john2018disentangled} and \textbf{CP-VAE}~\citep{DBLP:journals/corr/abs-1905-11975}) against: \textbf{DeleteRetrieve}~\citep{li2018delete}, \textbf{DualRL}~\citep{luo2019dual}, \textbf{B-GST}\citep{Sudhakar2020}.


\begin{table*}[th]
	\footnotesize
	\centering
	\begin{tabular}{c|cccccc|cccccc}
		\hline
		\multirow{2}{*}{\textbf{Model}} & \multicolumn{6}{c|}{\textbf{LT}} & \multicolumn{6}{c}{\textbf{GSD}} \\
		\cline{2-13}
		& \textbf{BLEU-3}$^{\uparrow}$  & \textbf{PPL}$^\downarrow$ & \textbf{ACC}$^\uparrow$ &\textbf{G3}$^\uparrow$ &\textbf{H3}$^\uparrow$ & \textbf{HE}$^\uparrow$ & \textbf{BLEU-3}$^\uparrow$ & \textbf{PPL}$^\downarrow$ & \textbf{ACC}$^\uparrow$ &\textbf{G3}$^\uparrow$ &\textbf{H3}$^\uparrow$& \textbf{HE}$^\uparrow$ \\
		\hline
%		Template & 8.10  & 5.4 & 0.3 & 4.7 &1.1 & 2.3 & 68.6 & 5.3 & 0.42 &9.2 &1.2 & 3.0 \\
%		\hline

		DeleteRetrieve & 0.27 & 63.3 & 0.33 &1.3 &0.3 & 1.9 & 0.71  & 28.8 & 0.41 &1.8 &0.4 & 2.9 \\
		DualRL & 0.01 & 1400.7 & 0.49 &0.3 &0.1 & 1.9 & 5.80  & 171.0 & 0.41 &3.4 &0.4 & 2.4 \\

		B-GST & 0.56 & 24.4 &0.50  &1.8 &0.4 & 1.9  & 15.62  & 31.1 &0.36  &4.8 &0.6 & 1.9  \\

		\hline
		\underline{CrossAlign} & 0.0  & 1895.6 & 0.45 &0.0 &0.0 & 1.8 & 0.0  & 1049.7 & 0.36 &0.0 &0.0 & 1.8 \\
		ST$^2$-CA & 0.26 & 54.8 & 0.54 &1.3 &0.3 & 2.4 & \textbf{17.6} & 21.4 & 0.45 &\textbf{5.3} &0.8 & 3.3 
		\\
		\hline
		\underline{VAE} & 0.11  & 8.5 & 0.49 &1.2 &0.2 & 1.8 & 0.35 & 21.5 & 0.45 &1.5 &0.4 & 2.9 \\
		ST$^2$-VAE & 0.34  & 8.2 & \textbf{0.62} &1.9 &0.5 & 3.3 & 0.80 & 10.9 & \textbf{0.71} &2.4 &0.6 & 3.2 \\
		\hline
		\underline{CP-VAE} & 0.57 & 11.4 &0.41  &1.8 &0.5 & 2.4  & 2.52  & 8.3 &0.64  &3.8 &0.8 &3.1  \\
		ST$^2$-CP-VAE &\textbf{0.71}   &\textbf{8.1}  &0.49  &\textbf{2.3}  &\textbf{0.7} &\textbf{3.4}  &2.87  &\textbf{4.6}  &0.66  &5.0 &\textbf{1.6} &\textbf{3.5}  \\
		\hline
	\end{tabular}
	\caption{Results for multi-task style transfer. The larger$^\uparrow$/lower$^\downarrow$, the better. Our base models are underlined.}\label{tb:exp1}
\end{table*}

\subsection{Main Results}
\label{sec:st}

%All the baseline models are trained on the single style pair. The ST$^2$ model is trained on all the tasks for both LT and GSD sets, and then fine-tuned using a specific style pair in the sets. The scores are calculated as the average among all sub-tasks for both ST$^2$ models and baselines. 

%We note that the BLEU and PPL scores for the template-based model appear
%to be superior to those of other models. This is because it directly
%modifies the original sentence by changing a couple of words (resulting in a large self-BLEU). So the modification
%is actually minimum under the small data setting (see Appendix). However, its transfer accuracy suffers, which is
%well expected. All other models are generative in nature, which makes the comparisons among them more convincing.

The results are shown in Table \ref{tb:exp1}. From the results, we notice that state-of-the-art models fail to achieve satisfying performances in few-shot style transfer tasks, and many baseline models fail to generate syntactically or logically consistent sentences. However, even without the help of large-scale pretrained language model~(e.g., GPT in B-GST), models equipped with ST$^2$ are able to generate more fluent sentences both in terms of automatic evaluation and human evaluation, meanwhile achieving a higher transfer accuracy.  By inspecting evaluation results of all base models in Table \ref{tb:exp1} before and after being equipped with ST$^2$, we show that ST$^2$ is robust to the choice of base models as all evaluation metrics regarding transfer effectiveness obtain consistent improvement after applying ST$^2$ to the base models. It is worth noting that both CrossAlign and VAE are significantly inferior to the two state-of-the-art models B-GST and CP-VAE in terms of overall performance before the enhancement by ST$^2$. By incorporating related tasks into a unified transfer model, the learned parameters can be better adapted to one specific task with minimal amount of data.  While ST$^2$-VAE yields better transfer strength and ST$^2$-CA achieves higher BLEU score on GSD, ST$^2$-CP-VAE learns to strike a balance and obtains the best overall performance. A similar phenomenon is also observed on LT. For qualitative analysis, we randomly select transferred sentences
by baseline models, pretrained base models and ST$^2$ models and show them in the Appendix.
%ST$^2$-CA achieves better performance on GSD because its binary style embedding mechanism suffices to transfer sentences between styles like sentiment polarity, while ST$^2$-VAE fuses disentangled style embeddings of all instances from target corpus hence yielding better transfer quality on LT.



We might be tempted to conclude that this is simply because the
ST$^2$ models learn better language models. 
Therefore, further experiments are required.
\subsection{Pretrained Base Models}
\label{sec:pretrain}
\begin{table}[ht]\footnotesize
	\centering
	\begin{tabular}{c|cccc}
		\hline
		\textbf{Model} & \textbf{BLEU-3}$^\uparrow$ & \textbf{PPL}$^\downarrow$ & \textbf{ACC}$^\uparrow$ & \textbf{HE}$^\uparrow$ \\
		\hline
		CA$^*$ &\textbf{24.21}   & 12.2 & 0.32 & 1.9 \\
		VAE$^*$ &1.84   & 22.4 & 0.48 & 2.0 \\
		\hline
		ST$^2$-CA$^*$ &14.62   & 23.2 & 0.37 &2.2  \\
		ST$^2$-VAE$^*$ &0.95   & 10.9 & 0.66  &2.9  \\
		\hline
		ST$^2$-CA & 17.60  & 21.4 & 0.45 & 3.3 \\
		ST$^2$-VAE & 0.80  & \textbf{10.9} & \textbf{0.71} & 3.2 \\
		\hline
	\end{tabular}
	\caption{Results on GSD for pretrained ($^*$) base models and ST$^2$. HE means human evaluation score.}\label{tb:exp2}
\end{table}
Based on the previous reasoning, we extract and pretrain the language model module in two of our base models (CrossAlign and VAE) on the union of training data from all sub-tasks. Starting with a well-trained language model, we then fine-tune the models for each style transfer task. By comparing pretrained base models with our ST$^2$ models, we verify that meta-learning framework can improve the style transfer accuracy in addition to language fluency.

In addition, to examine the effect of pretraining combined with meta-learning, we also add a pretraining phase to our ST$^2$ model. The results are included in Table \ref{tb:exp2}.
%\begin{figure}[t!]
%	\underline{\small Pretrained CrossAlign}\\
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/ca_pre_c.pdf}
%	\end{minipage}
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/ca_pre_s.pdf}
%	\end{minipage}
%	\underline{\small ST$^2$-CrossAlign}\\
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/ca_maml_c.pdf}
%	\end{minipage}
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/ca_maml_s.pdf}
%	\end{minipage}
%	\underline{\small Pretrained VAE}\\
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/vae_pre_c.pdf}
%	\end{minipage}
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/vae_pre_s.pdf}
%	\end{minipage}
%	\underline{\small ST$^2$-VAE}\\
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/vae_maml_c.pdf}
%	\end{minipage}
%	\begin{minipage}{0.45\linewidth}
%		\centering
%		\includegraphics[width=3.5cm]{./images/vae_maml_s.pdf}
%	\end{minipage}
%	\caption{t-SNE plots for content(left) and style(right) embedding}\label{fig:tsne}
%\end{figure}
By adding a pretraining phase, the models get a chance to see all the data and learn to generate fluent sentences via reconstruction. Therefore, it is not surprising that the BLEU and PPL gives significantly better results than before but at a cost of style transfer accuracy. In effect, the models tend to reconstruct the original sentence and do not transfer the style. In comparison, our ST$^2$ model learns to generate reasonable sentences and transfer styles jointly in the training phase. Therefore, it is still superior in terms of style transfer accuracy. This verifies that the success of ST$^2$ has not merely resulted from a larger training dataset. The way that the model updates its knowledge is parallel, rather than sequential, which contributes to better language models and more effective style transfer. We also notice that the pretraining is not crucial to ST$^2$, suggesting that it is the meta-learning framework that significantly contributes to the model's improvements in generating fluent sentences and efficacy in
style transfers.

%\subsection{Disentanglement of Style}
%\label{sec:disentangle}
%
%Following the experiments adapted by \citet{john2018disentangled}, we use t-SNE plots as shown in Figure \ref{fig:tsne} to analyze the effectiveness of disentanglement of style embedding and content embedding in the latent space~\citep{maaten2008visualizing}. In particular, we compare the pretrained base models (CrossAlign and VAE) and our ST$^2$ models.
%
%
%
%
%These two models, together with our ST$^2$ models attempt to disentangle style and content in latent space, and thus is well suited for this experiment, while it is unreasonable to treat hidden state vectors in other baseline models as content/style embedding. Therefore, they are excluded from this experiment.
%
%As we can see from the figures, the content space learned by all models are relatively clustered, while the style spaces are more separated in our ST$^2$ models than the pretrained base models. This verifies that the improvements of meta-learning framework is not limited to a better language model, but also in terms of the disentanglement of styles.


