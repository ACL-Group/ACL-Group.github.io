\section{Introduction}
\label{sec:intro}

Text style transfer aims at rephrasing a given sentence in a desired style. It can be used to rewrite stylized literature works, generate different styles of journals or news (e.g., formal/informal), and to adapt educational texts with 
specialized knowledge to learners at various levels.

Due to the lack of parallel data for this task, previous work~\citep{shen2017style,john2018disentangled,fu2018style} mainly focused on unsupervised learning of styles, usually assuming that there is a substantial amount of non-parallel corpora for each style and that the contents of the two corpora do not differ significantly. Existing state-of-the-art models either attempt to disentangle style and content in the latent space~\citep{shen2017style,john2018disentangled,fu2018style}, directly modifies the input sentence to remove stylized words~\citep{li2018delete}, or use reinforcement learning to control the generation of transferred sentences in terms of style and content~\citep{wu2019hierarchical,luo2019dual}. However, most of the approaches fail to generate fluent sentences with the desired style on low-resource datasets based on our experiments~(\secref{sec:st}).


Moreover, existing work has been limited to a small range of discrete styles such as sentiment polarity and textual formality, with no evidence to show that 
they can be generalized to more challenging out-of-domain transfer tasks. In real-world scenarios, the general notion of style is not restricted to the heavily studied discrete attribute labels, but also includes the writing style of a person. However, even the most productive writer can't produce a fraction of the text corpora commonly used for unsupervised training of style transfer today.
In the real world, there exists as many writing styles as you can imagine, making it impossible to train style transfer models tailored for each task from scratch. 

By viewing the transfer between each pair of styles as a separate domain-specific task, we propose to formulate a multi-task learning problem where each task corresponds to the transfer between a pair of styles. Based on the multi-task formulation, we further apply a meta-learning scheme to take advantage of data from other domains, i.e., other styles, to enhance the performance of few-shot style transfer~\citep{finn2017model}. To extend the scope of text style transfer beyond the coarse-grained styles, we take both personal writing styles and previously studied general styles, such as sentiment style, into account. We apply our framework to several state-of-the-art style transfer models on two collections of datasets, each with several style transfer tasks with small training data, and verify that information from different style domains can be effectively utilized to enhance the abilities in content preservation, style transfer accuracy, and language fluency.

Our contributions are listed as follows:
\begin{itemize}
	\item We create and release a literature writing style transfer dataset, which is the first of its kind that captures more fine-grained stylistic characteristics of text rather than discrete style label~(e.g., sentiment).
	\item We propose the Multi-task Small-data Text Style Transfer (ST$^2$) framework, which adapts meta-learning to enable flexible plug-in of existing state-of-the-art models, and this is the first work that applies meta-learning on text style transfer to the best of our knowledge.
	\item Experimental results demonstrate that the proposed algorithm substantially improves its base models in the few-shot text style transfer task for both traditional and our newly created datasets, in terms of content preservation, transfer accuracy and language fluency.
%	\item By testing the effect of pretraining of encoders/decoders in state-of-art methods, we verify that models with meta-learning framework is not only superior in language fluency because by setting they have more data to learn from, but also in style transfer accuracy.

\end{itemize}



