To R1:
Thanks for your careful reading and valuable feedback! We answer your questions as follows:

"In lines 83-84...": 
We consider the text style transfer task in the LT dataset to be finer grained, 
as opposed to the binary styles such as positive/negative or formal/information in most existing text style transfer benchmarks such as Yelp/Amazon.  
It is also different from "Zero-Shot Fine-Grained Style Transfer" paper by
Smith et al. (2019) because that task is defined over a fixed, albeit large,
discrete set of styles, whereas our task is defined on an unlimited, 
continuous space of writing styles by different authors.  
Therefore our definition of "fine-grain" is different from theirs. We will add
this piece of discussion if the paper is accepted.

"The described algorithm may be problematic...": 
Step 12 should not be included in the Step 2 for-loop because learning signal 
from all tasks is expected to contribute simultaneously to the 
meta-learner parameter. Otherwise, the learning process becomes 
sequential and fails to learn a good initialization that is optimized for 
all tasks.

"What's the basic experimental setting..." 
There are 7 pairs of styles in LT and GSD. During the training phase for each collection, the support set is composed of batches of sentences from each pair of styles. Testing is also performed for each style pair in both directions. We set the support set batch size as 64 and query set batch size as 16 for each style pair to control the memory footprint. We also attempted a few other options of batch sizes but the differences are not big so we didn't do 
exhaustive tuning.

"In most cases of Table 3..."
Our baseline models used to perform well
in previous works because they were applied on large datasets.
In our work, we have showed that they are not as effective when dealing with
small datasets like LT and GSD, which is evident from the small G3 and H3 scores,
correctly noted by Reviewer 1. However, when these models are integrated into
our ST2 framework, they are invariably improved, if you compare
the scores between base models and their ST2 versions. 
This is the main point we want to convey in Table 3. Therefore, we didn't show the
result of applying ST2 on other models including Smith (2019) and Li (2019)
mentioned by Reviewer 1, due to space constraint of this short paper.

"In Table 4..."
The BLEU score in style transfer literature is commonly used to measure content 
preservation. But GSD is a slight exception. Only 3 style pairs out of 7 in GSD 
have parallel data in the test set, which is needed to compute BLEU scores. 
For the remaining 4 style pairs, the BLEU is computed w.r.t the original sentences 
(also known as self-BLEU). As a result, the reported BLEU for GSD is actually a 
combination of real BLEU scores and the self-BLEU scores. 
As discussed in line 263-267 of our paper, 
after additional pretraining, base models tend to reconstruct the 
original sentences and their self-BLEU scores tend to be higher. 
Therefore, BLEU score is not a very good indicator for GSD. Instead, one
should look at PPL, ACC or human evaluation. 

"VAE is not a good baseline..."
Our VAE base model is not the vanilla VAE. It is augmented with multi-task 
and adversarial loss to achieve disentanglement in the 
latent space (John et al., 2019). We use VAE because it is a frequently adopted strong baseline.

To R2:
Thanks for your careful reading and useful feedback!

"The motivation for the newly introduced dataset is missing":
As explained in our introduction, most benchmarks used for unsupervised text style transfer are targeting binary style (e.g., sentiment, gender, etc.). In contrast, our proposed personal writing style transfer task is more fine-grained and poses a significant challenge for existing SOTA models. 
Another important point is the way we construct LT is scalable and can be applied to create 
other larger and higher-quality datasets for similar fine-grained text style transfer. Detailed characteristics about LT is shown in Table 1.

"LT seems like just an extra dataset": 
LT possesses distinctive characteristics from GSD. We experiment with both of them under the
small-data regime.

"Some details of LT is not clear...":
Each written literature work in LT has two translated English versions by two different writers. 
The choice of the writers is not important. In our case we chose the writers based on our reading experience while making sure that the two paired writers each have distinct styles.


To R3:
Thanks for your careful reading and useful feedback!

"Though by Kneser-Ney...":
You got it right: we choose the bigram KN language model to avoid excessive computational cost since there is actually 14 pairs of styles, consequently 28 target domains. 

"What was the reason behind...":
Budget constraint is one of the reasons. In our preliminary studies, we found that human scores for content preservation and transfer accuracy correlate fairly well with the automatic metrics. Therefore we didn't present these individual human scores but focus on the human evaluation of overall transfer effectiveness.


"The authors should perhaps...":
The BLEU-3 is indeed on a 1-100 scale. The rather low BLEU score on LT indicates that fine-grained text style transfer remains a challenging task to current models and is worthy of further study.

"How important were the GSD...":
GSD and LT collections are trained and tested separately. The good results on LT are obtained without any help from GSD.

"Any particular reason...": 
We also computed BLEU-4 but found the BLEU-4 results of all models to be close to 0, 
so we report BLEU-3 instead.
