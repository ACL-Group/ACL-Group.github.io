Review #1
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?

This work proposes a meta-learning framework to achieve text style transfer tasks with small data. The proposed method utilizes the basic MAML setting to carry out the meta-learning algorithm. To create a suitable meta-learning experiment setting, the author also collects a dataset with different writing styles from translation corpora, in which a source serves as the pivot to align the texts from different writers. The point and idea of the proposed work are very interesting, many claims, experimental settings, and results are questionable. In summary, the paper and experiments should be further revised in detail before acceptance. The main strengths to accept and weaknesses to reject are listed below.
Reasons to accept

The proposed work brings up an interesting and promising meta-learning topic, which is very important to the text style transfer community.
Reasons to reject

The proposed meta-learning algorithm is problematic. The experimental setting doesn't contain important details about meta-learning, which makes the whole experimental results questionable.

The experimental results are not favorable. Most cases demonstrate that the generated results are untrustable.

Some claims are unsuitable. In addition, the proposed work misses necessary discussion and experimental comparisons with important related works (listed in the Missing Reference section).

Questions for the Author(s)

In lines 83 - 84, the author claims that the crafted dataset has fine-grained stylistic characteristics? Could the author elaborate more about this? e.g., how to define the fine-grained stylistic characteristics in the proposed dataset, and what are the examples for these fine-grained characteristics?

The described algorithm may be problematic. Should step 12 be inside step 2 for loop (after step 10)? The meta leaner's parameter should be updated right after the sub learner loss on the query set.

What's the basic experimental setting for the text style transfer metal learning task in LT and GSD datasets？ e.g., how many style pairs used for training, testing respectively? In every style pair, how many instances used in the support set and query set, respectively? These details are very essential in the meta-learning task, while the author fails to provide these details in both the main text and supplementary material. According to line 172 - 173, the author only divides the training and testing sentence. However, this is not the general experimental setting for meta-learning.

In most cases of Table 3, the BLEU is much lower than 1, transfer acc is lower than 50% and the performance on G3 and H3 (much lower than 5) is too low to trust. Could the author explain whether the generated results are meaningful and significant enough to demonstrate the transfer quality, rather than just comparing with the baselines?

In Table 4, it seems that the proposed model degenerates the performance of the well-trained model, e.g., sacrifice the BLEU to improve the ACC metrics. It is well known to the community that, there is a trade-off between BLEU and ACC performance when tuning the model in training. I am wondering whether the performance gains come from model tuning instead of the proposed meta-learning method.

Missing References

Zero-Shot Fine-Grained Style Transfer: Leveraging Distributed Continuous Style Representations to Transfer To Unseen Styles

Domain adaptive text style transfer

Typos, Grammar, Style, and Presentation Improvements

VAE is not a good baseline to use in the text style transfer task.
Overall Recommendation:	2




Review #2
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?

This paper tackles the text style transfer problem with a meta-learning approach. The main contribution is that extensive empirical results on several datasets confirm the effectiveness of this approach as well as a new dataset introduced.
Strengths:

This paper presents the first work to introduce meta-learning to the text style transfer task and it is good to see it works on different base models
The paper is well organized and easy to follow.
The authors also introduce a new dataset based on the literature translation.
Weakness:

The motivation for the newly introduced dataset is missing. Why we need this dataset? How this dataset is different from others? Why is it challenging? From Table 3, LT seems like just an extra dataset. More discussions on the new dataset are needed for others to know the challenging part.
Some details of LT is not clear. How did the authors choose the writers? The explanation from Line 156 to Line 162 is hard to understand.
Reasons to accept

This paper presents the first work to introduce meta-learning to the text style transfer task and it is good to see it works on different base models
The paper is well organized and easy to follow.
The authors also introduce a new dataset based on the literature translation.
Reasons to reject

The motivation for the newly introduced dataset is missing. Why we need this dataset? How this dataset is different from others? Why is it challenging? From Table 3, LT seems like just an extra dataset. More discussions on the new dataset are needed for others to know the challenging part.
Some details of LT is not clear. How did the authors choose the writers? The explanation from Line 156 to Line 162 is hard to understand.
Overall Recommendation:	3.5


Review #3
What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?

Description:
This paper works on the problem of unsupervised text style transfer. Specifically, this paper aims to do two things a) Explore fine-grained style transfer, especially the setting where there are many individual style transfer problems to solve, with limited data available to do the same b) Investigate use of meta-learning frameworks for style transfer through the a) setup. To facilitate a), a new dataset of personal writing style annotated documents is collected and planned to be made public. For b), results from using the MAML framework atop different base architectures to solve a) are discussed. In addition to the set of tasks from the aforementioned writing-style-transfer benchmark, the more conventional coarse-grained style transfer datasets (e.g Sentiment and GYAFC [Rao and Tetreault, 18]) are also included in the mix of tasks on which to jointly carry out meta learning. ST^2 is shown to improve in each case over the respective plain base architecture on all the task metrics (perplexity, BLEU and transfer accuracy), with the experiments being carried out on three different base architectures. In particular, the ST^2 + CP-VAE variant manages to beat not just its parent method CP-VAE and the other two base architectures but also other competitive models such as (Sudhakar et al, 2020).

Strengths:

Application of the otherwise-widely-popular-in-other-tasks MAML-like framework has been rare in the task of language style transfer - this paper is a valuable artifact in that regard.

Furthermore, MAML can be used with any base arch, hence the findings are not specific to a certain set of architectures and are more widely applicable - the authors also present results likewise with multiple base architectures.

The multi-author literature writing style ST^2 dataset proposed and released here would certainly be a valuable resource, since current style transfer benchmarks either deal with less fine-grained and coarser styles (e.g sentiment, formality, male/female) or deal with only one particular literary style.

Weaknesses:

A lack of qualitative examples in the main body itself. This would have been pertinent to have given the nature of the dataset is itself new.
Reasons to accept

My reasons to accept are identical to the Strengths mentioned in the answer for the earlier section:
1) Introduction of a novel dataset for unsupervised style transfer with fine-grained, author-annotated documents for multiple authors. The presence of more realistic properties in the dataset, such as there being limited documents per author are also a salient aspect of the dataset.

2) Being one of the few papers to investigate and discuss meta-learning frameworks for text style transfer.

3) Performing experiments over multiple base architectures to confirm that the proposed meta learning framework improves things agnostic to the specific type of base architecture/model being used.

4) Clear writing and presentation of the dataset and the meta-learning experiments, including a well-organized and well-written Appendix.

Reasons to reject

I do not see any risks or concerns about this paper being presented at the venue.
Questions for the Author(s)

a) Though by Kneser-Ney bigram LM is by no means a non-standard LM approach, is there a particular reason why you chose a somewhat “weaker” language model (bigram, and non-neural) rather than not that unviable stronger variants (tri/4 gram or neural models, e.g awd-lstm-lm / GPT/GPT2). Was the ease of target finetuning the consideration? (which is not entirely un-understandable, though GPT-GPT2 are not that hard to finetune as long as there are say, a single digit number of target domains)
b) What was the reason behind choosing to ask the annotator about all three aspects together rather than through separate studies? Though budget constraints if they exist are understandable, it would have been nice to have separate evaluations for fluency and content preservation [whether annotators can at all predict transfer accuracy and “learn” a target style by looking at a few sentences, is in my opinion questionable anyway - but also happy to know the authors views on this]

c) The authors should perhaps clarify or mention at atleast one point whether the BLEU scores are on a 0-1 scale or a 1-100 scale. Since many BLEU scores are rather low and sometimes <1 [I gather the scale is 1-100 from my reading] , it gets a bit difficult to guess which scale is being followed by looking at the numbers alone.

d) How important were the GSD datasets in getting the MAML setup to work? Is it possible to train the MAML setup to any reasonable effect using only the LT-related tasks?

e) Any particular reason for the unusual choice of BLEU-3 as a metric? (rather than the more typical BLEU-4)

Missing References

Not all the sources of datasets mentioned in Table 2 have been cited - specifically the following citations are missing
[For the Shakespeare dataset]: Xu, W., Ritter, A., Dolan, B., Grishman, R., & Cherry, C. (2012, December). Paraphrasing for style. In Proceedings of COLING 2012 (pp. 2899-2914).

[For the Bible dataset]: Carlson, K., Riddell, A., & Rockmore, D. (2018). Evaluating prose style transfer with the Bible. Royal Society open science, 5(10), 171920.

Overall Recommendation:	4

