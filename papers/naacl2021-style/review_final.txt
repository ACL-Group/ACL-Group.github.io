Dear Siyu Ren:


We are sorry to inform you that the following submission was not selected by the program committee to appear at NAACL-HLT 2021:


    ST$^2$: Small-data Text Style Transfer via Multi-task Meta-Learning


The selection process was very competitive and unfortunately there were many quality papers that we could not accept. We considered a range of factors including the reviewers' assessment and scores, reviewer discussions, and careful assessment by the area chairs and senior area chairs.  


Please note that this year, NAACL-HLT 2021 added an ethics review process. If your paper was flagged for review by the ethics committee, it may have received ethics reviews in addition to the technical reviews.  This was done through a process run completely separately from the standard technical review, and the ethics reviews and scores were not considered when judging technical quality of the paper. 


We have enclosed all of the reviewer comments below, which we hope will be of use for future submissions.

We are still finalizing the format of the virtual NAACL-HLT 2021 conference, and will announce more details soon. We hope you will still be able to attend and contribute to the conference in other ways. Please let us know if you have any questions. 


Best Regards,

Program Chairs, NAACL 2021

NAACL-HLT 2021

------------ 
 ============================================================================ 
NAACL-HLT 2021 Reviews for Submission #373
============================================================================ 

Title: ST$^2$: Small-data Text Style Transfer via Multi-task Meta-Learning
Authors: Siyu Ren, Xiwen Chen and Kenny Zhu


============================================================================
                            REVIEWER #1
============================================================================

What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
---------------------------------------------------------------------------
This work proposes a meta-learning framework to achieve text style transfer tasks with small data. The proposed method utilizes the basic MAML setting to carry out the meta-learning algorithm. To create a suitable meta-learning experiment setting, the author also collects a dataset with different writing styles from translation corpora, in which a source serves as the pivot to align the texts from different writers. The point and idea of the proposed work are very interesting, many claims, experimental settings, and results are questionable. In summary, the paper and experiments should be further revised in detail before acceptance. The main strengths to accept and weaknesses to reject are listed below.
---------------------------------------------------------------------------


Reasons to accept
---------------------------------------------------------------------------
1. The proposed work brings up an interesting and promising meta-learning topic, which is very important to the text style transfer community.
---------------------------------------------------------------------------


Reasons to reject
---------------------------------------------------------------------------
1. The proposed meta-learning algorithm is problematic. The experimental setting doesn't contain important details about meta-learning, which makes the whole experimental results questionable. 

2. The experimental results are not favorable. Most cases demonstrate that the generated results are untrustable. 

3. Some claims are unsuitable. In addition, the proposed work misses necessary discussion and experimental comparisons with important related works (listed in the Missing Reference section).
---------------------------------------------------------------------------


Questions for the Author(s)
---------------------------------------------------------------------------
1. In lines 83 - 84, the author claims that the crafted dataset has fine-grained stylistic characteristics? Could the author elaborate more about this? e.g., how to define the fine-grained stylistic characteristics in the proposed dataset, and what are the examples for these fine-grained characteristics? 

2. The described algorithm may be problematic. Should step 12 be inside step 2 for loop (after step 10)? The meta leaner's parameter should be updated right after the sub learner loss on the query set. 

3. What's the basic experimental setting for the text style transfer metal learning task in LT and GSD datasets？ e.g., how many style pairs used for training, testing respectively? In every style pair, how many instances used in the support set and query set, respectively? These details are very essential in the meta-learning task, while the author fails to provide these details in both the main text and supplementary material. According to line 172 - 173, the author only divides the training and testing sentence. However, this is not the general experimental setting for meta-learning. 

4. In most cases of Table 3, the BLEU is much lower than 1, transfer acc is lower than 50% and the performance on G3 and H3 (much lower than 5) is too low to trust. Could the author explain whether the generated results are meaningful and significant enough to demonstrate the transfer quality, rather than just comparing with the baselines? 

5. In Table 4, it seems that the proposed model degenerates the performance of the well-trained model, e.g., sacrifice the BLEU to improve the ACC metrics. It is well known to the community that, there is a trade-off between BLEU and ACC performance when tuning the model in training. I am wondering whether the performance gains come from model tuning instead of the proposed meta-learning method.

#####################
After the rebuttal:
#####################

The author's response resolved a few of my questions and concerns, thus I updated my score accordingly. 

First I do admit the topic and the idea of the paper is very interesting, especially there is no existing meta-learning work in the text style transfer field right now.

However, I still feel a little bit disappointed about some other aspects:

1. Many details are missing. For example, both the main paper and the supplementary materials don't include ANY of the basic experimental settings for the meta-learning experiment. This will cause difficulty to reproduce the results for comparison in the community.

2. The motivation of the crafted dataset is unclear (same as R2). The author fails to explain the motivation of creating such an LT dataset for meta-learning. Why it needs different writing styles from different writers? What's the reason and potential applications that we want to do style transfer between different writers (For example, from plain language to different writing style makes more sense to me)? I feel like the author mainly "crafted" these datasets for the meta-learning task instead of a well-designed target. Similarly, GSD is a group of current standard datasets. Is it reasonable to group YELP with the Shakespeare dataset, since one refers to sentiment transfer and another refers to writing style transfer?

3. The experimental results are underperformed and are not convincing. Style transfer tasks require basic content preservation between the source and the transferred sentence. However, BLEU with 0.71 and 2.87 means there are few similarities between the source and the output. For reference, a reasonable BLEU in YELP from two years ago is around 20. Though the performance should be undermined since the task is proposed under limited data and meta-learning task, 0.71 BLEU indicates there is no successful content preservation and style transfer. In addition, the author responds about the BLEU metrics: "As a result, the reported BLEU for GSD is actually a combination of real BLEU scores and the self-BLEU scores. ". The author never mentioned this in the paper, and to my humble knowledge, I didn't know this is a good and standard usage of BLEU metrics.

Minor: The author claims the dataset and the model have fine-grained characteristics. To my best knowledge, this is an unsuitable claim, as there is no fine-grained degree that can be controlled in the generation. More precise examples for fine-grained text style transfer can be referred to https://www.aclweb.org/anthology/P19-1194.pdf, https://arxiv.org/abs/1811.00552.

In summary, I think if the author could fix the aforementioned issues, the submission of the paper would be really strong.
---------------------------------------------------------------------------


Missing References
---------------------------------------------------------------------------
1. Zero-Shot Fine-Grained Style Transfer: Leveraging Distributed Continuous Style Representations to Transfer To Unseen Styles

2. Domain adaptive text style transfer
---------------------------------------------------------------------------


Typos, Grammar, Style, and Presentation Improvements
---------------------------------------------------------------------------
VAE is not a good baseline to use in the text style transfer task.

################
After rebuttal:
###############

VAE's style transfer performance is not good as an auto-encoder network, e.g., Hu, et al's ControlGen switch VAE backbone to auto-encoder model as the second version in the Texar repo.
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                  Overall Recommendation: 2.5


============================================================================
                            REVIEWER #2
============================================================================

What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
---------------------------------------------------------------------------
This paper tackles the text style transfer problem with a meta-learning approach. The main contribution is that extensive empirical results on several datasets confirm the effectiveness of this approach as well as a new dataset introduced. 

Strengths:
1. This paper presents the first work to introduce meta-learning to the text style transfer task and it is good to see it works on different base models
2. The paper is well organized and easy to follow.
3. The authors also introduce a new dataset based on the literature translation.

Weakness:
1. The motivation for the newly introduced dataset is missing. Why we need this dataset?  How this dataset is different from others? Why is it challenging? From Table 3, LT seems like just an extra dataset. More discussions on the new dataset are needed for others to know the challenging part. 
2. Some details of LT is not clear. How did the authors choose the writers? The explanation from Line 156 to Line 162 is hard to understand.
---------------------------------------------------------------------------


Reasons to accept
---------------------------------------------------------------------------
1. This paper presents the first work to introduce meta-learning to the text style transfer task and it is good to see it works on different base models
2. The paper is well organized and easy to follow.
3. The authors also introduce a new dataset based on the literature translation.
---------------------------------------------------------------------------


Reasons to reject
---------------------------------------------------------------------------
1. The motivation for the newly introduced dataset is missing. Why we need this dataset?  How this dataset is different from others? Why is it challenging? From Table 3, LT seems like just an extra dataset. More discussions on the new dataset are needed for others to know the challenging part. 
2. Some details of LT is not clear. How did the authors choose the writers? The explanation from Line 156 to Line 162 is hard to understand.
3. The experimental results need to be improved as the current one is too low.
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                  Overall Recommendation: 2.5


============================================================================
                            REVIEWER #3
============================================================================

What is this paper about, what contributions does it make, and what are the main strengths and weaknesses?
---------------------------------------------------------------------------
Description:

This paper works on the problem of unsupervised text style transfer. Specifically, this paper aims to do two things  a) Explore fine-grained style transfer, especially the setting where there are many individual style transfer problems to solve, with limited data available to do the same b) Investigate use of meta-learning frameworks for style transfer through the a) setup.
To facilitate a), a new dataset of personal writing style annotated documents   is collected and planned to be made public. For b), results from using the MAML framework atop different base architectures to solve a) are discussed. In addition to the set of tasks from the aforementioned writing-style-transfer benchmark, the more conventional coarse-grained style transfer datasets (e.g Sentiment and GYAFC [Rao and Tetreault, 18]) are also included in the mix of tasks on which to jointly carry out meta learning.
ST^2 is shown to improve in each case over the respective plain base architecture on all the task metrics (perplexity, BLEU and transfer accuracy), with the experiments being carried out on three different base architectures. In particular, the ST^2 + CP-VAE variant manages to beat not just its parent method CP-VAE and the other two base architectures but also other competitive models such as (Sudhakar et al, 2020).

Strengths:
- Application of the otherwise-widely-popular-in-other-tasks MAML-like framework has been rare in the task of language style transfer - this paper is a valuable artifact in that regard. 
- Furthermore, MAML can be used with any base arch, hence the findings are not specific to a certain set of architectures and are more widely applicable - the authors also present results likewise with multiple base architectures.
- The multi-author literature writing style ST^2 dataset proposed and released here would certainly be a valuable resource, since current style transfer benchmarks either deal with less fine-grained and coarser styles (e.g sentiment, formality, male/female) or deal with only one particular literary style.
 
Weaknesses:

- A lack of qualitative examples in the main body itself. This would have been pertinent to have given the nature of the dataset is itself new.
---------------------------------------------------------------------------


Reasons to accept
---------------------------------------------------------------------------
My reasons to accept are identical to the Strengths mentioned in the answer for the earlier section:

1) Introduction of a novel dataset for unsupervised style transfer with fine-grained, author-annotated documents for multiple authors. The presence of more realistic properties in the dataset, such as there being limited documents per author are also a salient aspect  of the dataset.

2) Being one of the few papers to investigate and discuss meta-learning frameworks for text style transfer.

3) Performing experiments over multiple base architectures to confirm that the proposed meta learning framework improves things agnostic to the specific type of base architecture/model being used.

4) Clear writing and presentation of the dataset and the meta-learning experiments, including a well-organized and well-written Appendix.
---------------------------------------------------------------------------


Reasons to reject
---------------------------------------------------------------------------
I do not see any risks or concerns about this paper being presented at the venue.
---------------------------------------------------------------------------


Questions for the Author(s)
---------------------------------------------------------------------------
a) Though by Kneser-Ney bigram LM is by no means a non-standard LM approach, is there a particular reason why you chose a somewhat “weaker” language model (bigram, and non-neural) rather than not that unviable stronger variants (tri/4 gram or neural models, e.g awd-lstm-lm / GPT/GPT2). Was the ease of target finetuning the consideration? (which is not entirely un-understandable, though GPT-GPT2 are not that hard to finetune as long as there are say, a single digit number of target domains)

b) What was the reason behind choosing to ask the annotator about all three aspects together rather than through separate studies? Though budget constraints if they exist are understandable, it would have been nice to have separate evaluations for fluency and content preservation [whether annotators can at all predict transfer accuracy and “learn” a target style by looking at a few sentences, is in my opinion questionable anyway - but also happy to know the authors views on this] 

c) The authors should perhaps clarify or mention at atleast one point whether the BLEU scores are on a 0-1 scale or a 1-100 scale. Since many BLEU scores are rather low and sometimes <1 [I gather the scale is 1-100 from my reading] , it gets a bit difficult to guess which scale is being followed by looking at the numbers alone.

d) How important were the GSD datasets in getting the MAML setup to work? Is it possible to train the MAML setup to any reasonable effect using only the LT-related tasks?

e) Any particular reason for the unusual choice of BLEU-3 as a metric? (rather than the more typical BLEU-4)
---------------------------------------------------------------------------


Missing References
---------------------------------------------------------------------------
Not all the sources of datasets mentioned in Table 2 have been cited - specifically the following citations are missing

[For the Shakespeare dataset]: 
Xu, W., Ritter, A., Dolan, B., Grishman, R., & Cherry, C. (2012, December). Paraphrasing for style. In Proceedings of COLING 2012 (pp. 2899-2914).

[For the Bible dataset]:
Carlson, K., Riddell, A., & Rockmore, D. (2018). Evaluating prose style transfer with the Bible. Royal Society open science, 5(10), 171920.
---------------------------------------------------------------------------


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------
                  Overall Recommendation: 3.5

-- 
NAACL-HLT 2021 - https://www.softconf.com/naacl2021/papers
