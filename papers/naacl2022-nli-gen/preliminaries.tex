\section{Preliminaries}

In this section, we first show how to fine-tune models with prompts and demonstrations. Then we introduce the automatic template generation method and demonstration strategy proposed in LM-BFF~\citep{DBLP:conf/acl/GaoFC20}.

\subsection{Prompt-based Fine-tuning with Demonstration}

Starting from the GPT model~\citep{radford2018improving}, pre-train and fine-tune paradigm becomes the mainstream method. To solve this task, we can use an autoregressive language model $\mathcal{L}$ (eg., GPT-2~\citep{radford2019language}, BART~\citep{DBLP:conf/acl/LewisLGGMLSZ20}). For each sample $s_i=(p^i, c^i, h^i)$, we take input $x^i$ as:
\begin{align*}
	x^i\ =\ [c^i]\ \mbox{[SEP]}\ [p^i]
\end{align*}\label{eq:fine}
$\mathcal{L}$ is fine-tuned to maximize the objective:
\begin{align}
	P(h^i | p^i, c^i) = \prod_{j}^{N} P_{\theta_{lm}} (h_{j}^i | x^i, h_{1:j-1}^i)
	\label{eq:obj}
\end{align}
However, this paradigm is hard to learn from a small amount of data. 

Prompt is a possible solution for this problem. We can reformulate the task with a template so that the input sentences are closer to the inputs during pretraining. For a sample $s_i=(p^i, contradiction, h^i)$, we can create a prompt as:
\begin{align*}
	x_{prompt}^i\ =\ [p^i]\ But\ \_\_\_
\end{align*}\label{eq:exp}
$\mathcal{L}$ will fill in the blank based on the prefix, which is the same as the pretraining task. We use the template word ``But'' here, because we hope it can direct $\mathcal{L}$ to generate the rest part with opposite meanings.
As showed in \ref{fig:intro_prompt}(b), we can also add some demonstrations in the prompt $x_{prompt}^i$ as:
\begin{align*}
	[p^{(1)}]\ But\ [h^{(1)}]\ \mbox{[SEP]}\ ...\ \mbox{[SEP]}\ [r^i]\ But\ \_\_\_
\end{align*}\label{eq:expdemo}
These demonstrations are selected from $\mathcal{D}_{train}$.

\subsection{Automatic generation of template}\label{sec:agt}

\citet{DBLP:conf/acl/GaoFC20} generate templates automatically with a T5 model~\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20} for classification problems. 

For an input sample $s_i=(x_i, y_i) \in \mathcal{D}_{train}^{'}$\footnote{ $\mathcal{D}_{train}^{'}$ represents the training dataset for classification problems in their paper. We use symbol $'$ to distinguish it from the previously defined $\mathcal{D}_{train}$}, they build three different kinds of filled prompts $\mathcal{T}_g (s_i)$ whose template words are replaced by a [MASK] token.
Then a T5 model is used to fill in the [MASK] part. Because T5 is pre-trained to fill in the missing spans, there is no need to specific the number of [MASK] tokens here, which is more convenient than previous gradient-based prompt search methods~\citep{DBLP:conf/emnlp/WallaceFKGS19,DBLP:conf/emnlp/ShinRLWS20}. The goal is to find the template $\mathcal{T}$ which maximizes:
\begin{align}
	\sum_{s_i\in\mathcal{D}_{train}^{'}}log P_{T5}(\mathcal{T} | \mathcal{T}_g (s_i))
	\label{eq:decodet_old}
\end{align}

During the decoding, a wide beam search (e.g., 100) is used to obtain a large set of diverse templates. The reason why they need so many templates here is that the templates are affected by the architecture and pre-training tasks of the PLMs. Theses templates generated by T5 may not work on the $\mathcal{L}$ finally used. They fine-tuned each generated template on $\mathcal{D}_{train}^{'}$ and use $\mathcal{D}_{dev}^{'}$ to pick the best template $\mathcal{T}^{'}$:
\begin{align}
	\mathcal{T}^{'}=\mathop{\arg\max}\limits_{\mathcal{T}}\ S_{\mathcal{L}}(\mathcal{D}_{train}^{'}, \mathcal{D}_{dev}^{'}, \mathcal{T})
	\label{eq:best_old}
\end{align}
where $S_{\mathcal{L}}$ is a score function to measure performance using $\mathcal{L}$.

\subsection{Demonstration with Similarity}\label{sec:staticd}
To get the demonstrations, GPT-3 uses a random strategy to drawn from $\mathcal{D}_{train}$.  ~\citet{DBLP:conf/acl/GaoFC20} and ~\citet{DBLP:journals/corr/abs-2101-06804} use a BERT~\citep{DBLP:conf/naacl/DevlinCLT19}/RoBERTa~\citep{DBLP:journals/corr/abs-1907-11692} based retriever to sample examples with similarity. Their experiment results show (1) controlling the examples used in prompt is crucial for the model's performance; (2) using a model fine-tuned on task-related datasets is better than the original pre-trained
model.

They use SBERT~\citep{reimers-2019-sentence-bert}, which is fine-tuned on a large and diverse dataset, to build the retriever. The same SBERT model is used to encode both input samples and candidate demonstrations:
\begin{align}
	\textbf{emb}(s_i) = \mbox{SBERT}(p^i)
	\label{eq:emb}
\end{align}
The similarity is judged by cosine similarity:
\begin{align}
	Sim(s_i, s_j) = \frac{\textbf{emb}(s_i)\cdot \textbf{emb}(s_j)}{\left \| \textbf{emb}(s_i)\right \|_2\cdot \left \| \textbf{emb}(s_j)\right \|_2}
	\label{eq:sim}
\end{align}
The demonstrations examples for $s_i$ are sampled from the top 50\% instances.
%The demonstrations examples for $s_i$ are sampled from probability distribution:
%\begin{align}
%	P(s_j | s_i) = \frac{exp()}{\sum_j exp()}
%	\label{eq:simp}
%\end{align}