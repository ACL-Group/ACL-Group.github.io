\section{Related Work}

\textbf{NLI Generation}
~\citet{DBLP:journals/corr/KolesnykRR16} first proposed to serve NLI task as a generation task, while they only concentrated on entailment condition. Later ~\citet{DBLP:journals/csl/StarcM17} added two other conditions into this task to make it a conditional generation task. Our task definition follows many of their concepts except that we pay attention to few-shot settings. The previous works are all based on RNN architecture with attention mechanism, while in our work we use a stronger pre-trained language model as a baseline. There are some works exploring the inversion style of NLI generation task, which is to predict the premise based on the hypothesis and the condition. \citet{DBLP:journals/corr/KolesnykRR16} focused on entailment inversion style. They found that the model is learned to add more detailed information to the premise. \citet{DBLP:journals/corr/abs-1803-02710} used all three conditions to learn the conditional latent space over the representations of a logical antecedent of the given statement.

\noindent
\textbf{Prompt in Natural Language Generation}
Prompt-based methods is usually applied to NLG tasks by using prefix prompts together with autoregressive pre-trained LMs. \citet{radford2019language} demonstrated with prompts such as ``translate to french, [x], [z]'' or ``TL;DR'', and found that pre-trained LMs have impressive ability on generation tasks such as machine translation and summarization in the zero-shot setting. Further, \citet{DBLP:conf/nips/BrownMRSKDNSSAA20} showed that prompts also performed well in few-shot settings. \citet{DBLP:journals/corr/abs-2012-11926, DBLP:journals/corr/abs-2106-10715, DBLP:conf/acl/LiL20, DBLP:journals/corr/abs-2107-03374} explored how to adapt prompts for few-shot text summarization, machine translation, data-to-text and code generation tasks.

\noindent
\textbf{Automatic prompt search}
The automatically selected prompts consist of discrete prompts and continuous prompts (also called soft prompts). For the discrete prompts: \citet{DBLP:journals/tacl/JiangXAN20} used a text corpus to mine templates based on given training samples. Given a seed prompt, various paraphrasing methods can be exploited for generating more candidate prompts~\citep{DBLP:journals/corr/abs-2106-11520, DBLP:conf/eacl/HavivBG21}. \citet{DBLP:conf/emnlp/WallaceFKGS19} first proposed to search prompts automatically based on gradient. \citet{DBLP:conf/acl/GaoFC20} and \citet{DBLP:journals/corr/abs-2102-12206} explored how to use pre-trained T5 model to generate templates, which is more convenient compared with previous methods since it doesn't need extra corpus or modification of the model. For the continuous prompts: \citet{DBLP:conf/acl/LiL20} first proposed to use trainable continuous task-specific hidden representation vectors as prompts. There are some works making use of discrete prompts, like initializing continuous prompts with discrete prompts and hybrid prompts~\citep{DBLP:conf/naacl/ZhongFC21, DBLP:conf/naacl/QinE21, DBLP:journals/corr/abs-2103-10385}. 

\noindent
\textbf{Demonstration learning}
Demonstration learning is one of the methods to combine multi prompts. This method is first used by GPT series~\citep{radford2019language, DBLP:conf/nips/BrownMRSKDNSSAA20}. While in GPT models the demonstrations are selected randomly, researchers found that the selection with similarity would significantly improve the final performance~\citep{DBLP:conf/acl/GaoFC20, DBLP:journals/corr/abs-2101-06804}. \citet{DBLP:journals/corr/abs-2101-06804} and ~\citet{DBLP:conf/acl/KumarT21} also discovered that the order of prompts provided to the model has a great influence on the performance of the model. However, the above methods fail to select demonstrations dynamically.