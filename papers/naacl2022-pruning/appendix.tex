

\renewcommand\arraystretch{1.2}
\setlength\parskip{0.1\baselineskip}
\setlength{\textfloatsep}{0.5cm}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.

%\usepackage{geometry}
%\usetikzlibrary{automata,positioning}
%\geometry{left=2.0cm, right=2.0cm, top=2.5cm, bottom=2.5cm}


\appendix
\section{Derivation for Stochastic Pruning}
\begin{table*}[tb!]
	\centering
	\scriptsize
	\begin{tabular}{l|ccccccc}
		\toprule
		\textbf{Model/Task} & \textbf{RTE}&\textbf{COPA} &\textbf{CSQA} &\textbf{SWAG} &\textbf{HellaSWAG} &\textbf{aNLI} &\textbf{CosmosQA} \\
		\midrule
		BERT &$0\cup 6\cup 14$ &$5\cup 8\cup 14$ &$3\cup 4\cup 8\cup 12\cup 14$ &$1\cup 6\cup 10\cup 11$  &$0\cup 3\cup 5\cup 8\cup 14$ &$0\cup 3\cup 5\cup 8\cup 14$ &$0\cup 3\cup 5\cup 8\cup 14$ \\
		\bottomrule
	\end{tabular}
	\caption{Optimal fine-tuning knowledge type combination for BERT-base on commonsense reasoning tasks.}
	\label{table:finetuning}
\end{table*}
\begin{table*}[tb!]
	\centering
	\scriptsize
	\begin{tabular}{l|ccccccc}
		\toprule
		\textbf{Model/Task} &\textbf{COPA~(Dev.)} &\textbf{CSQA} &\textbf{CA} &\textbf{WSC} &\textbf{SM} &\textbf{ARCT1} &\textbf{ARCT2}\\
		\midrule
		DistilBERT  &$1\cup 6\cup 14$ &$2\cup 3\cup 13$ &$0\cup 1\cup 7\cup 9$  &$6\cup 7\cup 10$ &$2\cup 8\cup 13$ &$2\cup 3\cup 14$ &$1\cup 2\cup 7$\\
		BERT  &$4\cup 11\cup 15$ &$1\cup 2\cup 15$ &$6\cup 8\cup 12$  &$2\cup 9\cup 14$ &$6\cup 12\cup 15$ &$1\cup 9\cup 10$ &$1\cup 5\cup 8$\\
		RoBERTa &$2\cup 3\cup 8$  & $0\cup 2\cup 5$&$0\cup 1\cup 8$  &$1\cup 2\cup 4\cup 5\cup 11$ &$8\cup 11\cup 12$ &$2\cup 5\cup 11\cup 13$&$0\cup 8\cup 11\cup 13$\\
		MPNet  &$1\cup 6\cup 8\cup 10$ &$6\cup 12\cup 13$ &$2\cup 3\cup 10$  &$1\cup 3\cup 4\cup 9$ &$6\cup 10\cup 13\cup 15$ &$2\cup 5\cup 6\cup 11$ &$5\cup 6\cup 7\cup 11$ \\
		\bottomrule
	\end{tabular}
	\caption{Optimal zero-shot knowledge type combination for each PLM on each commonsense reasoning tasks.}
	\label{table:zero}
\end{table*}
\begin{table*}[tb!]
	\centering
	\footnotesize
	\begin{tabular}{l|ccc|c|c|c}
		\toprule
		\textbf{Model} & \textbf{P@1} & \textbf{P@2} & \textbf{P@3} & \textbf{Sparsity}  & $\bm{l_b-l_t}$ & \textbf{\# Param.}\\
		\midrule
		BERT-large w/o pruning &15.1  &20.9   &24.6  & 0\% & - &336M  \\
		BERT-large w/ stochastic pruning &22.1  &30.1   &35.4   & $\sim$30\% & 17-24 &336M \\
		BERT-large w/ deterministic pruning &69.2  &74.1   &76.3   & $\sim$50\% & 17-24 & 284M\\
		\bottomrule
	\end{tabular}
	\caption{Macro-averaged precision metrics of BERT-large on C-LAMA test set}
	\label{table:rank}
\end{table*}
\label{ap:derivation}
To re-parametrize the discrete binary Bernoulli variable $m_{i,j}^l\sim B(\sigma(g_{i,j}^l))$, denote the approaximate differentiable variable as $\tilde{m}_{i,j}^l=\sigma(\frac{g_{i,j}^l+\log{U}-\log{(1-U)}}{\tau})$ where $\tau$ is a real-valued temperature value, we have the following derivation holds for arbitrary $\epsilon \in (0, 0.5)$:
\begin{align}
	P(m_{i,j}^l=1) - P(\tilde{m}_{i,j}^l\geq 1-\epsilon) \leq (\frac{\tau}{4})\log{\frac{1}{\epsilon}}
\end{align}
Specifically, when temperature $\tau$ approaches $0$, $\tilde{m}_{i,j}^l = m_{i,j}^l$.

\noindent
\textit{\textbf{Lemma 1:}} $\sigma^{-1}(x)=\log{\frac{x}{1-x}}$.\\
\noindent
\textit{\textbf{Lemma 2:}} $\frac{\sigma(x)-\sigma(y)}{x-y} \leq \frac{1}{4}$.

\noindent
\textit{\textbf{Proof:}}
\begin{align}
	&P(\tilde{m}_{i,j}^l\geq 1-\epsilon) \\
	=&P(\sigma(\frac{g_{i,j}^l+\log{U}-\log{(1-U)}}{\tau}) \geq 1-\epsilon)\\
	=&P(\frac{g_{i,j}^l+\log{U}-\log{(1-U)}}{\tau} \geq \log{(\frac{1}{\epsilon}-1)}) \\
	=&P(g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)}\geq \log{(\frac{1}{U}-1)})\\
	=&P(e^{g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)}} \geq \frac{1}{U}-1)\\
	=&P(U\geq \frac{1}{1+e^{g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)}}})\\
	=&\sigma(g_{i,j}^l-\tau \log{(\frac{1}{\epsilon}-1)})
\end{align}
Then:
\begin{align}
	&P(m_{i,j}^l=1) - P(\tilde{m}_{i,j}^l\geq 1-\epsilon)\\
	=&\sigma(g_{i,j}^l)-\sigma(g_{i,j}^l-\tau \log{\frac{1}{\epsilon}-1})\\
	\leq&\frac{\tau}{4} \log{(\frac{1}{\epsilon}-1)} \\
	\leq&\frac{\tau}{4} \log{\frac{1}{\epsilon}}
\end{align}
The process for deriving $P(m_{i,j}^l=0) - P(\tilde{m}_{i,j}^l\leq \epsilon) \leq (\frac{\tau}{4})\log{\frac{1}{\epsilon}}$ can be analogously obtained.
$\square$

\section{Implementaiton Details}
\subsection{Templates}
The templates we used in single-relation scenario for different commonsense relations are defined as follows:

\noindent
\textit{AtLocation}: Something you find at \triple{obj} is \triple{subj}.  \\
\textit{CapableOf}: \triple{subj} can \triple{obj}. \\
\textit{Causes}: \triple{subj} causes \triple{obj}. \\
\textit{CausesDesire}: \triple{subj} would make you want to \triple{obj}. \\
\textit{Desires}: \triple{subj} wants to \triple{obj}. \\
\textit{HasPrerequisite}: \triple{subj} requires \triple{obj}. \\
\textit{HasProperty}: \triple{subj} can be \triple{obj}. \\
\textit{HasSubevent}: when \triple{subj}, \triple{obj}. \\
\textit{HasA}: \triple{subj} contains \triple{obj}. \\
\textit{IsA}: \triple{subj} is a \triple{obj}. \\
\textit{MadeOf}: \triple{subj} can be made of \triple{obj}. \\
\textit{MotivatedByGoal}: you would \triple{subj} because \triple{obj}. \\
\textit{NotDesires}: \triple{subj} does not want \triple{obj}. \\
\textit{PartOf}: \triple{subj} is part of \triple{obj}. \\
\textit{ReceivesAction}: \triple{subj} can be \triple{obj}. \\
\textit{UsedFor}: \triple{subj} may be used for \triple{obj}. \\
When performing CKBC task, we first fetch the template based on the relation of the triple to be complete and fill in the \triple{subj} and let the model predict the missing \triple{obj}. Concretely, the \triple{obj} placeholder is 
replaced by the mask token corresponding to different pre-trained language models.


\subsection{Notation for Knowledge Type}
\textit{HasSubevent}: 0\\
\textit{MadeOf}: 1\\
\textit{HasPrerequisite}: 2\\
\textit{MotivatedByGoal}: 3\\
\textit{AtLocation}: 4\\
\textit{CausesDesire}: 5\\
\textit{IsA}: 6\\
\textit{NotDesires}: 7\\
\textit{Desires}: 8\\
\textit{CapableOf}: 9\\
\textit{PartOf}: 10\\
\textit{HasA}: 11\\
\textit{UsedFor}: 12\\
\textit{ReceivesAction}: 13\\
\textit{Causes}: 14\\
\textit{HasProperty}: 15\\
In the remainder of this section, we use $\cup$ to indicate mask union operation upon multiple commonsense relations.
\subsection{Stand-alone Fine-tuning}
For fine-tuning on commonsense reasonging tasks, we only experiments with BERT-base due and perform hyper-parameter search only in terms of batch size in the range of $\{8, 16, 32\}$ and learning rate in the range of $\{3e^{-5}, 4e^{-5}, 5e^{-5}\}$ due to computational budget. We also adopt early stopping based on accuracy on the devlopment set. The combination achieving highest accuracy is shown in \tabref{table:finetuning}.

\subsection{Zero-shot Learning}
In constrast with fine-tuning, zero-shot evaluation is deterministic as long as the model does not involve any stochastic module, thereby averting extensive hyperparameter tuning.
Instead we perform exaustive search over knowledge combinations for each pretrained language model with number of knowledge types in $\{3,4,5\}$. The ConceptNet-grounded knowledge type combination achieving highest accuracy is listed in Table \ref{table:zero}.

\section{Extracted Commonsense Triples}
Here we present the additional experiment result of extracting novel relaitonal triples based on our specialized relation-specific knowledge models.
Applying the pruned DistilBERT-base model to predict missing objects for triples in ConceptNet-100K test set, we obtain commonsense triples deemed to be novel by three human annotators with Flessi's Kappa score $\kappa$ of $0.65$. We further filtered out triples that are included in the training or development set of ConceptNet-100K. Here we show some representative cases categorized by their relations:

\noindent
\textbf{\textit{CapableOf:}}\\
$(computer, crash)$, $(computer, communicate)$\\
\textbf{\textit{IsA:}}\\
$(sex, relationship)$, $(submarine, weapon)$, $(submarine, vessel)$\\
\textbf{\textit{AtLocation:}}\\
$(knife, war)$, $(knife, dinner)$, $(crab, dinner)$\\
\textbf{\textit{UsedFor:}}\\
$(stage, fun)$, $(stage, performance)$, $(literature, education)$, $(literature, research)$\\
\textbf{\textit{HasA:}}\\
$(book, index), (book, information)$\\
\textbf{\textit{HasProperty:}}\\
$(music, loud)$\\
Future work involves using seed triples beyond ConceptNet-100K dataset, e.g., the whole ConceptNet knowledge graph , and mining more novel and plausible commonsense knowledge.

\section{Limitations}
The major limitations of our work lie in how do we choose and combine the representation subspaces/subnetworks in
multi-relation scenario. We proposed a simple heuristic~(i.e., based on statistics of dataset and union operation upon masks) in the paper and it empirically works well, but more principled and optimal method should be 
further studied. Another potential limitation is that we limit our scope to commonsense relations only in this paper. We leave other binary relations~(e.g., factual relations defined in WikiData) as future work.
