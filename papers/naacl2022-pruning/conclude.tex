\section{Conclusion}
This study investigated specializing PLMs for better relational reasoning via network pruning. In the pilot experiment we find evidence of latent sparse subnetworks 
capable of representing grounded commonsense relations in various PLMs. Further experiments revealed 
that such subnetworks possess stronger relational reasoning capability than original PLMs. 
Our work provides a new vantage point about the internal mechanism as well as practical utilization of 
relational knowledge in PLMs, opening up avenues to better understanding and adapting pretrained language 
representations.
%This study explores the latent commonsense knowledge in PLMs by opening up new possibilities for distilling ``more'' knowledge from ``less'' parameters. Our findings confirm the conjecture that varied relational knowledge is blended in one shared parameter space due to mini-batch-based optimization during pretraining, and it is feasible to perform unstructured weights pruning upon PLMs to recover the latent subnetwork ad hoc.
%
%To examine the practical utility of these softly disentangled subnetworks on knowledge-intensive tasks, we additionally conducted a suite of experiments. The results show that: (i)~given enough supervision, choosing the ``right'' subnetwork ensembling makes it a good prior for better fine-tuning. (ii)~subnetworks exhibit notably superior performance than their full-scale counterparts under zero-shot setting, including commonsense reasoning and knowledge base completion.
%


%Future work includes applying the proposed procedure to other relational knowledge upon PLMs trained with more advanced objectives and structure. Exploring pretrained auto-regressive  models remains another promising research direction. 

