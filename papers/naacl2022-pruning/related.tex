\section{Related Work}
Since the emergence of large pre-trained language models, 
much work has focused on understanding their internal contextual representations. 
Most prior work~\cite{shi-etal-2016-string,belinkov-etal-2017-neural} 
pays attention to either using extraneous 
probing tasks to examine whether certain linguistic properties 
can be identified from those representations, or ablating the models 
to observe how behavior changes. More recently, some 
studies~\cite{DBLP:journals/corr/abs-1901-05287,DBLP:journals/corr/abs-1905-06316} have shown the existence 
of linguistic knowledge~(e.g., syntax) in various but 
generally lower layers of pre-trained transformers.

To shed more light on how PLMs 
memorize abstract knowledge rather than statistical co-occurrence 
patterns, we extend previous probe~\citep{Petroni2020} on relational 
knowledge. Specifically, we are concerned with commonsense knowledge 
that is grounded on ConceptNet relations. Our work differs in that 
we focus on not only probing but also bringing latent relational knowledge 
to the surface and unleashing more potential for better relation reasoning.

Another relevant line of research is network 
pruning~\cite{liu2018rethinking,Lin2020Dynamic} and lottery ticket 
hypothesis~\cite{conf/iclr/FrankleC19,Prasanna2020,Chen2020}. 
The former aims at reducing the size of model parameters without 
compromising accuracy and the latter reveals subnetworks whose 
initializations made them capable of being trained effectively comparable 
to the original model. In contrast, we seek to uncover subnetworks in 
over-parametrized PLMs that specializes on commonsense knowledge 
tailored for downstream tasks rather than focusing on good global
initialization, and achieve good results. 
