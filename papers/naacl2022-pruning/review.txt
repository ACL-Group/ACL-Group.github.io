Meta Review of Paper12 by Area Chair 5erx 
ACL ARR 2021 December Paper12 Area Chair 5erx
28 Jan 2022ACL ARR 2021 December Paper12 Meta ReviewReaders: Paper12 Senior Area Chairs, Paper12 Area Chairs, Paper12 Authors, Program Chairs
Metareview:
The authors are looking into how pruning affects knowledge in pretrained neural networks, specifically if you can prune away a lot of a pretrained MLM transformer without negatively affecting the performance on SVO relation triples from ConceptNet in a setup using a large combination of prompts. They propose two approaches for learning the pruning masks, and the trained deterministic one achieves results on par with finetuning or supervised knowledge base completion models at 50% sparsity.

Summary Of Reasons To Publish:
This is a novel approach/methodology that could be applied to other tasks as well; the amount and choice of experiments are well-suited to underline the promise and performance of the model on a relatively common task.

Summary Of Suggested Revisions:
The main critical comments concern details about the experiment design:

there are questions about the choice of prompt templates, which is not examined regarding the influence on the results (pm4y, hz4o)
concerns about the task setup (overlap in objects - not complete triples - between train/dev and test, combination of mask templates for multiple relations, lack of averages over multiple random initializations: nCej) but also questions about relation/applicability to other tasks (hz4o) or whether the sparsification approach could be applied to different (supervised fine tuning) models (nCej)
Overall all reviewers seem to agree that the approach of the paper is interesting and promising; but the different comments, while there is no one big weakness that would impact the value of the paper negatively, seem to indicate that a round of revisions based on fixing details of the experimental setup and addressing some of the questions would still make the paper stronger.

Overall Assessment: 3 = There are major points that may be revised
Suggested Venues:
This is a paper realizing an interesting and potentially broadly applicable idea and supported by an appropriate number of experiments; I'd want to see this in the main track of a *ACL conference.

[–]
Official Review of Paper12 by Reviewer pm4y 
ACL ARR 2021 December Paper12 Reviewer pm4y
19 Jan 2022ACL ARR 2021 December Paper12 Official ReviewReaders: Program Chairs, Paper12 Senior Area Chairs, Paper12 Area Chairs, Paper12 Reviewers Submitted, Paper12 Authors
Paper Summary:
This paper studies commonsense knowledge of pretrained language models from the perspective of the latent representation. In the literature, those studies are typically done by either model fine-tuning or prompt learning to distill the required knowledge from the model, but this work instead focuses on the latent representation of the model, trying to find a subspace responsible to each high-level commonsense relation. The subspace is designed by a sub-network of the original language model attained by a weight pruning technique, and hence there is no need to neither update the pretrained parameters nor train additional parameters. The sub-networks are substantially smaller than the original model, yet achieve very comparable results or even better in some datasets. Moreover, the sparseness of the models contributes to a better visualization leveraging the attention weights and the embedding distribution, which can facilitate the model interpretations.

Summary Of Strengths:
Interesting methodology to probe pre-trained language models’ commonsense knowledge
Code and all details shared
Careful design of experiments for specializing language models via network pruning
Summary Of Weaknesses:
Not detailed analysis/comparison of prompts

Comments, Suggestions And Typos:
Recent few years, weight pruning has gradually started catching attention, not only to achieve a sub-optimal yet sparse model to reduce the complexity, but rather to purify the model to find a sub-network that performs as good as the original model yet sparser at a huge scale. This is usually referred to as the lottery ticket hypothesis (LTH) [1], in which a deep neural network model is thought to contain a sub-network behaves equivalently as the original model. Inspired by such important findings of weight pruning, this work proposes a first-ever method to probe pre-trained language models’ commonsense knowledge, by identifying a sub-network representing each common-sense relation type. Being supported by strong evidence like LTH, the idea of applying weight pruning in model probing is naturally motivated and the methodology is well-explained.

Compared to existing methodologies to solve LAMA [2] including model finetuning and prompt learning, this approach has several obvious benefits due to the nature of weight pruning. First, parameters are fixed and never new parameters are introduced. This is so important in the model probing. We often evaluate a model's certain prior knowledge by the accuracy in a relevant downstream task, but methods with parameter updates or additional parameters, have no guarantee of whether the accuracy is accountable to the update or the model. On the other hand, since the sub-network is a part of the original model, the knowledge represented by the sub-network can also be regarded as the knowledge of the original model. Second, the sparsity enhances the interpretation of the model, which is an essential characteristic of model probing methods. The ultimate goal of model probing is to know how the knowledge is encoded in the model, and the embedding spaces of sub-networks can exhibit an interestingly distinguishable pattern of each relation type. This can be strong evidence to confirm the authors’ idea that the language model contains sub-networks, each of which is responsible for one specific type of knowledge. Finally, the paper covers not only LAMA but also multi-relation tasks, where the sub-network approach again establishes a strong baseline against other baselines.

However, through the experiments, prompts are fixed with the LAMA’s original hand designed prompts, so one concern would be how sensitive the reported results are depending on the prompt. To study the effect of prompt variations, one thing might be a simple experiment with other prompts such as one obtained from the AutoPrompt [3]. Also, a minor thing but there are relevant works in the relational knowledge probing of language models (e.g. [4]) that can be referred to in the paper.

Overall, I like the idea as it is simple and intuitive yet no one has done it yet before. This paper proves that weight-pruning can find a sub-network for a specific type of knowledge, which is quite novel finding but in fact quite a natural proposal considering the LTH.

Minor comment: Some tables look quite small

References:

[1] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. 2020. The lottery ticket hypothesis for pre- trained BERT networks. In Proceedings of the Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020.

[2] Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An- ton Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. 2020. Language Model as Knowledge Base. In Proceedings of EMNLP-IJCNLP 2019, pages 2463–2473.

[3] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of EMNLP, pages 4222–4235.

[4] Asahi Ushio, Luis Espinosa-Anke, Steven Schockaert, Jose Camacho-Collados. 2021. BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies? In Proceedings of ACL (Volume 1: Long Papers)

Overall Assessment: 4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.
Confidence: 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper12 by Reviewer nCej 
ACL ARR 2021 December Paper12 Reviewer nCej
19 Jan 2022 (modified: 19 Jan 2022)ACL ARR 2021 December Paper12 Official ReviewReaders: Program Chairs, Paper12 Senior Area Chairs, Paper12 Area Chairs, Paper12 Reviewers Submitted, Paper12 Authors
Paper Summary:
This paper introduces a method for identifying subnetworks of pretrained transformer LMs that better capture relational knowledge (operationalized here through cloze-style prompts transformed from KB triples). "Pruning mask generator" matrices (which determine which parameters are zeroed out) are learned using the cloze prompts for a particular relation, with transformer parameters held constant. The paper provides a number of experiments and analyses using existing KB & QA datasets to show that their pruning method indeed provides improved performance on predictions that cover specific relations.

Summary Of Strengths:
Although the paper focuses on commonsense relations, one could imagine this kind of approach being used to probe whether LMs capture other info.

No shortage of experiments and analysis: the paper tests a number of attributes and applications of the learned relation subnetworks (e.g., whether they indeed specialize, 1 & 2-hop link prediction, downstream learning on related NLI and QA tasks). The results are generally promising, outperforming the non-pruned LMs and achieving similar performance to supervised KB models.

Summary Of Weaknesses:
The breadth of the evaluation sometimes works against this paper; I felt that section 3 was unclear/missing some important details in places:

Given the role of random initialization of the pruning generator matrices (normal distribution parameterized w/sparsity value), I'm surprised that there isn't any mention of averaging over multiple runs in L270-290 (3.1).

It's not entirely clear what is being embedded in Fig. 4 -- are the masked words being included in the embedded prompts? there's a pruned network for each relation -- which one is being shown?

I'm not sure how the one-hop link prediction experiment (on ConceptNet triples) in 3.2 is intended to differ from the experiment in 3.1 (which seems to also be based on ConceptNet data). The paper states that the dev/test set does not overlap with the C-LAMA training data, but it's not clear whether that means the subject and/or object don't appear in the training data at all or just that the cloze template wording is different.

The experiments in 3.3 feel a bit undercooked:

a. For the stand-alone fine-tuning experiment, taking the union of mask matrices for selected relations (heuristically determined from task data) seems a bit counter to the original intuition (stripping away irrelevant information). I'd love something more rigorous that demonstrates the necessity of multiple relation matrices for the given tasks and the impact of combining matrices on network sparsity.

b. The integrated fine-tuning experiment needs more info in general.

c. For the zero-shot learning experiment, is it truly zero-shot if task data is used to determine the selected relations?

IMO would prefer a more thorough (a) rather than all three.

I like the ideas in the paper from an analysis/model probing point of view (can we identify LM subnetworks capable of capturing specific relations?) but am not fully sold on how they might be used in practice/for downstream tasks (3.3) -- my concern is mostly around how one selects relations, as the given heuristic feels kind of arbitrary.

Comments, Suggestions And Typos:
Typos:

3.1: "ConceptNet" (L380)
Appendix B: "implementation" (L860), "reasoning" (L910)
Appendix C: "relational" (L931), "Fleiss" (L936)
Overall Assessment: 3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.
Confidence: 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.
Best Paper: No
Replicability: 2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.
Datasets: 2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)
Software: 2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper12 by Reviewer hz4o 
ACL ARR 2021 December Paper12 Reviewer hz4o
17 Jan 2022ACL ARR 2021 December Paper12 Official ReviewReaders: Program Chairs, Paper12 Senior Area Chairs, Paper12 Area Chairs, Paper12 Reviewers Submitted, Paper12 Authors
Paper Summary:
This paper cares about specializing pretrained language models (PLMs) for relational reasoning via network pruning. It formulates relation reasoning as querying PLMs with cloze prompts (e.g., “you are likely to find [MASK] in a bus”) and presents a method to find subnetworks from PLMs to represent grounded commonsense relations at non-trivial sparsity. Specifically, among the selected layers of transformer PLMs, a learnable pruning mask generator is used to generate element-wise masks for each weight matrix. Two mask generators are introduced: Stochastic one uses re-parameterization to generate soft and differentiable masks from a Bernoulli distribution; Deterministic one generates hard masks via thresholding a sigmoid function. To learn such masks (i.e. to prune), they use a subset of ConceptNet, denoted as C-LAMA. In experiments, they evaluate the pruned PLMs on the test split of C-LAMA, where deterministic pruned PLMs outperforms the original PLMs, the stochastically pruned ones, and even the fine-tuned one. They also evaluate on datasets that require knowledge of single or multiple commonsense relations to show that the pruned PLMs are more generalizable than the original PLMs.

Summary Of Strengths:
The proposed network pruning method shows superior performance over the original PLMs and even the fine-tuned ones on the test split of C-LAMA. When it comes to the generalization to other datasets, fine-tuning the pruned PLMs also leads to better performance than fine-tuning the original PLMs, and this is more notable in the low-resource regime.

Summary Of Weaknesses:
The number of parameters shown in Tbl 2 seems problematic. Although the masks zero out some parameters (i.e. elements of weight matrix) in the original PMLs, the pruning mask generator itself introduces new parameters. As described in line 183, each weigh matrix W is associated with a learnable mask generator G of the same size. While these new parameters are not used during inference, at training time they are not negligible and should be noted in the paper.
The choice of prompt templates can be critical to the downstream performance. There is no discussion about how the prompt templates affect the performance of the pruned, the fine-tuned, and the original PLMs, and why the current choice of templates ensures a fair comparison.
No comparison with SOTA in single-relation and multi-relation scenarios. For example, providing SOTA of datasets in tbl 6 gives a clearer picture of where the proposed network is. Also, since most SOTA models are also based on the transformer architecture, it should be straightforward to replace the underlying transformers in these SOTA models with the pruned ones. A comparison of the replacement would be a better knowledge of improvement from the proposed method.
Comments, Suggestions And Typos:
Section 3.3 Multi-relation Scenario is not very clear, especially on how stand-alone and integrated fine-tuning are done.
Line 299 says increasing the number of pruned layers helps. Why are the pruned layers limited to top layers? What not prune all layers?
Overall Assessment: 3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 3 = Potentially useful: Someone might find the new datasets useful for their work.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
