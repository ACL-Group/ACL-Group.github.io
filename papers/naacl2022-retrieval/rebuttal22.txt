Sincere thanks to the reviewers comments and suggestions. And thanks to the senior area chair for reading our response.

1. First, We would like to clarify two factual errors made by Reviewer HjPV.
(1) Reviewer HjPV stated that "The basic hypothesis that development of such a tool (smaller model) is beneficial as a mobile application is the major weakness of this paper." and "The entire image pool (which is few TBs) cannot be loaded in a mobile device and if it is so how can we index the image set?". Both complaints are not 
true. First, the development of smaller and faster model with maximally retained performance is always useful and preferable for resource-constrained scnearios including servers with limited budget, embedded systems and mobile devices. If not, what is the point of all previous researches on model compression? 
Second, Reviewer HjPV ignores important scenarios where users do not want to share their private photos to the cloud and prefer a local solution to support fast and accurate search over their private photo collections.
(3) Reviewer HjPV rated 1(i.e., No usable software released) for the software availability. This is wrong as we have attached all necessary software in zip file to the ARR system. We also included the anonymous URL(at the end of Introduction section) to the repository containing the code, models, and even executable demo application. The other two reviewers both acknowledged the useful software and rated 4.

2. Second, we would like to respond to four critical comments raised by the reviewers and meta-reviewer.
(1) Reviewer HjPV stated that "Relevance of this paper for ACL community is also questionable", which is not true. On the one hand, we have seen about 7 papers working on cross-modal retrieval that were published at NLP conferences such as ACL/EMNLP/NAACL just recently in 2021. On the other hand, the relevant topics list from the official ACL call-for-papers website also includes: 1) Information Retrieval and Text Mining and 2) NLP Applications.
(2) We appreciate that Reviewer EeRM checked the CLIP paper for thorough review, but we would like to clarify that the results reported in the original CLIP paper correspond to the largest version of CLIP. This version of CLIP is not open-sourced, therefore we have no way to use it as our compression target. The results in our paper 
correspond to the second largest version of CLIP, and this version is publically availabel.
(3) Reviewer EeRM suggested fine-tuning ViT-small and TinyBERT with image-text pairs as a baseline. We did not include it as a baseline in current version of paper because we have conducted a similar ablation in Table 2, i.e., stage-1_InfoNCE. This ablation is almost identical to what Reviewer EeRM said except that 
the text encoder is not TinyBERT. TinyBERT is not pre-trained on massive image-text pairs as is done for CLIP hence we posit that it can hardly deliver satisfactory performance. Nonetheless, this baseline is plausible and straightforward to implement, and we are very willing to include it as our baseline in the final version.
(4) "No novel knowledge distillation technique" said by the meta-reviewer is contrary to other reviewers, e.g., "he loss function shows novelty" by reviewer HjPV, "An elegant way to compress large pre-trained dual-encoder" by reviewer EeRM.