\section{Related Work}
\textbf{Cross-encoder.}~Cross-encoder architecture~\cite{tan-bansal-2019-lxmert,uniter,oscar} adopts a single Transformer network~\cite{NIPS2017_3f5ee243} which is able to process inputs from different modalities, e.g., images and texts. Benefitting from the self-attention mechanism, the hidden states of images and texts 
interact with each other at the patch/token-level, therefore yielding state-of-the-art retrieval accuracy. Though effective, these models suffer from huge memory consumption and inference latency, making them inpractical in time-sensitive real-world scenarios.


\textbf{Dual-encoder.}~In contrast to cross-encoder, dual-encoder architecture~\cite{clip,align} trains two seperate encoders for vision and language modalities. The exact choices of encoder architecture may be different. For example, CLIP utilizes Transformers for both visual and text encoders, while ALIGN~\cite{align} uses pre-trained BERT as text encoder and EfficientNet as visual encoder. In dual encoder, interactions between different 
modalities take place only at the final encoder layer, resulting in slightly worse performance compared to cross-encoders. Nevertheless, this late-interaction scheme of dual-encoder allows for efficient similarity computation, thus rendering it suitable for prividing real-time searching.
