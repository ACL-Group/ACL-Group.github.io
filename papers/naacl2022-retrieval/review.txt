Leaner and Faster: Two-Stage Model Compression for Lightweight Text-Image Retrieval 
Anonymous
17 Nov 2021 (modified: 03 Dec 2021)ACL ARR 2021 November Blind SubmissionReaders: November, Senior Area Chairs, Area Chairs, Reviewers, Paper499 Authors
Abstract: Current text-image approaches~(e.g., CLIP) typically adopt the dual-encoder architecture using pretrained vision-language representation. However, these models still pose non-trivial memory requirements and substantial incremental indexing time, which makes them less practical on mobile devices. In this paper, we present an effective two-stage framework 
to compress large pretrained dual-encoder for lightweight text-image retrieval. The resulting model is smaller~(39\% of the original), 
faster~(1.6x/2.9x for processing image/text respectively), yet performs on par with or better than the original full model on Flickr30K and MSCOCO benchmarks.  We also open-source an accompanying realistic mobile image search application.
Software:   zip
Revealed to Siyu Ren, Kenny Q. Zhu
14 Nov 2021 (modified: 16 Nov 2021)ACL ARR 2021 November Submission
Authors:Siyu Ren, Kenny Q. Zhu
TL;DR: A novel and effective compression pipeline for dual-encoder based text-image retrieval.
Preprint: no
Preferred Venue: ACL 2022
Consent: yes
Consent To Review: yes
Add
Reply Type:all
Author:everybody
Visible To:all readers
Hidden From:nobody
3 Replies
Meta Review of Paper499 by Area Chair BkRT
ACL ARR 2021 November Paper499 Area Chair BkRT
06 Jan 2022ACL ARR 2021 November Paper499 Meta ReviewReaders: Paper499 Senior Area Chairs, Paper499 Area Chairs, Paper499 Authors, Paper499 Reviewers, Program Chairs
Metareview:
SUMMARY

Strengths: Successful distillation of a strong text-image retrieval model (CLIP), insightful ablation studies

Weaknesses: No novel knowledge distillation technique, lacking discussions of previous works and baselines

Summary Of Reasons To Publish:
See the metareview

Summary Of Suggested Revisions:
See the metareview

Overall Assessment: 4 = There are minor points that may be revised

[–]
Official Review of Paper499 by Reviewer HjPV 
ACL ARR 2021 November Paper499 Reviewer HjPV
29 Dec 2021ACL ARR 2021 November Paper499 Official ReviewReaders: Program Chairs, Paper499 Senior Area Chairs, Paper499 Area Chairs, Reviewers, Paper499 Authors
Paper Summary:
The paper is about model compression for a text-image retrieval application. The target model appeared in ICML 2021 and the present paper shows that a smaller model which 39% of the original model can achieve the similar performance for image retrieval from text query. The motivation for doing this engineering is to make this the task suitable for mobile devices that suffer from resource constraint.

Summary Of Strengths:
The engineering part is the real strength of the paper. Though the paper follows standard distillation model but successful implementation of such a model for the problem in hand requires innovation and the paper shows that strength. Desigining the loss function shows novelty. The paper can be cansidered as an NLP application for embedded systems (not as a mobile phone application as claimed in the paper).

Summary Of Weaknesses:
The basic hypothesis that development of such a tool (smaller model) is beneficial as a mobile application is the major weakness of this paper. Making edges intelligent by shifting some computational load from the could to an edge has been a new computing paradigm. However, there must be a plan on load sharing to know what part to be done by mobile and which to be carried out at the cloud. Unless this plan is given labelling the current system as an useful mobile application is not justified. The entire image pool (which is few TBs) cannot be loaded in a mobile device and if it is so how can we index the image set? The way in which the tool has been described in the paper looks like an old-fashioned stand-alone system. No study has been reported whether the compression acheived is good for a mobile application. Relevance of this paper for ACL community is also questionable. Only two previous papers from the CL/NLP community have been referenced and that too are not directly related to the work reported in the paper. Therefore, it will be difficult to connect this paper with the interest of ACL community.

Comments, Suggestions And Typos:
Suggestion: As an engineering application, the proposed method has merit. Could be nice for an embedded realisation but further implementation details are to be explored.
Comment: Many Arxiv papers have been referred but the following is left out: Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System by Yang et al. Isn't this paper is very relevant?

Overall Assessment: 2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 4 = From an allowed pre-existing preprint or workshop paper, I know/can guess at least one author's name.
Ethical Concerns:
None.

[–]
Official Review of Paper499 by Reviewer oowC 
ACL ARR 2021 November Paper499 Reviewer oowC
23 Dec 2021 (modified: 26 Dec 2021)ACL ARR 2021 November Paper499 Official ReviewReaders: Program Chairs, Paper499 Senior Area Chairs, Paper499 Area Chairs, Reviewers, Paper499 Authors
Paper Summary:
This paper proposes a two-stage training scenario for model compression in image-text retrieval tasks. In the first stage, two lite models are trained on an unpaired dataset by respectively knowledge distillation of an image encoder and text encoder. In the second stage, the original image encoder and text encoder are first sequentially fine-tuned on the target dataset, and then are used to teach the lite models by knowledge distillation. Experiments show that the lite models achieve similar retrieval performance to the original models with less capacity and computation.

Summary Of Strengths:
The writing is clear and easy to follow.
The lite models achieve comparable performance to the original ones with less capacity and computation.
Ablation studies are provided to justify the designs.
Summary Of Weaknesses:
It seems that a discussion about related work on compressing vision-language models is missing.

Comments, Suggestions And Typos:
The authors perform dataset-wise mining instead of batch-wise mining. Considering that the former is more computationally expensive, it might be useful to provide some experiment data to show its benefits.
The combined objective in Line 205 leaves the impression that lite models and original models are trained simultaneously when in fact they are not.
Can stage 2 be implemented by directly fine-tuning the lite models? This reduces the complexity of the pipeline and also the computational cost of training.
It is interesting to see how far we can go with the proposed approach by further reducing the capacity of the lite models. However, this might be too much work for a 4-page paper.
Overall Assessment: 3.5 
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper499 by Reviewer EeRM 
ACL ARR 2021 November Paper499 Reviewer EeRM
07 Dec 2021ACL ARR 2021 November Paper499 Official ReviewReaders: Program Chairs, Paper499 Senior Area Chairs, Paper499 Area Chairs, Reviewers, Paper499 Authors
Paper Summary:
This paper presents a two-stage model compression method for lightweight text-image retrieval, where the teacher models are from the large pretrained dual-encoder. The resulting model is smaller and faster, meanwhile achieving comparable results compared with the original models.

Summary Of Strengths:
An elegant way to compress large pre-trained dual-encoder
Public available code/models
Summary Of Weaknesses:
Lack of necessary baselines: • Just fine-tuning the pre-trained image (e.g. ViT small) and text (e.g. tinyBERT) encoders (of the same model size) with text-image pairs.

Comments, Suggestions And Typos:
The CLIP zero-shot setting on text-image retrieval (Radford et al., 2021) seems to be much higher than the results reported in this paper with large pre-trained dual-encoder. It is not clear to me why numbers reported in this paper for large pre-trained models are much lower.
It would be better if authors can elaborate more on the poor results on joint fine-tuning.
Overall Assessment: 2.5 
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
