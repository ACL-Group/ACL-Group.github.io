\section{Experiments}
In this section, we will introduce the evaluation results of psychiatrist chatbot and patient chatbot using our PsyDial framework. 

% \KZ{Every human or auto evaluation metric must target one of the
% objectives in Sec 2. Every objective in Sec 2 must be covered by at least
% a human or an auto evaluation. You can have a small table at the beginning
% of this section to align the eval metrics with the objectives. For example,
% the statistics (avg turns etc.) are actually for efficiency, but efficiency
% is not mentioned in the objectives. I think you can include them in sec 2 also.
% Right now the objectives in sec 2 seems to be all language style related.}
\subsection{Chatbots of Comparison}
% \KZ{this section should go into the evaluation framework?}

\begin{table}[h]
    \centering
    \footnotesize
    \begin{tabular}{m{0.1\columnwidth}|c|m{0.6\columnwidth}}
    \hline
    & Chatbot & Description \\
    \hline
    \multirow{4}{0.1\columnwidth}{Doctor} & D1 & use the full doctor prompt \\
    \cline{2-3}
    & D2 & remove empathy parts in prompt  \\
    \cline{2-3}
    & D3 &  remove aspect part in prompt \\
    \cline{2-3}
    & CPT & CPT model trained on domain data \\
    \hline
    \multirow{2}{0.1\columnwidth}{Patient} & P1 & remove emotion, resistance, and reminder mechanism \\
    \cline{2-3}
    & P2 & use the full patient prompt\\
    \hline
    \end{tabular}
    \caption{Brief description of the chatbots for comparison. Detailed description and prompt is in Appendix \ref{apd:prompts}.}
    \label{tab:cmp_chatbots}
\end{table}

Due to the complexity and high time cost of ``human-bot'' interactive chat, we select several representative prompt versions for comparison.

\paragraph{Psychiatrist Chatbot} 
Each patient will have a conversation with four different psychiatrist chatbots in a random order, and then rate them on four human evaluation metrics with 1-4 scale. 
Three of the chatbots are powered by ChatGPT. \texttt{D1} uses the full prompt, 
while the other two (i.e., \texttt{D2}, 
\texttt{D3}) have certain parts removed for ablation. 
The fourth chatbot, denoted as \texttt{CPT}, is a representative deep learning chatbot trained on domain-specific data \cite{yao-etal-2022-d4} using Chinese Pre-trained Un-balance Transformer (CPT)~\cite{shao2021cpt}. Notably, CPT stands out as a highly competitive model for generating Chinese text, boasting better performance in various tasks compared to BART. Thus, it serves as a strong baseline for psychiatrist chatbots, and is quite different from LLM-empowered chatbots.
% \MY{Make a table for these bots, including doctor and patient}
\paragraph{Patient Chatbot}
Each psychiatrist will have a conversation with two patient chatbots, and then rate their performance with 1-4 scale. 
The two patient chatbots are \texttt{P1} and \texttt{P2}. \texttt{P1} omits the instructions regarding emotional fluctuations, resistance, and the reminder mechanism. In contrast, \texttt{P2} utilizes the complete patient prompt outlined in Section \ref{sec:prompt}.
A brief description of these chatbots can be found in Table \ref{tab:cmp_chatbots}.
% The first one uses the full prompt while the second one omits additional parts for realistic in version 2\footnote{Detailed description and prompt of these doctor/patient chatbots are shown in Appendix \ref{apd:prompts}.}.
% COMMENT: 这两段要引述一下表1啊。此外1中的"xx parts"的表述，最好和前面一致，都用v1v2v3或者①②③这种方式来提，更容易理解。CPT这个模型的名字也应该在正文里写出来。

\subsection{Psychiatrist Chatbot Results}
% 随着轮数的symptom precision变化
\paragraph{Rating Metrics}
We present the human-rated results of different psychiatrist chatbots in \tabref{tab:human_doc}. It is evident that ChatGPT-based psychiatrist chatbots outperform the domain-specific data-trained \texttt{CPT} model across various metrics, especially ``fluency'' and ``empathy''. This highlights ChatGPT's ability to control the dialogue flow and provide emotional support. 

However, when comparing the performance of the three ChatGPT-based chatbots (i.e., \texttt{D1}, \texttt{D2}, \texttt{D3}), we find that
\texttt{D3}, which excludes symptom-related aspects from its prompts, outperform the rest in most metrics. Moreover, the chatbot without empathy components, \texttt{D2}, gets the highest score in the ``engagement'' metric. 
% In contrast, the D4 model, trained solely on domain-specific data, may fall short in these areas due to limited diversity and coverage in its training data. As a result, it fails to fully capture the qualities exhibited by human doctors during conversations.
% Chatbots utilizing prompts with empathy components (i.e., \texttt{D1} and \texttt{D3}) are scored higher in ``Empathy'' metrics than other chatbots. 
% Surprisingly, \texttt{D3}, which excludes symptom-related aspects from its prompts, outperform the rest in most metrics. Moreover, the chatbot without empathy components, \texttt{D2}, gets the highest score in the ``Engagement'' metric. 

\begin{table}[th]
    \small
    \centering
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc|c}
    \hline
     & D1 & D2 & D3 & CPT \\ 
     % & (full) & (w/o emp) & (w/o asp) & (cpt)\\
    \hline
    Fluency &3.00	&3.17	&\textbf{3.28}	&2.87 \\
    Empathy &3.36	&3.00	&\textbf{3.43}	&2.71 \\
    Expertise & 2.93	&3	&\textbf{3.71}	&3.29\\
    Engagement & 2.50	&\textbf{3.21}	&2.86	&2.64 \\
    \hline
    \end{tabular}
    % }
    \caption{Human-rated scores of psychiatrist chatbots}
    \label{tab:human_doc}
\end{table}

As we initially assume that \texttt{D1} with full prompt would deliver the best performance, we reviewed the dialogue history to understand the underlying reason. We found that \texttt{D1} often repetitively expresses empathy, relying on phrases like ``I understand your feelings'' multiple times within a single conversation\footnote{We provide repetitive dialogue examples of doctor chatbots in Appendix \ref{apd:error_anal}.}. This excessive repetition creates the impression that the chatbot lacks a genuine understanding of the patient's issues and relies on pre-written templates, which can negatively impact the user experience.

\paragraph{Computational Metrics}
% Then, we continue to explore the dialogue history and calculate some automatic metrics, hoping to find out more reasons of the unexpected human evaluation results. 
The data statistics and computational metric results are presented in \tabref{tab:auto_doc}, demonstrating a strong correlation with the human-rated metrics. 
For example, with these computational metrics, we can explain why \texttt{D3} performs the best in most human-rated metrics. It stands out by asking more in-depth questions while maintaining a lower question frequency per turn, indicating a higher level of professional skills. Furthermore, the symptom precision metric is the highest, implying that the chatbot's questions are highly efficient, with few ``no'' responses.  

These findings validate that the objectives for psychiatrist chatbots proposed with the guidance of professionals, as outlined in \secref{sec:objectives}, effectively capture the requirements of real patients. Therefore, the computational metrics designed based on these objectives serve as reliable indicators for assessing the performance of chatbots.

\begin{table}[th]
    \small
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc|c}
    \hline
     & D1 & D2 & D3 & CPT \\ 
     % & (full) & (w/o emp) & (w/o asp) & (cpt)\\ 
    \hline
    - \textbf{Statistics} & & & & \\
        ~~~~avg turns & 25.64&	24.00&	22.71 &	40.93  \\
        ~~~~avg doc utt len &56.84	&57.13	& 53.75	& 14.36\\
        ~~~~avg pat utt len & 8.68	& 10.34	&8.16	&4.87 \\
    \hline
    - \textbf{Function} & & & &   \\
        ~~~~diagnose acc & 42.85\%	& 35.71\%	&\textbf{50.00}\% & 21.43\% \\
    \hline
    - \textbf{Style} & & & & \\
    ~~~~avg \# of ques & 1.6	&1.9	&\textbf{1.22}	&0.92 \\
    ~~~~in-depth ratio & 25.08\%	&27.64\%	&\textbf{32.64}\%	&41.39\% \\
    ~~~~symp recall & 58.93\%	&\textbf{66.07}\%	&38.10\%	&61.90\%\\
    ~~~~symp precision & 72.40\% & 71.93\% & \textbf{92.24}\% & 49.61\% \\
    \hline
    \end{tabular}
    }
    \caption{Results of Computational Metrics for Psychiatrist Chatbots}
    \label{tab:auto_doc}
\end{table}

Furthermore, we observe that developing a good psychiatrist chatbot is an multi-objective optimization problem, as none of the chatbots can achieve the highest score in all metrics simultaneously.
While high symptom recall and in-depth ratio contribute to a more detailed inquiry, they may inadvertently create a mechanical impression and negatively impact symptom precision. Therefore, achieving a successful psychiatrist chatbot entails striking a careful balance between these objectives.

% However, we also notice some mismatches between the objectives of psychiatrists and the requirements of patients. 
% A notable example is \texttt{D3}, which falls short of psychiatrists' objective of ``comprehensiveness'' due to its low symptom recall caused by the absence of an explicit ``aspect'' component in the prompt. Nonetheless, it can create a more empathetic and understanding experience for patients through its flexible questions that are not constrained by predetermined aspects. In the face of such mismatches, we should prioritize experts' opinions to ensure that the diagnosis process adheres to medical standards, rather than solely catering to user preferences. 

\begin{figure*}[th]
	\centering
	\includegraphics[width=\linewidth]{Figures/topics.png}
	\caption{The proportion of symptoms asked by different psychiatrist chatbots (i.e., D1, D2, D3, CPT) and human psychiatrist (denote as ``doctor'').}
	\label{fig:symp_anno}
\end{figure*}


% We can find that \texttt{D3} has the fewest average number of dialogue turns and the least amount of text per turn among all the ChatGPT-based bots. Additionally, it tends to ask more in-depth questions while asking fewer questions each turn, both of which indicate higher professional skills as a psychiatrist. 
% \KZ{Asking in-depth questions seems to contradict with shorter text per turn.
% That's why you better show some examples to clear these doubts.}
% Furthermore, the symptom precision metric is the highest, suggesting that the chatbot's questions are highly efficient, with few ``no'' responses.  
% \KZ{Is there a particular order in which the doc bot follows in asking questions? I guess, when doc asks questions, they go from very general questions, down 
% to specific ones, like navigating down a tree structure. They should ask 
% questions adaptively according to the patients response. I wonder if our
% design permits this kind of style.}
% However, as the required aspects are not explicitly stated in the prompt, the symptom recall metric of this chatbot is relatively low, indicating that its diagnosis may not be comprehensive enough. Nevertheless, the chatbot's questions are more flexible and free-flowing, precisely because there are no predetermined aspects to ask. As a result, patients feel more understood, leading to a better experience overall.

% What's more, \texttt{D2} received the longest responses from patients, which is consistent with the human evaluation metric ``Engagement'', suggesting that patients are more willing to converse with this chatbot. 
% It also achieves the highest symptom recall among all the chatbots, even surpassing \texttt{D1} which also includes aspects in the prompt. 
% This could because \texttt{D1} contains too many instructions regarding empathy and other factors, which may have hindered its ability to thoroughly inquire about all the required symptoms.


\begin{figure*}[th]
 	\centering
	\begin{subfigure}{0.36\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Figures/empathy.png}
		\caption{Empathy Behaviors}
		\label{fig:emp_simu_doctor}%文中引用该图片代号
	\end{subfigure}
	\centering
	\begin{subfigure}{0.36\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Figures/indepth.png}
		\caption{In-depth Question} 
		\label{fig:indepth_simu_doctor}%文中引用该图片代号
	\end{subfigure}
	\caption{Dialogue act comparison between different psychiatrist chatbots and human doctor. The y-axis means the average number of the behavior occurs in the dialogue history.
 % \KZ{What is the y axis of these figs?}
 }
	\label{fig:doctor_chatbot_statistics}
\end{figure*}
% NEWCOMMENT: 图片的纵轴是什么？

\subsection{Human vs. Chatbot as Psychiatrist}
\label{sec:comp_doctor}

By involving human psychiatrists in conversations with patient chatbots, we can identify the limitations of chatbots by comparing them to the ideal benchmark represented by human psychiatrists. We analyze their behaviors based on three dimensions: topic proportion, empathy behaviors and in-depth questions, using the annotated dialogue history\footnote{We discuss the details of the annotation in Appendix \ref{apd:annotation}.}.
% COMMENT: 这里和patient chatbots的关系是？“标注question topic和dialogue act”也出现得有些突兀。应该先说要从这两方面对比评估，再说要标注其类型，可能会更顺畅一些？
% COMMENT: 有doctor chatbot与patient chatbot对话的实验吗？如果没有的话，这里的实验设计得会不会有什么问题？
% NEWCOMMENT: 这里的对比还是很奇怪，评价doctor chatbot是通过和人类病人聊天的，而人类医生是和机器人病人聊天的，该变量未控制的话，这个对比合理吗？
% NEWCOMMENT: 一个很自然的质疑就是，既然patient chatbot和真实病人是有差异的，那么在该变量未控制的情况下，有没有可能是谈话对象的不同导致了采取的对话策略不同呢？怎么证明这个对比反映的一定是doctor chatbot和人类医生的内在能力的差异呢？

\paragraph{Topic Proportion}
% Accordingly, we calculated the average proportion of question topics of different doctor chatbots, as well as human doctors. \figref{fig:symp_anno} displays the outcomes. 
In \figref{fig:symp_anno}, most psychiatrist chatbots tend to inquire more thoroughly about emotion and sleep-related symptoms. Human doctors, on the other hand, have a more even distribution of questions about various symptoms.
Moreover, human doctors often do ``screening'' to rule out other possible conditions (refer to Appendix \ref{apd:human_doc_example} for examples). In addition to exploring the typical symptoms of depression, they also inquire about symptoms related to other conditions, like bipolar disorder and anxiety, due to the high likelihood of comorbidity. However, chatbots rarely exhibit such behavior, indicating the possible limitations in multi-disease scenarios~\cite{Zhang2022SymptomIF}.

% \begin{table}[h]
%     \centering
%     \footnotesize
%     \begin{tabular}{m{0.15 \columnwidth}|m{0.7\columnwidth}}
%     \hline
%     Condition & Screening Question \\
%     \hline
%      Mania & Was there ever a time when you were feeling the opposite of how you feel now, like really upbeat, happy, and full of energy, with lots of plans and such?  \\
%     \hline
%      \multirow{2}{0.15\columnwidth}{Halluci-\\nation} & Do you hear strange voices or sound when you're alone? \\
%      & Do you worry about someone hurting you?\\
%     \hline
%      Social Phobia & Are you anxious about your social interactions specifically, or do you experience anxiety in other areas as well? \\
%     \hline
%     \end{tabular}
%     \caption{Example of screening questions in the dialogue history of human psychiatrist and patient chatbot.}
%     \label{tab:example_screen}
% \end{table}

\paragraph{Empathy Behaviors}
% Then we calculated the average number of empathetic strategies utilized by doctors in the dialogue history, as illustrated in Figure . 
\figref{fig:emp_simu_doctor} shows that \texttt{D1} and \texttt{D3} can utilize a range of empathetic strategies, while \texttt{D2} only offers suggestions to patients because the empathy instructions in prompt are removed. Moreover, though human doctors use all the strategies, their usage is less frequent than that of chatbots. This is because chatbots often strive to understand and empathize without considering appropriateness, whereas human doctors will choose suitable moments for empathetic interactions.

% When asked for the reasons behind this, doctors attributed it to the limited inquiry time in real outpatient scenarios and the bias resulting from the difference in interaction with chatbots compared to real people~\cite{Yun2021BehavioralAN}.

\paragraph{In-depth Questions}
% Further, we also calculated the various ways of in-depth questioning, and obtained Figure \ref{fig:indepth_simu_doctor}.
% \KZ{How did you count the number of in-depth questions? Automatically or manually? By looking for keywords?Is it accurate?} 
\figref{fig:indepth_simu_doctor} reveals that the frequency of asking about the duration or cause of symptoms is similar between human doctors and chatbots. However, human doctors ask significantly more questions about the specific manifestations of each symptom than chatbots do, as this helps to better understand the vague expressions of patients.

\subsection{Patient Chatbot Results}
\paragraph{Rating Metrics}

The human-rated results of patient chatbot are in Table \ref{tab:human_pat}. It can be observed that all metrics of \texttt{P2}
% \MY{Following my previous comment in listing these in a table, I would suggest you using very brief abbreviations and include them in this table, e.g. D1, D2 and P1, P2 and just use these abbr. for all the later texts.} 
are higher than \texttt{P1}. This suggests that the inclusion of resistance, colloquialism, etc., makes the chatbot more similar to real patients, according to the psychiatrists' perspective.
% COMMENT: and so on -> etc.?

\begin{table}[ht]
    \small
    \centering
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc}
    \hline
     & P1 & P2 \\ 
    \hline
    Resemblance & 1.93 & \textbf{2.21} \\
    ~~~~Mental State & 2.07 & \textbf{2.42} \\
    ~~~~Life Experience & 2.00 & \textbf{2.14}  \\
    ~~~~Expression style & 1.57 & \textbf{2.21} \\
    \hline
    Rationality & 2.42 & \textbf{2.57}  \\
    \hline
    \end{tabular}
    % }
    \caption{Human-rated scores of patient chatbots}
    \label{tab:human_pat}
\end{table}

\paragraph{Computational Metrics}
We show the results of computational metrics in Table \ref{tab:auto_pat}. It appears that ``unmentioned symptom ratio'' of \texttt{P2} is higher than \texttt{P1}, indicating a higher level of resistance. We also find that \texttt{P2} engages in slightly more dialogue turns with longer responses from the human psychiatrists than \texttt{P1}. This may be attributed to the inclusion of resistance in the prompt, which requires the psychiatrists to provide more guidance and encourage the patient chatbot to share more information. 
% The statistics of empathy strategies used by the psychiatrists in the conversation with these two patient chatbots can further support this point, which will be discussed in Section \ref{sec:comparison}.

%In informal conversations, people often rely on a smaller set of daily words and phrases. %This limited range of vocabulary contributes to less diversity in spoken language compared to written language.
% Since spoken language typically has less diversity than written language, \KZ{is there any ref for this? I don't see why spoken language is less diverse than
% written.}
Moreover, \texttt{P2} has more human-like language style and fewer robot-like words, echoing the higher human-rated scores in the dimension of ``expression style'', which indicates that its language style is more colloquial compared to \texttt{P1}. 
However, we observe that \texttt{P2} performs less competitively in the ``wrong symptom ratio'' metric, indicating that it may report more symptoms that are not included in the patient portrait. 
% One possible reason for this could be the excessive focus on language style and resistance in the prompt, which might cause ChatGPT to ``forget'' the actual symptoms of the patient. 

% We  in Appendix \ref{apd:examples}, which can provide a more intuitive and comprehensive understanding of different chatbots.
% xd\MY{there are too many newly-named metrics, we NEED to align the automatic and subjective metrics to objectives in Sec2. You can either categorise doctor and patient ones into function and style, or they can name as the objectives. After aligning these, our contribution can have corresponding adjectives for the doctor and patient bots.}


\begin{table}[th]
    \footnotesize
    \centering
    % \resizebox{1.05\linewidth}{22mm}{
    \begin{tabular}{l|cc}
    \hline
    & P1 & P2 \\ 
    \hline
    - \textbf{Statistics} & & \\
    ~~~~avg turns & 31.64 & 33.36\\
    ~~~~avg patient utt len & 40.38 & 40.94 \\
    ~~~~avg doctor utt len & 16.74 & 17.38 \\
    \hline
    - \textbf{Function} & & \\
    ~~~~wrong symp ratio & \textbf{15.07\%} & 18.38\%   \\
    \hline
    - \textbf{Style} & & \\
    % ~~~~Distinct-1 & 42.6\% & \textbf{37.3\%}  \\
    ~~~~human-like word num & 5.36 & \textbf{10.29}   \\
    ~~~~robot-like word num & 7.21 & \textbf{3.79} 	  \\
    ~~~~unmentioned symp ratio &  9.12\% & \textbf{12.28\%} \\
    \hline
    \end{tabular}
    % }
    \caption{Results of Computational Metrics for patient chatbot}
    \label{tab:auto_pat}
\end{table}

% \subsection{Human vs. Patient Chatbot}
% \label{sec:comparison}

% Similarly, as we invited real patients to participate in the evaluation, we can compare the language style of real patient and patient chatbots. 
\paragraph{Case Study}
To effectively showcase the resistance behaviors and emotional expressions of the patient chatbot, we include several examples of what the patient chatbot with full prompt and the reminder mechanism replied when asked by the doctor in Table \ref{tab:example_prompt_full}.

\begin{table}[h]
    \centering
    \footnotesize
    \begin{tabular}{m{0.03\columnwidth}|m{0.8\columnwidth}}
    \hline
     $S_1$ & It's like I toss and turn all night, and those darn nightmares sometimes creep in too.  \\
    \hline
     $S_2$ & \Pink{(Pauses for a moment)} I'm not sure, They're like dark clouds in my head, but don't worry, I would never let them take over. It's just that they freak me out, you know? I don't have a clue how to deal with them. \\
    \hline
    $S_3$ & \Pink{(Uneasy tone)} I'm not quite sure, but it seems like everything is fine with my family. I feel like these issues \Pink{aren't very serious}, so I don't think I need to worry too much about them.\\
    \hline
    $S_4$ & Well, I know, but honestly, \Pink{I don't think you can really help me.} I am a total loser, like I can't do anything right, and there's no hope for me. \\
    \hline
    $S_5$ & Yeah, I've been feeling a bit lonely lately, but \Pink{I think it's mostly because} I've been under a lot of work pressure and haven't had much time to hang out with friends. \Pink{I still feel capable of} socializing with people, but sometimes it can be tiring.\\
    \hline
    \end{tabular}
    \caption{Example utterance of patient chatbot powered by the full prompt with reminder mechanism.}
    \label{tab:example_prompt_full}
\end{table}

We can observe that the patient chatbot exhibits a natural and conversational language style.
Interestingly, sometimes the patient chatbot will give expressions or actions at the beginning of their sentences in parentheses (e.g., $S_2,S_3$). This could be attributed to ChatGPT's pre-training data, which may contain scripts utilizing this format. 
% These expressions and actions can make the conversation more immersive and realistic to some extent.
What's more, We find that the patient chatbot tends to exhibit resistance when faced with certain questions from the doctor, particularly when asked about suicide attempts or family medical history ($S_3$). Additionally, they sometimes downplay their symptoms or offer seemingly plausible reasons to conceal their true feelings, possibly to avoid appearing helpless or burdensome to others ($S_5$).

% However, if we remove the \textit{reminder} mechanism for patient chatbot, it will exhibit noticeable forgetfulness. We present an example in Table \ref{tab:example_prompt_v1}. This dialogue is from the 7th round of a conversation with a total of 23 rounds. We can observe that, without reminder, the patient chatbot will calmly shares symptoms even without being asked by the doctor, which does not align with the instructions in the prompt regarding ``resistance'' and ``emotional fluctuations''. Moreover, the script-like expression (e.g., sentence 2,3 in Table \ref{tab:example_prompt_full}) will not exist without reminder mechanism, as they typically emerge in the latter part of the conversation.

% \begin{table}[h]
%     \centering
%     \footnotesize
%     \begin{tabular}{m{0.12\columnwidth}|m{0.73\columnwidth}}
%     \hline
%      Doctor & Alright, you can try establishing a regular sleep schedule, aiming to go to bed and wake up at the same time every day. Additionally, you can engage in relaxing activities before bedtime, such as listening to soothing music, taking a warm bath, or reading light-hearted books.  \\
%      \hline
%      Patient & Okay, I will try these methods. Apart from sleep problems, I also have some other symptoms, such as difficulty getting out of bed in the morning, a lack of interest in anything, feeling like I can't do anything right, and experiencing a poor mood.\\
%     \hline
%     \end{tabular}
%     \caption{Example dialogue fragment of patient chatbot powered by the full prompt without reminder mechanism.}
%     \label{tab:example_prompt_v1}
% \end{table}

% \subsection{Human Doctor Examples}
% \label{apd:human_doc_example}
