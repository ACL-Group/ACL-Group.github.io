% !TEX root = ../main.tex

\section{Experiments}
\begin{table*}[t!]
\centering
\footnotesize
\begin{tabular}{l c c c c}
\hline
\textbf{Model} & \textbf{Model Size} & \textbf{Context length} & \textbf{Language} & \textbf{Access}\\
\hline
GPT-4~\cite{openai2023gpt4} & undisclosed & 8k & cn/en & API \\
GPT-3.5-turbo~\cite{schulman2022chatgpt} & undisclosed & 4k & cn/en & API\\
GPT-3.5-turbo-16k~\cite{schulman2022chatgpt} & undisclosed & 16k & cn/en & API\\
\hline
LLaMA2~\cite{touvron2023llama} & 7B & 4k & en & Weights\\
Alpaca~\cite{alpaca} & 7B & 2k & en & Weights\\
Vicuna-v1.5~\cite{chiang2023vicuna} & 7B & 4k & en & Weights\\
\hline
Chinese-LLaMA2~\cite{Chinese-LLaMA-Alpaca} & 7B & 4k & cn/en & Weights\\
Chinese-Alpaca2~\cite{Chinese-LLaMA-Alpaca} & 7B & 4k & cn/en & Weights\\
ChatGLM2~\cite{du2022glm, zeng2022glm} & 6B & 8k & cn/en & Weights\\
\hline
MedAlpaca~\cite{han2023medalpaca} & 7B & 2k & en & Weights\\
Mental-Alpaca~\cite{xu2023leveraging} & 7B & 2k & en & Weights\\
MentaLLaMA~\cite{yang2023mentalllama} & 7B & 2k & en & Weights\\
\hline
\end{tabular}
\caption{Models evaluated in this paper. The “access” columns show whether we have full access to the model weights or we can only access through API. Cn = Chinese. En = English.} 
%\MY{add refs to these models}
\label{tab:models}
\end{table*}
In this section, we conducted extensive experiments on PsyEval to assess a total of twelve up-to-date LLMs with carefully designed prompts for each task.

\subsection{Prompt Design}
We have designed corresponding concise prompts for each task. For tasks with clear classification criteria, such as mental health QA and diagnosis prediction via online text data, we employed a zero-shot approach. For tasks with uniquely designed labels, such as diagnosis prediction via dialogue and assessing levels of empathy and safety in dialogue, we utilized a few-shot approach. For tasks aiming to have the model provide its reasoning process in the answers, such as the two diagnosis tasks and the two assess tasks, we concurrently applied a chain-of-thought approach. Prompts tailored for each sub-task are detailed in Appendix \ref{app: prompt design}. 

%\KZ{It's very strange to tell people to go to appendix at the beginning of a section, without saying anything!}
%\MY{You can categorize tasks into different prompting styles, instead of iteratively go through every task. i.e. zero-shot; chain-of-thought, few-shot; then map your tasks into these prompting style categories. Find supporting evidences though, WHY we use these prompts for certain tasks is important.}
% \begin{itemize}
%     \item \textbf{Mental Health QA:} Utilizing zero-shot prompting, we aim to enhance the model's ability to respond accurately to mental health queries without specific training on question-answer pairs.
    
%     \item \textbf{Diagnosis Prediction via Online Text Data:} Employing chain-of-thought prompting, we dissect the model's decision-making process comprehensively to understand its diagnostic outputs better. This approach provides nuanced insights into the factors influencing predictions, offering a thorough evaluation of diagnostic capabilities.

%     \item \textbf{Diagnosis Prediction via Dialogue:} With labels like "depression\_risk" and "suicide\_risk", our methodology combines few-shot and chain-of-thought prompting. This dual approach helps the model learn classification rules and understand their application, enhancing its diagnostic precision.
    
%     \item \textbf{Therapeutic Conversations:} Combining zero-shot prompting with explicit guidance, we assess the model's adaptability in generating therapeutic responses across diverse scenarios. This evaluates its capability to apply general therapeutic knowledge in contextually varied patient interactions.
    
%     \item \textbf{Empathy Understanding in Therapeutic Conversations:} Integrating few-shot and chain-of-thought prompting, our methodology guides the model in learning empathy criteria. This dual-pronged approach enhances the model's proficiency in gauging and expressing empathy in therapeutic contexts.
    
%     \item \textbf{Safety Understanding in Therapeutic Conversations:} Utilizing a dataset with uniquely crafted safety labels, we employ chain-of-thought prompting to assess the model's proficiency in comprehending and addressing safety concerns. This methodological approach holistically evaluates the model's competence in ensuring conversation safety in diverse therapeutic scenarios.

% \end{itemize}

\subsection{Models}
%\MY{Model details can be moved to Appendix.}
To comprehensively assess the capabilities of LLMs in the context of mental health, we evaluated twelve high-performance LLMs that are widely accessible. \tabref{tab:models} summarizes information about these models. For a detailed introduction to the model, see Appendix \ref{app: model details}.
%\KZ{Use references!}



\subsection{Metrics}
%\MY{Metrics related to different tasks should be firs explained here, so that readers won't get confused later. Start like For xxxxx tasks, we incorporate xxx metrics following previous attempts (add refs.). For classification tasks like xx xx, accuracy is utilized (refs).}
For mental health QA task, accuracy is a suitable metric since all questions are objective. For multi-class tasks such as diagnostic tasks and assessing empathy and safety levels in conversations, we also use accuracy as a metric. For the task of simulating psychological counseling sessions, we adopted the evaluation approach from G-eval~\cite{liu2023geval}, using GPT-4 to score the model's outputs on a scale from 1 to 5.

\subsection{Experiments Results}

%\KZ{What do you mean by examples? Of tasks or of LLM's responses? I think you should show some example here, instead of the appendix. Readers may not read the appendix at all.}
Extensive results are presented based on different tasks, with specific observations drawn to discuss the features and drawbacks of the current models.


\subsubsection{Knowledge Tasks} We present a comprehensive performance analysis of various models on the QA task in \tabref{tab: mental health QA}. Upon analyzing the QA task results, GPT-4 emerges as the standout performer, demonstrating significantly superior performance in contrast to other models\footnote{Full results can be found in Appendix \ref{app: result example}.}. Notably, \textbf{only GPT-4 achieved an average accuracy exceeding 60\%}, underscoring the formidable challenges inherent in mental health QA. The performance of models with smaller parameter sizes in these QA tasks closely aligns with the random baseline, accentuating a substantial performance gap when compared to their larger counterparts. It becomes evident that LLMs with smaller parameter sizes lack the comprehensive mental health knowledge base exhibited by models with larger parameter sizes. 
\begin{table}[ht]
\centering
\footnotesize
\begin{tabular}{l c c c c c c c c}
\hline
\textbf{Model} & \textbf{USMLE} & \textbf{Step1} & \textbf{Step2} & \textbf{Crisis}\\
\hline
Chance & 20.00 & 20.00 & 20.00 & 25.00\\
\hline
GPT-4 & \textbf{67.68} & \textbf{71.10} & \textbf{65.16} & \textbf{92.81}\\
GPT-3.5-turbo & 45.12 & 49.68 & 41.77 & 88.24\\
GPT-3.5-turbo-16k & 45.39 & 50.32 & 41.77 & 89.54\\
\hline
LLaMA2 & 25.44 & 26.73 & 23.88 & 77.78\\
Alpaca & 24.76 & 25.97 & 23.87 & 56.21 \\
Vicuna-v1.5 & 23.38 & 23.38 & 23.39 & 64.71\\
\hline
Chinese-LLaMA2 & 20.08 & 23.05 & 17.90 & 60.78\\
Chinese-Alpaca2 & 20.77 & 22.73 & 19.33 & 63.40\\
ChatGLM2 & 20.77 & 23.05 & 19.09 & 76.47\\
\hline
MedAlpaca & 28.34 & 29.22 & 27.68 & 53.59 \\
Mental-Alpaca & 25.17 & 28.25 & 22.92 & 55.56 \\
MentalLLaMA & 25.58 & 27.27 & 24.34 & 53.59 \\
\hline
\end{tabular}
\caption{Models Performance on QA task (Metrics: Accuracy 100\%)%\MY{need a space before $($}
. USMLE represents the average accuracy of mental health-related questions extracted from the USMLE. %\MY{USMLE is the results on full medical dataset here?} 
Step 1 primarily focuses on foundational knowledge, while Step 2 is clinical-skill oriented.}
\label{tab: mental health QA}
\end{table}

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{l c c c c c c c c c c c}
\hline
\textbf{Model} & \textbf{Depression} & \textbf{Anxiety} & \textbf{Bipolar} & \textbf{Schiz.} & \textbf{Eating} & \textbf{PTSD} & \textbf{Autism} & \textbf{OCD} & \textbf{ADHD} & \textbf{Multi}\\
\hline
GPT-4 & 42 & 66 & 42 & 42 & 30 & 36 & 34 & 30 & 62 & 22\\
GPT-3.5-turbo & 68 & \textbf{86} & 54 & 48 & 62 & 48 & 54 & 60 & 64 & 24\\
GPT-3.5-turbo-16k & \textbf{74} & \textbf{86} & \textbf{62} & \textbf{62} & \textbf{68} & \textbf{50} & \textbf{60} & \textbf{66} & \textbf{68} & \textbf{28}\\
\hline
LLaMa2 & 62 & 70 & 50 & 40 & 54 & 42 & 52 & 38 & 52 & 10\\
Alpaca & 24 & 36 & 28 & 14 & 12 & 18 & 26 & 20 & 24 & 6\\
Vicuna-v1.5 & 64 & 78 & 50 & 42 & 56 & 40 & 48 & 50 & 48 & 8\\
\hline
Chinese-LLaMA2 & 52 & 68 & 44 & 36 & 42 & 44 & 38 & 40 & 44 & 10\\
Chinese-Alpaca2 & 54 & 70 & 48 & 40 & 46 & 42 & 46 & 42 & 44 & 12\\
ChatGLM2 & 66 & 80 & 56 & 40 & 56 & 44 & 56 & 44 & 46 & 12\\
\hline
MedAlpaca & 20 & 34 & 24 & 12 & 8 & 12 & 16 & 12 & 18 & 4\\
Mental-Alpaca & 32 & 44 & 32 & 20 & 20 & 32 & 34 & 22 & 30 & 8\\
MentalLLaMA & 30 & 42 & 32 & 22 & 24 & 30 & 30 & 20 & 28 & 10\\
\hline
\end{tabular}
\caption{Models Performance on Diagnosis Prediction via Online Text Data (Metrics: Accuracy 100\%). "Schiz." stands for schizophrenia.%\MY{bold your best results, to all tables}
}
\label{tab: SMHD}
\end{table*}
These models exhibit relatively superior proficiency in handling tasks falling under Step 1, emphasizing foundational scientific knowledge. However, their performance diminishes when confronted with tasks associated with Step 2, which involve more intricate clinical knowledge scenarios. The challenges presented in Step 2, leaning toward clinically relevant questions, introduce heightened complexity. This observed performance decrement in Step 2 suggests that the model \textbf{encounters difficulties when tasked with understanding and navigating the intricacies of real-world clinical scenarios}. The need for a more nuanced comprehension of clinical complexities, often encountered in diagnostic and therapeutic settings, becomes evident. Therefore, addressing the challenges presented in Step 2 becomes imperative for enhancing the model's applicability in clinical mental health contexts.


Comparing GPT-3.5-turbo's performance on our dataset with its performance on full medical USMLE (Step1: 55.8\%, Step2: 59.1\%)~\cite{kung2023performance} exposes specific challenges and limitations in mental health queries. This nuanced analysis provides insights for future improvements in mental health-oriented language models.
MedAlpaca, fine-tuned on medical text using Alpaca as a base, outperforms Alpaca, indicating the efficacy of fine-tuning for enhancing mental health-related knowledge. Mental-LLaMA and Mental-Alpaca, fine-tuned for mental health prediction, show moderate improvement, with a limited extent. Notably, their performance declines on the Crisis Response dataset, suggesting fine-tuning's task-specific impact.



\subsubsection{Diagnostic Tasks} We extensively compared various models for the Diagnosis Prediction via Online Text Data and Simulated Doctor-Patient Dialogue tasks, as presented in\tabref{tab: SMHD} and \tabref{tab: D4}.%\KZ{Tables 4 and 5}.

In the diagnosis prediction via online text data, models demonstrated strong predictive capabilities for \textbf{depression and anxiety}, leveraging explicit symptoms in social media posts. However, predicting conditions like \textbf{bipolar disorder, schizophrenia, PTSD, autism, and multiple disorders} posed challenges due to higher ambiguity. For instance, bipolar disorder might be misdiagnosed as depression, and symptoms might not be readily expressed in textual content, as in the case of schizophrenia. All models exhibited subpar performance in complex multiple disorder diagnoses, indicating a deficiency in handling intricate diagnostic tasks.

In a longitudinal comparison of model performance, GPT-4's results were inferior to those of GPT-3.5-turbo and GPT-3.5-turbo-16k. Through error analysis, it was discovered that GPT-4 tends to be fixated on the `symptom->disease' process during disease diagnosis, often overlooking the potential mental states of posting users, as shown in Appendix \ref{app: model comparison}. It only correctly predicts when users explicitly manifest depressive symptoms in their posts, whereas GPT-3.5 is more accurate in such situations.

In this task, \textbf{models with a 2k context struggled}, impacting the performance of models like mental-Alpaca and mental-LLaMA, despite secondary training. Longer context window models, like GPT-3.5-turbo-16k, showed better performance.
In the diagnosis prediction via simulated doctor-patient dialogue data, GPT-4 also displayed an inclination toward the 'symptom->disease' process, often overlooking the actual states of patients, as shown in Appendix \ref{app: model comparison}.
%\MY{give labels to different sections, then you can ref these sections.}

Furthermore, given that the task is inherently situated in a Chinese context, models that underwent secondary training on Chinese datasets exhibited superior performance, like chatglm2 and Chinese-Alpaca2. Notably, models like mental-Alpaca and mental-LLaMA, which received secondary training on relevant downstream tasks, outperformed LLaMA2 and Alpaca.


\begin{table}[htpb]
\centering
\footnotesize
\begin{tabular}{l c c }
\hline
\textbf{Model} & \textbf{Depression\_risk} & \textbf{Suicide\_risk}\\
\hline
GPT-4 & 36.92 & \textbf{69.23}\\
GPT-3.5-turbo & 51.54 & 64.62\\
GPT-3.5-turbo-16k & \textbf{53.08} & 67.69\\
\hline
LLaMa2 & 16.15 & 10.77\\
Alpaca & 12.31 & 9.23\\
Vicuna-v1.5 & 15.38 & 15.38\\
\hline
Chinese-LLaMA2 & 22.31 & 20.00\\
Chinese-Alpaca2 & 24.62 & 21.54\\
ChatGLM2 & 23.08 & 20.77\\
\hline
MedAlpaca & 11.54 & 9.23\\
Mental-Alpaca & 19.23 & 12.31\\
MentalLLaMA & 19.23 & 17.69\\
\hline
\end{tabular}
\caption{Models Performance on Diagnosis Prediction via Dialogue (Metrics: Accuracy 100\%)}
\label{tab: D4}
\end{table}
On PsyQA and DialogueSafety datasets, models lacking secondary training on Chinese corpora exhibited relatively poor performance. The deficiency in performance can be attributed to the lack of fine-tuning specifically on Chinese data, highlighting the importance of language-specific pretraining for mental health related tasks.

\subsubsection{Therapeutic Tasks}We conducted a comprehensive comparison of various models in Therapeutic Tasks, and the results are presented in Table 6. In the Therapeutic Conversation task, after obtaining the model's output, we had \textbf{GPT-4 score the model's output based on the ground truth from 1 to 5}~\cite{liu2023geval}. GPT-4 demonstrated exceptional performance across all metrics, achieving the highest GPT4 score, Empathy ACC, and Safety ACC. 
%\KZ{How did you measure empathy ACC and safety ACC? These should have been explained in the task setup section.} 
It's noteworthy that GPT-4, GPT-3.5-turbo, and GPT3.5-turbo-16k exhibited relatively similar performance. While GPT-4 showed advancements over GPT-3.5, it is conceivable that architectural improvements and parameter increases might not be substantial enough to significantly surpass GPT-3.5 in specific mental health therapeutic tasks. The additional complexity and parameters in GPT-4 may not have a pronounced impact on the nuanced differences in tasks related to mental health therapy, resulting in GPT-3.5 exhibiting similar performance. 

However, it is crucial to highlight that the performance of all models, including GPT-4 and GPT-3.5-turbo, was \textbf{consistently subpar in these Therapeutic Tasks}. This observation prompts further investigation into the specific challenges posed by mental health therapeutic scenarios and underscores the need for model enhancements tailored to this domain. 


\begin{table}
\centering
\footnotesize
\begin{tabular}{l c c c}
\hline
\textbf{Model} & \textbf{Response} & \textbf{Empathy} & \textbf{Safety}\\
\hline
GPT-4 & \textbf{4.20} & \textbf{46.92} & \textbf{48.28}\\
GPT-3.5-turbo & 3.66 & 44.62 & 39.90\\
GPT-3.5-turbo-16k & 3.82 & 46.15 & 42.36\\
\hline
LLaMa2 & 2.05 & 25.38 & 19.70\\
Alpaca & 1.84 & 15.38 & 14.78\\
Vicuna-v1.5 & 1.92 & 27.69 & 19.21\\
\hline
Chinese-LLaMA2 & 2.75 & 27.69 & 25.61\\
Chinese-Alpaca2 & 2.94 & 30.77 & 29.06\\
ChatGLM2 & 2.81 & 26.15 & 28.57\\
\hline
MedAlpaca & 1.63 & 13.84 & 12.32\\
Mental-Alpaca & 1.72 & 14.61 & 12.32\\
MentalLLaMA & 1.80 & 16.15 & 13.30\\
\hline
\end{tabular}
\caption{Models Performance on Therapeutic Tasks. The metric for Generated Response is GPT4 score. The metrics for Empathy and Safety are Accuracy 100\%.}
\end{table}