Meta Review of Paper1759 by Area Chair zkTb 
ACL ARR 2022 January Paper1759 Area Chair zkTb
28 Feb 2022ACL ARR 2022 January Paper1759 Meta ReviewReaders: Paper1759 Senior Area Chairs, Paper1759 Area Chairs, Paper1759 Authors, Paper1759 Reviewers Submitted, Program Chairs
Metareview:
This paper presents a curriculum learning strategy that makes predictions increasingly harder by increasing the amount of the sequence predicted. The method achieves improvements on 5 NLG tasks and can be generalized to more. There are numerous experiments and ablations to show the proposed methods performance. One reviewer highlighted some potential issues with the TCL baseline that could impact this paper’s results significantly.

Summary Of Reasons To Publish:
The proposed method shows improvements on 5 diverse NLG tasks.

The method is general and could be applied to more tasks.

The paper is clearly written.

Summary Of Suggested Revisions:
Reviewer 8bZG notes “The decision to not include TCL in the news summarization test is insufficiently justified and leads the reader to wonder whether the authors simply ran out of time to run the experiment.”

They also highlight issues with the TCL baseline that was worrisome and points to issues with the baseline. If the reimplementation is incorrect, than it could affect the entire paper’s results.

Overall Assessment: 3 = There are major points that may be revised

Official Review of Paper1759 by Reviewer hFdt 
ACL ARR 2022 January Paper1759 Reviewer hFdt
16 Feb 2022ACL ARR 2022 January Paper1759 Official ReviewReaders: Program Chairs, Paper1759 Senior Area Chairs, Paper1759 Area Chairs, Paper1759 Reviewers Submitted, Paper1759 Authors
Paper Summary:
The paper presents a curriculum learning strategy designed for natural language generation tasks which makes the learning task progressively harder by predicting more and more of the sequence. The results show improvements on a wide range of NLG tasks.

Summary Of Strengths:
Introduces a novel, simple and elegant way to do curriculum learning for natural language generation.

Significant Improvements on five NLG tasks.

Thorough analysis of the approach.

Summary Of Weaknesses:
Not clear whether the baselines were trained in a comparable way to the new method, i.e., was the number of updates the same, was the baseline trained to predict the same number of output tokens as ICT over the course of the entire training.

Comments, Suggestions And Typos:
Be clearer on the exact train parameters for ICT vs. the baselines: how many output tokens were predicted on average per batch for each method, and how often did you cycle over the train data in each setting.

Overall Assessment: 4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper1759 by Reviewer NLm2 
ACL ARR 2022 January Paper1759 Reviewer NLm2
15 Feb 2022 (modified: 15 Feb 2022)ACL ARR 2022 January Paper1759 Official ReviewReaders: Program Chairs, Paper1759 Senior Area Chairs, Paper1759 Area Chairs, Paper1759 Reviewers Submitted, Paper1759 Authors
Paper Summary:
This paper proposes a new curriculum learning method for natural language generation. Previous CL methods for natural language generation design human-crafted rules based on data features or train models to measure the difficulty scores, so they are task-specific and difficult to generalize to different tasks. Different from these methods, this paper considers an in-sample CL method, which re-orders the learning sequence within the sample. Specifically, it predicts the last few tokens given a long prefix first, and then gradually increase the tokens at the end while shortening the prefix, making the training order be an easy-to-hard style.

Summary Of Strengths:
The method is general, and can be easily applied on more language generation tasks.
The method and experiments are written clearly.
Summary Of Weaknesses:
On some of the tasks, the improvements of performance brought by this method are not very significant.
The principle of this method lacks some explanation. Without the supervision of the first half of the output (the prefix), why it is easy to calculate only the second half of the output (the target)?
Some details of the method are unclear, such as whether the prefix is concatenated or the original output of the model.
Comments, Suggestions And Typos:
The denominator in Formula 1 should be (n+1).
Line192-193: The parameters are also the same as dialogue summarization. -> The statement about the parameter setting of dialogue summarization comes later than this sentence. The order of these needs to be adjusted appropriately.
There is a spelling problem with line 366.
The meaning of “+” in table 4 is unclearly.
Overall Assessment: 3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper1759 by Reviewer QYD1 
ACL ARR 2022 January Paper1759 Reviewer QYD1
14 Feb 2022ACL ARR 2022 January Paper1759 Official ReviewReaders: Program Chairs, Paper1759 Senior Area Chairs, Paper1759 Area Chairs, Paper1759 Reviewers Submitted, Paper1759 Authors
Paper Summary:
This paper proposed a simple yet effective curriculum learning strategy for natural language generation. During training, they first predict the last few tokens given a long prefix and gradually increase the tokens at the end to predict while shortening the prefix. Their method outperforms baselines with similar setups on several NLG tasks.

Summary Of Strengths:
This paper is well written. Methods and experiments are clearly presented.
The in-sample curriculum learning is easy to implement and can generalize well to different NLP tasks. I think it is reasonable and would have a fair potential to be applied to other tasks and datasets.
The experiments and ablation studies are comprehensive. And the results are convincing.
Summary Of Weaknesses:
At the end of Section 4.1, the authors claim that ICL can further improve the performance when combined with traditional CL strategies, e.g. re-ordering sentences by sentence length, but there are no experiments in the paper supporting this claim.
The authors reported only the results of their reimplemented baselines, for fair comparison, I think they should also include numbers from original papers in the tables.
In section 3.2, the authors suggest that a strong baseline (TCL) that uses a similar curriculum learning strategy works better on machine translation task, because "the order of the corresponding tokens between input and output are [almost the same]". This is not very accurate. TCL was tested on English to Vietnamese, French, German and Romanian. And the word alignments are apparently not monotonic. I'm also curious about how ICL performs on MT tasks.
Comments, Suggestions And Typos:
Typos: line 460 "the gap between training objectives should be too large" -> should not

Overall Assessment: 4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.
Software: 4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper1759 by Reviewer 8bZG 
ACL ARR 2022 January Paper1759 Reviewer 8bZG
06 Feb 2022ACL ARR 2022 January Paper1759 Official ReviewReaders: Program Chairs, Paper1759 Senior Area Chairs, Paper1759 Area Chairs, Paper1759 Reviewers Submitted, Paper1759 Authors
Paper Summary:
This paper proposes an approach to curriculum learning for sequence-to-sequence natural language generation tasks that adapts the training examples rather than re-ordering them. The model initially only needs to generate the end of the output given the input and the beginning of the reference as context, and the length of the reference context provided is gradually reduced to none. They evaluate this approach on five NLG tasks with commonly available datasets: Reading Comprehension (DREAM), Dialogue Summarization (SAMSum), Style Transfer (STRAT), Question Generation (SQuAD), and News Summarization (CNNDM). They compare against re-implementations of baseline models without curriculum learning and with token-wise curriculum learning (Liang et al, 2021) on all five tasks using automated metrics and human evaluation. The approach shows slight improvement on several metrics for all tasks. They also compare against traditional curriculum learning approaches and explore hyperparameter choices using automated metrics on the dialogue summarization task.

Summary Of Strengths:
The approach is intuitive and could potentially be applied to any NLG task with minimal if any task-specific modifications. The evaluation is much more robust than most papers: they conduct both automated AND human evaluation on multiple tasks and compare against strong baselines. They even provide examples and some error analysis.

Summary Of Weaknesses:
The characterization of the token-wise curriculum learning (TCL) in Liang et al (2021) is incorrect. They state that TCL, "[produces] incorrect training samples because the output is cut short." (lines 72-74). The hard constraint TCL does not truncate the output, it simply does not include the end of the output in the loss calculation. The characterization is even more incorrect when applied to the soft constraint TCL, which only decays the impact of the end of the output in the loss calculation. This fundamental misunderstanding casts doubt on their reimplementation of TCL and thus the legitimacy of the comparison in the evaluation.
They do not explain how the prefix is provided to the model. Is it generated through teacher forcing or concatenated to the input (or some other approach)? This is a critical implementation detail.
The decision to not include TCL in the news summarization test is insufficiently justified and leads the reader to wonder whether the authors simply ran out of time to run the experiment.
Comments, Suggestions And Typos:
Although the baseline models are named in the text, it would be helpful to include them in one of the tables for reference (perhaps as a column in Table 1 or in place of "w/o CL" in Table 2). The abbreviation "TCL" should be introduced in the text rather than in the caption of Table 2. Section 3.2 would be more readable if the relevant sections of the results table were referenced in the text. Adding the baseline w/o CL scores to Table 4 would be a helpful courtesy to the reader (it was helpful to see in Table 6, although I generally expect to see a baseline at the top rather than the bottom).

There are may spelling and grammar errors, but most would be easily caught by an automated grammar checker (e.g., MS Word or Grammarly). If you care to look for them manually, the lines where I noticed them are: 6, 116, 153, 176, 209, 217, 269, 270, 278, 281, 283, 285, 305, 331, 354, 400, 442, 445, 448, 454, 456, 457, 460, 465, 479, 481, 484, 497, 506, 511, 519, 527, 529, 544, 555.

Some areas of the checklist are insufficiently described, but I do not think they pose ethical concerns so I am noting them here rather than under ethical concerns. These could be included on the checklist or in appendices if they don't fit in the text. Computational budget (e.g. GPU hours) is missing (note that because faster convergence is often a benefit of curriculum learning, this information might also be useful for judging the value of the approach). Descriptive statistics could be clearer (confidence interval, clear explanation of statistical tests used). Preprocessing steps (tokenization, etc.) are not mentioned. I believe they can be inferred based on the models used, but it is better to state explicitly whenever possible. Neither the pool the volunteer annotators were chosen from (students, researchers, members of the community) nor their familiarity with the tasks was specified. This information is helpful in determining their qualifications and biases in evaluating the output, so it could strengthen the results to include it.

Overall Assessment: 2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.
Confidence: 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.
Best Paper: No
Limitations And Societal Impact:
Because this paper is not introducing a new task or data set, it primarily inherits the limitations and societal impacts of the existing tasks and data. I do not believe the proposed approach raises any new concerns.

Ethical Concerns:
Although some areas of the checklist are insufficiently described, I do not think they pose significant ethical concerns, so I included them in the Comments, Suggestions, and Typos section.

Needs Ethics Review: No
Reproducibility: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)
Software: 4 = Useful: I would recommend the new software to other researchers or developers for their ongoing work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.