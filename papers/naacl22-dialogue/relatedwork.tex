\section{Related Work}
\label{sec:relatedwork}

%Consideration of our approach over previous approaches on 
Dialogue summarization and pretrained language models are discussed as follows.

%\subsection{Dialogue Summarization}

\textbf{Dialogue Summarization:} A growing number of works have been proposed for dialogue summarization in recent years~\cite{feng2021survey}. 
They widely explore dialogue features explicitly and input them as known labels to enhance the dialogue understanding ability of summarization models.
Features, including dialogue acts~\cite{goo2018abstractive}, topic transitions~\cite{chen2020multi}, discourse dependencies~\cite{chen2021structure}, coreference relations~\cite{liu2021coreference}, argument graphs~\cite{fabbri2021convosumm}, semantic structures or slots~\cite{lei-etal-2021-finer-grain,zhao-etal-2021-give-truth}, etc. are carefully designed and collected by transferring tools pre-trained on other corpus or unsupervised methods with multiple hyper-parameters. 
%Such specific features are not suitable for all scenarios and they highly relied on human labors or experts to redesign heuristic rules or hyper-parameters when implementing on a new dataset.
These work also modify the basic transformer-based models with additional encoders~\cite{chen2020multi} or attention layers~\cite{chen2021structure,liu2021coreference,lei-etal-2021-finer-grain,zhao-etal-2021-give-truth} to utilize the injected features.
\citet{liu-etal-2021-topic-aware} propose a contrastive learning approach for dialogue summarization with multiple training objectives. They also introduce a number of hyper-parameters for contrastive dataset construction and balancing among training objectives.

%Different from previous approaches, we propose to do post-training before dialogue summarization tasks, exploiting the vanilla model's representation ability. And our approach is proved to be more effective with less implementation costs. 


%Multi-view~\cite{chen2020multi} proposes to add separators between input utterances by topic segments and conversational stages with a double-encoder design.
%Structure-BART~\cite{chen2021structure} constructs inter-utterance discourse graphs and intra-utterance action graphs with a graph encoder layer between the encoder and decoder structure.
%ConvoSumm~\cite{fabbri2021convosumm} improves summarization by constructing an argument graph for each dialogue session and linearizing the graph as the input for models.
%DialoBART~\cite{feng2021language} takes advantage of the pretrained response generator model DialoGPT\cite{zhang2020dialogpt} to label features with special tokens including keywords extraction, redundancy detection and topic segmentation.
%Coref-Attention~\cite{liu2021coreference} utilizes coreference resolution tools accompanied with human-designed rules to get coreference links between words in dialogues.

%and may also modify the vanilla pretrained encoder-decoder model to utilize their additional labels.



%\subsection{Pretrained Language Models}

\textbf{Pretrained Language Models:} Previous pretrained seq-to-seq models can be divided into two categories by training data formats.
One is models pretrained on narrative text, such as BART~\cite{lewis2020bart}, PEGASUS~\cite{zhang2020pegasus}, and T5~\cite{raffel2020exploring}. They use training data from Wikipedia, BookCorpus~\cite{zhu2015aligning} and C4~\cite{raffel2020exploring}. These models show great potentials on tasks such as translation and story ending generation. %, where both the input and output are in narrative text format.
The other is models pretrained on dialogue, such as DialoGPT~\cite{zhang2020dialogpt} and PLATO~\cite{bao2020plato}. Their training data are general-domain dialogues, such as Reddit~\cite{henderson2019repository} and Twitter~\cite{cho2014learning}. These models work for dialogue response selection and generation tasks. %, aiming at finding or generating the most suitable utterance given the dialogue history.
All of the above models are trained to exploit language features within the same data format, with pre-training tasks such as masked token/sentence prediction and utterance permutation.
%on tasks including masked language modeling, next utterance or sentence prediction, and utterance or sentence permutation, exploiting the language modeling features within the same data format.
Pretraining with cross-format data hasn't been researched so far. 
%With the rapid growth of single format data understanding and generation mentioned above, cross-format tasks become new hot points, such as dialogue summarization. 
%Different from the previous language modeling training target, 
%such cross-format pretraining focuses on rephrasing between a third-person point of view and multi-speaker views.
As a first step, we focus on narrowing the gap by learning to rephrase unidirectionally from dialogue to narratives.




