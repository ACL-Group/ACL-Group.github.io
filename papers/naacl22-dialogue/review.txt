Post-training to Rephrase from Dialogue to Narrative Text for Abstractive Dialogue Summarization 
Anonymous
17 Nov 2021 (modified: 03 Dec 2021)ACL ARR 2021 November Blind SubmissionReaders: November, Senior Area Chairs, Area Chairs, Reviewers, Paper505 Authors
Abstract: Previous dialogue summarization techniques adapt large language models pretrained on the narrative text by injecting dialogue-specific features into the models. These features either require additional knowledge to recognize or make the resulting models harder to tune. To bridge the format gap between dialogues and textual summaries, we propose a simple method to post-train pretrained language models (PLMs) by learning to rephrase from dialogue to narrative text. %using no additional training data. After that, the model is fine-tuned for dialogue summarization as usual. To this end, we provide a group of methods to construct rephrasing datasets based on the dialogue summarization data itself and newly introduce a prefix-guided generation task. Comprehensive experiments show that our approach significantly improves vanilla PLMs on dialogue summarization and outperforms other SOTA models by the summary quality and implementation costs.
Software:   zip
Data:   zip
Revealed to Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. Zhu
14 Nov 2021 (modified: 16 Nov 2021)ACL ARR 2021 November Submission
Authors:Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. Zhu
Preprint: no
Preferred Venue: ACL 2022
Consent: yes
Consent To Review: yes
Add
Reply Type:all
Author:everybody
Visible To:all readers
Hidden From:nobody
4 Replies
[–]
Official Review of Paper505 by Reviewer 23oZ 
ACL ARR 2021 November Paper505 Reviewer 23oZ
06 Jan 2022 (modified: 06 Jan 2022)ACL ARR 2021 November Paper505 Official ReviewReaders: Program Chairs, Paper505 Senior Area Chairs, Paper505 Area Chairs, Reviewers, Paper505 Authors
Paper Summary:
This paper proposes fine-tuning pre-trained language models (PLMs) for dialogue summarization in two stages. The first fine-tuning stage aims at bringing the gap between the language of the dialogues (first person) and the language of the summaries (third person). The second stage is the standard dialogue summarization fine-tuning. Several different dataset construction methods are proposed for the first stage (based on prior work). In addition, the authors propose ignoring the loss of the first few tokens in the output sequence during training, which they term prefix-guided generation (PGG). They show that incorporating PGG into the training leads to improved performance on SAMSum and DialSumm.

Summary Of Strengths:
Tried several different dataset construction approaches in order to find out what would work best for 1st stage pre-training.
Liked the different ablations to show the effectiveness of the proposed PGG method.
Summary Of Weaknesses:
There seems to be very little in terms of novelty in the paper. The idea of adapting language models for downstream tasks/domains has been done (for example Gururangan et al. 2020). PGG isn't really a novel task -- it's essentially just teacher forcing except you ignore the loss from the first few tokens -- though it is interesting that ignoring the first few tokens seems to improve performance.
The significance tests are compared against BART, instead of the SOTA baseline. Are the improvements significantly better than the best SOTA baseline?
Running the models multiple times and selecting the best result seems pretty odd! It would be better to report the mean along with the variation.
Comments, Suggestions And Typos:
The paper was very difficult to follow. I had to go back and re-read several times. Way too many unintuitive abbreviations in the paper. The paper needs a significant rewrite before it can be published.
Post-training is an odd term, since it would tend to imply something you do after training. I would suggest that you change that to something else (maybe two-stage finetuning?).
You should move the human evaluation description down to where you present the results. Had to scroll back a lot to look up the different acronyms.
Overall Assessment: 2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.
Confidence: 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.
Best Paper: No
Replicability: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper505 by Reviewer 6Qnp 
ACL ARR 2021 November Paper505 Reviewer 6Qnp
06 Jan 2022ACL ARR 2021 November Paper505 Official ReviewReaders: Program Chairs, Paper505 Senior Area Chairs, Paper505 Area Chairs, Reviewers, Paper505 Authors
Paper Summary:
The paper proposes an intermediate training step where the PLM is trained to rephrase dialogue into narrative text. As there does not exist such a dataset of dialogue -> narrative test, the authors explore variations for automatically creating such data, finding a variation consisting of (document, summary sentence) pairs to perform best.

The authors apply a prefix-guided generation task for this data. The model is then fine-tuned on the original dialogue -> summary data.

Results are shown across two dialogue datasets, and human evaluation is performed on 100 examples of one of the datasets.

Summary Of Strengths:
State-of-the-art results are achieved on the DialSumm dataset for automatic ROUGE evaluation. The approach largely only requires data modifications as opposed to architectural changes, so its relative simplicity is a positive.

Summary Of Weaknesses:
I do not find the results very convincing - the authors do mention how differences in tagging and parsing accuracy can affect results around L401, but do not really provide an explanation as to why Ling-Root might do better on DialSumm or Ling-Noun on SamSum, yet the difference between these choices is the difference between outperforming previous results or not. Related to this, from what I can tell, all ablation results are provided on the test set. This, along with choosing the best of three experiments (mentioned in L261), seems like a recipe for overfitting to the test set, even if some prior work did similarly. For the human evaluations, for Mid|Red the proposed model shows a fair number more errors and a smaller gap is found in Cor|Rea. Including statistical significance analysis for the overall human evaluation would be useful.

The idea of transforming the input to be more narrative isn't necessarily novel. A related approach was done in "Restructuring Conversations using Discourse Relations for Zero-shot Abstractive Dialogue Summarization" (unpublished so the authors may not know about it, but it's mentioned in Feng et al., 2021a, which the authors cite). "A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining" does the opposite direction of converting news data to more dialogue-structured data for pretraining.

Comments, Suggestions And Typos:
Misc clarification questions:

L223, you say how b is used as a constant equaling 4 during inference - when is this inference used and which tokens are used to begin generation then? It seems like you do post-training on your data and then do not need to do inference for the second stage finetuning.

L257 "The minimum and maximum lengths are set..." - I didn't quite understand that - you just set the min and max lengths to single numbers for each dataset, based on that dataset's statistics, right?

Misc writing and other suggestions L036: delete "and so on" L175: done* between D and S L182 "broken by" L239: The standard way to report compression ratio is DW/SW, not the other way around. L284: Better to use PERL-based ROUGE script L339: noise L341: does not* (don't use contractions); also L481, 563 L435: Rouge* Table 9: trials* L529: etc.,* L532: These works* Table 3: do not include decimal places for IW and OW

Overall Assessment: 2.5 
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 2 = Documentary: The new datasets will be useful to study or replicate the reported research, although for other purposes they may have limited interest or limited usability. (Still a positive rating)
Software: 2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
Ethical Concerns:
No ethical concerns found.

[–]
Official Review of Paper505 by Reviewer pMdv 
ACL ARR 2021 November Paper505 Reviewer pMdv
27 Dec 2021 (modified: 27 Dec 2021)ACL ARR 2021 November Paper505 Official ReviewReaders: Program Chairs, Paper505 Senior Area Chairs, Paper505 Area Chairs, Reviewers, Paper505 Authors
Paper Summary:
This paper aims to improve dialogue summarization by first letting a model learn to generate a third-person narrative based on dialogues. The authors designed several ways to automatically convert dialogue summarization dataset into utterance-narrative ones, and they also explore two tasks (vanilla generation and prefix-guided generation) to use the data. After this stage, a model is fine-tuned on the downstream dialogue summarization datasets. Experiments show that gains can be obtained on two datasets: SAMSum and DialSumm.

Summary Of Strengths:
This paper is well-written and well-motivated.

The authors designed several methods to convert a dialogue summarization dataset into dialogue-narrative data.

The authors conducted extensive experiments and analyzed the errors carefully. Human evaluation is also used.

The authors provide a method to better use existing datasets for improvements.

Summary Of Weaknesses:
Some details about data are missing. For example, it is not clear a single dialogue can be used to generate how many utterance-narrative pairs in average. And the average number of utterances is not clearly described.

The authors should conduct a manual check to evaluate the quality (e.g., reference between utterances and narratives) of the automatically generated data. Also, for DialSent, given the same input, it seems that there can exist multiple “correct” outputs. Will it cause any negative impacts on the final performance?

The authors might need to cite previous work about meeting summarization, which is also highly relevant.

To demonstrate the effectiveness of the post-training, the authors also need to explore other ways to leverage dialogue summarization datasets.

Comments, Suggestions And Typos:
Line 175: down -> done
Move DSum to the first row in Table 1 as others are variations of the original dataset.
Please also report the average performance, instead of the best one out of three runs only.
Please clearly describe what types of generation tasks (vanilla or PGG) (Lines 224-227) are used for different types of data in Table 4.
Overall Assessment: 3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 3 = Potentially useful: Someone might find the new datasets useful for their work.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
[–]
Official Review of Paper505 by Reviewer 2ruE 
ACL ARR 2021 November Paper505 Reviewer 2ruE
14 Dec 2021ACL ARR 2021 November Paper505 Official ReviewReaders: Program Chairs, Paper505 Senior Area Chairs, Paper505 Area Chairs, Reviewers, Paper505 Authors
Paper Summary:
In this paper, the author proposed to use a 2nd-phase pre-training with the known prefix for dialogue summarization. This method allows the speaker information to appear in the inputs text. It also helps the performance of SamSum and DialSumm datasets.

Summary Of Strengths:
Good performance on SamSum and DialSumm
Summary Of Weaknesses:
Not enough novelty
No explanation of why the method works.
Comments, Suggestions And Typos:
Need more experiments on longer inputs/outputs datasets. SamSum and DialSumm are too short.
Explain why the method is working.
Overall Assessment: 2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Replicability: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 1 = No usable datasets submitted.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
