% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{algorithm,algpseudocode}
\usepackage{subcaption}
\usepackage{hhline}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
%\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
% This assumes your files are encoded as UTF8
%\usepackage[utf8]{inputenc}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% Added
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{makecell}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\apxref}[1]{Appendix \ref{#1}}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}
\usepackage{color}
\newcommand{\KZ}[1]{\textcolor{red}{Kenny: #1}}
\newcommand{\YZ}[1]{\textcolor{red}{Yizhu: #1}}
\newcommand{\JQ}[1]{\textcolor{green}{JQ: #1}}


\title{In-sample Curriculum Learning for Natural Language Generation}

\iffalse
\author{
First Author$^1$\footnote{Contact Author}\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$\\
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation\\
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi
\begin{document}

\maketitle

\begin{abstract}
%Natural language generation has achieved great success with autoregressive language models. The training objective is maximizing the likelihood of output given the input from beginning to end. However, predicting the whole output sequence is quite tough at the start of training iterations. Inspired by curriculum learning that training models from easy to 
Curriculum learning has shown promising improvements in multiple domains by 
training machine learning models from easy samples to hard ones. 
Previous work either designing rules or training models for calculating difficulty scores are highly relied on task-specific expertise which is difficult for generalization. Inspired by the ``easy-to-hard'' intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e. do sentence completion, and gradually extends to generate the whole output sequence.
%Unfortunately, designing sampling strategy requires significant domain expertise,
%and when it comes to natural language generation tasks that have limited training
%samples, sample-wise curriculum learning is not that effective.
%Previous work either designing rules or 
%training models for calculating difficulty scores are highly relied on task-specific 
%expertise which are difficult for generalization. 
%Inspired by the ``easy-to-difficult'' intuition, 
%Inspired by the common principles of auto-regressive language generation models, 
%we propose a simple, right-to-left, in-sample curriculum learning strategy that 
%works for any auto-regressive models. 
%Our learning strategy starts training the model to generate the last few words, and gradually extends to generate the whole output sequence.
Comprehensive experiments show that it generalizes well to different generation 
tasks and achieves significant improvements over strong baselines.
\end{abstract}
\input{intro}
\input{approach}
\input{eval}
\input{results}
\input{relatedwork}
\input{conclusion}


\bibliographystyle{named}
\bibliography{ijcai22}

\end{document}

