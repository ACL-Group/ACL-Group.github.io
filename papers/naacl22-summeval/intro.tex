\section{Introduction}
Summarization evaluation metrics that measure the quality of generated summaries are very important for the development of summarization systems
~\cite{RushCW15,ChopraAR16,NallapatiAAAI17,LiuLZ18,LiuJZ22,BART19,PEGASUS20,liu2021keyword}.
Most previous summarization evaluation metrics need human-annotated summaries as reference 
and measure summary quality through the similarity between generated summaries and their reference summaries~\cite{bleu2002,rouge,rouge2006,rouge15,bertscore,moverscore}.
Such reference-based evaluation metrics cannot accurately evaluate the summary,
because a document has many correct but different summaries.
It is difficult and expensive to write many reference summaries 
by human for evaluation. Thus, it is useful to develop 
reference-free evaluation metrics for this task. 

\begin{figure}[th]
	\centering
	\includegraphics[width=1.0\linewidth]{example1.pdf}
	\caption{A document with its high-quality and low-quality summaries. The
		heat map marks the salient content in the document.
		The darker the colour, the more salient the content.}
	\label{fig:exp}
\end{figure}

In this paper, we focus on reference-free evaluation metrics.
As shown in \figref{fig:exp}, a high-quality summary should be concise and contain the most important information of its document.
Some reference-free evaluation metrics~\cite{shao2017efficient,gao-etal-2020-supert} unsupervisedly construct a pseudo reference summary by selecting salient sentences from the source document,
which also ignore the variety of summaries.
Others evaluate the summary quality by measuring how much information
from the document is represented in the summary. 
QA-based evaluation metrics~\cite{chen2018semantic,scialom2019answers,durmus2020feqa}  achieve this possibility by first asking the same questions to document and summary and then comparing their answers. 
The performance of these metrics depends on the quality of question generation and question-answering systems.
Shannon score~\cite{shannon22}
intuitively uses a language model to autoregressively generate a document both with and without a summary as a prompt, and then computes the difference in information content between two generated documents.
The information of document generated with a better summary, 
which is better restored,
is more similar to the document generated without summary.
Although Shannon score is the state-of-the-art (SOTA) summarization evaluation metric, 
its estimation of information content of the document 
cannot reflect the position and importance of each token in document.
However, the position of tokens will impact coherence and the importance will impact salient information,
which are very important for summarization evaluation. 
For example, 
the low-quality summary contains similar words to the high-quality summary
but it is unreadable and loses important information of the source document. 

To tackle the problem in Shannon score, we present a new reference-free evaluation metric ({\bf SDC}) which computes the correlation ({\bf semantic distribution correlation}) between the probability distribution of tokens in predicted documents with and without a prepended summary. 
Such sequential probability take account of the position and importance of the tokens.
As {\bf compression ratio} reflects the difficulty of summarization,
we introduce compression ratio into SDC ({\bf SDC*}) and penalize the long summary.

Our contribution are as follows:
\begin{itemize}
\item We propose a reference-free summarization evaluation metric (SDC*) which evaluates summaries considering {\em semantic distribution correlation} and {\em compression ratio} between source document and summary.
\item Our proposed SDC and SDC* achieve better performance than the SOTA summarization evaluation metric on CNN/Daily Mail and TAC 2010 datasets.
\end{itemize}
