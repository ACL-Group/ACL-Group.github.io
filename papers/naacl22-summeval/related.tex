\section{Related Work}
Summarization evaluation metrics to measure the overall quality of summaries
can be divided into {\em reference-based} evaluation metric and 
{\em reference-free} evaluation metric.

Reference-based evaluation metrics evaluate summaries by computing their similarity
with respect to reference summaries.
% of their corresponding document.
ROUGE~\cite{rouge} is the most popular evaluation metric in summarization, 
which evaluates the token sequence overlapping.
% between generated summaries and reference summaries. 
%It also has many variants, such as ROUGE 2.0~\cite{rouge2006} and ROUGE-WE~\cite{rouge15}.
Similarly, BLEU~\cite{bleu2002} and METEOR~\cite{meteor2005} 
calculate n-gram overlap.
BLEU focuses on precision with a brevity penalty. 
METEOR allows word stems, synonyms and paraphrases matching.
%backing-off from exact unigram matching to matching word stems, synonyms, and paraphrases.
Others utilize language model to measure semantic
similarity.
%between the references and the summary.
BERTScore~\cite{bertscore} greedily 
maximize the cosine similarity between token embeddings.
%contextualized token embeddings.
%from BERT.
MoverScore~\cite{moverscore} operates
%adopts the Word Mover’s Distance~\cite{kusner2015word} operating 
over n-gram embeddings.
However, reference-based methods ignore the variaty of summary.
% pooled from BERT representations.
%These evaluation metrics are also widely used in other text generation tasks, 
%such as machine translation (Papineni et al., 2002; Zhang et al., 2020) 
%and dialogue systems (Papineni et al., 2002; Gao et al., 2021; Xiang et al., 2021). 

Reference-free evaluation metrics can
avoid the dependency on reference summaries.
%, which obtain more and more attention in recent years (Bohm et al. ¨ , 2019; Gao set al., 2020; Wu et al., 2020; Chan et al., 2021).
Some of them build the pseudo references from the source
document and compute the relevance score between the pseudo
reference and the summary~\cite{gao-etal-2020-supert,chen2021training}.
Others measures semantic similarity between documents and summaries by language models. 
%BLANC~\cite{blanc} computes the accuracy of unmasking document tokens with a summary.
Shannon score~\cite{shannon22}, as an extention of BLANC~\cite{blanc}, estimates the information content shared between a document and its summary.
Reference-free evaluation metrics avoid the cost of human-annotation reference summaries.

In this paper,  
%we focus on reference-free evaluation metrics.
%We improve Shannon score 
we improve reference-free evaluation metric by computing the semantic distribution correlation (SDC) between document and summary instead of the information content of text. 
Meanwhile, we combine SDC and compression ratio, which makes the more similar the semantic and the shorter the length of summary get higher score.

