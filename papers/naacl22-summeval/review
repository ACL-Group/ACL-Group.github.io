Metareview:
This paper describes a reference-free evaluation metric for text summarization, named SDC, that computes correlation (Pearson’s coefficient) between the semantic distributions of the source document with and without a prepended summary based on a pre-trained LM (GPT-2). A variant of SDC that accounts for the compression rate is also proposed. Experiments are conducted on two commonly-used datasets (CNN/Daily Mail and TAC 2011) against reference-based metrics (ROUGE, BLEU, Meteor) and two reference-free metrics (BLANC and Shannon, the metric on which SDP is based on). Reported results show higher correlation with human ratings than other metrics and improvement over Shannon score.

Overall, reviewers agree about the paper’s strengths: clearly written, proposes a straightforward, yet effective reference-free evaluation metric as demonstrated by reported experiments.

The main concerns are about the lack of thorough comparison (e.g. including other DUC and TAC datasets, reference-free metrics, small overall improvement without significance test) and details about the experimental setup.

Summary Of Reasons To Publish:
The paper focuses on a challenging problem, and proposes a sound metric that achieves improved results over the recently proposed Shannon score. Substance is sufficient for a short paper.

Summary Of Suggested Revisions:
Including additional experiments on DUC-’03->’07 and TAC-’08->’10 benchmark datasets would substantially strengthen the paper and make the comparison with previous work easier.

As reported improvements are small, calculating significance tests are needed to make the results more convincing.

Overall Assessment: 4 = There are minor points that may be revised
Suggested Venues:
The second workshop on Natural Language Generation, Evaluation, and Metrics (GEM) @ NAACL 2022

Official Review of Paper1592 by Reviewer Wcye
ACL ARR 2022 January Paper1592 Reviewer Wcye
16 Feb 2022ACL ARR 2022 January Paper1592 Official ReviewReaders: Program Chairs, Paper1592 Senior Area Chairs, Paper1592 Area Chairs, Paper1592 Reviewers Submitted, Paper1592 Authors
Paper Summary:
This short paper presents a new reference-free measure of automatic summaries evaluation. This score computes the correlation between probability distributions of tokens of documents generated with or without the addition of the summary at the input of an autoregressive language model. A variation of the measure takes summary length into account.

The new measure is compared to several other measures on two corpora using three correlation measures between the available scores in each corpus.

Summary Of Strengths:
Automatic summary evaluation without reference summary is an important topic which is not solved in an adequate manner up to now. This new method is simple to use and obtains better results than the other in nearly all reported results.

Summary Of Weaknesses:
Even if several other evaluation measures are compared with, other reference-free works are not taken into account: • JS-2 (Lin et al., 2006) • FRESA (Torres-Moreno et al., 2010) • SummTriver (Cabrera-Diego and Torres-Moreno, 2018) • GeSERA (López Espejel et al., 2021)

Unfortunately results from this tools seems not easily comparable with those from the current paper but it should be checked more deeply.

You do not say if your software will be released and under which license. This is a shame because a tool easy to use and efficient would be very useful for the community to replace the long criticized ROUGE.

Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2006. An information-theoretic approach to automatic evaluation of summaries. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric Sanjuan, and Patricia Velázquez-Morales. 2010. Summary evaluation with and without references. Polibits, 42:13–20. Luis Adrián Cabrera-Diego and Juan-Manuel Torres-Moreno. 2018. Summtriver: A new trivergent model to evaluate summaries automatically without human references. Data Knowl. Eng., 113:184–197. J. López Espejel, G. de Chalendar, J. Garcia Flores, T. Charnois, and I. V. Meza Ruiz, “GeSERA: General-domain Summary Evaluation by Relevance Analysis,” in Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), Held Online, Sep. 2021, pp. 856–867.

Comments, Suggestions And Typos:
None

Overall Assessment: 4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.
Confidence: 5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.
Best Paper: No
Limitations And Societal Impact:
Yes.

Ethical Concerns:
None

Needs Ethics Review: No
Reproducibility: 4 = They could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.

Official Review of Paper1592 by Reviewer HFSU
ACL ARR 2022 January Paper1592 Reviewer HFSU
16 Feb 2022ACL ARR 2022 January Paper1592 Official ReviewReaders: Program Chairs, Paper1592 Senior Area Chairs, Paper1592 Area Chairs, Paper1592 Reviewers Submitted, Paper1592 Authors
Paper Summary:
A new reference-free summarization metric which takes into account new variables, correlation and a variation on the language modeling that underlies one type of reference-free summarization, and a meta-evaluation of the new metric against reference-based metrics (ROUGE, BLEU, Meteor and BERT) and 2 reference-free metrics (BLANC and Shannon). The results mainly replicate the success of Shannon (in that there is much more correlation than reference-based metrics), but add a slight advantage over Shannon.

Summary Of Strengths:
Good points: a) strong evaluation with two good datasets (although a bit small, but it's hard getting enough data). b) I like the idea of including compression as it matters in Summarization c) results achieved, although paper-thin advantage. d) written in a logical, understandable way e) novelty is not very large, but in my opinion enough for a short paper f) experiment performed in a methodologically sound manner

Summary Of Weaknesses:
a) improvement paper-thin but no significance test performed, although such tests readily exist. b) the examples in table 2 are questionable. The authors state they intentionally created some wrong information to illustrate the advantages of their approach. However, their definition of truth does not agree with mine. Why exactly should the red material in the third example be true? It is a statement that is directly entailed by the text, so I would be thankful if I had a summarization system that could achieve this inference. c) Terminology is not used properly. Any introduction of a new evaluation metric requires a meta-evaluation, but this is mentioned nowhere in the text. Instead, the existing best evaluation is called "SoA". This is not appropriate -- systems for a particular task represent SoA, but evaluation metrics needs to be "true" in some sense of the word, i.e., approximate truth as closely as possible. Once such an evaluation metric has been found and adopted by the community, it is considered fixed and all actual summarization systems' SoA performance is measured against this.

Comments, Suggestions And Typos:
The paper is nicely written, but needs to be thoroughly proof-read for missing articles and plural/singular mistakes.

Overall Assessment: 2.5
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 5 = They could easily reproduce the results.
Datasets: 1 = No usable datasets submitted.
Software: 3 = Potentially useful: Someone might find the new software useful for their work.
Author Identity Guess: 1 = I do not have even an educated guess about author identity

Official Review of Paper1592 by Reviewer Qoc1
ACL ARR 2022 January Paper1592 Reviewer Qoc1
15 Feb 2022ACL ARR 2022 January Paper1592 Official ReviewReaders: Program Chairs, Paper1592 Senior Area Chairs, Paper1592 Area Chairs, Paper1592 Reviewers Submitted, Paper1592 Authors
Paper Summary:
This paper presents a reference-free automatic evaluation metric, semantic distribution correlation (SDC), for summarization. SDC computes Pearson’s correlation coefficient between a semantic distribution of a document P(D) and that of summary S given the document D, P(S|D) based on a pre-trained language model. To consider the compression ratio of the summary, the authors propose a normalized SDC score. The evaluation results obtained from CNN/Daily Mail dataset and TAC-2011 dataset show that the proposed metric obtained better correlation scores than conventional metrics.

Summary Of Strengths:
This paper tackles a challenging problem. High-quality reference-free evaluation metrics are crucial for automatic summarization. The proposed metric is straightforward. The experimental evaluation demonstrates the effectiveness of the method. The proposed metric obtained better correlation scores than conventional metrics. In particular, the gains against conventional metrics are remarkable on coherence.

Summary Of Weaknesses:
My main concerns are below:

The author mentioned “CNN/DailyMail dataset, each paired with 16 summaries generated by different systems” in lines 166-167. Did the author perform the experiments with only abstractive summarization systems? Why did the author exclude extractive summarization systems (seven systems)?

Furthermore, there are many publicly available evaluation results on extractive summarization systems such as DUC-2003 to 2007 and TAC-2008 to 2011. The experiments on these datasets make the paper more solid. Why did the author not perform experiments on these datasets?

Why did the author not compute correlation coefficients against pyramid scores and responsibleness scores on TAC-2011? These are well-known manual content evaluations for summarization.

Comments, Suggestions And Typos:
None

Overall Assessment: 3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Needs Ethics Review: No
Reproducibility: 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.

Official Review of Paper1592 by Reviewer pHTi
ACL ARR 2022 January Paper1592 Reviewer pHTi
14 Feb 2022ACL ARR 2022 January Paper1592 Official ReviewReaders: Program Chairs, Paper1592 Senior Area Chairs, Paper1592 Area Chairs, Paper1592 Reviewers Submitted, Paper1592 Authors
Paper Summary:
This paper introduces a new metric to evaluate summaries, building on previous work from Egan et al (2022). Egan et al proposed the use of "Shannon Score", which uses a language model to quantify the similarity between source documents and generated summaries. In this paper, the authors updated the same approach to account for the relative ordering of tokens within text. Another variant builds on this to also consider the compression ratio achieved.

The new metric was evaluated on CNN/Daily Mail, as well as TAC 2011. By looking at the correlation of the scores of multiple baseline metrics and human judgements, the authors concluded that the proposal is one of the most effective metrics for summarization evaluation.

Summary Of Strengths:
Proposal is shown to correlate better wit human judgements
Targeted improvement to Shannon score, helping to improve what is an already reasonably effective metric.
Summary Of Weaknesses:
A lot of details are left under-specified (e.g. human evaluations, values in Table 2), which makes it hard for me to be totally convinced
For easier comparisons, it could be useful to consider running the same experiments on the SummEval (Fabbri et al 2021) and DUC datasets, since this is what the Shannon score paper has used
Comments, Suggestions And Typos:
Table 1 - There is enough space, could be good to spell out the column headings in full (e.g. coh -> coherence)
Table 2 - I might have missed it, but it could be clearer what "Comp" refers to.
Table 2 - It's not clear what the numbers in the table are - pearson? or raw metric scores?
Overall Assessment: 3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.
Confidence: 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.
Best Paper: No
Limitations And Societal Impact:
No

Ethical Concerns:
No

Needs Ethics Review: No
Reproducibility: 2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.
Datasets: 1 = No usable datasets submitted.
Software: 1 = No usable software released.
Author Identity Guess: 1 = I do not have even an educated guess about author identity.
