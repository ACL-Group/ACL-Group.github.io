Format Inference of Very Large Ad Hoc Data

1. Introduction
- Ad hoc data is pervasive
- Ad hoc data is difficult to process traditionally
- PADS improves the efficiency of working with ad hoc data because:
  (a) declarative specification of data formats and some semantics
  (b) automatically generates a suites of commonly useful tools and programming
     libs
- However producing PADS desc by hand is very tedious and error-prone, 
  especially when data sources are very large
- This motivates us to "incrementally" learn data formats
- Main contributions...

2. Review of PADS & LearnPADS
- PADS language (use a running example - free_clickthrough.dat?)
  (a) base types
  (b) compound/dependent types
  (c) constraints
  (d) tool suites
- LearnPADS
  (a) multi-phase algorithm - histogram, relative entropy, MDL, rewriting
      union-clustering(?)
  (b) limitations: main memory, quadratic to determine dependency

3. Main algorithm
- high level desc of the main phases of the algo
  options of generating initial desc: 
   (a) learning a chunk
   (b) user provides a desc
- Use an example or two to run thru the algo 
- IR (note that we will make no distinction between IR and pads)
- Initial description
- Parsing into rep: sync tokens, partial success, etc 
- Aggregate structure (only store error data), opts table
- Translation of aggregate back to IR
- Rewriting rules 
  New ones: 
  (a) blob-finding
  (b) option-to-union, etc
  (c) simulated annealing(?)

4. Implementations 
- Optimization in parsing and aggregation:
  (a) the "clean" function to limit the number of parses
      setting the number k : using free_click_through example
  (b) memoization
  (c) parse cut-off
  (d) struct parsing optimizations
  (e) partly deterministic array and union semantics (similar to PADS)

- Various parameter tuning (with running example)

- Parsing regex in sml/nj is slow, the implementation is optimized for that

5. Evaluation
- Data sets (large and medium sizes)
- System specs
- Experiments:
  (a) union clustering or not?
  (b) determining best params: discussion on the effect of init/inc sizes
  (c) scaling tests
  (d) large experiments (learning time, re-parse time, pads parsing time,
      blob-parsing time, wc -l time, edit distance to gold)
  (e) show learned description and gold description for running examples

6. Related Work

7. Conclusion
  1. parsing efficiency is important
  2. scales up to large numbers. The degree of scale depends in part on
     the complexity of the desciption...

http://www.padsproj.org/incremental-learning.html
