% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{newfloat}
\usepackage{listings}
%\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{multirow}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{color, colortbl}
%\usepackage{url}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{verbatim}
%\usepackage[linesnumbered, boxed, ruled]{algorithm2e}
%
%\usepackage[noend]{algpseudocode}


\usepackage{bbding}
%\usepackage{subfigure}
%\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
%\captionsetup[sub]{compatibility=false}
%\hypersetup{
%	colorlinks=true,
%	linkcolor=blue
%}

%\newtheorem{example}{Example}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. (\ref{#1})}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\exref}[1]{Example \ref{#1}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\Roy}[1]{\textcolor{red}{Roy: #1}}
\newcommand{\crosssymbol}{{\color{red} \XSolidBrush} }
\newcommand{\checksymbol}{{\color{green} \Checkmark} }
\newcommand{\cut}[1]{}

\usepackage{makecell}

\newcommand\BibTeX{B\textsc{ib}\TeX}
\definecolor{Gray}{gray}{0.9}


\begin{document}
%
\title{Reducing Short Circuits in Multiple-Choice Natural Language Reasoning Models with Data Augmentation}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\iffalse
\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Princeton University, Princeton NJ 08544, USA \and
Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}
\fi
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Statistical biases in the training data may lead to fragility in 
neural models that makes choices in multiple-choice natural language 
reasoning problems without referring to the context or premises. 
To encourage the models to pay more attention to the relations between 
the premise and the choices, we propose two biologically inspired operations 
that can generate new training data that ``forces'' the model
to look at the premises and reducing short circuits. They can augment
any type of multiple choice reasoning dataset, and can be applied to
any supervised learning models. Results show that models trained
with the augmented data become more robust against both stress test  
and original test.
%beating the strong back-translation baseline.


\keywords{Natural language reasoning \and Data augmentation \and Robustness.}
\end{abstract}
%
%
\input{intro}
\input{method}
\input{experiments}
\input{related}
\input{discussion}
\input{conclude}

\bibliographystyle{splncs04}
\bibliography{pkdd23}
\end{document}
