\section{Preliminary}
\label{sec:formulation}
\subsection{Task Definition}
%\textcolor{red}{Hongru: these are concrete examples, which i think should be given after the problem
%formulation} 
We define an instance $x$ of an NLU task 
dataset $X$ as
\begin{equation}
    x = (p, h, l) \in X, \label{eq:nli}
\end{equation}
\noindent
where $p$ is the context against which to do the reasoning ($p$ corresponds 
to ``premise'' in~\exref{exp:snli});
$h$ is the hypothesis given the context $p$; 
$l \in \mathcal{L}$ is the label that 
depicts the type of relation between $p$ and $h$. 
The size of the relation set $\mathcal{L}$ varies with tasks. 
%We argue that 
%most of the discriminative NLR tasks can be formulated into this general form. 
%which will bring us much convenience to evaluate the cues of a dataset. 
%For example, an NLI question consists of a \textit{premise}, a \textit{hypothesis} 
%and a \textit{label} on the relation between premise and hypothesis. 
%The formulation of this task will be $p =$ \textit{premise}, $q =$ \textit{hypothesis} and $l =$ \textit{label}. 
%$|\mathcal{L}| = 3$ for three different relations: 
%\textit{entailment}, \textit{contradiction} and \textit{neutral}. 
%We will talk about how to transform into this form in \secref{sec:dynamic}. 
%The ROCStory~\cite{mostafazadeh2016corpus} dataset
%such as in~\exref{exp:roc}, 
%consists of a \textbf{context} and two possible story \textbf{endings}. 

%We will formulate this task by setting $p = $ \textit{context}, $h = $ \textit{ending1/ending2}. In this case, $|\mathcal{L}| = 2$. because $l$ is ``true'' or ``false'' indicating whether 
%the ending is a plausible ending of the story.


%\subsection{Transformation of MCQs with dynamic choices}
%\label{sec:dynamic}
%So far the multiple-choice questions we targeted such as NLI problems
%are actually classification problems with fixed set of choices. 
There is another type of natural language reasoning tasks which 
are also in the form of multiple-choice questions, 
but their choices are a fixed set of labels, as shown below. 

\begin{center}
    \fbox{
        \begin{minipage}{\textwidth}
\begin{example}\label{exp:roc}
A story in ROCStory dataset, with ground truth bolded~\cite{mostafazadeh2016corpus}.
\begin{description}
\item{Context:} Rick grew up in a troubled household. 
He never found good support in family, and turned to gangs.           
It was n't long before Rick got shot in a robbery.             
The incident caused him to turn a new leaf.
\item{Ending 1:} He joined a gang. 
\item{Ending 2:}  \textbf{He is happy now.}
\end{description}
\end{example}
\end{minipage}
    }
\end{center}
We can transform the this case into two separate 
problem instances, still
in the same form as in \eqnref{eq:nli}, 
$u_1=(context, ending1, false)$ and $u_2=(context, ending2, true)$, where $L = {true, false}$.

\subsection{Linguistic Features}
\label{sec:extract}

As demonstrated in previous work~\cite{naik2018stress,checklist2020acl}, 
we consider the following linguistic features: 
%Word (unigram tokens in the input sentences), 
%Typos, NER (named entity recognition), 
%Tense (temporal order of events), Negation, 
%Sentiment, and Overlap (words occurring both in the premise and 
%the hypothesis). 
%The above list is by no means exhaustive, but just a starting point for users 
%who can come up with additional features that are relevant
%to their task or domain. 

\subsubsection{Word} 
For a dataset $X$, we collect a set of all words 
$V$ that ever exist in $X$. 
A word feature is defined as the existence of a word $w \in V$
either in the premise or the hypothesis. 
%The cross-unigrams, such as  ``swimmer-sea'' in~\exref{exp:snli}, 
%represent the 
%relational unigrams in a dataset. 
%The ``swimmer-sea'' cross-unigram can be identified as a cue if it always appear in the instances with 
%one label, like entailment.
Because $V$ is generally very large, in practice, we may narrow it down
to words that are sufficiently popular in $X$. That is, we may remove
words that seldom appear in $X$.
%the feature set to a smaller subset of words in $V$. Intuitively, 
%we want to pick those words that are strongly biased in
%the dataset, which means their occurrence is strongly
%correlated with some fixed label. There are a few choices for
%computing this bias. In this work, we choose to use the probabily
%of seeing word $w$ conditioned on label $l_i$:
%\begin{equation}
%   cp_{w,l} = p(w | l) = \frac{\#(w, l)}{\#(w)}
%\end{equation}
%where $\#(x)$ denotes the number of 
%instances in the dataset that contain $x$. \KZ{$w$ only in $h$ also?}
%
%The bias in a word is then computed as the mean square error over 
%$cp_{w,l}$:
%\begin{equation}
%bias(w) = \frac{1}{L} \sum_{l \in \mathcal{L}} (cp_{w,l} - \overline{cp_w})^2
%\end{equation}
%where $\overline{cp_w}$ is the average of $cp_{w,l}$ over all $l$ in $\mathcal{L}$.
%Words in $V$ with the top $bias(w)$ scores will be used as 
%word features.

%of $w$ with respect to label $l$ as
%%\KZ{Consider changing $\mathcal{F}$ to $\mathcal{B}$ to avoid confusion with $f$?}
%
%\begin{equation}
%    f_{\mathcal{F}}^{l} = f_{\mathcal{F}}(w, l),    
%\end{equation}
%%We call $f_{\mathcal{F}}^{(k,l)}$ \textit{cue metric}, 
%where $f_{\mathcal{F}}^{l}$ is a function which measures how much spurious information can be conveyed by token $w_k$ for a particular label $l$. 
%$\mathcal{F}$ is a set of cue metrics that we used for computing the \textit{cue score}. 

\subsubsection{Sentiment}

For each data instance $x$, we can compute its sentiment value as:
\begin{equation}
S(x) = \sgn(\sum_{w \in x} polar(w),
\end{equation}
where $polar(w)$ is the sentiment polarity (-1, 0, or 1)
of $w$ determined by a look-up from a pretrained sentiment 
lexicon~\footnote{NLTK: \url{https://www.nltk.org}}.
We say $x$ has a positive/negative/neutral sentiment feature if $S(x)$ = 1, -1 or 0,
respectively.

\subsubsection{Tense}
We say that an instance $x$ has  
\textit{past}, \textit{present} or \textit{future} tense feature if $x$
carries one of these tenses, respectively, by the POS tag of the root verb
in $p$ or $h$. 

\subsubsection{Negation}
Previous work has observed that negative words (``no'', ``not'' or ``never'') 
may be indicative of a certain label in NLI tasks for some models.
The existence of a negation feature in $x$ is decided by dependency 
parsing~\footnote{Scipy: \url{https://spacy.io}}. 

\subsubsection{Overlap}
In many models, substantial word-overlap between the premise and the
hypothesis sentences causes incorrect inference, 
even if they are unrelated~\cite{mccoy2019right}. 
%Very little word overlap causes a prediction
%of neutral instead of entailment.
We define that an overlap feature exists in $x$ if there's at least one word
(except for stop words) that occurs both in $p$ and $h$. 

\subsubsection{NER}
We define the NER feature as the existence of either PER,
ORG, LOC, TIME, CARDINAL entity in $x$.
We use the NLTK ner toolkit for this purpose. 
%For example anchoring on named entities too strongly
%instead of understanding named entities and their
%impact on whether questions are duplicates. 

\subsubsection{Typos}
We say an instance $x$ has typo feature if there exists at least one
typo in $x$.
We use a pretrained spelling model~\footnote{\url{https://github.com/barrust/pyspellchecker}} 
to detect all typos in a sentence. We don't distinguish the types of misspellings here. 
%We only pay attention to the features which have enough test data for model evaluation.
%In addition, we can analysis the credibility of test data through 
%the Kullback-Leibler (KL) Divergence. If a train dataset distribution is unbalance, 
%a similar distribution between train and T
%test can lead to insufficient test which test the cues mostly with a certain label. 

In Example \ref{exp:roc}, we noted that multiple-choice 
questions are split into two instances with opposite 
labels (T or F) and identical premises. Thus, 
detecting features within premises alone is unproductive. 
For MCQ datasets, all features except Overlap are applied exclusively to hypotheses.


