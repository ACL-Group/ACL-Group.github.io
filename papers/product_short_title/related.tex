\section{Related Work}

% document summarization 
The task of summarization aims at generating short summaries for 
long documents or sentences. 
This research problem has drove lots of attentions in recent years and exsiting methods can be categorized mainly into two perspectives. 

% extractive 
\textit{Extractive summarization} methods \cite{erkan2004lexpagerank,mcdonald2007study,wong2008extractive} produce summaries by concatenating several sentences or words found directly in the original texts. 
Several methods have been used to select the summary-worthy sentences, 
including binary classifiers \cite{kupiec1995trainable}, Markov models \cite{conroy2001text}, graphic model \cite{erkan2004lexpagerank,mihalcea2005language} and integer linear programming (ILP) \cite{woodsend2010automatic}. 
Compared to traditional methods which heavily rely on human-engineered 
features, neural network based approaches \cite{kaageback2014extractive,cheng2016neural,nallapati2017summarunner,narayan2017neural} have gain 
popularity rapidly. The general idea of these methods is to regard the 
extractive summary as a sequence classification problem
and adopt RNN like networks. 
Besides, attention-based framework can perform better by attending to the whole document when extracting a word (or sentence) \cite{cheng2016neural}.
Our work is based on extractive framework as well. Besides a deep attentional RNN network, 
we also employ explicit semantic features such as TF-IDF score and NER tags, 
making our model more informative.

% abstractive
On the other hand, \textit{abstractive summarization} methods \cite{chen2016distraction,nallapati2016abstractive,see2017get}, 
which have the ability to generate text beyond the original input text, 
in most cases, can produce more coherent and concise summaries. 
These approaches are mostly centered on the attention mechanism and 
augmented with recurrent decoders\cite{chopra2016abstractive}, 
Abstract Meaning Representations \cite{takase2016neural}, 
hierarchical networks \cite{nallapati2016abstractive} 
and pointer network \cite{see2017get}. 
However, It is not necessary to use abstractive methods in 
short title extraction problem in this paper as the input is product title, 
which already contains the required informative words. 
%Thus, there is no need to generate words from scratch.

% summarization for short text
Our work can be comfortably set in the area 
of \textit{short text summarization}. 
This line of research is in fact essentially \textit{sentence compression}
working on short text inputs such as tweets, microblogs or single sentence. 
Recent advances typically contribute to improving seq-to-seq learning, 
or attentional RNN encoder-decoder structures \cite{chopra2016abstractive,nallapati2016abstractive}.
While these methods are mostly abstractive, 
we use an extractive framework,
combining with deep attentional RNN network and explicit semantic features, due to different scenarios of summarization problem.
Besides, while existing summarization systems focus on 
news, article or other ducuments, 
we are the first to do summarization in a mobile E-commerce setting.
 
%There is usually a predefined length constraint in sentence simplification tasks. 
%As it is difficult to pose hard constraints on decoder generation, 
%one recent work \cite{kikuchi2016controlling} studies various solutions, 
%including direct truncation on generated sequence, 
%discarding out-of-range generations in the decoding beam, 
%and directly embedding length information in the LSTM units. 
%Comparing to this work, we instead use a char-limit constrained inference algorithm with the idea of dynamic programming after the model prediction.
