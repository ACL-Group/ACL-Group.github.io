\section{Experiments and Results}
In this section, we show the experiments that we conducted 
to analyze the performance of our proposed approach. Due to 
the lack of reference set to evaluate the system, we created 
our own reference set that contains 12 user queries together 
with their relevant results. The relevant results were taken 
from the query log, with each of the result is given a score, 
indicating the degree of their relevancy to the user query. 
The performance of the system are then evaluated based on this 
score.

\subsection{Query Log}
The query log that we used were taken from 10\% of a week of 
Bing's search log. It contains around 14 million queries. The 
query log contains additional information of frequency and the 
number of click-through. We utilized this information to measure 
the quality of the query, since some query may be a wrong typed 
query, with low frequency number or a query that returns bad 
result, hence having low number of click-through.

\subsection{Reference Set}
The reference set was built by selecting 12 user queries as shown 
in Table \ref{tab:referenceset}. We selected the user queries 
randomly by querying into the system and took the user query that 
have at least one relevant query returned in their top ten results. 
The user queries that we selected contain at least one concept and 
maximum of two concepts. We select two as maximum because we consider 
query having more than two concepts are not common or not intuitively 
made, and is hardly composed by daily users.

\begin{table}[h]
\centering
\caption{Reference Set}
\begin{tabular}{ | p{5.3 cm} | c | }
\hline
\centering\textbf{User Query} & \textbf{\#Queries}\\ \hline
\textit{actresses got cancers} & 4 \\ \hline
\textit{cancers and cancer treatments} & 54 \\ \hline
\textit{cancers treatment}  & 58 \\ \hline
\textit{cannibalism in countries} & 2 \\ \hline
\textit{celebrities got cancers} & 13 \\ \hline
\textit{celebrities on vogue cover} & 11 \\ \hline
\textit{diseases in domesticated animals} & 81 \\ \hline
\textit{greek philosophers and their theories} & 16 \\ \hline
\textit{hurricanes in states} & 2 \\ \hline
\textit{natural disasters japan 2011} & 14 \\ \hline
\textit{storms hit states} & 29 \\ \hline
\textit{video games banned} & 1 \\ \hline
\end{tabular}
\centering
\label{tab:referenceset}
\end{table}

The list of relevant queries for each user query were created 
by first collecting queries from query log that contain instances 
of the concepts found in user query. The queries are then sorted 
by their modified word distance to the user query. We filtered out 
queries that has distance greater than 15, based on the observation 
that the queries having distance greater than 15 are mostly not
 relevant and contain too many additional information, compared 
 with the original user query. The number of relevant queries for 
 each user query is also shown in Tabel \ref{tab:referenceset}.

For each of the query in the reference set, we give a score indicating 
the degree of relevancy to their respected user query, ranging from 0-5. 
The query is scored by 2 human judges independently and the final 
score for the query is taken as an average. In scoring the query, 
some guidelines that we follow are that whether the query contains 
correct or proper instances of user query, and whether the query as 
a whole reflects the true intention of original user query in a more 
specific form.

\subsection{Evaluations}



\subsection{Discussions}
\textit{some result discussion}

\textit{Dynamic Increase discussion}
