\section{Evaluation Plan}
\label{sec:eval}

\subsection{Data collection}
Since we haven't deployed our system on TV broadcast network, we can't get
real time feedbacks of users. So we will use an offline method to test our system.
We download three weeks' TV schedule of 10 channels from web and ask 12 users to finish a
survey. Users go through TV schedules of three weeks
and pick up programs they are interested in or they have watched. They rank
these programs from 1 to 5, where 1 means that they dislike the program
and 5 means that they like it very much.
%\subsection{Data filter}
%Survey results need to be filtered to suit the real fact better. Some users
%select more than one program during the same time interval. For this, we delete
%the programs that rank lower in score for the specific user since a user can't
%watch more than one diagram at the same time.
We will divide the survey results into two parts, one for our system to learn users'
behaviours and one to test our recommendation results. Using the first part,
our system can generate a user model, which is used to recommend programs for
that user. Then we compare the second part with our recommendation result to
evaluate the accuracy of our system.

\subsection{Baseline systems}
We will develop several baseline systems for comparison purposes:
\begin{itemize}
\item \textbf{Random recommendation} This system provides completely random recommendations.
\item \textbf{Repeated recommendation} This system recommends programs/channels that a user has viewed in the recent past.
\item \textbf{Category recommendation} This system uses very coarse-grained categories to first classify the TV programs into different types, e.g. movies, drama series, talk shows,
etc., and then recommend types of programs that a user has recently viewed.
\item \textbf{Collaborative filtering} This system improves the above category recommendation by looking at the viewing history of each user, and if user A and user B watched same
``type'' of programs in the past, they are considered ``similar'' users and
the system recommends similar programs to them.
\end{itemize}

\subsection{Metrics}
Under real circumstances, TV recommender systems aim to attract users to watch
more TV programs. Since the survey is not real time, we can't influence the user.
So we will compare the recommended results with the programs actually selected by user
and calculate the difference. The smaller the difference, the better the recommendation
system works.

Besides, we can compute the timing and complexity of our system.
\begin{itemize}
\item Throughput and response times of the communication module
\item Efficiency of the offline extraction module
\item Timing of the online recommendation module
\item System scalability
\end{itemize}
%And during the survey, some users always select quite few programs of
%specified channels from the first day of the week to the last day of the week.
%These kind of user is less useful to the evaluation of recommendation systems because
%they vary too little in the programs watched.
%So we need to get the weight that
%different users contribute to the evaluation by calculating the variation of all
%the diagrams every user select as follows:
%
%Define {\em precision} and {\em recall} here. Or shall we use {\em F-measure}?
%
%\subsubsection*{Accuracy of the IE module}
%
%\subsubsection*{End-to-end accuracy}
%Compare the accuracy of our system with the baseline systems.
%\subsection{Timing/Complexity}
%
%\begin{itemize}
%\item Throughput and response times of the communication module
%\item Efficiency of the offline extraction module
%\item Timing of the online recommendation module
%\item System scalability
%\end{itemize}



