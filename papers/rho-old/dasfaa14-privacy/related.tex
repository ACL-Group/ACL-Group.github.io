\section{Related Work}
\label{sec:related}
%{\bf KZ: please shorten the citatation labels in the bib. Just include the Author + year + brief word to indicate the paper. Also note that don't over cite a paper, e.g. Cao's paper has been cited too many times in this section. Also
%I think you might need to discuss the $k^m$-anonymity a bit more since they are
%also set-valued models?}

Privacy-preserving data publishing of relational tables has been well
studied in the past decade since the original proposal of $k$-anonymity by
Sweeney \etal \cite{Sweeney2002:k-anonymity}.
%Since then, many other privacy models and anonymization
%techniques \cite{Kifer:l-diversity,Bayardo:optimal-k-anonymity,Fung:2005:TopdownSpe,LeFevre:2005:Incognito,LeFevre:2006:Mondrian,Wong:2010:NGP} for relational data were proposed.
%
Recently, privacy protection of set-valued data has received increasing
interest. The original set-valued data privacy problem was defined in the
context of association rule hiding 
\cite{atallah99:disclosure,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding}, 
in which the data publisher
wishes to ``sanitize'' the set-valued data (or {\em micro-data}) so that all
sensitive or ``bad'' associate rules cannot be discovered while all (or most)
``good'' rules remain in the published data.
%One of the first attempts at set-valued data anonymization was by Ghinita
%\etal~ \cite{GhinitaTK:2008:sparse}.
%In that work, data publishers only publish the original non-sensitive items
%along with a summary of the frequencies of sensitive items, similar to
%Anatomy \cite{Xiao:2006:ASE}. They also assume that attackers can only
%acquire background knowledge about non-sensitive items.
Subsequently, a number of privacy models
including $(h,k,p)$-coherence \cite{Xu:2008:ATD},
$k^m$-anonymity \cite{Terrovitis:2008:PAS},
$k$-anonymity \cite{He:2009:ASD} and
$\rho$-uncertainty \cite{Cao:2010:rho} have been proposed.
$k^m$-anonymity and $k$-anonymity are carried over directly from
relational data privacy,
while $(h,k,p)$-coherence and $\rho$-uncertainty protect the
privacy by bounding the confidence and the support of
any sensitive association rule inferrable from the data. This is
also the privacy model this paper adopts.

A number of anonymization techniques
were developed for these models. 
These generally fall in four categories: {\em
global/local generalization} 
\cite{Terrovitis:2008:PAS,He:2009:ASD,Cao:2010:rho}, {\em global suppression} \cite{Xu:2008:ATD,Cao:2010:rho},
{\em permutation} \cite{2011:TKDE:Anonymous} and {\em perturbation}
\cite{Zhang:2007:agg,ChenMFDX11:Diff}. Next we briefly discuss the pros and
cons of these anonymization techniques.

Generalization replaces a specific value by
a generalized value, e.g., ``milk'' by ``dairy product'',
according to a generalization hierarchy \cite{FungWCY10:Survey}.
While generalization preserves the correctness of the data,
it compromises accuracy
and preciseness. Worse still, association rule mining is impossible
unless the data users have access to the same generalization taxonomy
and they agree to the target level of generalization. For instance, if
the users don't intend to mine rules involving ``dairy products'', then
all generalizations to ``dairy products'' are useless.

Global suppression is a technique that deletes all items of some types
so that the resulting dataset is safe. 
The advantage is that it preserves the support of
existing rules that don't involve deleted items and hence retains these rules
\cite{Xu:2008:ATD}, and also it doesn't introduce
additional/spurious association rules.
The obvious disadvantage is that it can cause unnecessary
information loss. In the past, partial suppression
has not been attempted mainly due to its perceived side effects of 
changing the support of inference rules in the original data 
\cite{Xu:2008:ATD,Cao:2010:rho,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding}. 
But our work shows that partial suppression introduces limited
amount of new rules while preserving many more original ones than
global suppression. Furthermore,
it preserves the data distribution much better than
other competing methods.

%Partial suppression draws great controversy in
% mainly for its side-effects in rule mining tasks, as its inconsistent change in itemset's support.
%There is no doubt on the importance to keep the consistency of residual itemsets' supports. But it has a strong prediction the downside application is rule mining. What if global suppression kept so few rules while $partial$ offers more rules to find good ones, which is a better idea?

Permutation was introduced by Xiao \etal \cite{Xiao:2006:Anatomy} for
relational data and was extended by
%. With generalization technique severely compromising the
%accuracy of data aggregation analysis, Xiao \etal propose the
%\textit{Anatomy} which releases quasi-identifier and sensitive values in two
%separate tables. Specifically quasi-identifier values are not changed and
%organized into groups, and for every such group the corresponding sensitive
%values are aggregated. After that, 
Ghinita \etal \cite{2011:TKDE:Anonymous}
for transactional data.
Ghinita \etal propose two novel anonymization techniques for sparse
high-dimensional data by introducing two representations for transactional
data. However the limitation is that the quasi-identifier is restricted to
contain only {\em non-sensitive items}, which means they only
consider associations between quasi-identifier
and sensitive items, and not {\em among} sensitive items.
Manolis \etal \cite{terrovitis:privacy} introduced ``disassociation''
which also severs the links between values attributed to the 
same entity but does not
set a clear distinction between sensitive and non-sensitive attributes.
%They set those frequent itemsets into a cluster and partition the table into
%several parts, which eliminates the rules with
%a high confidence and a certain support.
In this paper, we consider all kinds of associations and try best to
retain them.
%While in our paper, we consider all kinds of associations and try best to
%retain the accuracy of those associations
%
%\PC{Moreover, the model introduced in their method is not strong. The attack
%effects in the following steps. Step 1:The attacker gains some background
%knowledge such that the number of people buying creams and pregnancy tests
%are around five times more than people buying butter and pregnancy tests. .
%Step 2: The attacker downloads the result processed by permutation and one of
%the group in the result has such form that only contains people buying cream
%or butter with probability of sensitive item pregnancy test $\frac{1}{3}$.
%Step 3: with a simple equation $\frac{1}{3}(x+y)=5Px+Py$, where x represents
%people buying butter and y represents people buying cream, the attacker can
%get the exact probability of people who buy cream buy the pregnancy
%test($5P$) which is likely to be a high value with different x and y.
% }

Perturbation is developed for statistical disclosure control
\cite{FungWCY10:Survey}. Common perturbation methods include {\em additive
noise}, {\em data swapping}, and {\em synthetic data generation}. Their
common criticism is that they damage the data integrity by adding noises and
spurious values, which makes the results of downstream analysis unreliable.
Perturbation, however, is useful in non-deterministic privacy model such as
differential privacy \cite{Dwork08:diff:survey}, as
attempted by Chen \etal~ \cite{ChenMFDX11:Diff} in a probabilistic top-down
partitioning algorithm based on a context-free taxonomy.
 %Interestingly, the
%algorithm proposed in this paper is probabilistic in nature as well.
%Considering the fact that the noise introduced by randomization leads to
%severe data utility, some work related with differential privacy focuses on
%releasing certain data mining results
%\cite{Barak:2007:PAC:1265530.1265569,Bhaskar:2010:DFP:1835804.1835869,Friedman:2010:DMD:1835804.1835868,Korolova:2009:RSQ:1526709.1526733}.
%However, the usability of the published data is constrained by the pattern
%the owner decide to release and moreover the assumption that the data owner
%is able to perform data mining tasks is also weak. In addition, Leoni \etal
%\cite{DBLP:journals/corr/abs-1205-2726} also indicates the weakness of
%differential privacy model itself.
%
%\textbf{
%Recently, a new method called slicing was firstly proposed in \cite{10.1109/TKDE.2010.236} and was
%further developed in \cite{terrovitis:privacy}. Slicing, also called disassociation, aims to protect
%identity or attribute disclosure using identify combinations. However, the privacy model they introduce is
%different from ours and the type of targeted
%data utility is also completely different from ours.
%(I am in a dilemma. Since their methods are totally different
%from ours, I can't make comparison with ours.
%Their methods are somewhat useful in association rule mining and distribution remaining.
%The only disadvantage is that they changed the original structure of the table, but
%according to their data utility such change is acceptable.  )}

The most relevant work to this paper is by Xu \etal \cite{Xu:2008:ATD} and
Cao \etal \cite{Cao:2010:rho}. The $(h,k,p)$-coherence model by Xu \etal~
requires that the attacker's prior knowledge to be no more than $p$ public
(non-sensitive) items, and any inferrable rule must be supported by at least
$k$ records while the confidence of such rules is at most $h$\%. They believe
private items are essential for research and therefore only remove public
items to satisfy the privacy model. They developed an efficient greedy
algorithm using global suppression. In this paper, we do not restrict the
size or the type of the background knowledge, and we use a partial
suppression technique to achieve less information loss and also better retain
the original data distribution.
%\KZ{One notable error made by Xu \etal \cite{Xu:2008:ATD} is in
%their proof of NP-hardness of the problem. They incorrectly
%claim to have reduced the vertex cover problem to the
%global suppression problem, when their definition of vertex cover problem
%is not minimizing the vertex but finding {\em any} vertex cover.}
%\XH{
%Assume their vertex cover is minimum vertex cover problem, their proof is correct since in their proof they announced that $IL(e)=1$, but it didn't mean the hardness of this problem when $IL(e)=support(e)$.
%In their information loss section "In
%particular, IL(e)=1 charges one unit of information loss for the
%item e suppressed, and IL(e)=Sup(e) charges one unit of
%information loss for each occurrence of the item e suppressed."
%As they say, $IL(e)$ is a customized penalty, so the proof only works when $IL(e)=1$.
%}
%
%\XJ{I think the error is that they are trying to minimize the overall information loss
%on item occurrences but for vertex cover problem it should consider
%information loss on item types.}

%This model restrict the amount of knowledge $p$ that attacker may acquire,
%and the prior knowledge can only contain public items.
%The suppression on public items is also necessary to negotiate in nature.
%Yabo Xu et al. \cite{Xu:2008:ATD} measure the information loss

%created by their algorithm by amount of items suppressed.
%On the contrary, in our paper, to limit the inconsistent change of itemset support and the information loss, we prefer to perform partial suppression on only sensitive items.

Cao \etal \cite{Cao:2010:rho} proposed a similar $\rho$-uncertainty model
which is used in this paper.
They developed a global suppression method and a top-down
generalization-driven global suppression method (known as TDControl)
to eliminate all sensitive inferences with confidence above
a threshold $\rho$.
Their methods suffer from same woes discussed earlier for generalization and
global suppression.
Furthermore, TDControl 
assumes that data exhibits some monotonic property under a generalization
hierarchy. This assumption is questionable.
Experiments show that our algorithm significantly outperforms the 
two methods in preserving data distribution and useful
inference rules, and in minimizing information losses.
