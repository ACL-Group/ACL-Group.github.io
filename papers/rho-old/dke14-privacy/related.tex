\section{Related Work}
\label{sec:related}

\subsection{Privacy Models}

Privacy-preserving data publishing of relational tables has been well
studied in the past decade since the original proposal of $k$-anonymity by
Sweeney \etal \cite{Sweeney2002:k-anonymity}.
Recently, privacy protection of set-valued data has received increasing
interest. The original set-valued data privacy problem was defined in the
context of association rule hiding 
\cite{atallah99:disclosure,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding}, 
in which the data publisher
wishes to ``sanitize'' the set-valued data (or {\em micro-data}) so that all
sensitive or ``bad'' associate rules cannot be discovered while all (or most)
``good'' rules remain in the published data.
Subsequently, a number of privacy models
including $(h,k,p)$-coherence \cite{Xu:2008:ATD},
$k^m$-anonymity \cite{Terrovitis:2008:PAS},
$k$-anonymity \cite{He:2009:ASD} and
$\rho$-uncertainty \cite{Cao:2010:rho} have been proposed.
$k^m$-anonymity and $k$-anonymity are carried over directly from
relational data privacy,
while $(h,k,p)$-coherence and $\rho$-uncertainty protect the
privacy by bounding the confidence and the support of
any sensitive association rule inferrable from the data. This is
also the privacy model this paper adopts.

\subsubsection{The $k$-anonymity Model}

Many datasets are published simply with key identifiers (e.g. name and
social-security number) removed so the records are not related to specific
people.  However, some pseudo-identifiers (e.g. age and zip-code) can be
combined to narrow down to or even identify a small number of individuals.
In order to prevent identification, the $k$-anonymity model requires that
every such combination in the dataset occur at least $k$ times so that
every record is indistinguishable from at least $k-1$ other records.

\subsubsection{The $l$-diversity Model}

Kifer \etal \cite{Kifer:l-diversity} showed using two simple attacks that a
$k$-anonymized dataset has some subtle but severe privacy problems, and
proposed a novel and powerful privacy criterion called $l$-diversity that
can defend against such attacks.

While $k$-anonymity is effective in preventing identification of a record,
$l$-diversity focuses on maintaining the diversity of the sensitive attributes \cite{aggarwal2008general}.
Therefore, the $l$-diversity model is defined as follows:

\begin{definition}
  Let a $q^*$-block be a set of tuples such that its non-sensitive values
  generalize to $q^*$.  A $q^*$-block is $l$-diverse if it contains $l$
  ``well-represented'' values for the sensitive attribute $S$.  A table
  is $l$-diverse, if every $q^*$-block in it is $l$-diverse.
\end{definition}

A number of different instantiations for this definition are discussed
in \cite{Kifer:l-diversity}, where ``well-representated'' is assigned with
different meanings.

\subsubsection{The $(h,k,p)$-coherence Model}

The $(h,k,p)$-coherence model by Xu \etal \cite{Xu:2008:ATD}
requires that the attacker's prior knowledge to be no more than $p$ public
(non-sensitive) items, and any inferrable rule must be supported by at least
$k$ records while the confidence of such rules is at most $h$\%. They believe
private items are essential for research and therefore only remove public
items to satisfy the privacy model. They developed an efficient greedy
algorithm using global suppression. In this paper, we do not restrict the
size or the type of the background knowledge, and we use a partial
suppression technique to achieve less information loss and also better retain
the original data distribution.

\subsubsection{The $\rho$-uncertainty Model}

Cao \etal \cite{Cao:2010:rho} proposed a similar $\rho$-uncertainty model
which is used in this paper.
They developed a global suppression method and a top-down
generalization-driven global suppression method (known as TDControl)
to eliminate all sensitive inferences with confidence above
a threshold $\rho$.
Their methods suffer from same woes discussed earlier for generalization and
global suppression.
Furthermore, TDControl 
assumes that data exhibits some monotonic property under a generalization
hierarchy. This assumption is questionable.
Experiments show that our algorithm significantly outperforms the 
two methods in preserving data distribution and useful
inference rules, and in minimizing information losses.

\subsubsection{Differential Privacy}

Differential privacy \cite{Dwork08:diff:survey} aims to maximize query 
accuracy while minimizing the chances of breaching privacy, 
which is more general than $\rho$-uncertainty as well as many other
previously proposed privacy models, and can thus handle different attacks.  
However, differential privacy is mostly used
interactively such that the users do not have exclusive access to
all of the data set, but have to query the data as a blackbox through some
predefined interfaces, only to gain some statistics of the data.  
Such interactive use is less often efficient and harder to
deploy in real world than non-interactive use, 
where the users can load the whole data set into
memory and do arbitrary computations on the data.  
The model of differential privacy requires that the existence of individual items
should not affect the statistics of the whole data set, and does not make a
distinction between sensitive items and non-sensitive items.  Because of this,
anonymization techniques for differential privacy will affect distribution of
non-sensitive items and add noise data that leads to spurious rules
in order to meet its privacy guarantees.

\subsection{Anonymization Techniques}

A number of anonymization techniques
were developed for these models. 
These generally fall in four categories: {\em
global/local generalization} 
\cite{Terrovitis:2008:PAS,He:2009:ASD,Cao:2010:rho}, {\em global suppression} \cite{Xu:2008:ATD,Cao:2010:rho},
{\em permutation} \cite{2011:TKDE:Anonymous} and {\em perturbation}
\cite{Zhang:2007:agg,ChenMFDX11:Diff}. Next we briefly discuss the pros and
cons of these anonymization techniques.

\subsubsection{Generalization}

Generalization replaces a specific value by
a generalized value, e.g., ``beer'' by ``drink'',
according to a generalization hierarchy \cite{FungWCY10:Survey}.
Table \ref{tab:samesample} illustrates generalization by reusing the
same dataset in Table \ref{tab:sample}, where both ``beer'' and ``coffee''
are generalized to ``drink'', according to some generalization hierarchy.
While generalization preserves the correctness of the data,
it compromises accuracy
and preciseness. Worse still, association rule mining is impossible
unless the data users have access to the same generalization taxonomy
and they agree to the target level of generalization. For instance, if
the users don't intend to mine rules involving ``drink'', then
all generalizations to ``drink'' are useless.

\begin{table*}[tb]
\caption{The Same Dataset in Table \ref{tab:sample} and Generalization Anonymization Result}
\label{tab:samesample}
\small
\centering
\subtable[Original Dataset]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, {\bf beer}, {\em condom} \\ \hline
2 & {\bf coffee}, fruits  \\ \hline
3 & {\bf beer}, {\em condom}  \\ \hline
4 & {\bf coffee}, fruits  \\ \hline
5 & flour, {\em condom}\\ \hline
6 & bread, {\bf coffee}  \\ \hline
7 & fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:orig-sample-same}
}
\subtable[Generalization Result]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, {\bf drink}, {\em condom} \\ \hline
2 & {\bf drink}, fruits  \\ \hline
3 & {\bf drink}, {\em condom}  \\ \hline
4 & {\bf drink}, fruits  \\ \hline
5 & flour, {\em condom}\\ \hline
6 & bread, {\bf drink}  \\ \hline
7 & fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sample-generalization}
}
\end{table*}

\subsubsection{Global Suppression}

Global suppression is a technique that deletes all items of some types
so that the resulting dataset is safe. 
The advantage is that it preserves the support of
existing rules that don't involve deleted items and hence retains these rules
\cite{Xu:2008:ATD}, and also it doesn't introduce
additional/spurious association rules.
The obvious disadvantage is that it can cause unnecessary
information loss. In the past, partial suppression
has not been attempted mainly due to its perceived side effects of 
changing the support of inference rules in the original data 
\cite{Xu:2008:ATD,Cao:2010:rho,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding}. 
But our work shows that partial suppression introduces limited
amount of new rules while preserving many more original ones than
global suppression. Furthermore,
it preserves the data distribution much better than
other competing methods.

\subsubsection{Permutation}

Permutation was introduced by Xiao \etal \cite{Xiao:2006:Anatomy} for
relational data and was extended by
%. With generalization technique severely compromising the
%accuracy of data aggregation analysis, Xiao \etal propose the
%\textit{Anatomy} which releases quasi-identifier and sensitive values in two
%separate tables. Specifically quasi-identifier values are not changed and
%organized into groups, and for every such group the corresponding sensitive
%values are aggregated. After that, 
Ghinita \etal \cite{2011:TKDE:Anonymous}
for transactional data.
Ghinita \etal propose two novel anonymization techniques for sparse
high-dimensional data by introducing two representations for transactional
data. However the limitation is that the quasi-identifier is restricted to
contain only {\em non-sensitive items}, which means they only
consider associations between quasi-identifier
and sensitive items, and not {\em among} sensitive items.
Manolis \etal \cite{terrovitis:privacy} introduced ``disassociation''
which also severs the links between values attributed to the 
same entity but does not
set a clear distinction between sensitive and non-sensitive attributes.
%They set those frequent itemsets into a cluster and partition the table into
%several parts, which eliminates the rules with
%a high confidence and a certain support.
In this paper, we consider all kinds of associations and try best to
retain them.
%While in our paper, we consider all kinds of associations and try best to
%retain the accuracy of those associations
%
%\PC{Moreover, the model introduced in their method is not strong. The attack
%effects in the following steps. Step 1:The attacker gains some background
%knowledge such that the number of people buying creams and pregnancy tests
%are around five times more than people buying butter and pregnancy tests. .
%Step 2: The attacker downloads the result processed by permutation and one of
%the group in the result has such form that only contains people buying cream
%or butter with probability of sensitive item pregnancy test $\frac{1}{3}$.
%Step 3: with a simple equation $\frac{1}{3}(x+y)=5Px+Py$, where x represents
%people buying butter and y represents people buying cream, the attacker can
%get the exact probability of people who buy cream buy the pregnancy
%test($5P$) which is likely to be a high value with different x and y.
% }

\subsubsection{Pertubation}

Perturbation is developed for statistical disclosure control
\cite{FungWCY10:Survey}. Common perturbation methods include {\em additive
noise}, {\em data swapping}, and {\em synthetic data generation}. Their
common criticism is that they damage the data integrity by adding noises and
spurious values, which makes the results of downstream analysis unreliable.
Perturbation, however, is useful in non-deterministic privacy model such as
differential privacy \cite{Dwork08:diff:survey}, as
attempted by Chen \etal~ \cite{ChenMFDX11:Diff} in a probabilistic top-down
partitioning algorithm based on a context-free taxonomy.
 %Interestingly, the
%algorithm proposed in this paper is probabilistic in nature as well.
%Considering the fact that the noise introduced by randomization leads to
%severe data utility, some work related with differential privacy focuses on
%releasing certain data mining results
%\cite{Barak:2007:PAC:1265530.1265569,Bhaskar:2010:DFP:1835804.1835869,Friedman:2010:DMD:1835804.1835868,Korolova:2009:RSQ:1526709.1526733}.
%However, the usability of the published data is constrained by the pattern
%the owner decide to release and moreover the assumption that the data owner
%is able to perform data mining tasks is also weak. In addition, Leoni \etal
%\cite{DBLP:journals/corr/abs-1205-2726} also indicates the weakness of
%differential privacy model itself.
%
%\textbf{
%Recently, a new method called slicing was firstly proposed in \cite{10.1109/TKDE.2010.236} and was
%further developed in \cite{terrovitis:privacy}. Slicing, also called disassociation, aims to protect
%identity or attribute disclosure using identify combinations. However, the privacy model they introduce is
%different from ours and the type of targeted
%data utility is also completely different from ours.
%(I am in a dilemma. Since their methods are totally different
%from ours, I can't make comparison with ours.
%Their methods are somewhat useful in association rule mining and distribution remaining.
%The only disadvantage is that they changed the original structure of the table, but
%according to their data utility such change is acceptable.  )}

\subsection{Adversarial Attacks}

It is transparent to see that our anonymized data is immune from the {\em
record linking attack} \cite{FungWCY10:Survey}.
To show more safeness of our technique, we will also
demonstrate that our anonymized data is also immune from the {\em minimality
attack} \cite{Wong:2007:Minimality} and the {\em composition attack} \cite{Ganta:2008:Composition}.

The minimality attack \cite{Wong:2007:Minimality}
is proposed for relational data. Assume an adversary knows the whole original
quasi-identifier values as external data, also knows the privacy model and
anonymization technique, by comparing the generalized version of
quasi-identifier values with the original quasi-identifier values, the
adversary can successfully predict some privacy. The minimality attack relies
on the generalization anonymization technique, while our method uses
suppression technique. Also for set-valued data it doesn't have fixed
combination of items as quasi-identifiers, so it's unrealistic for an
adversary to obtain the satisfactory external data. 

The composition attack
\cite{Ganta:2008:Composition} is proposed for relational data by using the
overlap population of multiple organizations' independent release of
anonymized data, through intersection the privacy can still be breached in
relational data. For example $l$-diversity \cite{Ganta:2008:Composition}
model can be violated by composition attack. The reason why composition
attack can succeed is that quasi-identifier attribute values are generalized
and sensitive attribute values are retained, when performing intersection the
probability of a correlation between quasi-identifier and sensitive values
will definitely increase. On the contrary, our partial suppression algorithm
anonymizes set-valued data by randomly suppressing some sensitive items. In
the same way to perform intersection,
 the probability that a sensitive item correlated with quasi-identifier items can
both be higher or lower than before, which made the composition attack
untrusted. So in a summary our partial suppression technique is ideal to
avoid composition attack depending on the randomized characteristic of
suppression.
