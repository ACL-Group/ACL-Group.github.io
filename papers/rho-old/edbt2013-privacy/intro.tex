\section{Introduction}
\label{sec:intro}

Set-valued data sources are valuable in many data mining and data analysis
tasks. 
%For example, retail companies may want to know what items are top
%sellers (e.g., milk), or whether there is an association between the purchase
%of two or more items (e.g., people who buy flour also buy milk), to help them
%make better decisions in purchasing, marketing and even store arrangements.
%Search engines may analyze the pattern of queries issued and sites clicked
%within the same user session to identify good keywords and indexing
%strategies. In many cases, analysis tasks are not performed by the
%organizations or entities that produce the original datasets, but are instead
%{\em outsourced} to other external companies or individuals. In other cases,
%data are simply {\em published} to the general masses for scientific and
%public research purposes.
Publishing set-valued, and especially transactional data,
can pose significant privacy risk to the
individuals involved in those transactions, even though
there is no explicit identification of the individuals.
Set-valued data items can be divided into two types:
{\em non-sensitive} (or public) and {\em sensitive} (or private).
%\cite{Xu:2008:ATD}.
Privacy is in general associated with the sensitive items.
Table \ref{tab:retail-ex} shows
an example of retail transaction data in which each record (row) represents
a set of items purchased in a single transaction by an individual,
  all the items are non-sensitive, except the {\em pregnancy tester}
which is sensitive. An individual's privacy is breached if he or she can be
{\em re-identified}, or associated with a record in the data which contains
one or more sensitive items. Past research has shown that such breach is
possible through {\em linking attacks} \cite{FungWCY10:Survey}. For example,
from Table \ref{tab:retail-ex}, the attacker can infer that anybody who buys
chocolate and shampoo also buys the pregnancy tester.
This is evident from the fact that
\[{\rm Prob}(pregnancy~tester | chocolate,~ shampoo) = 1.0\]
By this inference (with a confidence of 1), the attacker {\em links}
chocolate and shampoo with the pregnancy tester, and if he also possesses the
background knowledge that Alice, an individual who is in the data, has indeed
bought chocolate and shampoo, then he can conclude with high confidence that
Alice bought the pregnancy tester as well.

\begin{table}[th]
\centering
\caption{A Retail Transaction Dataset}\label{tab:retail-ex}

\begin{tabular}{|r|l|} \hline
{\bf TID} & {\bf Items} \\ \hline \hline
1 & yogurt, milk \\ \hline
2 & milk, flour, potato chips \\ \hline
3 & bread, milk, fruits \\ \hline
4 & flour, milk \\ \hline
5 & chocolate, shampoo, {\em pregnancy tester} \\ \hline
\end{tabular}
\end{table}

There are four general approaches to anonymizing set-valued data.
One popular approach is {\em suppression} \cite{Xu:2008:ATD,Cao:2010:rho}
which involves deleting some of the data items.
For instance, by deleting pregnancy tester from Table \ref{tab:retail-ex}, 
Alice's privacy is preserved. Another popular approach is {\em generalization}
%which is also
%widely used in the anonymization of relational data
\cite{Cao:2010:rho}. In generalization, data items are ``generalized'' into a
different item or concept which subsumes the original item. Such subsumption
relation is governed by a pre-defined item taxonomy.
For instance, milk and yogurt can be generalized into ``dairy product''.
The third approach is {\em permutation} (or called {\em anatomization})
\cite{Xiao:2006:Anatomy,2011:TKDE:Anonymous} which separates the non-sensitive
and sensitive attributes into two schemas, while leaving non-sensitive
attribute values unchanged and offering aggregated values for sensitive
attributes among each group. The fourth approach is {\em perturbation} 
\cite{ChenMFDX11:Diff} which changes some of the items into
something completely different or adds other items into the data as ``noises''.

In this paper, we focus on the suppression approach. 
Most existing work in this space uses a technique called ``global
suppression'' \cite{Cao:2010:rho} in which once an item of type $t$ is
determined to be removed from one record, all items of the same type are
removed from the whole dataset. We instead study the problem of {\em partial
suppression} which seeks to delete only {\em some} of the items of type $t$.
%To the best of our knowledge, the partial suppression technique has not been
%studied in the context of set-valued data anonymization before.
Our observation is that set-valued data are primarily used either in
{\em association rules mining} or {\em statistical analysis}. We
argue that partial suppression is a better choice to preserve the 
number of original rules mineable and to preserve the statistical 
distribution of the data after anonymization,
because 1) it doesn't abruptly change the support for association rules like
generalization techniques do; 2) it makes fewer deletions of the data than
global suppression and thus incurs less information loss;
and 3) it better preserves the original data distribution because there is more 
flexibility in which item to suppress.

%According to our observations there are two main categories of set-valued
%data analysis: one is {\em mining of association rules}; the other is {\em
%statistical analysis}. In the first case, data users want to establish useful
%association rules among item sets, with a certain support. In other words,
%useful rules are the precious information that should be retained. In the
%second case, users simply compute the statistics of the data such as max,
%min, average and standard deviation, etc. In this case, the distribution of
%the data is the useful information.

%We attack
%this problem for the following reasons.

%A (very simple) two-phase algorithm that would always outperform 
%the proposed method. The algorithm works as follows. 
%
%First Phase: Given a transaction database to be anonymized, 
%the algorithm groups the transactions by their non-sensitive items. 
%That is, two transactions would be in the same group if and only 
%if the non-sensitive items in the two transactions are the same.
% 
%Second Phase: The algorithm suppresses a minimum number of sensitive items 
%to make each group privacy preserving. In particular, let us consider a 
%group g that contains n transactions, and a sensitive item x that appears 
%in m transactions in g. If m/n is larger than \rho 
%(i.e., the item x leads to a privacy breach in g), 
%then we remove the items x in g one by one (in an arbitrary order), 
%until m/n <= \rho. The algorithm terminates, once the aforementioned 
%suppression procedure is applied on each group.

%First, generalization itself losses information.
%% , e.g., by generalizing
%%yogurt and milk both into dairy products, there is no distinction between the
%%two items. The association rules thus learned will be very different from
%%those learned from the original data.
%The support for those rules will be
%changed unexpectedly as well. This renders rule mining almost impossible with
%generalized datasets. What's more, generalization techniques involve
%additional information (the taxonomy) which is proprietary to the data
%publisher and may not be available or agreeable to the data users.
%
%Second, perturbation techniques alters the original data in more drastic ways
%as it introduces random noises which may not be acceptable to the unknown
%downstream applications, since perturbation technique is restricted by the
%specified downstream application.
%
%Third, global suppression tends to delete more items than necessary,
%and the removal of all items of the same type not only changes the
%data distribution more severely but also makes mining association rules
%involving the deleted item impossible.
%
%Fourth, permutation also assumes the downstream
%application in advance and is highly restricted by the assumption that the
%external knowledge can only be nonsensitive items.
%On the contrary, partial suppression has the best potential of
%preserving both rules and distribution of the original data.
%
%The problem of anonymization by suppression (global or partial) is very
%challenging and has been shown to be NP-hard \cite{atallah99:disclosure,
%Xu:2008:ATD}, exactly because, first, the number of possible inferences from
%a given dataset is exponential and second, the size of the search space, i.e.
%the number of ways to suppress the data is also exponential to the number of
%data items.

%We are the first to propose an effective \emph{partial} suppression method
%for anonymizing set-valued data according to $\rho$-uncertainty model
%\cite{Cao:2010:rho}, which prevents the anonymized data from not only {\em
%record/attribute linkage attack} \cite{FungWCY10:Survey}. To satisfy
%$\rho$-uncertainty model, Cao \etal developed a global suppression method and
%a top-down generalization-driven global suppression method which suffer from
%same woes discussed earlier for generalization and global suppression.
%$\rho$-uncertainty model is relevant to $(h,k,p)$-coherence model proposed by
%Xu \etal \cite{Xu:2008:ATD}, while the work of Xu \etal assumes that all
%prior knowledge can only be public items and only public items can be
%suppressed.
%but also
%both {\em minimality attack} \cite{Wong:2007:Minimality} and {\em composition
%attack} \cite{Ganta:2008:Composition} (Section \ref{sec:related}).

The main contributions of this paper are: 
1) we propose the first partial suppression framework for anonymizing
set-valued data which includes heuristics to optimize for data distribution
or for association rule mining while minimizing the number of items suppressed; 
and 2) unlike previous work on suppression which 
assumes that attacker's knowledge contains {\em only} non-sensitive data, 
the framework allows the background knowledge to be combination of 
{\em any} items.
%\item The framework adopts a ``pay-as-you-go'' approach based on 
%divide-and-conquer, which can be adapted to achieve both 
%space-time and quality-time trade-offs (Section
%\ref{sec:app} and \ref{sec:eval}).
%\item We implemented an online demo system that anonymizes any set-valued
%data in real time (Section \ref{sec:demo}).
%\end{itemize}

%The main contributions of this paper are as follows.
%\begin{enumerate}
%\item To the best of our knowledge, we are the first to propose an
%    effective \emph{partial} suppression framework for anonymizing
%    set-valued data (Section \ref{sec:prob} and
%    \ref{sec:algo}), which %, to the best of our knowledge, is first such
%    %attempt in privacy preserving data publishing research, and
%    prevents
%    the anonymized data from not only {\em record/attribute linkage
%    attack} \cite{FungWCY10:Survey} but also both {\em minimality
%    attack} \cite{Wong:2007:Minimality} and {\em composition attack}
%    \cite{Ganta:2008:Composition} (Section \ref{sec:related}).
%%    \PC{Why don't we mention that we are the first?}
%\item We adopt a ``pay-as-you-go'' approach based on divide-and-conquer,
%    which can be adapted to achieve both space-time and quality-time
%    trade-offs (Section \ref{sec:algo} and \ref{sec:eval}), and devise two
%    categories of novel heuristics to either preserve the original data
%    distribution or retain mineable useful rules with limited spurious
%    rules invented, while minimizing the items deletions (Section
%    \ref{sec:algo}).
%\item We conduct extensive experiments to validate key properties of the
%    algorithm, to compare the performance of our algorithm with
%    state-of-the-art suppression and generalization techniques, and to
%    show that our algorithm out-performs the peers in preserving the
%    original data distribution(more than 100 times better
%    than previous work on average)
%    or retaining mineable useful association
%    rules with limited spurious rules invented (retain 70\% rules
%    but introduce only 8\% spurious rules on average),
%     while minimizing the item
%    deletions (a 30\% improvement on average), by large margins and
%    achieves this in reasonable time (Section \ref{sec:eval}).
%\end{enumerate}

%Recently, privacy protection of set-valued data has also received increasing
%interest. The original set-valued data privacy problem was defined in the
%context of association rule hiding \cite{atallah99:disclosure,
%tkde:VerykiosEBSD04:ARH, tkde:WuCC07:hiding}, in which the data publisher
%wishes to ``sanitize'' the set-valued data (or {\em micro-data}) so that all
%sensitive or ``bad'' associate rules cannot be discovered while all (or most)
%``good'' rules remain in the published data.

%Subsequently, a number of privacy models including $(h,k,p)$-coherence
%\cite{Xu:2008:ATD}, $k^m$-anonymity \cite{Terrovitis:2008:PAS}, $k$-anonymity
%\cite{He:2009:ASD} and $\rho$-uncertainty \cite{Cao:2010:rho} have been
%proposed. $k^m$-anonymity and $k$-anonymity are models carried over directly
%from relational data privacy. They require that any record must be
%indistinguishable from a set of $k$ records with $m$ being the upper bound of
%attacker's background knowledge. $(h,k,p)$-coherence and $\rho$-uncertainty,
%on the other hand, protect the data from malicious inferences by bounding the
%confidence and the support of any sensitive association rule inferrable from
%the data. This is also the privacy model this paper adopts.

%In response to these privacy models, a number of anonymization techniques
%were developed. These can be generally divided into four categories: {\em
%global/local generalization} \cite{Terrovitis:2008:PAS, He:2009:ASD,
%Cao:2010:rho}, {\em global suppression} \cite{Xu:2008:ATD, Cao:2010:rho},
%{\em permutation} \cite{2011:TKDE:Anonymous} and {\em perturbation}
%\cite{Zhang:2007:agg, ChenMFDX11:Diff}. Next we briefly discuss the pros and
%cons of these anonymization techniques.

%Generalization replaces a specific value by a generalized value, e.g.,
%``milk'' by ``dairy product'', according to a generalization hierarchy
%\cite{FungWCY10:Survey}. Global generalization not only converts all
%occurrences of an item to a higher level item type $t_h$ in the taxonomy
%hierarchy, but also converts all occurrences of items under that the sub-tree
%of $t_h$ in the taxonomy to $t_h$ as well. For example, if an instance of
%``milk'' is generalized to ``dairy product'', then all other instances of
%``milk'' as well as all instances of ``yogurt'' and ``cheese'' are also
%generalized to ``dairy product''. Local generation refers to the application
%of the technique on some occurrences of an item only. While generalization
%preserves the correctness of the data, it compromises its accuracy and
%preciseness. Worse still, association rule mining is impossible unless the
%data users have access to the same generalization taxonomy and they agree to
%the target level of generalization. For instance, if the users don't intend
%to mine rules involving ``dairy products'', then all generalizations to
%``dairy products'' are useless.
%with the arbitrary change of support in the rules learned and massive
%reduction in the number of original rules (evident from our experiments
%in Section \ref{sec:eval}). \XH{Rules are only useful when data publisher and data user have the same generalization target level for each item in the final generalization result, but then no need generalization any more, so it's a huge mismatch} Moreover, data users have to have access
%to the generalization hierachy they may or may not agree to.

%Global suppression is a technique that deletes some of the items so that the
%resulting dataset is safe. However, it removes all occurrences of an item
%type even if just one of the occurrences causes the privacy violation. The
%advantage of such approach is that it preserves the support of existing rules
%that don't involve deleted items and hence retains these rules
%\cite{Xu:2008:ATD}, and at the same time it doesn't introduce additional and
%spurious association rules. The obvious disadvantage is that it can cause
%tremendous unnecessary information loss. In our work, we focus on partial
%suppression which has not been attempted in data anonymization mainly due to
%its perceived side effects of changing the support of inference rules in the
%original data \cite{Xu:2008:ATD, Cao:2010:rho, tkde:VerykiosEBSD04:ARH,
%tkde:WuCC07:hiding}. Our experiments in this paper has demonstrated that
%partial suppression introduces very limited amount of new rules while
%preserving many more original rules than the global suppression techniques.
%In addition, our algorithm has been shown to preserve the data distribution
%much better than other competing methods.

%Permutation is introduced by Xiao \etal \cite{Xiao:2006:Anatomy} for
%relational data. With generalization technique severely compromising the
%accuracy of data aggregation analysis, Xiao \etal propose the
%\textit{Anatomy} which releases quasi-identifier and sensitive values in two
%separate tables. Specifically quasi-identifier values are not changed and
%organized into groups, and for every such group the corresponding sensitive
%values are aggregated. After that, Ghinita \etal \cite{2011:TKDE:Anonymous}
%extend the permutation technique to publish anonymous transactional data.
%Ghinita \etal propose two novel anonymization techniques for spares
%high-dimensional data by introducing two representations for transactional
%data. However the limitation is that the quasi-identifier is restricted to
%contain only {\em non-sensitive items}, which means their work only focus to
%approximately guarantee the utility of associations between quasi-identifier
%and sensitive items, but not consider the associations among sensitive items.
%While in our paper, we consider all kinds of associations and try best to
%retain the accuracy of those associations

%Perturbation is developed for statistical disclosure control
%\cite{FungWCY10:Survey}. Common perturbation methods include {\em additive
%noise}, {\em data swapping}, and {\em synthetic data generation}. Their
%common criticism is that they damage the data integrity by adding noises and
%spurious values, which makes the results of downstream analysis unreliable.
%Perturbation, however, is useful in non-deterministic privacy model such as
%differential privacy \cite{Dwork:2006:diff, Dwork08:diff:survey}, as
%attempted by Chen \etal~ \cite{ChenMFDX11:Diff} in a probabilistic top-down
%partitioning algorithm based on a context-free taxonomy. Interestingly, the
%algorithm proposed in this paper is probabilistic in nature as well.
%Considering the fact that the noise introduced by randomization leads to
%severe data utility, some work related with differential privacy focuses on
%releasing certain data mining results
%\cite{Barak:2007:PAC:1265530.1265569,Bhaskar:2010:DFP:1835804.1835869,
%Friedman:2010:DMD:1835804.1835868,Korolova:2009:RSQ:1526709.1526733}.
%However, the usability of the published data is constrained by the pattern
%the owner decide to release and moreover the assumption that the data owner
%is able to perform data mining tasks is also weak. In addition, Leoni \etal
%\cite{DBLP:journals/corr/abs-1205-2726} also indicates the weakness of
%differential privacy model itself.

%The most relevant work to this paper is by Xu \etal \cite{Xu:2008:ATD} and
%Cao \etal \cite{Cao:2010:rho}. The $(h,k,p)$-coherence model by Xu \etal~
%requires that the attacker's prior knowledge to be no more than $p$ public
%(non-sensitive) items, and any inferrable rule must be supported by at least
%$k$ records while the confidence of such rules is at most $h$\%. They believe
%private items are essential for research and therefore only remove public
%items to satisfy the privacy model. They developed an efficient greedy
%algorithm using global suppression. In this paper, we do not restrict the
%size or the type of the background knowledge, and we use a partial
%suppression technique to achieve less information loss and also better retain
%the original data distribution.
%
%Cao \etal \cite{Cao:2010:rho} proposed a similar $\rho$-uncertainty model
%which is also used in this paper.
%%Their model is related to association rule hiding problem.
%They developed a global suppression method and a top-down
%generalization-driven global suppression method (known as TDControl) to
%eliminate all strong sensitive inferences with confidence above a threshold
%$\rho$. The generalization is performed on non-sensitive items while deletion
%is performed on sensitive items only. The $\rho$-uncertainty model, like this
%paper, makes no assumption about the type of items in the background
%knowledge. However, their methods suffer from same woes discussed earlier for
%generalization and global suppression.
%
%Furthermore, TDControl method terminates when the information gain provided
%by specialization is out-weighed by the information loss incurred by global
%suppression. But this is based on the assumption that specialization always
%makes the data less safe and hence requires more suppression. In other words,
%data is assumed to exhibit some monotonic property under a generalization
%hierarchy. Such assumption makes the applicability of TDControl questionable.
%In our work, most of the comparative experiments were done against the two
%methods by Cao \etal. And our results clearly show that our new algorithm
%significantly outperforms the two in preserving data distribution and useful
%inference rules, as well as in minimizing information losses.
%
%It's transparent to see that our anonymized data is immune from {\em
%record/attribute linkage attack} \cite{FungWCY10:Survey} by Definition
%\ref{def:safety_table}. To show more safeness of our technique, we will also
%demonstrate that our anonymized data is also immune from {\em Minimality
%attack} \cite{Wong:2007:Minimality} and {\em Composition attack}
%\cite{Ganta:2008:Composition}. Minimality attack \cite{Wong:2007:Minimality}
%is proposed for relational data. Assume an adversary knows the whole original
%quasi-identifier values as external data, also knows the privacy model and
%anonymization technique, by comparing the generalized version of
%quasi-identifier values with the original quasi-identifier values, the
%adversary can successfully predict some privacy. The Minimality attack relies
%on the generalization anonymization technique, while our method uses
%suppression technique. Also for set-valued data it doesn't have fixed
%combination of items as quasi-identifiers, so it's unrealistic for an
%adversary to obtain the satisfactory external data. Composition attack
%\cite{Ganta:2008:Composition} is proposed for relational data by using the
%overlap population of multiple organizations' independent release of
%anonymized data, through intersection the privacy can still be breached in
%relational data. For example $l$-diversity \cite{Ganta:2008:Composition}
%model can be violated by composition attack. The reason why composition
%attack can succeed is that quasi-identifier attribute values are generalized
%and sensitive attribute values are retained, when performing intersection the
%probability of a correlation between quasi-identifier and sensitive values
%will definitely increase. On the contrary, our partial suppression algorithm
%anonymizes set-valued data by randomly suppressing some sensitive items. In
%the same way to perform intersection,
% the probability that a sensitive item correlated with quasi-identifier items can
%both be higher or lower than before, which made the composition attack
%untrusted. So in a summary our partial suppression technique is ideal to
%avoid composition attack depending on the randomized characteristic of
%suppression.

%\XH{I leave out two points: one is why our method will be immune from
%Minimality attack and Composition attack; the other is to stress that we
%consider the case that sensitive items can also be known by attacker as prior
%knowledge which makes the two-phase anonymization algorithm incorrect. }
