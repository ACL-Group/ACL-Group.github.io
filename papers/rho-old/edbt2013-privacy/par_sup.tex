To ensure that each partition is safe, we must make sure all \qids in the
partition are safe. Since a \qid is a combination of different item types,
the number of distinct \qids is large. It is thus impractical to enumerate all
distinct \qids in one go. Instead, we enumerate one set of \qids at a time
and fix these \qids by suppressing just enough items so that these \qids are
safe. Once this batch of \qids are fixed, we move on to fix the next batch,
until all \qids are safe. We do this by introducing a \qid buffer of fixed
capacity $\bmax$. The \qid buffer contains the set of \qids to be checked and
fixed in each batch. The value of $\bmax$ is significant. Small $\bmax$
values can cause repetitive enumeration of \qids because a \qid made safe in
one batch may become unsafe again after processing another batch, a
phenomenon known as {\em regression}. Large $\bmax$ values causes unnecessary
enumeration of massive \qids, many of which do not exist
by the time all previous \qids in the buffer are fixed, due to the item
suppressions.

%\begin{algorithm}[th]
%\caption{$\PartialSuppressor(T, D_S, \rho, \bmax)$}
%\label{algo:partialsuppressor}
%\begin{algorithmic}[1]
%    \STATE Initialize $safe\leftarrow\TRUE$, $i\leftarrow 1$;
%%    \STATE Initialize $safe\leftarrow\TRUE$
%    \LOOP
%        \STATE Initialize $sup$, $\linked$, $S$, $L$;
%        \WHILE {$|B|<b_{max}$ \AND $i\leq |T|$} \label{algo:enu_s}
%             \STATE Fill $B$ with \qids generated by $T[i]$, \label{algo:enumerate1}
%             \STATE update $sup$, $\linked$, $S$, $L$; \label{algo:enumerate2}
%             \STATE $i\leftarrow i+1$;
%        \ENDWHILE \label{algo:enu_e}
%                \STATE update $sup$, $\linked$, $S$, $L$;
%%        \STATE Calculate $\rho$ of each $qid$ in $|B|$ \label{algo:update}
%        \IF {$B$ contains an unsafe \qids}\label{line:containunsafe}
%            \STATE Sanitize Buffer $B$;\label{line:sanitizebuffer}
%            \STATE $safe\leftarrow\FALSE$;
%        \ENDIF
%        \IF {$i \ge |T|$ \AND $safe$}
%            \STATE \textbf{break};\label{algo:partialbreak}
%        \ELSIF {$i \ge |T|$}
%                \STATE $i\leftarrow 1$;
%                \STATE $safe\leftarrow\TRUE$;
%%                \STATE \textbf{continue}
%        \ENDIF
%    \ENDLOOP
%\end{algorithmic}
%\end{algorithm}

%Algorithm \ref{algo:partialsuppressor} gives the pseudo-code of the
%partial suppressor.
Starting from the beginning of the partition,
the partial suppressor scans through the records and performs
the following two-phase sanitization repeatedly. In phase 1,
it takes a number of records starting from the current position, and enumerates
\qids to fill up the \qid buffer. Meanwhile, the sensitive items associated with
each \qid is also determined and the support of the \qid computed.
In phase 2, it sanitizes all the unsafe \qids in the buffer.
The algorithm terminates when the whole partition is scanned in one go
and there is no unsafe \qid discovered.

If a \qid is unsafe, it is involved in at least one {\em unsafe} sensitive
association rule. Sanitizing the \qid buffer is equivalent to disabling all
the unsafe sensitive rules associated with the \qids in the buffer by
suppressing some items in the original data. To disable a rule $q \rightarrow
e$, the number of items of type $t \in q\cup e$ that need to be suppressed is
given by
\[N_s(t, q\rightarrow e)=
\begin{cases}
sup(q\cup \{e\})-sup(q)\rho & t=e  \\
\frac{sup(q\cup \{e\})-sup(q)\rho}{1-\rho} & t\in q %\\
% \infty & otherwise
\end{cases} \]
Of all item types in $q\cup e$, there exists an item type $t_{min} \in q\cup
e$ which results in to minimum suppression. In other words, for each unsafe
sensitive rule $r$, we need to delete $N_s(t_{min}, r)$ items of type
$t_{min}$ to make it safe. In this work, we delete {\em any} $N_s(t_{min},
r)$ items from the original data.

Since there are multiple unsafe rules associated to the buffer, we disable one
rule at a time. Each time we pick a rule to disable according to one of the following
two heuristics.

The first heuristic prefers to focus the suppression on a few item types and
retain the support of other items as much as possible.
This is in the same spirit of global
suppression though we don't have to suppress all items of the same type. The
advantage of this heuristic is that it doesn't introduce {\em spurious rules}
and therefore produces data that is more suitable in rule mining tasks.

The second heuristic prefers to disable a rule which results in a data distribution
which is closest to the orginal data distribution, by Kullback-Leibler divergence.
The effect of this heuristic is to preserve the original data distribution and hence
is more suitable for statistical analysis tasks.

%The above two heuristics not only determine the best strong sensitive
%association rule to fix, also need to fully decide how to fix a specific
%rule. As shown below, for a strong sensitive association rule $q\rightarrow
%e$ whose confidence $conf(q,e)$ is bigger than $\rho$, we have two
%conservative ways to fix it: one is suppressing $sup(q\cup \{e\})-sup(q)\rho$
%replications of e, the other one is suppressing $\frac{sup(q\cup
%\{e\})-sup(q)\rho}{1-\rho}$ replication of any $t\in q$. In this way the
%heuristics follow the strategy explained above and fixed a picked rule.
%we follow the Definition \ref{def:minimum} which decides the minimum
%information loss it must incur.
%
%\begin{definition}[Minimum suppression]
%\label{def:minimum} Minimum suppression of item $t$ to make strong sensitive
%association rule $q\rightarrow e$ safe, i.e. $conf(q,e)\leq\rho$:
% \hspace{4mm}
%\[MS(t, q\rightarrow e)=
%\begin{cases}
%sup(q\cup \{e\})-sup(q)\rho & t=e  \\
%\frac{sup(q\cup \{e\})-sup(q)\rho}{1-\rho} & t\in q \\
% \infty & otherwise
%\end{cases} \]
%\end{definition}

%After fixing one strong association rule with several replication of one
%item type suppressed, it'll determine the set of \qids in \qid buffer that
%would be affected.
