
 \section{Appendix}
\label{sec:appendix}

Here are some ideas I have come up with.

Let us consider the function in analysis part
\begin{align*}
  n_1 r
+ \frac{n_1 (r |T| - \bmax)}{r_d}
+ \frac{n_1 |T| r l r_r (\bmax t + l)}{r_d} \\
+ l_m |T| n_2 \dnum v_3
+ l |T| n_2 \dnum v_3 r_r (l + \dnum t)
\end{align*}

First of all, this function is considered as the worst condition in my opinion.
Let's recheck the analysis part.
First ,  $v_1=\frac{n_1\cdot r}{\bmax r_d}$ is  considered all distinct qids
in short records for a single pass ,which is definitely the worst case based on the assumption that
all qids are distinct which are generally not.

There are four parts in our algorithm, we analyzed them one by one.

The first part is regarded as the worst case because it is  the maximum number of iterations.
The second part is the worst case because we assume that all qids in a buffer are independent, which
means we need to update each of them. (times $\bmax$)
The third part is also the worst case, which is analyzed in the analysis part. I have a question about it.
Why is the maximum number of intersections  $v_2 v_3 \dnum l_m |T|$? What does $l_m$ represent?
I think that $l_m$ represents the number of items in one qid, but do we just enumerate qids with 
$l_m$ items?


I think that the forth part is a little confused part. To see why it is the worst situation, let's check the 
two sub-part.The first sub-part is the intersection calculation on Line 5 in Algorithm 4,
which costs roughly $l |T|$. This part is the worst case because $l |T|$ represents the total number of items,
which indicates that if we want to calculate $X \leftarrow \container(q\cup\{e\})$ and the worst case is that 
we need to check all of the items one by one(the dataset is ordered,so if one item doesn't exist in one record,
we can realize it by discovering a item whose ID is bigger than that of the  object item and we do not find 
the object item before).  $k |B|$ is also the worst case based on the assumption that
each qid in the buffer is independent, which means we have to eliminate a item in the record whose qids are not 
contained in the buffer and this is not necessarily the case.$k= t |T|$. is also the worst case which is mentioned 
in the analysis part.

So the function
\begin{align*}
  n_1 r
+ \frac{n_1 (r |T| - \bmax)}{r_d}
+ \frac{n_1 |T| r l r_r (\bmax t + l)}{r_d} \\
+ l_m |T| n_2 \dnum v_3
+ l |T| n_2 \dnum v_3 r_r (l + \dnum t)
\end{align*}
represents the worst case.






First of all, let's consider the scalability result.

We divide the dataset into five parts. It is obvious to see that all parameters in this equation remained except $|T|$, 
which means
the time complexity is proportional to $|T|^{2}$. 
So, the time cost of each part should be$\frac{1}{25}$ ,$2*\frac{1}{5}^{2}$,$4*\frac{3}{20}^{2}$,$4*\frac{1}{5}^{2}$,$8*\frac{1}{8}^{2}$, which is 0.04,0.08,0.09,0.16,0.125 
respectively. These numbers explain
the result perfectly!

As for the result of $b_max$
It is used for memory limit. So the result of it cannot be explained by this equation. 
However, we can do further experiment on a computer with a smaller memory and set Buffersize extremely large and 
I think the time performance will increase with the buffersize.


As for $l_m$, I can't find a solution. I can write a function about the time of HandleShort with $l_m$, and it is
exponential to $l_m$, but I can't find a function about the time of HandleLong. 
The function about the time of HandleShort is 
\begin{align*}
 A((\frac{2}{e^{a}})^{l_m}-1)(1-e^{-al_m})
\end{align*}
by setting other parameters constant. I omit the procedure, since it is 
a little complicated and it is useless if we can't find a function about the time of HandleLong.
More further discussion is needed for this part.

Anyway, I think the time complexity function is true and we can explain our result with this equation. 
If we are succeed in doing this, I think that our result is much more convincing.

Next I will try to think about the regression ratio.
