\section{Introduction}
\label{sec:intro}
%\begin{itemize}
%
%\item Motivation of the problem (why is set-valued privacy problem important)
%
%\item traditional approaches (state of the art) and their deficiencies.
%
%\item Challenge of the problem (why is it hard? Exponential space,
%NP-hard, etc. E.g. it takes a server 2 days to solve a problem size of only
%10 records using naive approach.)
%
%\item Briefly introduce our approach and why it's better
%
%\item Summarize the main contributions of the paper (and refer to the relevant
%sections instead of explicitly naming the organization of the paper)
%\end{itemize}
%

Set-valued data are valuable data sources generated in
such occasions as retail transactions, web search sessions, 
web browsing and medical activities. 
While a relational data source is
a set of rows of equal cardinality and schema,
a set-valued data source consists of multiple records, each of which is
a set of items drawn from a finite domain. Table \ref{tab:retail-ex} shows
an example of retail transaction data in which each record (row) represents
a set of items purchased in a single transaction by an individual.

\begin{table}[th]
\centering
\caption{A Retail Transaction Dataset}\label{tab:retail-ex}

\begin{tabular}{|r|l|} \hline
{\bf TID} & {\bf Items} \\ \hline \hline
1 & yogurt, milk \\ \hline
2 & milk, flour, potato chips \\ \hline
3 & bread, milk, fruits \\ \hline
4 & flour, milk \\ \hline
5 & chocolate, shampoo, {\em pregnancy tester} \\ \hline
\end{tabular}
\end{table}

These data sources are valuable in many data mining and data analysis tasks.
For example, retail companies may want to know what items are top sellers
(e.g., milk), or whether there is an association between the purchase of
two or more items (e.g., people who buy flour also buy milk),
to help them make better decisions in purchasing, marketing and even store
arrangements. Search engines may analyze the pattern of queries issued and
sites clicked within the same user session to identify good keywords and
indexing strategies. In many cases, analysis tasks are not performed by the
organizations or entities that produce the original datasets, but are instead
{\em outsourced} to other external companies or individuals. In other cases,
data are simply {\em published} to the general masses for
scientific and public research purposes.

Publishing set-valued data can pose significant privacy risk to
the individuals involved in
those transactions recorded in the data \cite{FungWCY10:Survey}, even though
there is no explicit identification of the individuals. This is
because data items in set-valued sources can be divided into two types:
{\em non-sensitive} (or public) and {\em sensitive} (or private)
\cite{Xu:2008:ATD}. In the above example, all the items
are non-sensitive, except the {\em pregnancy tester} which is sensitive.
Privacy is in general associated with the sensitive items.
An individual's privacy is breached if he or she can be {\em re-identified},
or associated with a record in the dataset
which contains one or more sensitive items.
Past research has shown that such breach is possible through {\em linking
attacks}\cite{FungWCY10:Survey}. For example, from Table \ref{tab:retail-ex},
the attacker can infer that anybody who buys chocolate and shampoo also
buys the pregnancy tester. This is evident from the fact that
\[{\rm Prob}(pregnancy~tester | chocolate,~ shampoo) = 1.0\]
By this inference (with a support of 1), the attacker {\em links}
chocolate and shampoo with the pregnancy tester,
and if he also possesses the background knowledge that Alice,
an individual who is in the dataset, has indeed bought chocolate and shampoo,
then he can conclude with high confidence that Alice bought the
pregnancy tester as well.

To protect individuals privacy from linking attacks, datasets must be
{\em anonymized} first before publishing. This is a process which
alters the original data in such a way that no strong inference about any
sensitive items can be obtained from the resulting anonymized data.
Because the data publisher cannot make any assumption about the amount of
background knowledge an attacker may have, they must prepare for the worst
and remove {\em all} the sensitive strong inferences.

There are three general approaches to anonymizing set-valued data. One
popular approach is {\em suppression} which involves deleting some of the
data items. For instance, by deleting pregnancy tester from Table
\ref{tab:retail-ex}, Alice's privacy is preserved.
The other popular approach is {\em generalization} which is also
widely used in the anonymization of relational data
\cite{Sweeney2002:k-anonymity}. In generalization, data items are
``generalized'' into a different item or concept which subsumes the original
item. Such subsumption relation is governed by a pre-defined item taxonomy.
For instance, milk and yogurt can be generalized into ``dairy product''.
The last approach is {\em perturbation} which changes some of the items into
something completely different or adds other items into the dataset as
``noises'' \cite{ChenMFDX11:Diff}.

There are obviously many different ways
to change a given dataset so that no strong inference can be obtained.
We want to find an optimal way that inflicts the {\em minimal} change to
the original data, because we would like the recipient of the data to make
maximal {\em utility} of the data. In other words, the data anonymization
is an optimization problem of minimizing {\em information loss}
while preserving the users privacy. According to our observations, there are
two main categories of set-valued data analysis: one is {\em mining of
association rules}; the other is {\em statistical analysis}. In the first
case, data users want to establish useful association rules among
item sets, with a certain support. In other words, useful rules are the
precious information that should be retained.
In the second case, users simply compute the statistics of the data
such as max, min, average and standard deviation, etc. In this case,
the distribution of the data is the useful information.

In this paper, we focus on the suppression approach. Most existing work
in this space uses a technique called ``global suppression''
\cite{Cao:2010:rho} in which once an item of type $t$ is determined to
be removed from one record, all items of the same type
are removed from the whole data set. We instead study the problem of
{\em partial suppression} which seeks to delete only {\em some} of
the items of type $t$. To the best of our knowledge,
the partial suppression technique has not been studied in the context of
set-valued data anonymization before. We attack this problem for
the following reasons. First, generalization itself losses information,
e.g., by generalizing yogurt and milk both into dairy products,
there is no distinction between the two items. The association rules thus
learned will be very different from those learned from the original data.
The support for those rules will be changed unexpectedly as well.
This renders rule mining almost impossible with generalized datasets.
What's more, generalization techniques involve additional
information (the taxonomy) which is proprietary to the data publisher
and may not be available or agreeable to the data users.
Second, perturbation
techniques alters the original data in more drastic ways as it introduces
random noises which may not be acceptable to the downstream applications.
They also mainly target different attack models
(e.g. differential privacy model \cite{ChenMFDX11:Diff})
than the linking attack which is our main concern in this paper.
Third, global suppression tends to delete more items than necessary,
and the removal of all items of the same type not only changes the
data distribution more severely but also makes mining association rules
involving the deleted item impossible. On the contrary, partial
suppression has the best potential of preserving both rules and
distribution of the original data.

The problem of anonymization by suppression (global or partial)
is very challenging and has been shown to be NP-hard
\cite{atallah99:disclosure, Xu:2008:ATD}, exactly because, first,
the number of possible inferences from a given dataset is exponential
and second, the size of the search space, i.e. the number of ways
to suppress the data is also exponential to the number of data items.
In fact, we experimented with finding the optimal solution for anonymizing
a very small data set of merely 15 records and a total of 75 items.
A straight-forward search of all possible suppression solutions
takes two and half days on a reasonably powerful server!

In this paper, we propose an iterative partial suppression framework that
minimizes the number of item suppressions while ensuring that the confidence
of any association rule that positively links a sensitive item is below
a threshold $\rho$. Our main idea is although the total number of
``dangerous'' inference rules maybe worst case exponential in the original
data, incremental ``invalidation'' of some of the rules through
partial suppression of small number of affected items can
dramatically decrease the number of these bad inferences,
which leads to quicker convergence to a solution.
In each iteration, our algorithm deletes at least one item from the dataset, 
and since the dataset is finite, the algorithm is
guaranteed to terminate. In addition, one can employ
a divide-and-conquer approach by approximating the global solution with
the combination of anonymization of a number of smaller subsets of the data.
Our experiments show that this technique is very effective in producing
a ``safe'' data set with fewer suppressions than previously reported
methods while better preserving the original data distribution and
useful (non-sensitive) association rules without introducing many spurious
rules. The framework adopts a ``pay-as-you-go'' approach, and can be
adapted to achieve both space-time and quality-time trade-offs.

The main contributions of this paper are as follows.
\begin{enumerate}
\item We propose an effective partial suppression framework for
 anonymizing set-valued data (Section \ref{sec:prob} and \ref{sec:algo}),
which, to the best of our knowledge, is
first such attempt in privacy preserving data publishing research.
\item We give detailed analysis of the partial suppression problem to show
that it is NP-hard and prove a number of interesting properties of our
algorithm (See Section \ref{sec:analysis}).
\item We conduct extensive experiments to validate key properties of the
algorithm, compare the performance of our
algorithm with state-of-the-art suppression and generalization techniques,
and show that our algorithm out-performs the peers in
all measures of information losses by large margins and achieves this in
very reasonable time (See Section \ref{sec:eval}).
\end{enumerate}


%
%includes shopping transactions, web search queries, click streams, medical activities and other transaction data. Set-valued data is a multi-set of transactions, each transaction is a set of items drawn from a universe of items. Items are distinguished by sensitive (private) and non-sensitive (public). Set-valued data is shared to support all kinds of data mining and data analysis tasks, including mining association rules, predicting user interests, and doing other statistics \cite{Cao:2010:rho, DBLP:journals/pvldb/ChenMFDX11}. However direct publishing original set-valued data threatens to breach individual's privacy through linking attacks \cite{FungWCY10:Survey}. Example is that if an attacker has partial knowledge $PK$ of a certain victim's transaction $t$, by combining $PK$ with the released data $T$, the attacker may locate $t$ from $T$ and obtain other sensitive information belonging to $t$. Through this way the individual's privacy is violated. As a result it's very important and necessary for data publisher to do the anonymization before publishing.\\
%
%There are diverse ways \cite{Xu:2008:ATD, Terrovitis:2008:PAS, He:2009:ASD, Cao:2010:rho, DBLP:journals/pvldb/ChenMFDX11} already proposed to anonymize set-valued data.
%Most of existing research \cite{Xu:2008:ATD, Terrovitis:2008:PAS, He:2009:ASD, Cao:2010:rho} use deterministic techniques to anonymize set-valued data.
%Yabo Xu et al. \cite{Xu:2008:ATD} propose an efficient algorithm which eliminates all damaging prior knowledge that violate (\textit{h}, \textit{k}, \textit{p})-coherence from database. They \cite{Xu:2008:ATD} assume prior knowledge is all public items and within the power of $p$, they use a greedy global suppression on public items and measure information loss by amount of items suppressed.
%J. Cao et al. \cite{Cao:2010:rho} introduce $\rho$-uncertainty to protect sensitive item inference. Although paradigm of global suppression in \cite{Cao:2010:rho} is similar to the greedy algorithm of Yabo Xu's \cite{Xu:2008:ATD}, they distinguish items from sensitive between non-sensitive, and allow sensitive items in prior knowledge, to make it a more natural problem setting. J. Cao et al. \cite{Cao:2010:rho} not only use global suppression on sensitive items, also use a selective mixed scheme of global suppression on sensitive items and generalization on non-sensitive items as approaches.
%M. Terrovitis et al. \cite{Terrovitis:2008:PAS} define $k^m$-anonymity to prevent a transaction from been recognized in a set of k published transactions. $k^m$-anonymity \cite{Terrovitis:2008:PAS} is a loosen version of $k$-anonymity, since attacker's prior knowledge is upper-bounded by m. M. Terrovitis et al.
%While Y. He et al. \cite{He:2009:ASD} propose the real $k$-anonymity model in the following year for set-valued data publishing.
%$k^m$-anonymity\cite{Terrovitis:2008:PAS} and $k$-anonymity \cite{He:2009:ASD} assume all items are equally to be sensitively, so they employ generalization on all items if possible. While this assumption is not true in nature. On the other side, $k$-anonymity can well protect record linkage attack \cite{FungWCY10:Survey,Kifer:l-diversity}, but cannot well guard attribute linkage attack \cite{FungWCY10:Survey,Kifer:l-diversity}. Attribute linkage attacker is described as attacker may not need to precisely identify victim's transaction but to infer his/her sensitive information \cite{FungWCY10:Survey,Kifer:l-diversity}. Example is even though a transaction is crowded in a identical $k$ transaction group, while if there is a high percent that all $k$ transactions contain a common sensitive item, then attacker may infer the victim also has a high probability having this sensitive item. Privacy is not guaranteed by $k$-anonymity in this way.
%
%On the contrary to deterministic approaches, Rui Chen et al. \cite{DBLP:journals/pvldb/ChenMFDX11} addressed the privacy concern \cite{Kifer09attackson, Narayanan:2008:RDL, Wong:2009:AAP} for their deterministic nature and propose to use differential privacy \cite{Dwork08:diff:survey, Dwork:2006:diff} to publish set-valued data. Rui Chen et al. \cite{DBLP:journals/pvldb/ChenMFDX11} propose a probabilistic top-down partitioning algorithm based on a context-free taxonomy tree.
%
%As we see, most previous research work use generalization and global suppression techniques to anonymize set-valued data.
%Generalization preserves the correctness of data, but it compromises data's accuracy roughly along the generalization hierarchy. When generalize set-valued data for rule mining, the amount of rules highly decreases and the generalized rules are much less specific (see Section \textbf{Experimental section id}). We doubt the real utility of those few even less specific rules, and doubt the ambiguous inferences it would lead.
%
%To protect the consistency of residual itemsets' supports \cite{Xu:2008:ATD}, for example if items $a$ and $b$ exist in anonymized data, then the supports of $a$, $b$, and $ab$ in anonymized data should be the same in original data. Global suppression technique follows this constraint and removes all occurrences of one item if it is determined to remove. The good side is it would't affect other itemsets' support, while the bad side is it is a dramatic deletion which would cause much unnecessary information loss. Same conclusion as generalization technique, many rules would be lost during the dramatic deletion (also see Section \textbf{Experimental section id}).
%
%There is no doubt on the importance to keep the consistency of residual itemsets' supports. But it has a strong prediction the downside application is rule mining. Assume the anonymized data is to be used in rule mining, few rules are kept with global suppression's crude deletion, so what's the real utility of global suppression result?
%
%In this paper, we propose a $partial suppression$ framework (later call it $partial$) to anonymize set-valued data. $Partial$ is the opposite side of global suppression. Simplest to say it desires to suppress as few occurrences of an item as possible instead of globally suppress all occurrences of an item. In this paper we consistently use definitions in $\rho$-uncertainty \cite{Cao:2010:rho}. To give a concrete example using $partial$, given a sensitive inference $K \rightarrow \alpha$ appears in data $T$, $K$ is the combination of either sensitive an non-sensitive items as prior d knowledge, $\alpha$ is a sensitive item. If the \textbf{confidence?} of this inference is above $\rho$, $partial$ may fix this inference by deleting some occurrences of $\alpha$ in transactions containing $K$, then this sensitive inference's confidence may less than the threshold $\rho$ \textbf{(Is this example clear)}.
%
%$Partial$ doesn't do assumptions about the downside applications, while applications can be both data statistics and data mining tasks. As we know, it is true that $partial$ draws controversy \cite{Xu:2008:ATD,Cao:2010:rho,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding} for its inconsistent change in itemset's support and may create spurious rules in rule mining. $Partial$ tries to minimize the items suppressed, as a result not only well preserves the original data's distribution, also keeps many rules even with a small amount of tolerable spurious rules created.
%
%About the complexity of $partial$, since $partial$ needs to ensure all confidences of sensitive inferences using all prior knowledge below threshold $\rho$, and the combination of different items as prior knowledge is exponential, so the problem is a really hard one (see Section \textbf{Analysis}). What's more, there is a situation that the already fixed sensitive inference would violate again (later call it $regression$) during the process fixing other sensitive inferences \textbf{Give a example?}. So one sensitive inference may be repeatedly fixed by $partial$. $Partial$ potentially is a very time consuming and heavy memory consumption process. Compared with global suppression, it is also a heavy memory load approach, but the scale of combinations decreases dramatically since all occurrences of one item is suppressed. Also global suppression won't cause $regression$s. However in this paper, we design an iterative paradigm using a fixed size buffer as prior knowledge stores, and make $partial$ as a consumer to fix sensitive inferences one by one. And use partition policy to speedup the $partial suppression$ if necessary. The termination condition is the confidences of all sensitive inferences are below $\rho$.
%
%To protect sensitive inferences attack (similar to attribute linkage attack \cite{FungWCY10:Survey}) in set-valued data, we follow the steps of rho-uncertainty \cite{Cao:2010:rho} and extend definitions for set-valued data to a formal generalized setting (see section \textbf{The Problem Section}. Then we design an iterative paradigm to fix all sensitive inferences, use a fixed buffer as a pool of prior knowledge to restrict/customize the memory cost, use prune techniques to remove redundant work and finally devise an time cost estimation function to determine to partition data to speedup $partial$ or not (see detail in Section \textbf{Algorithm}). At last we do abundant experiments on different datasets to show both our $partial$'s efficiency and effectiveness. We do various comparisons between our results with $\rho$-uncertainty's results on item suppressed, distribution similarity with original dataset, and number of rules mined (see Section \textbf{Experimental Results}).
%
%compare with (h,k,p)
%compare with rho
%stress that we are publishing data for unknown downside, while rho-uncertainty why not directly publishing all rules for rule mining?? may not that clearable
