\section{Introduction}
\label{sec:intro}
%\begin{itemize}
%
%\item Motivation of the problem (why is set-valued privacy problem important)
%
%\item traditional approaches (state of the art) and their deficiencies.
%
%\item Challenge of the problem (why is it hard? Exponential space,
%NP-hard, etc. E.g. it takes a server 2 days to solve a problem size of only
%10 records using naive approach.)
%
%\item Briefly introduce our approach and why it's better
%
%\item Summarize the main contributions of the paper (and refer to the relevant
%sections instead of explicitly naming the organization of the paper)
%\end{itemize}

%Privacy-preserving data publishing aims to publish data
%in a hostile environment so that the published data
%remains practically useful while individual privacy is
%preserved \cite{FungWCY10:Survey}.
%Set-valued data are %valuable data sources
%generated in such occasions as retail transactions, web search sessions,
%web browsing and medical activities where a lot of private and sensitive
%information is involved.
%While a relational data source is
%a set of rows of equal cardinality and schema,
%a set-valued data source consists of multiple records, each of which is
%a set of items drawn from a finite domain. Table \ref{tab:retail-ex} shows
%an example of retail transaction data in which each record (row) represents
%a set of items purchased in a single transaction by an individual.

Set-valued data sources are valuable in many data mining and
data analysis tasks. For example, retail companies may want to know what
items are top sellers (e.g., milk), or whether there is an association
between the purchase of two or more items (e.g., people who buy flour also
buy milk), to help them make better decisions in purchasing, marketing and
even store arrangements. Search engines may analyze the pattern of queries
issued and sites clicked within the same user session to identify good
keywords and indexing strategies. In many cases, analysis tasks are not
performed by the organizations or individuals that produce the original
datasets, but are instead {\em outsourced} to other external companies or
individuals. In other cases, data are simply {\em published} to the general
masses for scientific and public research purposes.

According to our observation there are two main categories of set-valued
data analysis: one is {\em statistical analysis}; 
the other is {\em mining of association rules}. 
In the first case, users simply compute the statistics of the data such as max,
min, average and standard deviation, etc. In this case, the distribution of
the data is the useful information.
In the second case, data users want to establish useful
association rules among item sets, with a certain support,
so rules are the useful information that should be retained. 

Publishing set-valued, and especially transactional data,
can pose significant privacy risk to the
individuals involved in those transactions recorded in the data, even though
there is no explicit identification of the individuals.
Set-valued transactions consist of one or more data items, which can be divided into two categories:
{\em non-sensitive} (or public) and {\em sensitive} (or private).
%\cite{Xu:2008:ATD}.
Privacy is in general associated with the sensitive items.
Table \ref{tab:orig-sample} shows
an example of retail transactions in which each record (row) represents
a set of items purchased in a single transaction by an individual,
  all the items are non-sensitive, except the {\em condom}
which is sensitive. An individual's privacy is breached if he or she can be
{\em re-identified}, or associated with a record in the data which contains
one or more sensitive items. Past research has shown that such breach is
possible through {\em linking attacks} \cite{FungWCY10:Survey}. For example,
from Table \ref{tab:orig-sample}, the attacker can infer that anybody who buys
chocolate and milk also buys the condom.
This is evident from the fact that
\[{\rm Prob}(condom ~|~ milk) = 2/3\]
By this inference (with a confidence of 2/3), the attacker {\em links}
milk with the condom, and if he also possesses the
background knowledge that Alice, an individual who is in the data, has indeed
bought milk, then he can conclude with high confidence that
Alice bought the condom as well.

%\begin{table}[th]
%\centering
%\caption{A Retail Transaction Dataset}\label{tab:retail-ex}
%\begin{tabular}{|c|l|} \hline
%{\bf TID} & {\bf Items} \\ \hline
%1 & yogurt, milk \\ \hline
%2 & milk, flour, potato chips \\ \hline
%3 & bread, milk, fruits \\ \hline
%4 & flour, milk \\ \hline
%5 & chocolate, milk, {\em condom} \\ \hline
%\end{tabular}
%\end{table}

%\begin{table}
%\caption{A Retail Transaction Dataset}
%\centering
%\subtable[Original]{
%\begin{tabular}{|r|l|} \hline
%{\bf TID} & {\bf Items} \\ \hline \hline
%1 & yogurt, milk \\ \hline
%2 & milk, flour, potato chips \\ \hline
%3 & bread, milk, fruits \\ \hline
%4 & flour, milk \\ \hline
%5 & chocolate, milk, {\em pregnancy tester} \\ \hline
%\end{tabular}
%\label{tab:original}
%}
%\subtable[Anoymized Dataset]{
%\begin{tabular}{|r|l|} \hline
%{\bf TID} & {\bf Items} \\ \hline \hline
%1 & yogurt, milk \\ \hline
%2 & milk, flour, potato chips \\ \hline
%3 & bread, milk, fruits \\ \hline
%4 & flour, milk \\ \hline
%5 & chocolate, milk\\ \hline
%\end{tabular}
%\label{tab:Dataset1}
%}
%\end{table}


\begin{table*}[th]
\caption{A Retail Dataset and Anonymization Results}
\label{tab:sample}
\centering
\subtable[Original Dataset]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, {\em condom} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, {\em condom}  \\ \hline
4& flour, fruits  \\ \hline
5& flour, {\em condom}\\ \hline
6& bread, fruits  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:orig-sample}
}
\subtable[Global Suppression]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, \sout{\em condom} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, \sout{\em condom}  \\ \hline
4& flour, fruits  \\ \hline
5& flour, \sout{\em condom}\\ \hline
6& bread, fruits  \\ \hline
7& fruits, \sout{\em condom}  \\ \hline
\end{tabular}
\label{tab:sample2}
}
\subtable[Our Approach 1]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, \sout{{\em condom}} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, {\em condom}  \\ \hline
4&flour, fruits  \\ \hline
5& \sout{flour}, {\em condom} \\ \hline
6& bread, fruits  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sample3}
}
\subtable[Our Approach 2]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, \sout{{\em condom}} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, {\em condom}  \\ \hline
4&flour, fruits  \\ \hline
5& flour, \sout{{\em condom}}\\ \hline
6& bread, fruits  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sample4}
}
\end{table*}

%To protect individuals privacy from linking attacks, data must be {\em
%anonymized} first before publishing. This is a process which alters the
%original data in such a way that no strong inference about any sensitive
%items can be obtained from the resulting anonymized data. Because the data
%publisher cannot make any assumption about the amount of background knowledge
%an attacker may have, they must prepare for the worst and remove {\em all}
%the sensitive strong inferences.

The privacy model we want to achieve is called $\rho$-uncertainty, where
no sensitive association rules can be inferred with a confidence higher than
$\rho$ \cite{Cao:2010:rho}.
There are four general approaches to achieve this goal.
One popular approach is {\em suppression} \cite{Xu:2008:ATD,Cao:2010:rho}
which involves deleting some of the data items. For instance, by deleting
condom from Table \ref{tab:orig-sample}, Alice's privacy is
preserved. Another popular approach is {\em generalization}
%which is also
%widely used in the anonymization of relational data
\cite{Cao:2010:rho}. In generalization, data items are ``generalized'' to a
different item or concept which subsumes the original item. Such subsumption
relation is governed by a pre-defined item taxonomy. For instance, milk and
yogurt can be generalized to ``dairy product''. The third approach is {\em
permutation} (or called {\em anatomization})
\cite{Xiao:2006:Anatomy,2011:TKDE:Anonymous} which separates the non-sensitive
and sensitive attributes into two schemes, while leaving non-sensitive
attribute values unchanged and offering aggregated values for sensitive
attributes among each group. The
fourth approach is {\em perturbation} \cite{ChenMFDX11:Diff}
which changes some of the items into
something completely different or adds other items into the data as ``noises''.

%We want to \textcolor{red}{find an optimal way that inflicts the {\em
%minimal} change to the original data, because we would like the recipient of
%the data to make maximal {\em utility} of the data}. In other words, the data
%anonymization is an optimization problem of minimizing {\em information loss}
%while preserving the users privacy.

%\textcolor{red}{?Is this true, can our
%work beat differential privacy and Anatomy in this kind of analysis? The
%argument may start from the restriction of differential privacy and Anatomy
%tech since they restricted by knowing the specific kind of statistics in
%advance. Can you compare our result with some specific differential privacy
%result or Anatomy result?}


%%
%% Following para is considered unnecessary. (Xiao, Chao)
%There are  many different ways to change a given dataset so that no
%strong inference can be obtained.

In this paper, we focus on the suppression approach.
Most existing work in this field uses a technique called ``global
suppression'' \cite{Cao:2010:rho} in which once an occurrence of an item $t$ is
determined to be removed from one record, all occurrences of $t$ are
removed from the whole dataset. We instead opt to {\em partially}
suppress the data set so only {\em some} occurrences of item $t$ are
deleted.
Table \ref{tab:sample} shows an example dataset
and three anonymized datasets produced by global suppression and our approaches.
The orginal dataset is not safe because sensitive rules $\{bread,~milk\} \rightarrow condom$, 
$flour \rightarrow condom$ and $fruits \rightarrow condom$ can be
inferred with confidence great than 1/3, which is our threshold.
Table \ref{tab:sample2} is the anonymized dataset where all the occurrences of
the sensitive item {\em condom} are deleted due to global suppression.
Table \ref{tab:sample3} shows the anonymized dataset produced by our first
approach, which is optimized to preserve data distribution, so different
items ({\em condom} and flour) are deleted to make the dataset safe.
Table \ref{tab:sample4} is the result of our second approach, which is optimized
to preserve important data association in the original dataset, so only
two occurrences of the item {\em condom} are deleted for safety.
Even in such a small dataset, both our approaches outperform global suppression
in the number of deletions (2 vs. 4) while retaining useful information at the same time.

To the best of our knowledge, the partial suppression technique
has not been studied in the context of set-valued data anonymization before.
We choose to solve the set-valued data anonymization problem by partial
suppression for the following reasons.

First, global suppression tends to delete more items than necessary,
and the removal of all occurrences of the same item not only changes the
data distribution significantly but also makes mining association rules
about the deleted items impossible.

Second, generalization itself losses
information, e.g., by generalizing yogurt and milk both into dairy products,
there is no distinction between the two items. The association rules thus
learned will be very different from those learned from the original data. The
support for those rules will be affected unexpectedly as well. This renders
rule mining almost impossible with generalized datasets. Furthermore,
generalization techniques require additional information (the taxonomy) which
is proprietary to the data publisher and may not be available or agreeable to
the data users.

Third, permutation assumes the downstream
application in advance and is highly restricted by the assumption that the
external knowledge can only be {\em nonsensitive} items.
On the contrary, partial suppression has the best potential of
preserving both rules and distribution of the original data.

Fourth, perturbation alters the original
data in more unpredictable ways as it introduces random noises.
This may not be acceptable to the unknown downstream applications.
%since perturbation
%is restricted by the specific downstream application.
Also, it mainly targets differential privacy model \cite{ChenMFDX11:Diff},
rather than the linking attack, which is our main concern in this paper.

The problem of anonymization by suppression (global or partial) is very
challenging \cite{atallah99:disclosure,
Xu:2008:ATD}, exactly because, (i) the number of possible inferences from
a given dataset is exponential, and (ii) the size of the search space, i.e.
the number of ways to suppress the data is also exponential to the number of
data items. We therefore propose two heuristics in this paper to anonymize
input data in two different ways, giving rise to the two kinds of output
in \tabref{tab:sample}.  %\XJ{not sure whether the reason is true}
%\textcolor{red}{In fact, we experimented with finding the optimal
%solution for anonymizing a very small data set of merely 15 records and a
%total of 75 items. A straight-forward search of all possible suppression
%solutions takes two and half days on a reasonably powerful server!}

%%
%% rephrased and moved to other paras/secs (Xiao)
%%
% In this paper, we propose an iterative partial suppression framework which
% (i) ensures the confidence of any association rule that positively
% links to a sensitive item is below a threshold, and
% (ii) prevents the anonymized data from both
% {\em Minimalitiy attack} \cite{Wong:2007:Minimality} and {\em Composition
% attack} \cite{Ganta:2008:Composition}.
% %\MakeRed{In each iteration, our algorithm deletes at least one item
% %from the dataset, and since the dataset is finite, the algorithm is
% %guaranteed to terminate (remove it).}
% In addition, a divide-and-conquer approach can be employed
% by approximating the global solution with the
% combination of anonymization of a number of smaller subsets of the data. Our
% experiments show that this technique is very effective in producing a
% ``safe'' data set with fewer suppressions than previously reported methods
% while better preserving the original data distribution and useful
% (non-sensitive) association rules without introducing many spurious rules.
% The framework adopts a ``pay-as-you-go'' approach, and can be adapted to
% achieve both space-time and quality-time trade-offs.

The main contributions of this paper are as follows.
\begin{enumerate}
\item To the best of our knowledge, we are the first to propose an
    effective \emph{partial} suppression framework for anonymizing
    set-valued data (Section \ref{sec:prob} and
    \ref{sec:algo}), which %, to the best of our knowledge, is first such
    %attempt in privacy preserving data publishing research, and
    prevents
    the anonymized data from not only {\em record/attribute linking
    attack} \cite{FungWCY10:Survey} but also both {\em minimality
    attack} \cite{Wong:2007:Minimality} and {\em composition attack}
    \cite{Ganta:2008:Composition} (Section \ref{sec:related}).
%    \PC{Why don't we mention that we are the first?}
\item We adopt a ``pay-as-you-go'' approach based on divide-and-conquer,
    which can be adapted to achieve both space-time and quality-time
    trade-offs (Section \ref{sec:algo} and \ref{sec:eval}). We also devise two
    categories of novel heuristics to either preserve the original data
    distribution or retain mineable useful rules with limited spurious
    rules invented, while minimizing the items deletions (Section
    \ref{sec:algo}).
\item We conduct extensive experiments to validate key properties of the
    algorithm, to compare the performance of our algorithm with
    state-of-the-art suppression and generalization techniques, and to
    show that our algorithm out-performs the peers in preserving the
    original data distribution (more than 100 times better
    than previous work on average)
    or retaining mineable useful association
    rules with limited spurious rules invented (retain 70\% rules
    but introduce only 8\% spurious rules on average),
     while minimizing the item
    deletions (a 30\% improvement on average), by large margins and
    achieves this in reasonable time (Section \ref{sec:eval}).
\end{enumerate}

%\XJ{move this para to algo sec}
%\MakeRed{Our main idea is although the total number
%of ``dangerous'' inference rules maybe the worst case exponential in the original
%data, incremental ``invalidation'' of some of the rules through partial
%suppression of small number of affected items can dramatically decrease the
%number of these bad inferences, which leads to quicker convergence to a solution.}

%
%includes shopping transactions, web search queries, click streams, medical activities and other transaction data. Set-valued data is a multi-set of transactions, each transaction is a set of items drawn from a universe of items. Items are distinguished by sensitive (private) and non-sensitive (public). Set-valued data is shared to support all kinds of data mining and data analysis tasks, including mining association rules, predicting user interests, and doing other statistics \cite{Cao:2010:rho, DBLP:journals/pvldb/ChenMFDX11}. However direct publishing original set-valued data threatens to breach individual's privacy through linking attacks \cite{FungWCY10:Survey}. Example is that if an attacker has partial knowledge $PK$ of a certain victim's transaction $t$, by combining $PK$ with the released data $T$, the attacker may locate $t$ from $T$ and obtain other sensitive information belonging to $t$. Through this way the individual's privacy is violated. As a result it's very important and necessary for data publisher to do the anonymization before publishing.\\
%
%There are diverse ways \cite{Xu:2008:ATD, Terrovitis:2008:PAS, He:2009:ASD, Cao:2010:rho, DBLP:journals/pvldb/ChenMFDX11} already proposed to anonymize set-valued data.
%Most of existing research \cite{Xu:2008:ATD, Terrovitis:2008:PAS, He:2009:ASD, Cao:2010:rho} use deterministic techniques to anonymize set-valued data.
%Yabo Xu et al. \cite{Xu:2008:ATD} propose an efficient algorithm which eliminates all damaging prior knowledge that violate (\textit{h}, \textit{k}, \textit{p})-coherence from database. They \cite{Xu:2008:ATD} assume prior knowledge is all public items and within the power of $p$, they use a greedy global suppression on public items and measure information loss by amount of items suppressed.
%J. Cao et al. \cite{Cao:2010:rho} introduce $\rho$-uncertainty to protect sensitive item inference. Although paradigm of global suppression in \cite{Cao:2010:rho} is similar to the greedy algorithm of Yabo Xu's \cite{Xu:2008:ATD}, they distinguish items from sensitive between non-sensitive, and allow sensitive items in prior knowledge, to make it a more natural problem setting. J. Cao et al. \cite{Cao:2010:rho} not only use global suppression on sensitive items, also use a selective mixed scheme of global suppression on sensitive items and generalization on non-sensitive items as approaches.
%M. Terrovitis et al. \cite{Terrovitis:2008:PAS} define $k^m$-anonymity to prevent a transaction from been recognized in a set of k published transactions. $k^m$-anonymity \cite{Terrovitis:2008:PAS} is a loosen version of $k$-anonymity, since attacker's prior knowledge is upper-bounded by m. M. Terrovitis et al.
%While Y. He et al. \cite{He:2009:ASD} propose the real $k$-anonymity model in the following year for set-valued data publishing.
%$k^m$-anonymity\cite{Terrovitis:2008:PAS} and $k$-anonymity \cite{He:2009:ASD} assume all items are equally to be sensitively, so they employ generalization on all items if possible. While this assumption is not true in nature. On the other side, $k$-anonymity can well protect record linkage attack \cite{FungWCY10:Survey,Kifer:l-diversity}, but cannot well guard attribute linkage attack \cite{FungWCY10:Survey,Kifer:l-diversity}. Attribute linkage attacker is described as attacker may not need to precisely identify victim's transaction but to infer his/her sensitive information \cite{FungWCY10:Survey,Kifer:l-diversity}. Example is even though a transaction is crowded in a identical $k$ transaction group, while if there is a high percent that all $k$ transactions contain a common sensitive item, then attacker may infer the victim also has a high probability having this sensitive item. Privacy is not guaranteed by $k$-anonymity in this way.
%
%On the contrary to deterministic approaches, Rui Chen et al. \cite{DBLP:journals/pvldb/ChenMFDX11} addressed the privacy concern \cite{Kifer09attackson, Narayanan:2008:RDL, Wong:2009:AAP} for their deterministic nature and propose to use differential privacy \cite{Dwork08:diff:survey, Dwork:2006:diff} to publish set-valued data. Rui Chen et al. \cite{DBLP:journals/pvldb/ChenMFDX11} propose a probabilistic top-down partitioning algorithm based on a context-free taxonomy tree.
%
%As we see, most previous research work use generalization and global suppression techniques to anonymize set-valued data.
%Generalization preserves the correctness of data, but it compromises data's accuracy roughly along the generalization hierarchy. When generalize set-valued data for rule mining, the amount of rules highly decreases and the generalized rules are much less specific (see Section \textbf{Experimental section id}). We doubt the real utility of those few even less specific rules, and doubt the ambiguous inferences it would lead.
%
%To protect the consistency of residual itemsets' supports \cite{Xu:2008:ATD}, for example if items $a$ and $b$ exist in anonymized data, then the supports of $a$, $b$, and $ab$ in anonymized data should be the same in original data. Global suppression technique follows this constraint and removes all occurrences of one item if it is determined to remove. The good side is it would't affect other itemsets' support, while the bad side is it is a dramatic deletion which would cause much unnecessary information loss. Same conclusion as generalization technique, many rules would be lost during the dramatic deletion (also see Section \textbf{Experimental section id}).
%
%There is no doubt on the importance to keep the consistency of residual itemsets' supports. But it has a strong prediction the downside application is rule mining. Assume the anonymized data is to be used in rule mining, few rules are kept with global suppression's crude deletion, so what's the real utility of global suppression result?
%
%In this paper, we propose a $partial suppression$ framework (later call it $partial$) to anonymize set-valued data. $Partial$ is the opposite side of global suppression. Simplest to say it desires to suppress as few occurrences of an item as possible instead of globally suppress all occurrences of an item. In this paper we consistently use definitions in $\rho$-uncertainty \cite{Cao:2010:rho}. To give a concrete example using $partial$, given a sensitive inference $K \rightarrow \alpha$ appears in data $T$, $K$ is the combination of either sensitive an non-sensitive items as prior d knowledge, $\alpha$ is a sensitive item. If the \textbf{confidence?} of this inference is above $\rho$, $partial$ may fix this inference by deleting some occurrences of $\alpha$ in transactions containing $K$, then this sensitive inference's confidence may less than the threshold $\rho$ \textbf{(Is this example clear)}.
%
%$Partial$ doesn't do assumptions about the downside applications, while applications can be both data statistics and data mining tasks. As we know, it is true that $partial$ draws controversy \cite{Xu:2008:ATD,Cao:2010:rho,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding} for its inconsistent change in itemset's support and may create spurious rules in rule mining. $Partial$ tries to minimize the items suppressed, as a result not only well preserves the original data's distribution, also keeps many rules even with a small amount of tolerable spurious rules created.
%
%About the complexity of $partial$, since $partial$ needs to ensure all confidences of sensitive inferences using all prior knowledge below threshold $\rho$, and the combination of different items as prior knowledge is exponential, so the problem is a really hard one (see Section \textbf{Analysis}). What's more, there is a situation that the already fixed sensitive inference would violate again (later call it $regression$) during the process fixing other sensitive inferences \textbf{Give a example?}. So one sensitive inference may be repeatedly fixed by $partial$. $Partial$ potentially is a very time consuming and heavy memory consumption process. Compared with global suppression, it is also a heavy memory load approach, but the scale of combinations decreases dramatically since all occurrences of one item is suppressed. Also global suppression won't cause $regression$s. However in this paper, we design an iterative paradigm using a fixed size buffer as prior knowledge stores, and make $partial$ as a consumer to fix sensitive inferences one by one. And use partition policy to speedup the $partial suppression$ if necessary. The termination condition is the confidences of all sensitive inferences are below $\rho$.
%
%To protect sensitive inferences attack (similar to attribute linkage attack \cite{FungWCY10:Survey}) in set-valued data, we follow the steps of rho-uncertainty \cite{Cao:2010:rho} and extend definitions for set-valued data to a formal generalized setting (see section \textbf{The Problem Section}. Then we design an iterative paradigm to fix all sensitive inferences, use a fixed buffer as a pool of prior knowledge to restrict/customize the memory cost, use prune techniques to remove redundant work and finally devise an time cost estimation function to determine to partition data to speedup $partial$ or not (see detail in Section \textbf{Algorithm}). At last we do abundant experiments on different datasets to show both our $partial$'s efficiency and effectiveness. We do various comparisons between our results with $\rho$-uncertainty's results on item suppressed, distribution similarity with original dataset, and number of rules mined (see Section \textbf{Experimental Results}).
%
%compare with (h,k,p)
%compare with rho
%stress that we are publishing data for unknown downside, while rho-uncertainty why not directly publishing all rules for rule mining?? may not that clearable
