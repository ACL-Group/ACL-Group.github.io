\section{Problem Definition}
\label{sec:prob}

This section first defines a few preliminary concepts and then
introduces the privacy model before formally describing
the problem of partial suppression.

\subsection{Preliminaries}
%A {\em multiset} $S$ is a set which allows repetitive elements, while the
%{\em power multiset} $\mathbb{N}^S$ is the set of all subsets of the multiset
%$S$.
%Examples or formal definitions of these are given below.
%\begin{description}
%  \item[Multiset] $[a,a,b]$ is the same as $(\{a,b\},\{(a,2),(b,1)\})$
%  \item[Power set] $2^S$ is the power set of the set $S$
%  \item[Power multiset] $\mathbb{N}^S$ is the power multiset of the set $S$
%\end{description}

%A {\em multiset} is a set which allows repetitive elements, while a {\em
%power set} is the set of all subsets of a set. Examples or formal definitions
%of these are given below.
%\begin{description}
%  \item[Multiset] \hspace{2em} $[a,a,b]$ is the same as
%      $(\{a,b\},\{(a,2),(b,1)\})$
%  \item[Power set]  \hspace{2em}  $2^S$ is the power set of the set $S$
%  \item[Power multiset] \hspace{4em} $\mathbb{N}^S$ is the power multiset
%      of the set $S$
%\end{description}

\begin{table}[th]
\centering
\caption{Notations}
\label{table:problem_notations}
\begin{tabular}{m{0.28\columnwidth}|m{0.6\columnwidth}}
  \hline
  \textbf{Symbol} & \textbf{Definition} \\
  \hline
$T$ %\in\natnum^{2^D}$
	& a set-valued table, which is a multiset of $m$ transaction records \\ \hline
  $D(T)$ & the domain of all items in $T$ \\ \hline
  $D_S(T)$ & the domain of sensitive items \\ \hline
  $D_N(T)$ & the domain of non-sensitive items \\ \hline
   % $T[i]\in T$ & the $i$-th record of $T$ \\ \hline
  $R$ & a transaction record in $T$ \\ \hline
 % $|R|$ & the number of items contained in $R$ \\ \hline
  $I$ & an itemset, which is a subset of $D(T)$\\ \hline
  $e$ & an item in $T$ and $e \in D(T)$ \\ \hline
  $q$ & a \emph{quasi-identifier} (also \qid), which is any itemset
(\textbf{including sensitive items}) drawn from any record in table $T$ \\ \hline
%  $e\in D$ & \KZ{a data item} \\ \hline
  $ X \rightarrow Y $ & an association rule where $X \subset R$, $Y \subset R$
and $X \cap Y = \emptyset$\\ \hline
  $nr(T)$ & set of all non-sensitive associations rules mineable from $T$
with sufficient support and confidence \\ \hline
  $\enum(R) = 2^R - \{\emptyset\}$ & the \qid enumeration of $R$, which is the power set of $R$ except the $\emptyset$ \\ \hline
  $\displaystyle Q(T)=\bigcup_{R\in T} \enum(R)$ & the set of all \qids in table $T$ \\ \hline
%  $\rho$ & the strong association rule threshold \\ \hline
  %$\mathcal{A}(q,e)$ & an inference/association rule $q\rightarrow e$\\  \hline
%  $\SA(q,e)$ & a sensitive association rule $\mathcal{A}(q,e)$  if \KZ{$e$ contains at least one sensitive item, or is a sensitive item?}\\ \hline
  $sup_{T}(I)$ & the support of $I$ is the number of transactions $t\in T$ such that $I\subset t$\\ \hline
  $conf_{T}(X \rightarrow Y)$ & the confidence of
$X \rightarrow Y$ from table $T$, given by $sup_T(X \cup Y)/sup_T(X)$\\ \hline
\end{tabular}
\end{table}

%A set-valued table $T$ is a multiset of transaction records,
%  each record $R \in T$ is a set of items drawn from domain $D$.
%  $D$ is the union of two non-intersecting set, sensitive domain $D_S$ and non-sensitive domain $D_N$.
%We follow the step of \cite{Sweeney2002:k-anonymity} and
%  extend the definition \emph{quasi-identifier} ($qid$) in relational database for set-valued data.
%Then we give a series of other definitions related with $qid$.
%As simple as you can imagine, a $qid$ is just a set of items taken from $D$.
%$Q$ is a set of $qid$s, $\Omega(R_i)$ is the $qid$ enumeration of $R$ which is the power set of $R$ except the $\emptyset$.
%The column count $cc$ of row $R$ is the number of items contained in $R$.
Table \ref{table:problem_notations} lists some notations used in the
rest of this paper. We define {\em container} and {\em linked items}
as follows.

\begin{definition}[Container]
The \emph{container} of an itemset $I$ in table $T$ is defined as
\begin{equation}
\container_T(I) = \{ i ~|~ I \subseteq T[i],~ 1 \leq i \leq |T| \}
.
\end{equation}
\end{definition}

\begin{definition}[Linked Items]
The set of all sensitive items linked by a \qid $q$ in table $T$ is defined as
\begin{equation}
\linked_T(q) = \{ e ~|~ e \in D_S(T) ~\backslash~ q,~ sup_{T}(q\cup\{e\}) > 0 \}.
\end{equation}
\end{definition}

According to the definition, $sup_{T}(I)$=$|\container_T(I)|$.
Also we will use $\container(I)$, $\linked(q)$, $sup(I)$,
$conf(X \rightarrow Y)$ to represent $\container_T(I)$,
$\linked_T(q)$, $sup_{T}(I)$ and $conf_{T}(X \rightarrow Y)$ respectively
when $T$ is the only table within discussion.

\subsection{Privacy Model}
$X \rightarrow Y$ is a {\em sensitive association rule} iff $Y$ contains
at least one sensitive item.
The principle privacy model in this paper
maintains that a table is {\em safe} iff no sensitive rules can be inferred
with a confidence higher than threshold $\rho$ \cite{Cao:2010:rho}.
%Let $T$ be a set-valued table and the domain $D$ is
%divided into two subsets: the sensitive domain $D_S$ and the non-sensitive
%domain $D_N$. We assume that an attack will know any \qids $q$ such that
%$q\subset R, where~ R \in T$. The dataset is safe if and only if the attack
%will not infer any items , with a high probability (e,g,$\geq \rho$),
% $e$ from $D_S$ such that if $q \subset R$ then $e\in R$. Such an inference can be defined
%as a sensitive inference or sensitive association rule if $e \in D_S$,
% Our objective is to prevent the attack from mining any sensitive association rules.

%The model defined is immune to
%{\em
%record/attribute linkage attack} \cite{FungWCY10:Survey}
%However, we  prove that our technique promises a strong privacy result immune from  {\em Minimalitiy
%attack} \cite{Wong:2007:Minimality} and {\em Composition attack}
%\cite{Ganta:2008:Composition}
%To reach our objective, we have to keep the $conf(q,e)$ lower than $\rho$
%for any $\mathcal{A}(q,e)$. Therefore, we introduce the following definitions.
We say sensitive association rule $X \rightarrow Y$ inferred from table $T$
is {\em safe} iff $conf(X \rightarrow Y) \leq \rho$. It is easy to show that

\begin{equation}
conf(X \rightarrow Y) \leq \min_{e \in Y} conf(X \rightarrow e),
\end{equation}
where $e \in D_S$. Therefore, if all sensitive association rules with
one-item consequent from $T$ are safe then all sensitive association rules
are safe and hence $T$ is safe. Next we formalize this privacy requirement.

\begin{definition}[Breach Probability]
\label{def:probability} The \emph{breach probability} of a \qid $q$ is
\begin{equation}
\breach(q) = \max_{e\in\linked(q)} conf(q \rightarrow e).
\end{equation}
%where $P(e|q) = P(q\rightarrow
%e) = \csize(q\cup \{e\})/\csize(q)$, and $P(e|q) = conf(e,q)$.
\end{definition}

\begin{definition}[Safety of qid]
\label{def:safety_qid}
A \qid $q$ is safe \wrt~$\rho$ if and only if $\breach(q)\leq\rho$.
\end{definition}

\begin{definition}[Safety of Table]
\label{def:safety_table}
A table $T$ is safe \wrt~$\rho$ iff $q$ is safe \wrt~$\rho$ for any
\qid $q\in Q(T)$.
\end{definition}

%Cao \etal \cite{Cao:2010:rho} has proved that by ensuring confidences of all
%sensitive association rules with one consequent below $\rho$,
%confidences of all sensitive association rules with more consequents are also
%made below $\rho$. As a result, Definition \ref{def:safety_table} can satisfy
%the privacy model.


\begin{definition}[Suppressor]
\label{def:suppressor}
Let $T$ be the original table, and $T'$ be the table obtained by deleting
one or more items from $T$, then $T'$ is called the {\em suppressed table}.
A \emph{suppressor} is a function
$S : T \rightarrow T'$ where $T'$ is a suppressed table which is
safe \wrt~ $\rho$.
\end{definition}

Of course, there are many different ways to suppress a table. The goal
is to find a suppressor that maximizes the {\em utility} of the suppressed
table.
%\begin{property}
%\label{prop:container_size} For two\qids $p$ and $q$, if $p\subseteq q$ then
%$\container(q)\subseteq\container(p)$ and $\csize(q)\leq \csize(p)$.
%\end{property}
%
%\begin{proof}
%  $\forall i\in\container(q), q\subseteq T[i]\Rightarrow p\subseteq q\subseteq T[i] \Rightarrow i\in\container(p)$,
%  so $\container(q)\subseteq\container(p)$. Hence, $\csize(q)\leq \csize(p)$.
%\end{proof}
%
%Property \ref{prop:container_size} guarantees that $P(e|q)$ in Definition
%\ref{def:probability}, we can only consider sensitive association rules with
%a singleton consequent then all

\subsection{Data Utility}
\label{sec:du}
%Xu \etal \cite{Xu:2008:ATD} requires the data publisher assigning a certain
%information loss function to the global suppression of an item $e$, denoted
%$IL(e)$.
%\textcolor{red}{ \emph{Classification Metric}
%\cite{Iyengar:2002:TDS} is better for anonymizing data for classifier
%training. \emph{Normalized Certainty Penalty} \cite{Xu:2006:UAU} is proposed
%for anonymization using generalization. Remove them?}
%Cao \etal \cite{Cao:2010:rho} introduces a metric
%for computing the information loss
%caused by generalization.
%We have adopted the information loss metric used by Xu \etal \cite{Xu:2008:ATD}
%and have revised it for the convenience of comparative evaluation of our
%algorithms against other state-of-the-art methods.
In this paper, we identify two major uses of an anonymized table: 
statistical analysis and association rule mining. 
In the first case, we want the anonymized table to have a distribution 
as close to the original table as possible, which can be measured by 
the {\em Kullback-Leibler divergence} 
(a.k.a relative entropy) \cite{kl-divergence} 
between the original distribution and the new 
distribution.  
In the second case, 
we would like the anonymized data to
retain all non-sensitive association rules while introducing few 
or no spurious rules,
which can be measured by the {\em Jaccard similarity} \cite{jaccard-sim}
between the rule set before and after the suppression.
In both scenarios, there is also a common goal which is
to minimize the {\em information loss}, i.e.,
the total number of items suppressed from the original table.

With these two scenarios in mind, we define two variants of an objective function $f(T, T')$ as:
\begin{equation}
f(T, T') =
\begin{cases}
NS(T, T')\cdot KL(T'~||~T) & \rm{(data~ distribution)} \\
& \\
\frac{NS(T, T')}{J(nr(T), nr(T'))} & \rm{(rule~ mining)}
\end{cases}
\end{equation}
where
\begin{eqnarray}
NS(T,T') &=& \frac{\sum_{e\in D(T)}(sup_T({e}) - sup_{T'}({{e})})}{\sum_{e\in D(T)}sup_{T}(\{e\})} \\
KL(P~||~Q)&=&\sum_{i}Q(i)log\frac{Q(i)}{P(i)} \label{eq:kl}\\
J(A, B) &=& \frac{|A \cap B|}{|A \cup B|}.
\label{eq:kl-dis}
\end{eqnarray}
Here the functions $NS$, $KL$ and $J$ represent total number of suppressions 
(normalized to 1), K-L divergence and Jaccard similarity, respectively. K-L
divergence measures the distance between two probability distributions, while
Jaccard similarity measures the similarity between two sets.
%
%The exact definition of $f$ depends on specific downstream utility of
%the data \cite{Xu:2008:ATD}. In this paper, we assume there are two variants of
%$f$, namely $f_{mine}$ which focuses on preserving the association rules
%mineable from the data, and $f_{dist}$ which focuses on preserving the
%data distributions. These two variants can be defined as,
%
%where $J$ is the Jaccard similarity function:
%\[J(A, B) = \frac{|A \cap B|}{|A \cup B|}\]
%and $KL$ is the Kullback-Leibler divergence which measure the
%similarity between two distributions:
%\[KL(P||Q)=\sum_{i}Q(i)log\frac{Q(i)}{P(i)}.\]
%
%is the \emph{symmetric relative entropy} which measures the change in
%data distribution. The third is the \emph{number of rules mined}
%(including original and spurious rules) from the anonymized data. The first
%measure is more general and used by other work. The last two metrics target
%the utility of the anonymized data for statistical analysis and rule mining,
%and they will be introduced in Section \ref{sec:eval}.

%Information loss of $T$ is essentially the number of items deleted in $T$
%divided by total number of items in $T$.
%Our goal is to find a suppressor
%defined in Definition \ref{def:suppressor} which reduces information loss as
%much as possible.


%\textcolor{red}{ We'll consider these three metrics in our
%heuristic solution later. }
%\begin{definition}[Optimal Suppression Problem]
%\label{def:osp}
%The optimal suppression problem is to find an optimal suppressor $S_\text{OPT}$ for a given table $T$ such that
%\[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor $S$.
%\end{definition}
%\begin{definition}[Minimum suppression]
%Minimum occurrence suppression of item type $t$, to make confidence of
%inference $\mathcal{A}(q,e)$ below $\rho$:
% \hspace{4mm}
%\[MS(t,\mathcal{A}(q,e))=
%\begin{cases}
%sup(q\cap \{e\})-sup(q)\rho & t=e  \\
%\frac{sup(q\cap \{e\})-sup(q)\rho}{1-\rho} & t\in q \\
% \infty & otherwise
%\end{cases} \]
%\end{definition}
%\PC {
%\begin{definition}[Remaining probability of item $i$]
%The Remaining probability of item $i$ is defined as
%\[ remain(i)=\frac{\kappa_{T^\prime}(\{i\})}{\kappa_T(\{i\})} \]
%where $T$ is the original
%dataset and $T^\prime$ is the current dataset processed by suppression but
%not finished
%\end{definition}
%}

\subsection{Optimal Partial Suppression Problem}
The optimal partial supression problem is to find a {\em Partial Suppressor} $S$
which anonymizes an input set-valued table $T$ to minimize
the objective function:
\[\min_S f(T, S(T))\]
such that $S(T)$ is {\em safe} w.r.t. to our privacy model.

In effect, this is an optimization problem that attempts to minimize the
number of supressions while maximizing the similarity between the set of mineable
non-sensitive rules before and after the suppression, or minimizing the distributional distance
before and after the suppression.
%preserves the original data distribution or retains mineable useful
%association rules with limited spurious rules invented, and also minimizes
%item deletions.
%\begin{definition}[Optimal Partial Suppressor]
%The \emph{optimal partial suppressor} $S_\text{OPT}$ is the suppressor such that
%\[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor $S$.
%\end{definition}

%\PC {We have to mention that actually this metric of information loss makes sense, since the value of it is exactly the value
%calculated by the avgloss\cite{Cao:2010:rho} under the condition that only suppression is executed}
%Because most of the existing metrics of information loss are proposed
%for anonymization using generalization,
% \begin{definition}[Optimal Partial Suppressor for Distribution]
% \label{def:distribution}
% \[ Dist_{distance}(S_\text{OPT}(T), T) \leq  Dist_{distance}(S(T), T)\]
% while
% \[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor.
% \end{definition}
% \begin{definition}[Optimal Partial Suppressor for Mining]
% \[ Spurious(S_\text{OPT}(T)) \leq  Spurious(S(T)) \] and
% \[OriginalRule(S_\text{OPT}(T)) \geq OriginalRule(S(T))\]
% while
% \[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor.
% \end{definition}
