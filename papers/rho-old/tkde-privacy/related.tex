\section{Related Work}
\label{sec:related}
%{\bf KZ: please shorten the citatation labels in the bib. Just include the Author + year + brief word to indicate the paper. Also note that don't over cite a paper, e.g. Cao's paper has been cited too many times in this section. Also
%I think you might need to discuss the $k^m$-anonymity a bit more since they are
%also set-valued models?}

Privacy-preserving data publishing of relational tables has been well
studied in the past decade since the original proposal of $k$-anonymity by
Sweeney \etal \cite{Sweeney2002:k-anonymity}.
Since then, many other privacy models and anonymization
techniques \cite{Kifer:l-diversity, Bayardo:optimal-k-anonymity,
Fung:2005:TopdownSpe, LeFevre:2005:Incognito, LeFevre:2006:Mondrian,
Wong:2010:NGP} for relational data were proposed.

Recently, privacy protection of set-valued data has also received increasing
interest. The original set-valued data privacy problem was defined in the
context of association rule hiding \cite{atallah99:disclosure,
tkde:VerykiosEBSD04:ARH, tkde:WuCC07:hiding}, in which the data publisher
wishes to ``sanitize'' the set-valued data (or {\em micro-data}) so that all
sensitive or ``bad'' associate rules cannot be discovered while all (or most)
``good'' rules remain in the published data.
%One of the first attempts at set-valued data anonymization was by Ghinita
%\etal~ \cite{GhinitaTK:2008:sparse}.
%In that work, data publishers only publish the original non-sensitive items
%along with a summary of the frequencies of sensitive items, similar to
%Anatomy \cite{Xiao:2006:ASE}. They also assume that attackers can only
%acquire background knowledge about non-sensitive items.
Subsequently, a number of privacy models
including $(h,k,p)$-coherence \cite{Xu:2008:ATD},
$k^m$-anonymity \cite{Terrovitis:2008:PAS},
$k$-anonymity \cite{He:2009:ASD} and
$\rho$-uncertainty \cite{Cao:2010:rho} have been proposed.
$k^m$-anonymity and $k$-anonymity are models carried over directly from
relational data privacy. They require that any record must be
indistinguishable from a set of $k$ records
%\XH{this is the final effect, the initial require is to make all kinds of prior knowledge indistinguishable from a set of $k$ records, maybe it's ok to say in this way?},
with $m$ being the upper bound of attacker's background knowledge.
$(h,k,p)$-coherence and $\rho$-uncertainty, on the other hand, protect the
data from malicious inferences by bounding the confidence and the support of
any sensitive association rule inferrable from the data. This is
also the privacy model this paper adopts.

In response to these privacy models, a number of anonymization techniques
were developed. These can be generally divided into four categories: {\em
global/local generalization} \cite{Terrovitis:2008:PAS, He:2009:ASD,
Cao:2010:rho}, {\em global suppression} \cite{Xu:2008:ATD, Cao:2010:rho},
{\em permutation} \cite{2011:TKDE:Anonymous} and {\em perturbation}
\cite{Zhang:2007:agg, ChenMFDX11:Diff}. Next we briefly discuss the pros and
cons of these anonymization techniques.

Generalization replaces a specific value by
a generalized value, e.g., ``milk'' by ``dairy product'',
according to a generalization hierarchy \cite{FungWCY10:Survey}.
Global generalization not only converts all occurrences of an item
to a higher level item type $t_h$ in the taxonomy hierarchy,
but also converts all occurrences of items under that the sub-tree of
$t_h$ in the taxonomy to $t_h$ as well. For example,
if an instance of ``milk'' is generalized to ``dairy product'',
then all other instances of ``milk'' as well as all instances of
``yogurt'' and ``cheese'' are also generalized to ``dairy product''.
Local generation refers to the application of the technique
on some occurrences of an item only.
While generalization preserves the correctness of the data,
it compromises its accuracy
and preciseness. Worse still, association rule mining is impossible
unless the data users have access to the same generalization taxonomy
and they agree to the target level of generalization. For instance, if
the users don't intend to mine rules involving ``dairy products'', then
all generalizations to ``dairy products'' are useless.
%with the arbitrary change of support in the rules learned and massive
%reduction in the number of original rules (evident from our experiments
%in Section \ref{sec:eval}). \XH{Rules are only useful when data publisher and data user have the same generalization target level for each item in the final generalization result, but then no need generalization any more, so it's a huge mismatch} Moreover, data users have to have access
%to the generalization hierachy they may or may not agree to.

Global suppression is a technique that deletes some of the items so that the
resulting dataset is safe. However, it removes all occurrences of an item
type even if just one of the occurrences causes the privacy violation.
The advantage of such approach is that it preserves the support of
existing rules that don't involve deleted items and hence retains these rules
\cite{Xu:2008:ATD}, and at the same time it doesn't introduce
additional and spurious association rules.
The obvious disadvantage is that it can cause tremendous unnecessary
information loss. In our work, we focus on partial suppression
which has not been attempted in data anonymization
mainly due to its perceived side effects of changing the support of
inference rules in the original data \cite{Xu:2008:ATD, Cao:2010:rho,
tkde:VerykiosEBSD04:ARH, tkde:WuCC07:hiding}. Our experiments in
this paper has demonstrated that partial suppression introduces very limited
amount of new rules while preserving many more original rules than
the global suppression techniques. In addition, our algorithm has
been shown to preserve the data distribution much better than
other competing methods.

%Partial suppression draws great controversy in
% mainly for its side-effects in rule mining tasks, as its inconsistent change in itemset's support.
%There is no doubt on the importance to keep the consistency of residual itemsets' supports. But it has a strong prediction the downside application is rule mining. What if global suppression kept so few rules while $partial$ offers more rules to find good ones, which is a better idea?

Permutation is introduced by Xiao \etal \cite{Xiao:2006:Anatomy} for
relational data. With generalization technique severely compromising the
accuracy of data aggregation analysis, Xiao \etal propose the
\textit{Anatomy} which releases quasi-identifier and sensitive values in two
separate tables. Specifically quasi-identifier values are not changed and
organized into groups, and for every such group the corresponding sensitive
values are aggregated. After that, Ghinita \etal \cite{2011:TKDE:Anonymous}
extend the permutation technique to publish anonymous transactional data.
Ghinita \etal propose two novel anonymization techniques for sparse
high-dimensional data by introducing two representations for transactional
data. However the limitation is that the quasi-identifier is restricted to
contain only {\em non-sensitive items}, which means their work only focus to
approximately guarantee the utility of associations between quasi-identifier
and sensitive items, but not consider the associations among sensitive items.
Manolis \etal \cite{terrovitis:privacy} developed a new method called disassociation
which also severs the links between values attributed to the same entity but does not
set a clear distinction between sensitive and non-sensitive attributes.
They set those frequent itemsets into a cluster and partition the table into
several parts, which eliminates the rules with
a high confidence and a certain support.
While in our paper, we consider all kinds of associations and try best to
retain the accuracy of those associations
%While in our paper, we consider all kinds of associations and try best to
%retain the accuracy of those associations
%
%\PC{Moreover, the model introduced in their method is not strong. The attack
%effects in the following steps. Step 1:The attacker gains some background
%knowledge such that the number of people buying creams and pregnancy tests
%are around five times more than people buying butter and pregnancy tests. .
%Step 2: The attacker downloads the result processed by permutation and one of
%the group in the result has such form that only contains people buying cream
%or butter with probability of sensitive item pregnancy test $\frac{1}{3}$.
%Step 3: with a simple equation $\frac{1}{3}(x+y)=5Px+Py$, where x represents
%people buying butter and y represents people buying cream, the attacker can
%get the exact probability of people who buy cream buy the pregnancy
%test($5P$) which is likely to be a high value with different x and y.
% }

Perturbation is developed for statistical disclosure control
\cite{FungWCY10:Survey}. Common perturbation methods include {\em additive
noise}, {\em data swapping}, and {\em synthetic data generation}. Their
common criticism is that they damage the data integrity by adding noises and
spurious values, which makes the results of downstream analysis unreliable.
Perturbation, however, is useful in non-deterministic privacy model such as
differential privacy \cite{Dwork:2006:diff, Dwork08:diff:survey}, as
attempted by Chen \etal~ \cite{ChenMFDX11:Diff} in a probabilistic top-down
partitioning algorithm based on a context-free taxonomy.
 %Interestingly, the
%algorithm proposed in this paper is probabilistic in nature as well.
Considering the fact that the noise introduced by randomization leads to
severe data utility, some work related with differential privacy focuses on
releasing certain data mining results
\cite{Barak:2007:PAC:1265530.1265569,Bhaskar:2010:DFP:1835804.1835869,
Friedman:2010:DMD:1835804.1835868,Korolova:2009:RSQ:1526709.1526733}.
However, the usability of the published data is constrained by the pattern
the owner decide to release and moreover the assumption that the data owner
is able to perform data mining tasks is also weak. In addition, Leoni \etal
\cite{DBLP:journals/corr/abs-1205-2726} also indicates the weakness of
differential privacy model itself.
%
%\textbf{
%Recently, a new method called slicing was firstly proposed in \cite{10.1109/TKDE.2010.236} and was
%further developed in \cite{terrovitis:privacy}. Slicing, also called disassociation, aims to protect
%identity or attribute disclosure using identify combinations. However, the privacy model they introduce is
%different from ours and the type of targeted
%data utility is also completely different from ours.
%(I am in a dilemma. Since their methods are totally different
%from ours, I can't make comparison with ours.
%Their methods are somewhat useful in association rule mining and distribution remaining.
%The only disadvantage is that they changed the original structure of the table, but
%according to their data utility such change is acceptable.  )}


The most relevant work to this paper is by Xu \etal \cite{Xu:2008:ATD} and
Cao \etal \cite{Cao:2010:rho}. The $(h,k,p)$-coherence model by Xu \etal~
requires that the attacker's prior knowledge to be no more than $p$ public
(non-sensitive) items, and any inferrable rule must be supported by at least
$k$ records while the confidence of such rules is at most $h$\%. They believe
private items are essential for research and therefore only remove public
items to satisfy the privacy model. They developed an efficient greedy
algorithm using global suppression. In this paper, we do not restrict the
size or the type of the background knowledge, and we use a partial
suppression technique to achieve less information loss and also better retain
the original data distribution.
%\KZ{One notable error made by Xu \etal \cite{Xu:2008:ATD} is in
%their proof of NP-hardness of the problem. They incorrectly
%claim to have reduced the vertex cover problem to the
%global suppression problem, when their definition of vertex cover problem
%is not minimizing the vertex but finding {\em any} vertex cover.}
%\XH{
%Assume their vertex cover is minimum vertex cover problem, their proof is correct since in their proof they announced that $IL(e)=1$, but it didn't mean the hardness of this problem when $IL(e)=support(e)$.
%In their information loss section "In
%particular, IL(e)=1 charges one unit of information loss for the
%item e suppressed, and IL(e)=Sup(e) charges one unit of
%information loss for each occurrence of the item e suppressed."
%As they say, $IL(e)$ is a customized penalty, so the proof only works when $IL(e)=1$.
%}
%
%\XJ{I think the error is that they are trying to minimize the overall information loss
%on item occurrences but for vertex cover problem it should consider
%information loss on item types.}

%This model restrict the amount of knowledge $p$ that attacker may acquire,
%and the prior knowledge can only contain public items.
%The suppression on public items is also necessary to negotiate in nature.
%Yabo Xu et al. \cite{Xu:2008:ATD} measure the information loss

%created by their algorithm by amount of items suppressed.
%On the contrary, in our paper, to limit the inconsistent change of itemset support and the information loss, we prefer to perform partial suppression on only sensitive items.

Cao \etal \cite{Cao:2010:rho} proposed a similar $\rho$-uncertainty model
which is also used in this paper.
%Their model is related to association rule hiding problem.
They developed a global suppression method and a top-down
generalization-driven global suppression method (known as TDControl)
to eliminate all strong sensitive inferences with confidence above
a threshold $\rho$.
The generalization is performed on non-sensitive items
while deletion is performed on sensitive items only. The $\rho$-uncertainty
model makes no assumption about the type of items in
the background knowledge. However, their
methods suffer from same woes discussed earlier for generalization and
global suppression.
%Because both algorithms rely on global suppression,
%it's clear that they are meant for datasets in which the number of
%distinct items per record is relatively small.\KZ{Why?}
%\XH{needs
%to iteratively compute all SARs formed for increasing antecedent
%size. This process can be efficiently preformed for relative small
%antecedent sizes,
%actually it's not that a big weak point, may be we can not relate this,
%we also have related concern about the convergence
%}

Furthermore, TDControl method terminates when the information gain provided
by specialization is out-weighed by the information loss incurred by global
suppression. But this is based on the assumption that specialization always
makes the data less safe and hence requires more suppression. In other words,
data is assumed to exhibit some monotonic property under a generalization
hierarchy. Such assumption makes the applicability of TDControl questionable.
In our work, most of the comparative experiments were done against the two
methods by Cao \etal. And our results clearly show that our new algorithm
significantly outperforms the two in preserving data distribution and useful
inference rules, as well as in minimizing information losses.
%\PC{ I think
%the related work occupies the paper too much. e.g We discuss global and local
%generalization in details, since our paper is not related with
%generalization, I think that we only need to indicate the weakness of
%generalization methods. Also, we introduce our advantage in global
%suppression part. I think that it is good. However, given that the lack of
%space, do we need to put it in related work? In addition, the relevant work
%also takes too much space. We can put them in the previous introduction? }

It's transparent to see that our anonymized data is immune from {\em
record/attribute linkage attack} \cite{FungWCY10:Survey} by Definition
\ref{def:safety_table}. To show more safeness of our technique, we will also
demonstrate that our anonymized data is also immune from {\em Minimality
attack} \cite{Wong:2007:Minimality} and {\em Composition attack}
\cite{Ganta:2008:Composition}. 

Minimality attack \cite{Wong:2007:Minimality}
is proposed for relational data. Assume an adversary knows the whole original
quasi-identifier values as external data, also knows the privacy model and
anonymization technique, by comparing the generalized version of
quasi-identifier values with the original quasi-identifier values, the
adversary can successfully predict some privacy. The Minimality attack relies
on the generalization anonymization technique, while our method uses
suppression technique. Also for set-valued data it doesn't have fixed
combination of items as quasi-identifiers, so it's unrealistic for an
adversary to obtain the satisfactory external data. 

Composition attack
\cite{Ganta:2008:Composition} is proposed for relational data by using the
overlap population of multiple organizations' independent release of
anonymized data, through intersection the privacy can still be breached in
relational data. For example $l$-diversity \cite{Ganta:2008:Composition}
model can be violated by composition attack. The reason why composition
attack can succeed is that quasi-identifier attribute values are generalized
and sensitive attribute values are retained, when performing intersection the
probability of a correlation between quasi-identifier and sensitive values
will definitely increase. On the contrary, our partial suppression algorithm
anonymizes set-valued data by randomly suppressing some sensitive items. In
the same way to perform intersection,
 the probability that a sensitive item correlated with quasi-identifier items can
both be higher or lower than before, which made the composition attack
untrusted. So in a summary our partial suppression technique is ideal to
avoid composition attack depending on the randomized characteristic of
suppression.

%not only depends on the right terminiation condition \KZ{what?}
%\XH{The algorithm terminates when info gain by particularization is smaller than info loss caused
%by suppression to prevent breach, they said this "
%In effect, particularization
%ends at, and hence reveals, the point where a privacy
%risk was at stake if it were carried forward. This revelation is reminiscent
%of a state of affairs that arises in the problem of microdata
%anonymization [26]. Throughout this paper, we have made arguments
%akin to the ones made in [26] to justify our choices of global
%suppression and generalization, and of not generalizing sensitive
%items. Still, a discussion about the relevance of the exact argument
%made by [26] is also due. We provide such a discussion here.
%"
%it's actually not a big problem as I think previously
%so we should not include this in our paper}
%but also
%relies on the monotonicity of the dataset \KZ{what?} \XH{
%generalizing to a higher level in the generalization
%hierarchy does not necessarily lead to generalized forms of
%derivable SARs with confidence equal to or lower than the specific
%SARs they stand for; it may also lead to generalized [22] SARs
%of higher confidences.
%},

%As global suppression's dramatic deletion, much useful information lost would be caused. What's more, the generalization-driven method in \cite{Cao:2010:rho} not only depends on the right termination condition also relies on monotonicity dataset, so the generalization method's applicability is worried.
%Although paradigm of global suppression in \cite{Cao:2010:rho} is similar to the greedy algorithm of Yabo Xu's \cite{Xu:2008:ATD}, they distinguish items from sensitive between non-sensitive, set no restrict on prior knowledge power $p$, and allow sensitive items in prior knowledge, to make it a more natural problem setting.

%$k^m$-anonymity\cite{Terrovitis:2008:PAS} and $k$-anonymity \cite{He:2009:ASD}
%assume all items are equally sensitive, so they employ generalization
%on all items if possible. While this assumption is not true in nature.
%On the other side, $k$-anonymity can well protect record linkage attack
%\cite{FungWCY10:Survey,Kifer:l-diversity}, but cannot well guard attribute
%linkage attack \cite{FungWCY10:Survey,Kifer:l-diversity}. Attribute linkage
%attacker is described as attacker may not need to precisely identify
%victim's transaction but to infer his/her sensitive information
%\cite{FungWCY10:Survey,Kifer:l-diversity}.
%Example is even though a transaction is crowded in a identical $k$
%transaction group, while if there is a high percent that
%all $k$ transactions contain a common sensitive item, then attacker may
%infer the victim also has a high probability having this sensitive item.
%Privacy is not guaranteed by $k$-anonymity in this way.

%Rui Chen et al. addressed the privacy concern \cite{Kifer09attackson, Narayanan:2008:RDL, Wong:2009:AAP} for their deterministic nature and propose to use differential privacy to publish set-valued data. Rui Chen et al. \cite{ChenMFDX11:Diff} propose a probabilistic top-down partitioning algorithm based on a context-free taxonomy tree.

%To the best of our knowledge, most of the prior work
%used perturbation , global/local generalization  and global suppression
%methods  to anonymize set-valued data. We discuss the merits and
%drawbacks of these approaches below.
%Different techniques have different applicable usages,
%while on the contrary different techniques also have different drawbacks.


%However in this paper, we opt for $partial suppression$ technique to anonymize set-valued data. To our knowledge, we are the first to use partial suppression in set-valued data anonymization. As we see, $partial$ has two advantages: one difference from perturbation and generalization techniques, it doesn't damages data's integrity through changing value into fake or less specific one, the other difference compared with global suppression, it will prevent much information loss and preserve result's distribution.
%
%But since sensitive items take the few part in original dataset, while we suppress only sensitive items, so it won't create as many spurious rules as you imagine. \textbf{We'll prove this in experimental results?}.

%So in a summary, our contribution is the first to propose a reasonable and scalable way performing partial suppression in set-valued data anonymization. We are the first to define a general safety definition for set-valued data following the steps of \textit{l}-diversity \cite{Kifer:l-diversity} and $\rho$-uncertainty \cite{Cao:2010:rho}. Our safety definition is an generalization of $\rho$-uncertainty \cite{Cao:2010:rho} since we're not restricted by the rule viewpoint any more. We take the good part of \textit{quasi-identifier} in relational database anonymiztion, and generalize the safety definition to fit for set-valued data.

%M. Terrovitis et al. \cite{Terrovitis:2008:PAS} define $k^m$-anonymity to prevent a transaction from been recognized in a set of k published transactions. $k^m$-anonymity \cite{Terrovitis:2008:PAS} is a loosen version of $k$-anonymity, since attacker's prior knowledge is upper-bounded by m. M. Terrovitis et al.
%While Y. He et al. \cite{He:2009:ASD} propose the real $k$-anonymity model in the following year for set-valued data publishing which frees the limitation on power $m$ in $k^m$-anonymity \cite{Terrovitis:2008:PAS}.
%
%Above are described as deterministic approaches \cite{Xu:2008:ATD, Terrovitis:2008:PAS, He:2009:ASD, Cao:2010:rho},
%With the different structural characteristics between set-valued data and relational table, the difficulties in anonymization algorithms vary vastly.
