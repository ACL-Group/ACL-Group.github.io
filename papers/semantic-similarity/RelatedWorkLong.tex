\section{Related work}
We introduce some related work in measuring semantic similarity between terms in this section.
Contrary to the semantic relatedness relying on the more general relationships (e.g., part-of and the co-occurrence), semantic similarity relies on the degree of taxonomic
likeness between concepts considering relationships such as hyponymy and hyperonymy. We will only discuss methods used to assess semantic similarity here, however most of them can be adapted and generalized to deal with semantic relatedness. To measure the semantic similarity between terms, existing efforts mainly follow two directions. The first approach calculates the semantic similarity based on the conceptual distance in a preexisting thesauri, taxonomy or encyclopedia, such as WordNet. The second approach relies on a large corpus of text (such as the search snippets and web documents) and the term similarities are often derived from distributional properties of words from corpora. Details are as follows.

Regarding the first approach, most works focus on adopting the popular taxonomy such as WordNet as the knowledge database to measure the semantic similarity between terms. We can divide these WordNet-based approaches into two categories. One is the concept likeness based approach regarding the relationships in WordNet and the other is the the graph-based learning approach built on the isA-relationship structure of WordNet.

More specifically, considering the concept likeness based approach, Rada et. al. \cite{Rada:1989} and Lee et. al. \cite{Lee:1993} proposed to calculate similarity between two terms by finding the length of the path connecting the two terms in the taxonomy. Under this approach, semantic similarity is evaluated in terms of the distance between the nodes corresponding to the items being compared in a taxonomy: the shorter the path from one node to another, the more similar they are. Given multiple paths, the shortest path is taken as involving a stronger similarity. This is a very simple and straightforward approach, but it has a low accuracy in the measuring semantic similarity between terms due to the following two problems. First, it relies on the notion that all links in the taxonomy represent a uniform distance. Second, it ignores the information content hidden in the concept nodes. To overcome the problems in the path-length-based approach, researchers proposed the  refined approach, called the information theory based approach.

The first work applying information theory to semantic similarity
computation was proposed by Resnik \cite{Resnik:1995}. It states concept
similarity depends on the amount of shared information
between two concepts. That is, given two concepts $c_1$ and $c_2$, the algorithm first finds the most specific common ancestor subsuming both concepts (denoted as the Least Common Subsumer: LCS) by exploiting a background ontology. Secondly, it uses the information theoretic evaluation function to compute the IC (Information Context) value of LCS, namely,
$sim(c_1, c_2) = IC(LCS(c_1, c_2))$.
The $IC$ of a concept \emph{c} is achieved by computing $IC(c) = -log(p(c))$.
where $p(c)$ indicates the probability of encountering \emph{c} or any of its taxonomical hyponyms in the given corpus, such as the SemCor texts distributed with WordNet 3.0, Brown corpus and British National Corpus. In Resnik's algorithm, there is a problem that any pair of concepts with the same LCS will result in exactly the same semantic similarity. Thus, to tackle this problem, researchers have extended the Resnik's work by considering the IC of each of the evaluated concepts. More specifically, Jiang and Conrath proposed to quantify the length of the taxonomical links as the difference between the IC of a concept and its subsumer\cite{Jiang:1997}. Instead of subtracting the IC of their LCS from the sum of the
IC of each concept as the similarity between two concepts, Lin adopted the ratio between the amount of the IC of their LCS and the sum of the IC of each concept to measure their similarity.

In the analysis of the above corpora-based IC algorithms, researchers found that to accurately compute the information therapy, the contents of corpora should be adequate with respect
to the ontology scope and big enough.
Large and general purpose corpora such as Brown corpus may be suitable for WordNet, but more specific corpora may be
needed for domain ontologies covering concrete terminology.

Due to the limitations imposed by the use of corpora, some authors proposed computing IC from an ontology in an
intrinsic manner\cite{Seco:2004}. These works rely on the assumption that
there is a well-organized taxonomic structure of ontologies like WordNet, according to the principle of cognitive saliency
\cite{Blank:2001}. This states that concepts are specialised when it is needed to
differentiate them from already existing ones. Hence, concepts with
many hyponyms (i.e., specializations) provide less information
than concepts at the leaves of the hierarchy, which provide maximum
information as they are not further differentiated. Comparing corpora-based IC computation models, the intrinsic IC-based models take a function of the
number of hyponyms in a taxonomy as the appearance probability of a concept (and the amount of information it provides). For example, Seco et al. first proposed to compute the base IC calculations on the number
of concept hyponyms \cite{Seco:2004}, namely, $IC(c) = 1-log(hypo(c)+1)/log(max_{nodes})$, where $hypo(c)$ refers to the number of hyponyms in the taxonomical tree below the concept \emph{c} and $max_{nodes}$ refers to the maximum number of concepts in the taxonomy. In the analysis of the intrinsic IC computation algorithms, researchers found that firstly, the more taxonomical leaves that a concept has, the more generality of the current concept, it indicates the lower $IC$ value because it subsumes the meaning of
many salient terms. Secondly, the depth of a concept in a taxonomy
corresponds to its number of taxonomical subsumers (when no multiple inheritance is considered). So, the larger the amount of subsumer concepts above a given one, the higher its
degree of concreteness as it is the result of many specializations.
Following these principles, S$\acute{a}$nchez et. al. proposed an advanced intrinsic IC computation algorithm \cite{Snchez:2011}, namely, $IC(c) = -log((|leaves(c)|/|subsumers(c)|+1)/(max_leaves+1))$, where $|leaves(c)|$ indicates the number of leaves in all hyponyms of the concept $c$ and $|subsumers(c)|$ indicates the number of taxonomical subsumers that the concept $c$ belongs to, and $max_leaves$ is a normalization factor.

Considering the graph-based learning approach built on the isA-relationship structure of WordNet, there are several representative works below. Alvarez and Lim presented a novel algorithm for scoring the semantic similarity (SSA)
between terms \cite{Alvarez:2007}. That is, given two terms $t_1$ and $t_2$, SSA exploits
their corresponding concepts, relationships, and descriptive glosses available in WordNet to build a
rooted weighted graph \emph{Gsim}, and then calculates the output score by exploring the concepts present in \emph{Gsim} and selecting
the minimal distance between any two concepts $c_1$ and $c_2$ of $t_1$ and $t_2$ respectively. The definition of distance is
a combination of: 1) the depth of the nearest common ancestor between $c_1$ and $c_2$ in \emph{Gsim}, 2) the intersection of the
descriptive glosses of $c_1$ and $c_2$, and 3) the shortest distance between $c_1$ and $c_2$ in \emph{Gsim}. A correlation of 0.913 has been
achieved between the results by SSA and the human ratings reported by Miller and Charles for a dataset of 28 pairs of nouns. Agirre and Soroa sequently proposed a WordNet-based personalized PageRank algorithm to measure the semantic similarity between terms\cite{Soroa:2009}. That is, given a pair of terms and a graph-based representation of WordNet, the proposed algorithm first computes the personalized PageRank over WordNet separately for each of the words, producing a probability distribution over WordNet synsets. Second, it compares how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors.
This algorithm can achieve the spearson correlation coefficient of \emph{0.89} predicted on the Miller \& Charles benchmark database \cite{Miller:1998}.

Regarding the distributional context based similarity evaluation approaches, main works are below. Sahami et al. measured semantic similarity between
two queries using search snippets\cite{Sahami:2006}. For each query, the proposed approach first collects snippets from
a search engine and represents each snippet as a TF-IDF weighted term vector (each vector is a $L_{2}$ normalized), and then it defines the semantic
similarity between two queries by the inner product between the corresponding centroid vectors.
Chen et al. proposed a double-checking model using
text snippets returned by a Web search engine to compute
semantic similarity between words \cite{Chen:2006}. That is, after collecting the search snippets given two terms \emph{X} and \emph{Y},
the proposed approach first count the occurrences of terms \emph{X} and \emph{Y} in the
snippets of \emph{Y} and \emph{X} respectively. {\color{red}This method
depends heavily on the search engine's ranking algorithm.
Although two terms \emph{X} and \emph{Y} might be very similar, there
is no reason to believe that one can find \emph{Y} in the snippets for
\emph{X}, or vice versa. This observation is confirmed by the experimental results which report zero similarity
scores for many pairs of words in the Miller and Charles dataset\cite{Miller:1998}.}
Contrary to the above distributional context based methods, Bollegala et.al. proposed a semantic similarity measure using
page counts and snippets retrieved from a Web
search engine \cite{Bollegala:2007}\cite{Bollegala:2011}. To compute the similarity between
two terms \emph{X} and \emph{Y}, the proposed approach first collects the snippets from a web search engine by querying '\emph{X} AND \emph{Y}', and then extracts lexical
patterns that combine \emph{X} and \emph{Y} from snippets. Correspondingly, it uses frequencies of 200 lexical patterns in snippets and four
co-occurrence measures: Dice coefficient, overlap
coefficient, Jaccard coefficient and pointwise mutual
information to create the feature vectors. Finally, it trains a two-class support
vector machine using automatically selected synonymous
and non-synonymous word pairs from WordNet. This method reports a Pearson correlation
coefficient of 0.837 with Miller-Charles ratings.
{\color{red} All approaches mentioned above do not depend on any taxonomy. However, these approach requires a proper disambiguation and annotation of each noun found in the snippets. It is hence necessary to perform a manual and time-consuming analysis of text.}

{\color{red} Unlike many of the aforementioned approaches, our approach uses
semantic context for sparse extraction and our approach is lightweight
and supports web scale data.  To improve its accuracy, we aggregate
three different semantic contexts to increase the data redundancy,
including the attribute-based context, isA-based context and
concept-based context. All semantic contexts are extracted from the
Web by specified patterns and preprocessed as the knowledge databases
in Probase. As we know that the scale of Probase is one order of
magnitude larger than the previously known large
corps\cite{12MSRA:Probase}, such as WordNet\cite{Ritter:What},
Freebase and WikiPedia. Thus, our approach is more scalable in assessing
sparse extractions compared to the aforementioned knowledge-based
approaches~\cite{Wu:Open}\cite{Yao:Collective}. Lastly, experimental
results conducted on the Hearst pattern database in Probase
demonstrate our approach is more efficient and effective compared to
the HMM-based method and other five syntactic and semantic
context-based baseline approaches.

This paper discuss the existing semantic similarity assessing methods and identify how these could be exploited to calculate accurately the semantic similarity of WordNet concepts. The semantic similarity approaches could broadly be classified into three different categories:Ontology based approaches (structural approach),information theoretic approaches (corpus based approach) and hybrid approaches.
All of these similarity measures are expected to preferably adhere to certain basic properties of information. The survey revealed the following drawbacks The information theoretic measures are dependent on the corpus and the presence or absence of a concept in the corpus affects the information content metric. For the concepts not present in the corpus the value of information content tends to become zero or infinity and hence the semantic similarity measure calculated based on this metric do not reflect the actual information content of the concept. Hence in this paper we propose a new information content metric which provides a solution to the sparse data problem prevalent in corpus based approaches. The proposed measure is corpus independent and takes into consideration hyponomy and meronomy relations. Empirical studies of finding similarity of R\&G data set using existing Resnik, linand J\&C semantic similarity methods with the proposed information content metric is to be studied.We also propose a new semantic similarity measure
}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

