\section{Semantic Network}
\label{sec:knowledge}
%\KZ{In this section we give the prelim info about probase.}

To compute the similarity between two terms, we compute the similarity
between their contexts. The context that we use in this paper comes from a
large-scale, general-purpose semantic network, known as Probase
\cite{12MSRA:Probase}. Besides other knowledge, Probase contains isA
relations between concepts, sub-concepts, and entities.  For example,
``Microsoft is a company''. Here ``company'' is a {\em concept} and
``Microsoft'' is an {\em
  entity}. % , because ``company'' serves as the
% hypernym while ``Microsoft'' is the hyponym.
We refer to concepts and entities collectively as terms in this
paper. % } can
% be both a concept and an entity, e.g., ``company'' is an entity under
% ``organization''.
  Probase has the following important properties:
\begin{itemize}
\item Probase introduces a very large concept space with over 2.7
  million concepts;
\item It is not a tree structured taxonomy, but a network: An entity
  or concept may have many super-concepts. For example, the term
  ``banana'' is connected to concepts such as ``fruit'' and ``food''
  directly. The benefit is that such links are data driven rather than
  handcrafted. There is no need to transitively find all super
  concepts, but the distance between two terms cannot be easily
  measured by the number of steps it takes to reach each other.
\item Each isA relation (e isA c) is associated with conditional
  probabilities $P(e|c)$ and $P(c|e)$ (a.k.a.  typicality scores).
\end{itemize}

%In other words, we need open domain, probabilistic isA knowledge (e.g., Microsoft is a company etc.)
%\KZ{I moved the following from intro.}
%That is, modeling a probability distribution over contexts. In this processing,
%to improve the efficiency of collecting contexts,
%we want to create a large, open domain semantic network,
%whose scale or coverage is especially important to the
%applications built on top of them.
%Because manually constructed taxonomies cannot reach sufficient scale and coverage,
%recent work~\cite{S:YAGO2, Carlson:NELL, Etzioni:Unsupervised, Ponzetto:Wiki,
%12MSRA:Probase} uses data driven approaches to automatically acquire
%taxonomies from large corpus such as the World Wide Web.
%In other words, to extend the coverage in the measuring semantic similarity
%between terms, the data and the relationships in
%this knowledgebase are acquired from a huge web corpus through syntactic-based
%information extraction. Hence, we can get the sufficient context given a term
%using this knowledgebase.

Probase attains these properties because it was constructed by an
iterative semantic bootstraping algorithm using a set of
syntactic-based patterns such as the the Hearst
patterns~\cite{Hearst:Automatic}. It extracts isA pairs from 1.68
billion web pages and 2-years of Bing search log.
%\begin{displaymath}
%\begin{aligned}
%~~~~~~~~~~~~~~~~~~~~Y~such~as~X\\
%such~Y~as~X\\
%Y~including~X\\
%Y,~especially~X~\\
%X~and~other~Y\\
%X~or~other~Y\\
%X~is~a/an~Y\\
%\end{aligned}
%\end{displaymath}
For example, ``... European artists such as Pablo Picasso ...'' matches a
Hearst pattern and serves as an evidence that Pablo Picasso is an instance of
European artists.

For a concept/entity pair $\langle c, e\rangle$, Probase provides
two {\it typicality} scores: $P(e|c)$ and $P(c|e)$.
The scores are known as typicality because, for example,
$P(robin~|~bird)$ is greater than $P(penguin~|~bird)$
because $robin$ is a more typical than $penguin$ as a $bird$.
Typicality scores are derived as follows:
\begin{displaymath}
{
\begin{aligned}
P(e|c) = \frac{\mbox{occurrences of }(c,e)\mbox{ in isA pattern extraction}}{\mbox{occurrences of }c\mbox{ in isA pattern extraction}}\\
P(c|e) = \frac{\mbox{occurrences of }(c,e)\mbox{ in isA pattern extraction}}{\mbox{occurrences of }e\mbox{ in isA pattern extraction}}
\end{aligned}
}
\end{displaymath}


%After obtaining all candidate instances including the concept set
%$\{\emph{Y}\}$ and the entity set $\{\emph{X}\}$,
%we require clustering all similar instances lexically and semantically.
% Because Probase is extracted from the whole web. Noises, redundancies and
% irregularities are abundant.
Before we deal with similarity between any two terms, we first look at
terms that have the same meaning. Intuitively, they should have the
highest similarity.  A single term may have many surface forms:
\begin{description}
\item[synonyms:] ``GE'' and ``General Electric''; ``corporation'', ``firm'', and ``company'';
\item[spelling styles:] ``neighbor'' vs. ``neighbour'',
``2d barcode'' vs. ``2d bar code'' and  ``accomplished artist'' vs.
``accomplished artiste';
\item[singular/plural forms:] ``shoe'' vs. ``shoes'';
\end{description}

We address this issue in two steps.  First, we use sources such as
Wikipedia Redirects, Wikipedia Internal Links, and synonym data set in
WordNet to group terms that are synonyms.  Second, we use the edit
distance function to evaluate the distance between terms as follows.
\begin{equation*}
dis_{lex}(t_{1}, t_{2}) = \frac{EditDistance(t_{1}, t_{2})}{MaxLength(t_{1}, t_{2})}
\label{eq:lexDist*}
\end{equation*}
If $dis_{lex}(t_{1}, t_{2}) < \varphi$, the two terms in the current pair are ones with very similar surface forms, and we group them together.
In this paper, we set the value of $\varphi$ to 0.1 according to empirics.

At this point, all lexically similar or synonymous terms are grouped into a
cluster which is analogous to the notion of ``synset'' in WordNet. As a result,
the isA pairs between terms are also mapped logically into isA relations
between synsets.
The set of all synsets is called $\Gamma_{ssyn}$ which provides a mapping between
any Probase term, to its synset and hence all the other terms in that synset.
When computing the semantic similarity between two terms which belong to
the same synset, e.g., General Electric and GE, the similarity is
set to the highest score, namely 1.
%
%of which the most frequent entity representative one of the cluster, correspondingly, we merge the entity or concept distribution of all instances in each cluster as the context of the representative instance. We call these clusters as $\Gamma_{ssyn}$, namely the data set containing the synonym data and the data with very similar surface forms.
%In this case, if the given pair IS in the same cluster, its similarity is set to the highest score, namely 1.
%
%Finally, we collect 16 million
%unique isA relationships, and 2.8 million unique concepts. The concept
%space is big enough to cover almost every aspect of worldly facts.
%According to the above data, we get a huge semantic network of isA
%relationships between concepts and entities, details refer to \cite{12MSRA:Probase}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
