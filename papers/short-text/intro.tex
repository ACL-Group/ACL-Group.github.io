\section{Introduction}
\label{sec:intro}
With the explosive growth in popularity of online social networks
such as Facebooks and Twitter, in addition to the traditional bulletin boards
and forums, a large proportion of Internet users are creating and adding new 
content to the Web on a regular basis. Much of this content is in the
form of short textual messages. For example, Twitter imposes a restriction
of 140 characters per tweet 
\footnote{This restriction resembles that of Short Messaging Service (SMS) 
in mobile telecommunications, and therefore
Twitter is known as the SMS of the Internet.}; 
Facebook limits the number of characters in
a status update to 420; average user comment on Amazon.com is no more than 
a few sentences. Short texts prevails in the digital era for good reasons.
On one hand, service providers prefer to limit the message size to save
precious communication bandwidth and storage capacity. On the other hand,
people today are just too busy to write formal articles. 
This trend is also highlighted by the recent downfall of the previously
florishing Blogging services and the diminishing 
use of Emails among younger people.

Short text messages are important sources of information. US government
agencies are already analyzing Twitter messages for pandemic monitoring
and disaster control. Buyers' review comments on online shopping web sites 
can help merchants follow sales trends and control quality of their 
goods and services. Facebook uses messages post by users to connect people
of similar interests together. Search queries, which can also be regarded
as short text, are constantly being processed and studied by search engines
to provide better search results or more targeted advertising.

However, automatic machine understanding and processing of short texts is
a challenging problem. First, short texts come with limited semantic 
signals such as words and phrases. 
Traditional text mining and classification techniques are centered around 
the {\em bag of words} model, which assumes that a piece of text can 
be represented by a simple, unordered collection of words and a
certain distribution of word frequencies carries a semantic meaning. Well
known methods such as Naive Bayes\cite{Lewis98:Naive}, 
Latent Dirichlet Allocation (LDA) \cite{BleiNJ03} and the more recent 
explicit semantic analysis (ESA) \cite{GabrilovichM07:ESA} 
are all based on this model. Without enough words, it is difficult for
these methods to capture the accurate meaning of the text.
Probase conceptualization \cite{Song11:Conceptualize} deviated from
the bag-of-words approach by mapping a short text from a bag of words 
to a bag of concepts each with an associated probability. But this approach
still ignores the internal relationship among the concepts within 
a message.
Second, words and phrases used in short texts can be highly ambiguous due to
lack of proper context. Both ESA and Probase conceptualization implicitly 
disambiguated a word or entity term by giving it a pre-computed 
distribution in its senses. A sense takes the form of a Wikipedia article
in ESA and a Probase concept in Probase conceptualization. But the 
superimposition of these distributions can dilute a strong sense and render
a whole sentence ambiguous. Consider the following example sentence:

{\em ``Superfresh sells apples and oranges.''}

{\bf Kaiqi to add more content and figure here.}

In this paper, we present a novel approach to detect concepts
and entities and {\em explicitly} disambiguate their senses in short texts.
We do this by leveraging the category hierarchy and the
hyperlink structure among almost 4 million terms in Wikipedia \cite{wikipedia}. 
We use Wikipedia because first it is the largest manually built
knowledge base that combines huge number of concepts and their
descriptions. The quality of this knowledge base is believed to be
much higher than automatically constructed knowledge bases and
at the same time its coverage is likely the most comprehensive
among its peers.

The key contributions of this paper are:
\begin{itemize}
\item automatic construction of a complete category ontology and 
a term sense graph from the raw Wikipedia dump (Section \ref{sec:ontology});
\item an algorithm to parse a given short text into known concepts and 
entities in and disambiguate their senses using the above structured
knowledge (Section \ref{sec:detect} and \ref{sec:tsd}); 
\item state-of-the-art experimental results in word sense disambiguation
and clustering of short messages for SENSEVAL-3 datasets, 
a Twitter dataset and a Topix forum message dataset.
\end{itemize}

Next we will define the short text understanding problem in Section
\ref{sec:problem}, before presenting our approach for the problem (Section
\ref{sec:approach}) and some evaluation results (Section \ref{sec:eval}). 
More complete discussion of the related work
will be deferred till Section \ref{sec:related}.

%********************************************
%
%{\bf Outline:}
%
%* What are short texts and why are they important? (Driving example)
%
%* The challenges of short text understanding
%
%* Simply state previous approaches to text mining/clustering and why
%they do not work well with short texts: (1) bag-of-words similarity
%(2) Topic model (3) ESA (4) Probase Conceptualization
%Defer the detailed discussion of these in the related work.
%
%* Our contributions:
%Our main approach is to disambiguate the known concepts and entity terms
%in the text first before computing similarity based on the information
%content score defined over the Wikipedia category structure.
%In particular:
%
