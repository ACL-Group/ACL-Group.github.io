\section{Related Work}
\label{sec:related}

%Multi-document summarization~\cite{abs-2011-04843} is used for generating an informative summary of multiple topic-related texts, such as news~\cite{FabbriLSLR19} and emails~\cite{ZajicDL08}.
% Wikipedia articles ~\cite{LiuSPGSKS18} and so on. 
Opinion summarization~\cite{GeraniMCNN14,rtopi21} is a typical multi-document summarization problem~\cite{abs-2011-04843,FabbriLSLR19,ZajicDL08},
which can generate a summary covering the salient opinions
of multiple reviews. It inherently has a special focus on aspects of the product or service, 
making it different from other review summarization tasks, 
such as the summarization on tourist reviews~\cite{tourist20} and twitters~\cite{DBLP:conf/www/KeswaniC21}.

Opinion summarization suffers from a lack of training pairs. 
Some work~\cite{MeanSum19, Copycat20,tree21} used auto-encoder to train the model by 
reconstructing loss or sentence embeddings. 
%\citet{TianY019} classified words into three types, including aspect, opinion and context and predicted the work type as a first step. 
Others construct synthetic datasets for supervised training. 
The input format of the synthetic datasets can be divided into 
textual input and structured input.  
For the textual input,
%the most intuitive way 
%proposed by some approaches
some approaches~\cite{Fewshot20,transsum21}
regarded one review for a product as a summary 
and took all or part of the rest as input. 
\citet{transsum21} computed the distance between the summary and all remaining reviews as weights of review embeddings.
%So, further works~
\citet{Plansum20} took the nearest neighbors as inputs based on review representations.
%Adding noise to the sampled summary and taking the disturbed ones as the input reivews is another way to generate synthetic datsets. 
\citet{Denoise20} added noise to the sampled summary from two aspects: the segment noising at token and chunk level, and document noising by replacing the whole review with a similar one. 
\citet{prefix21} labeled input reviews and sampled summaries with control tokens as additional input and took control tokens as prefixes at decoding,
which cannot be compared fairly with the methods without external labels.
%Different from their simple replacing, removing and inserting operations, 
However, the quality of such datasets is limited by biased reviews, 
which cannot be summarized from other reviews. 


%Although above mentioned work mainly focused on data construction and ignored the characteristics of reviews, aspects and opinions are quite important for opinion summarization~\cite{MukherjeePVGBG20}. 
The aspects and opinions are quite important for opinion summarization.
Some approaches~\cite{AngelidisL18,MukherjeePVGBG20} classified the sentences of reviews into different aspects and collected the most salient sentence of each class as the summary.
\citet{TianY019} 
classified words into three types (i.e., aspect, opinion and context) and predicted words by the probability distribution on these types.
Inspired by these works,
\citet{OpiDig20} extracted opinion-aspect phrases from each review for filtering information and transformed the task into single document summarization. 
%Besides, previous work~\cite{Plansum20} shows that project each review into aspect and sentiment distributions can also help.
\citet{amplayo-etal-2021-aspect} used predefined aspects to construct synthetic training data and trained a controllable model to generate summaries based on aspects.
However, all of these works are limited by the accuracy of 
the opinion-aspect extractor and also
neglect some other information in the sentences which cannot be explicitly formulated as opinion-aspect pairs. 

Thus, we propose a method to create a semi-structured synthetic dataset consisting of
opinion-aspect pairs and implicit sentences. 
Taking the noisy opinion-aspect pairs and noisy implicit sentences as input, we can get more accurate and comprehensive summaries.  
