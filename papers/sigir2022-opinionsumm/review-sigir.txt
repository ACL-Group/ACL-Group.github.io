SUBMISSION: 79
TITLE: Opinion Summarization by Weak-Supervision from Semi-Structured Data

-------------------------  METAREVIEW  ------------------------
This paper addresses an interesting problem --automatic abstractive aggregated review summarization-- and provides what appears to be a novel and promising solution. However, it is our general opinion that the paper needs some work before it can be published at SIGIR. Specifically, there are issues with presentation, motivation of the work and also more details are required to understand the extent of the improvement over the state-of-the-art. We encourage the authors to work on the comments provided by the reviewers, in hope that they will be successful in publishing their work in the near future.



----------------------- REVIEW 1 ---------------------
SUBMISSION: 79
TITLE: Opinion Summarization by Weak-Supervision from Semi-Structured Data
AUTHORS: Yizhu Liu, Qi Jia, Haifeng Tang and Kenny Zhu

----------- Relevance to SIGIR -----------
SCORE: 3 (fair)
----------- Technical soundness -----------
SCORE: 4 (good)
----------- Quality of presentation -----------
SCORE: 3 (fair)
----------- Adequacy of citations -----------
SCORE: 4 (good)
----------- Reproducibility of methods -----------
SCORE: 4 (good)
----------- Strengths -----------
S1: Simple idea and good performance. Their key idea is to use both key-value pairs extracted from review sentences and review sentences themselves from which no key-value pairs are extracted. They interacts with each other by taking a weighted summation of embeddings. According to the evaluation results, their proposed method outperformed seven existing methods.

S2: Evaluation methodology is solid. The authors compared their proposed methods and seven existing methods on not only well-known datasets but also datasets used in the original papers of the existing methods.

S3: Because their method is a combination of two encoder-decoder models, an ablation study is desirable and is actually conducted in this paper.
----------- Weaknesses -----------
W1: Motivation is not clearly stated in this paper. In other words, what are applications of generated summaries of reviews? I just assumed that there are some demand on review summarization because of a number of existing methods and datasets.

W2: There are a couple of questions on the summary sampling step at training data creation (2.1.1). However, these are not critical because their proposed methods performed well for real data even if the created training data was not realistic.
(a) The authors removed reviews with first-person singular pronouns or with non-alphanumeric symbols from pseudo-summaries. This process is ad-hoc. Are both of the conditions necessary? Are there no other possible conditions?
(b) The authors claim that they ensured that all the aspects in each pseudo-summary are in the other reviews on the entity. How did they ensure that?

W3: There are also a couple of (not critical) questions on the noisy opinion-aspect pairs sampling step (2.1.3).
(a) In this sub-subsection, The authors define "matched" OAs are with the same aspects as summary. After that, they regard the aspects appearing in more than one review on the same entity as matched aspects. Are they consistent?
(b) Number of reviews (N_r) for each entity is a constant. This assumption is not realistic.
----------- Overall recommendation -----------
SCORE: 0 (borderline paper)
----------- Detailed comments to authors -----------
This work aims to summarize multiple review posts on an entity (e.g., a consumer service, a product, a movie, and so on) into a single review post. The novelty of this work is to use not only key-value pairs (Opinion-Aspect pairs, OAs) extracted from the review text, but also sentences in natural language themselves from which no explicit key-value pairs are extracted (Implicit Sentences, ISs). Because their methods are only weakly supervised, the authors first explained how to construct training data from a dataset of just review posts. Their methods are based on two encoder-decoder models, one is for OAs and the other for ISs. The two models interacts with each other by taking weighted summation of intermediate dense vector expressions of OAs and ISs. They proposed two variations of their methods. The TBA-T is based on Transformer without pre-training, and TBA-B is based on pre-trained BART (not BERT).
The authors conducted detailed evaluation of their proposed methods and seven existing methods. Based on three datasets (Yelp, Amazon, and RottenTomatoes) and three evaluation metrics (ROUGE, diversity, and aspect coverage), their proposed TBA-B method significantly outperformed the other methods. Human assessors also judged that the results of TBA-B are better than the ones of TranSum, the existing method strongest among the seven. The authors also conducted ablation study. They first confirmed the validity of semi-structuring of input by semi-structuring the existing datasets which were used in the original papers of the existing methods, and then training their TBA models on the semi-structured data. Their TBA models on the semi-structured data consistently outperformed the pairs of original method and dataset. They second confirmed that both OAs and ISs were effective for improving the quality of summaries.

Other Comments:
C1: Just after reading the title and abstract, I thought that the target of this study is semi-structured documents, e.g., HTML or XML documents. The target is actually a combination of fully-structured and not structured information, and the two are clearly separated. A combination of fully-structured and not structured information is different from semi-structured information, I think.

C2: In 3.5.1, the authors claim that a single gold summary may not cover all important information in multiple reviews. Is this an observation on the quality of Yelp dataset?

C3: In 3.5.2, the authors suggest most of tokens in OA-sentences are OAs or stopwords. They are quite different. How is the ratio between them?
----------- Nominate for Best Paper -----------
SELECTION: no



----------------------- REVIEW 2 ---------------------
SUBMISSION: 79
TITLE: Opinion Summarization by Weak-Supervision from Semi-Structured Data
AUTHORS: Yizhu Liu, Qi Jia, Haifeng Tang and Kenny Zhu

----------- Relevance to SIGIR -----------
SCORE: 4 (good)
----------- Technical soundness -----------
SCORE: 3 (fair)
----------- Quality of presentation -----------
SCORE: 3 (fair)
----------- Adequacy of citations -----------
SCORE: 4 (good)
----------- Reproducibility of methods -----------
SCORE: 2 (poor)
----------- Strengths -----------
1. Abundant empirical data studies and comparisons.
2. detailed explanations about concepts and results by using real-world examples.
3. The proposed solution about review summarization shows better performance than the state-of-the-art methods, which could be useful for the SIGIR community.
----------- Weaknesses -----------
1. The presentation of the work needs improvement.
----------- Overall recommendation -----------
SCORE: -1 (weak reject)
----------- Detailed comments to authors -----------
This work presents a novel solution for reviews/opinions summarization. Instead of using plain text s, the authors propose to use semi-structured data such as opinion-aspect (OA) pairs and implicit sentences (IS) as the processed input to machine learning models and prove the effectiveness by empirical studies. The presented experimental results show better performance than the state-of-the-art baselines.

This paper presents a significant amount of content about the proposed solution. Several parts of it are distracting and confusing for a general reader. For example,

1, for the "semi-structured synthetic data" part in Section 3.5.2, if the authors try to show the advantage of semi-structured data, it's better to discuss the performance of using the same model but different types of inputs (text, structured, semi-structured).

2, the concept of OA is introduced at the beginning of the paper, but we suddenly have the matched (MA) OA and mismatched (MM) OA in the paragraph under Table. 7 of Page 8. The exact definitions of them are missing.

The overall better performance of this work is the result of the proposed semi-structured input data and the introduced machine learning model. It would be better if the authors can provide experimental results of the basic aspect-guided (BM) model and the advanced aspect-guided (AM) model in Table 4. Moreover, In Table 9, it seems the AM model shows lower Accuracy comparing the BM model. The authors may want to analyze and discuss why this happens. Since the text, structured and semi-structured are different ways of representing the information, the authors are encouraged to do an analysis from a theory perspective, like how accurate the information is carried by the OA and the IS compared to the original text info.
----------- Nominate for Best Paper -----------
SELECTION: no



----------------------- REVIEW 3 ---------------------
SUBMISSION: 79
TITLE: Opinion Summarization by Weak-Supervision from Semi-Structured Data
AUTHORS: Yizhu Liu, Qi Jia, Haifeng Tang and Kenny Zhu

----------- Relevance to SIGIR -----------
SCORE: 4 (good)
----------- Technical soundness -----------
SCORE: 4 (good)
----------- Quality of presentation -----------
SCORE: 3 (fair)
----------- Adequacy of citations -----------
SCORE: 4 (good)
----------- Reproducibility of methods -----------
SCORE: 2 (poor)
----------- Strengths -----------
The paper proposes a method of preprocessing text review to a semi-structured version for abstractive opinion summarization of multiple reviews. Specifically, authors utilize plain sentences from reviews in addition to opinion-answer pairs extracted from them. They also adapt transformer models to use this new information type and, through automatic and human evaluation, show that the proposed model performs better in generating summaries from reviews.
----------- Weaknesses -----------
Despite its potential, some caveats in the paper would be hard to address in a short period.

First of all, the paper's contribution seems to be overstated. Specifically, I would expect more evaluation and justification for a statement that the model generates high-quality summaries. The overall quality of summaries shown in the human evaluation is close to 0 and far from the gold ones, which also do not seem to be high-quality summaries. Moreover, a quick study of random examples from the model result data provided by the link in the paper shows many relatively meaningless generations. Examples are given in detailed comments. Secondly, the proposed approaches are not very novel and relatively simple. The evaluation metrics are basic and often unreliable for summary evaluation. The human-evaluation part is missing details such as participants' background and assessment scales. Finally, the paper presentation could be significantly improved by providing more motivation for the approach, adding more examples, using fewer custom abbreviations. The space could be saved by
, for instance, skipping well-known formulas.
----------- Overall recommendation -----------
SCORE: 0 (borderline paper)
----------- Detailed comments to authors -----------
The introduction seems too specific and lacks a detailed explanation to let the reader understand the problem correctly. For instance, the difference between the implicit sentence and opinion aspects pairs is unclear. The given example for "staff" from Table 1 could be an opinion aspect pair (unfriendly, staff). I failed to understand the difference with  (burned, beef) pair, which is an example of an opinion-aspect pair.

Presentation: The custom abbreviation is hugely overused and makes reading very complicated with constant looking back and force, for instance, "noisy MA OAs, we sample some matched OAs from O's". There are many statements about reviews structure, such as "In some actual multi-reviews, some people may have inconsistent comments on the same aspect of an entity." which would be nice to support with examples where possible.

The accompanying code is very messy. Most of the files are templates; the readme is minimal, the only file containing some training code contains details not described in the paper. In addition, the data folder has too few instances (only 30 in QA yelp train).

Section 2.1. What is the motivation behind using this specific dataset creation process? For instance, at this point, it is not clear why do we need noisy OAs and ISs and how they are different from actual OAs and ISs. Overall, I failed to understand the exact procedure, benefits, and motivation: the narration is hard to follow as there are many specific abbreviations and no examples.

Section 2.1.1. "we ensure that all aspects of the summary can be found in R" - what is the exact method behind this? Do you calculate any metric like novel n-grams percentage?

Section 2.2: What is the length distribution of concatenated OAs? As the input of the models is limited, it could be a problem if the lengths of OAs is usually longer as some random OAs will be truncated to the maximum token length of the model.

Section 2.2.1:  
"consequently tends to generate general and simple summaries." - with the text-generation models, it is better to link actual examples of generated text. You can save space for that by removing well known and unchanged in your case formulas, such as self-attention.  

Section 2.2.2:  
The "Advanced Aspect-guided Model." is a not very model and a pretty standard model. I would suggest changing the name to
 something less overstated, like Basic/Baseline Aspect-guided Model with IS.

Section 3.4:
"Previous methods used different versions of ROUGE, so the ROUGE scores in some published papers cannot be compared." - the libraries do not use different versions of ROUGE as there are no different rouge versions. However, the libraries differ in handling text preprocessing (for example, \n). So please, clarify this in your text. You can find more info, for instance, in this issue https://github.com/huggingface/datasets/issues/617.

More information on Human Evaluation is needed: how human annotators were hired, their demography, was it approved by a  Research Ethics Committee?

The overall usefulness of the generated summaries in Table 6 is very low despite being higher than the other model. What was the scale of evaluation criteria?

Examples in Table 5 seem to be cherry-picked. Based on the random samples from the data you provided in the repository, the generated summaries on RT for movies often look meaningless:
"the film is a triumph of fun, and the kind of movie that will leave you wondering why it's not a bad movie.",
"it follows is a film that is not only a great deal of a movie, but it's a very good movie."
One would expect an analysis of model failures.
----------- Nominate for Best Paper -----------
SELECTION: no


