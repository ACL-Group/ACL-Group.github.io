Review 1
W1: Motivation is not clearly stated in this paper. In other words, what are applications of generated summaries of reviews? I just assumed that there are some demand on review summarization because of a number of existing methods and datasets.

W2: There are a couple of questions on the summary sampling step at training data creation (2.1.1). However, these are not critical because their proposed methods performed well for real data even if the created training data was not realistic.
(a) The authors removed reviews with first-person singular pronouns or with non-alphanumeric symbols from pseudo-summaries. This process is ad-hoc. Are both of the conditions necessary? Are there no other possible conditions?
(b) The authors claim that they ensured that all the aspects in each pseudo-summary are in the other reviews on the entity. How did they ensure that?

W3: There are also a couple of (not critical) questions on the noisy opinion-aspect pairs sampling step (2.1.3).
(a) In this sub-subsection, The authors define "matched" OAs are with the same aspects as summary. After that, they regard the aspects appearing in more than one review on the same entity as matched aspects. Are they consistent?
(b) Number of reviews (N_r) for each entity is a constant. This assumption is not realistic.

C1: Just after reading the title and abstract, I thought that the target of this study is semi-structured documents, e.g., HTML or XML documents. The target is actually a combination of fully-structured and not structured information, and the two are clearly separated. A combination of fully-structured and not structured information is different from semi-structured information, I think.

C2: In 3.5.1, the authors claim that a single gold summary may not cover all important information in multiple reviews. Is this an observation on the quality of Yelp dataset?

C3: In 3.5.2, the authors suggest most of tokens in OA-sentences are OAs or stopwords. They are quite different. How is the ratio between them?

Review 2
1, for the "semi-structured synthetic data" part in Section 3.5.2, if the authors try to show the advantage of semi-structured data, it's better to discuss the performance of using the same model but different types of inputs (text, structured, semi-structured).

2, the concept of OA is introduced at the beginning of the paper, but we suddenly have the matched (MA) OA and mismatched (MM) OA in the paragraph under Table. 7 of Page 8. The exact definitions of them are missing.

The overall better performance of this work is the result of the proposed semi-structured input data and the introduced machine learning model. It would be better if the authors can provide experimental results of the basic aspect-guided (BM) model and the advanced aspect-guided (AM) model in Table 4. Moreover, In Table 9, it seems the AM model shows lower Accuracy comparing the BM model. The authors may want to analyze and discuss why this happens. Since the text, structured and semi-structured are different ways of representing the information, the authors are encouraged to do an analysis from a theory perspective, like how accurate the information is carried by the OA and the IS compared to the original text info.

Review 3
First of all, the paper's contribution seems to be overstated. Specifically, I would expect more evaluation and justification for a statement that the model generates high-quality summaries. The overall quality of summaries shown in the human evaluation is close to 0 and far from the gold ones, which also do not seem to be high-quality summaries. Moreover, a quick study of random examples from the model result data provided by the link in the paper shows many relatively meaningless generations. Examples are given in detailed comments. Secondly, the proposed approaches are not very novel and relatively simple. The evaluation metrics are basic and often unreliable for summary evaluation. The human-evaluation part is missing details such as participants' background and assessment scales. Finally, the paper presentation could be significantly improved by providing more motivation for the approach, adding more examples, using fewer custom abbreviations. The space could be saved by, for instance, skipping well-known formulas.

The introduction seems too specific and lacks a detailed explanation to let the reader understand the problem correctly. For instance, the difference between the implicit sentence and opinion aspects pairs is unclear. The given example for "staff" from Table 1 could be an opinion aspect pair (unfriendly, staff). I failed to understand the difference with (burned, beef) pair, which is an example of an opinion-aspect pair.

Presentation: The custom abbreviation is hugely overused and makes reading very complicated with constant looking back and force, for instance, "noisy MA OAs, we sample some matched OAs from O's". There are many statements about reviews structure, such as "In some actual multi-reviews, some people may have inconsistent comments on the same aspect of an entity." which would be nice to support with examples where possible.

The accompanying code is very messy. Most of the files are templates; the readme is minimal, the only file containing some training code contains details not described in the paper. In addition, the data folder has too few instances (only 30 in QA yelp train).

Section 2.1. What is the motivation behind using this specific dataset creation process? For instance, at this point, it is not clear why do we need noisy OAs and ISs and how they are different from actual OAs and ISs. Overall, I failed to understand the exact procedure, benefits, and motivation: the narration is hard to follow as there are many specific abbreviations and no examples.

Section 2.1.1. "we ensure that all aspects of the summary can be found in R" - what is the exact method behind this? Do you calculate any metric like novel n-grams percentage?

Section 2.2: What is the length distribution of concatenated OAs? As the input of the models is limited, it could be a problem if the lengths of OAs is usually longer as some random OAs will be truncated to the maximum token length of the model.

Section 2.2.1:
"consequently tends to generate general and simple summaries." - with the text-generation models, it is better to link actual examples of generated text. You can save space for that by removing well known and unchanged in your case formulas, such as self-attention.

Section 2.2.2:
The "Advanced Aspect-guided Model." is a not very model and a pretty standard model. I would suggest changing the name to
something less overstated, like Basic/Baseline Aspect-guided Model with IS.

Section 3.4:
"Previous methods used different versions of ROUGE, so the ROUGE scores in some published papers cannot be compared." - the libraries do not use different versions of ROUGE as there are no different rouge versions. However, the libraries differ in handling text preprocessing (for example, \n). So please, clarify this in your text. You can find more info, for instance, in this issue https://github.com/huggingface/datasets/issues/617.

More information on Human Evaluation is needed: how human annotators were hired, their demography, was it approved by a Research Ethics Committee?

The overall usefulness of the generated summaries in Table 6 is very low despite being higher than the other model. What was the scale of evaluation criteria?

Examples in Table 5 seem to be cherry-picked. Based on the random samples from the data you provided in the repository, the generated summaries on RT for movies often look meaningless:
"the film is a triumph of fun, and the kind of movie that will leave you wondering why it's not a bad movie.",
"it follows is a film that is not only a great deal of a movie, but it's a very good movie."
One would expect an analysis of model failures.
