\section{Related Work}
\label{sec:related}

The most related work to our problem is class-based approaches solving selectional preferences(SP). Selectional preferences aims at computing preferences over arguments indicated by verb, given the fact that some arguments are more suitable for certain verb than others. For example,
``drink water'' is more plausible than ``drink desk''.
While our problem determines the appropriate abstraction
of instances of verb argument, selectional preference
doesn't necessarily output a representation for plausible arguments.
Class-based approaches are based on the idea of
generalizing from witnessed arguments extracted from corpus.
They first extract class-representation the verb's seen arguments
and then generalize to unseen arguments according
to extracted classes.
The intermediate class result from class-based SP
could be seen as conceptualization of verb arguments. 
One distinct difference with our work is that it doesn't attempt to
satisfy the coverage and overlap constraint as we defined in
our problem.
We have compared our result and class-based SP intermediate result
in \secref{sec:eval} and conclude that adding requirement would
generate better result.

First such approach was proposed by Resnik\cite{resnik1996selectional},
where WordNet\cite{wordnet}, manually produced classes corresponding
to synsets, is used as candidate set for representation of
verb's arguments. He calculated preference score for a predicate as
$$S(c,p)=\sum_{c}P(c|p)log\frac{P(c|p)}{P(c)},$$
where $p$ is predicate and $c$ are WordNet synsets,
thus obtained the associational strength of synset $c$
on the predicate by measuring the difference between the prior for $c$
and the probability of $c$ given predicate.
To rank words, which belong to several synsets, for predicate,
Resnik went through all the possible $c$ for word $w$,
and assign the maximum preference score to $w$. We implemented
this method and compared its accuracy with our algorithms.

Li and Abe\cite{li1998generalizing} proposed a cut-based SP method,
aiming to find an appropriate level of generalization of classes
on WordNet hierarchy, preventing class representation of arguments
from being too specific or too general. They induced the problem to
estimating a tree cut model of a thesaurus tree, and the method is
based on Minimum Description Length (MDL) principle.
The thesaurus tree was built based on WordNet where leaf nodes are
noun and internal nodes are noun classes.
MDL serves as a criterion for the best cut, representing the
tradeoff between model complexity and goodness of fit to the data
by minimising a sum of model description length and
data description length. Then the preference score for a synset
$c$ would be $$S(c)=P(c|cut_{c,p})P(cut|p),$$ where $cut_{c,p}$
is the cut that dominates $c$, $P(c|cut_{c,p})$ is a uniform
distribution and $P(c|p)$ comes from a maximum likelihood estimation.

Clar and Weir\cite{clark2001class} proposed another method
which focuses on determining the good level of generalization.
They use Chi-square test to determine whether a class should
be maintained or refined to some of its children. Specifically,
Chi-test measures how similar the probabilities between the parent
and children are, given the same argument slot and verb.
A significant difference indicates an appropriate cut should
be between the parent and children, otherwise the algorithm
goes on testing the children because they well-represent their parent.

Pantel et al.\cite{pantel2003clustering} used
automatically generating classes instead of WordNet.
They proposed a general-purpose clustering algorithm called
CBC (Clustering By Committee) to overcome the rare senses and the
lack of domain specific senses in WordNet.
In CBC, the centroid of a cluster is constructed by
averaging the feature vectors of a subset of the cluster members.
And the subset is thus the committee that determines what
other elements belong to the cluster. In the CBS algorithm,
committee is constructed as follows: first compute each element's
top-$k$ similar elements ($k$ is small), then construct
a collection of tight clusters using the elements above,
and now those elements serve as committee in each cluster.
If a newly formed committee is similar to existed committee,
then it is discarded. In the end, each element $e$ is
assigned to its most similar clusters.

Other approaches to solve SP include
similarity-based methods \cite{dagan1999similarity, erk2007simple},
discriminative approach \cite{bergsma2008discriminative},
and generative probabilistic models \cite{rooth1999inducing,
ritter2010latent, seaghdha2010latent}.
These methods don't use classes as intermediate representation,
thus less related to our work.

Another related problem is Semantic Role Labeling (SRL), which aims at
assigning semantic roles to verb's related arguments.
Normally the set of roles are universal and predefined,
and consequently SRL provides coarse-grained output comparing
to our results. The task is commonly solved
by a supervised learning process with manually labeled data,
which are two significant deviations from our problem.

Some grammar rule based solutions were first introduced to solve
the problem, including Link parser\cite{sleator1995parsing} and
MiniPar\cite{lin1994principar}. Rule-based approaches suffers from
labor-intensive and limited domains. Thanks to large scale annotated
corpora, such as FrameNet\cite{baker1998berkeley}
and PropBank\cite{kingsbury2002treebank}, statistical techniques
incorporating machine learning algorithms
\cite{gildea2002automatic,pradhan2004shallow,pradhan2005semantic,
marquez2008semantic} are introduced to this task
to build domain independent system.

FrameNet defines around 1,200 semantic frames based on
the theory of Frame Semantics, each specifying an event
with all related elements. Thus a sentence may evoke
one or more frames, and for each frame, elements in the sentence
which are involved in the frame are annotated as frame elements.
For example, in ``Will you help the Government find your brother?'',
``help'' evoked a frame {\em Assistance}.
{\em Assistance} has several frame elements,
including {\em Helper}: ``you'', {\em Benefited party}:
``the Government'', and {\em Goal}: ``find your brother''.
Different words can evoke the same frame, e.g., ``assistance''
and ``aid'' may also evoke frame {\em Assistance}.
In our work, action frames which are similar can be generated
automatically and the results was shown in \secref{sec:eval}.
Propbank particularly labeled verbs and their arguments
in the sentence without conceptualization.
VerbNet\cite{KipperDP00} further mapped verbs
to other lexical resources such as FrameNet and WordNet\cite{wordnet}.

%In order to do SRL automatically, Jurafsky and Gildea\cite{gildea2002automatic} proposed one of the fundamental approaches to the problem by using WordNet and FrameNet. A supervised classifier is trained based on both syntactic and semantic features extracted from corpus, including
%\begin{description}\setlength{\itemsep}{-\itemsep}
%\item[phrase type]Directly from constituent parse
%\item [governing category] Indication if a Noun Phrase(NP) is subject or object of the verb
%\item [parse tree path] Path from target word
%\item [position] Where is the constituent regarding to the predicate
%\item [voice]Active/passive concluding from passive identifying patterns
%\item [head word]Head word of constituent
%\end{description}
%
%More works are done following the idea (\cite{pradhan2004shallow,
%pradhan2005semantic}), they cooperating more features in
%svm training including name entities in constituents, part of
%speech tag of the headword, etc, and explore more on automatic SRL
%by extending basic features and changing machine learning algorithms,
%and state-of-art performance is proposed by Marquez et al
%\cite{marquez2008semantic}.
%
%
%One of our approaches applying The Strength Pareto Evolutionary Algorithm 2 (SPEA2) to our problem, in which we induce our problem to a multi-objective issue and aiming at finding the approximation of the Pareto set. In our problem, we consider several different objectives to achieve both statistical representative and human plausible results. Considering objectives as vector, defined by Pareto dominance, objective vertor $y^1$ dominate objective vector $y^2$ indicate that all components of $y^1$ is larger or equal to $y^2$ and at least one component of $y^1$ is larger than that of $y^2$. Thus optimal solutions are solutions not dominated by any other solutions, i.e. Pareto set, and such solutions may not be unique. To deal with the large search space of optimal solutions, evolutionary process is introduced to approximate Preto Set. The basic idea is keeping good results while randomly generating new "population" expecting some of them are results worth remaining, during the procedure two selections have to be done:mating selection, which decides which solutions will be used in generating population; and environmental selection, which decides which solutions are survived.
%}

PATTY\cite{nakashole2012patty} builds taxonomy of a binary relation's
which are not limited to predicates.
Similar to conceptualizing verb arguments in this paper,
binary relation patterns in PATTY are represented by
a linguistic pattern and and ontological types, which is
a semantic class name coming from YAGO2\cite{SuchanekKW07}.
PATTY can also generate subject-predicate-object triple from
its result\cite{nakashole2013discovering}, however, the PATTY
work focus on mining relations, with no constraints on the
numbers of possible ontological types at the same position of
the same pattern. This goes against our main objective of minimizing
the number of concepts for each arguments.
There's also no constraint on overlap between two ontological
types.

ReVerb are introduced in the relation extraction process, too.
During the construction of relational extractions knowledge base,
Velardi et al.\cite{velardi2013open} combined ReVerb and Ontology
induction Ontolearn by getting instances of relations from ReVerb
then build ontology among the instances, thus extracting concept
level relation. Bonan Min\cite{min2012ensemble} use ReVerb data
as well as corpus and do unsupervised relation extraction.
ReVerb instances are then clustered to get semantic classes,
and the final output would be a $\langle$ semantic class,
relation, semantic class $\rangle$ triple. Relations in
both pieces of work are limited to ReVerb, thus suffering
its insufficient scale.

%\KZ{Is there any work that makes use of ReVerb or Google n-gram data
%to do verb semantic analysis in general?}

Google n-gram data was used in some semantic understanding work:
Polajnar et al.\cite{polajnar2013learning} particularly used the
verb's subjects and objects in the data, as we do,
to construct verb tensor; Welke et al.\cite{welkegrounded} focused
on preposition between objects and locations for semantic
understanding of robots; others
\cite{borin2013mining,riedlscaling,kaiserextracting}
use the data as general corpus.

%As for search algorithm, we apply Simulated annealing(SA) to solve our problem. SA is a probabilistic metaheuristic obtaining approximation of the global optimum in a large search space. SA gets satisfying approximation for our NLP problem within reasonable time comparing to other search algorithm such as gradient descent.



