\subsection{Verb Sense Matching}
\label{sec:synset}
The sense of a verb may be identified by the semantic class of its argument.
For example, the verb ``play'' in ``play sports'' holds different
sense to ``play music'' or ``play someone''.
Our lexicon contains strong relations between
verbs and argument concepts, which may indicate certain senses of
the verbs. Here, we evaluate ActionNet(WN) on its ability 
of identifying verb senses (synsets) with different 
action concepts.

Each synset of a verb in WordNet contains a gloss, which is an example
sentence illustrating the use of the verb in a particular sense.
From these glosses, we extract verb-subject and verb-object pairs
to construct two test sets, namely Verb-Subject and Verb-Object, respectively.
When a verb has both subject and object in the gloss,
we put its verb-object relation into the Verb-Object set.
For each verb $v$ in each test set, we run AC and SP setting number of concepts ($k$) equals to the
number of senses of $v$,
which is essentially the number of glosses containing $v$.
Let the set arguments (from glosses) covered by the 
$k$ concepts ($C_k$)
of ActionNet(WN) $A_{g,v}$.
We define the precision
and recall of the mapping as:
\begin{eqnarray*}
Precision(v) &=& \frac{\mbox{min \# of}\ c\in C_k\ \mbox{covering}\ A_{g,v}}{k},\\
Recall(v) &=& \frac{|A_{g,v}|}{k}.\\
\end{eqnarray*}

The precision indicates how many action concepts are needed to
capture all senses of the verb, while recall measures how many
senses can be covered by the action concepts.
%Then, we perform a maximum matching with the Hungary algorithm
%between the $k$ concepts and the arguments extracted from the $k$ glosses
%to check how many senses of the verb we can match.
We also compute the $F_1$ score for the two algorithms and summarize the results
in \tabref{tab:verbmatch}.
%\begin{table}[th]
%\centering
%%\scriptsize
%\caption{Percentage of Maximum Matching between Action Concepts and Arguments in Glosses}
%\begin{tabular}{|l|l|l|}
%\hline
%Test Set & AC & SP \\
%\hline \hline
%Verb-Subject & {\bf 19.54\%} & 11.97\%\\
%\hline
%Verb-Object & {\bf 27.95\%} & 18.67\%\\
%\hline
%Average & {\bf 27.06\%} & 17.26\%\\
%\hline
%\end{tabular}
%\label{tab:verbmatch}
%\end{table}
\begin{table}[th]
\centering
\scriptsize
%\scriptsize
\caption{Precision (Pre), Recall (Rec) and F1 for Verb Sense Matching (ActionNet v.s. SP)}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Test Set} & \multicolumn{3}{c|}{ActionNet} & \multicolumn{3}{c|}{SP}\\
\cline{2-7}
& Pre & Rec & $F_1$ & Pre & Rec & $F_1$\\
\hline
Verb-Subject &\bf 0.21 &\bf 0.23 &\bf 0.22 & 0.11 & 0.12 & 0.11\\
\hline
Verb-Object &\bf 0.24 &\bf 0.26 &\bf 0.24 & 0.19 & 0.19 & 0.18\\
\hline
Average &\bf 0.24 &\bf 0.26 &\bf 0.24 & 0.17 & 0.18 & 0.17\\
\hline
\end{tabular}
\label{tab:verbmatch}
\end{table}

ActionNet(WN) achieves $0.27$ $F_1$ score, while SP gets $F_1=0.17$.
The reason is that SP selects general
concepts which may have higher overlap against each other,
hence they focus on a few senses but miss out other smaller, 
less popular senses.
