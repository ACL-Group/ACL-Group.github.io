
\subsection{E-commerce Concept Classification}

In this subsection,
we mainly investigate how each component of our model influences the performance in the task of judging whether a candidate e-commerce concept satisfy the criteria or not (\secref{sec:classification}).

We randomly sample a large portion of e-commerce concepts from the candidate set and ask human annotators to label \footnote{The annotation task lasts for several months until we get enough training samples.}. The final dataset consists of $70k$ samples (positive: negative= 1: 1). Then we split the dataset into 7:1:2 for training, validation and testing. 

\begin{table}[th]
	\centering
	%\scriptsize
	\begin{tabular}{l|c}
		\hline
		Model & Precision   \\
		\hline
		Baseline (LSTM + Self Attention) & 0.870 \\
		+Wide  & 0.900 \\
		+Wide \& BERT & 0.915 \\
		+Wide \& BERT \& Knowledge & \textbf{0.935} \\
		\hline 
	\end{tabular}
	\caption{Experimental results in shopping concept generation. }
	\label{tab:concept}
\end{table}

Results of ablation tests are shown in \tabref{tab:concept}.
Comparing to the baseline, which is a base BiLSTM with self attention architecture, adding wide features such as different syntactic features of concept improves the precision by $3\%$ in absolute value.
When we replace the input embedding with BERT output,
the performance improves another $1.5\%$, 
which shows the advantage of rich semantic information
encoded by BERT.
After introducing external knowledge into our model,
the final performance reaches to $0.935$, improving by a relative gain of $7.5\%$ against the baseline model, indicating that leveraging external knowledge benefits commonsense reasoning on short concepts.


