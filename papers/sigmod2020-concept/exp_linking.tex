
\subsection{E-commerce Concept Tagging}

To associate those e-commerce concepts which are directly mined from text corpus to the layer of primitive concepts,
we propose a text-augmented NER model with fuzzy CRF mentioned in \secref{sec:tagging}
to link an e-commerce concept to its related primitive concepts.
We randomly sample a small set ($7,200$) of e-commerce concepts
and ask human annotators to label the correct class labels for each primitive concepts within the e-commerce concepts.
To enlarge the training data, 
we use the similar idea of distant supervision mentioned in \secref{sec:eval_mining}
to automatically generate $24,000$ pairs of data.
Each pair contains a compound concept and its corresponding gold sequence of domain labels.
We split $7,200$ pairs of manually labeled data into 
$4,800/1,400/1,000$ for training, validation and testing.
$24,000$ pairs of distant supervised data are added into training set to help learn a more robust model.

\begin{table}[th]
	\centering
	%\scriptsize
	\begin{tabular}{l|c|c|c}
		\hline
		Model & Precision &  Recall & F1  \\
		\hline
		Baseline & 0.8573 & 0.8474 &	0.8523 \\
		+Fuzzy CRF  & 0.8731 &  0.8665 & 0.8703 \\
		+Fuzzy CRF \& Knowledge & \textbf{0.8796} &  \textbf{0.8748} & \textbf{0.8772} \\
		\hline
	\end{tabular}
	\caption{Experimental results in shopping concept tagging.}
	\label{tab:linking}
\end{table}

Experimental results are shown in \tabref{tab:linking}.
Comparing to baseline which is a basic sequence labeling model with Bi-LSTM and CRF, 
adding \textit{fuzzy CRF} improves 1.8\% on F1, 
which indicates such multi-path optimization in CRF layer actually contributes to disambiguation.
Equipped with external knowledge embeddings to further enhance the textual information, 
our model continuously outperform to $0.8772$ on F1.
It demonstrates that introducing external knowledge can benefit tasks dealing with short texts with limited contextual information.