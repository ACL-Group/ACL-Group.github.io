%!TEX root = paper.tex
\section{Background}
\label{sec:background}
This section presents some preliminary knowledge about probabilistic graphical
models and Bayesian inference algorithms.
  
%\subsection{Statistical Inference}
%
%Statistical inference is a common machine learning task of obtaining the
%properties of the underlying distribution of data. For example, one can infer
%from many coin tosses the probability of the coin turning up head by counting
%how many tosses out of the all tosses are head. There are two different
%approaches to model the number of heads: the frequentist approach and the
%Bayesian approach.
%
%
%Let $N$ be the total number of tosses and $H$ be the number of heads.  In
%frequentist approach, the probability of coin turning up head is viewed as an
%unknown \emph{fixed} parameter so the best guess $\phi$ would be the number of heads $H$ in the
%results over the total number of tosses $N$.
%\begin{equation*}
%	\phi = \frac{H}{N}
%\end{equation*}
%
%In Bayesian approach, the probability of head is viewed as a hidden \emph{random
%variable} drawn from a prior distribution, e.g., $\mathrm{Beta}(1, 1)$, the
%uniform distribution over [0, 1]. According to the Bayes Theorem, the
%posterior distribution of the probability of coin turning up head can be
%calculated as follows:
%
%\begin{align*}
%	p(\phi|x) &= \frac{ \phi^H (1-\phi)^{N-H} f(\phi; 1, 1)}{\int_0^1 \phi^H
%	(1-\phi)^{N-H} f(\phi; 1, 1)\mathrm{d}\phi} \\ &= f(\phi; H+1, N-H+1) \numberthis
%	\label{eqn:coin_posterior}
%\end{align*}
%
%\noindent
%where $f(\cdot; \alpha, \beta)$ is the probability density function (PDF) of
%$\mathrm{Beta}(\alpha, \beta)$ and $x$ is the outcome of $N$ coin tosses.
%
%The frequentist approach needs smoothing and regularization techniques to
%generalize on unseen data while the Bayesian approach does not because the
%latter can capture the uncertainty by modeling the parameters as random
%variables. 

\subsection{Probabilistic Graphical Model}

Probabilistic graphical model \cite{pgm} (PGM) is a graphical representation of the
conditional dependencies in statistical inference. Two types of PGM are widely
used: Bayesian networks and Markov networks. Markov networks are undirected
graphs while Bayesian networks are directed acyclic graphs. Each type of PGM
can represent certain independence constraints that the other cannot represent. 
InferSpark currently supports Bayesian networks and regards Markov networks 
as the next step.  
%\begin{figure}[h]
%	\centering
%	\includegraphics[scale=0.38]{figs/one_coin.eps}
%	\caption{Bayesian network of the coin flip model (observed/unobserved random
%	variable are in dark/white)}
%	\label{fig:coin_bn}
%\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.4]{figs/two_coins_latent.eps}
	\caption{Bayesian network of the two-coin model}
	\label{fig:two_coins}
\end{figure}

In a Bayesian network, the vertices are random variables and the edges
represent the conditional dependencies between the random variables.  The
joint probability of a Bayesian network can be factorized into conditional
probabilities of each vertex $\theta$ conditioned on their parents
$\mathrm{F}(\theta)$.  

\figref{fig:two_coins} shows the Bayesian network of the coin flip mode, where
$\phi$'s are two random variable and $x$'s are N coin flip outcomes with the
probability of turning head being the value of one of the $\phi$. The random
variable $z$'s are the choice of coin in each toss, which takes the first one
and the second one with probability $\pi$ and $1-\pi$ respectively.

\subsection{Bayesian Inference Algorithms}


Most real-world models do not have posteriors of familiar forms.
Even calculating the
probability mass function or probability density function at one point is hard
because of the intractability of calculating the probability of the
observed data in the denominator of the posterior. 
Since solving for the exact posterior is
intractable, approximate inference algorithms are used instead.
Although approximate inference is also NP-hard, it performs well in 
practical applications. 
Approximate inference techniques include Markov Chain Monte Carlo (MCMC) 
method, variational inference and so on.
%MCMC algorithms are more
%accurate than the variational inference but the running time is usually much
%longer. 
MCMC algorithms are inherently non-deterministic, and single random number
generator is required to ensure randomness. In a distributed setting, sharing
a single random number generator across the nodes in a cluster is a serious
performance bottleneck. Having different generators on different nodes would
risk the correctness of the MCMC algorithms.
On the other hand, variational inference methods such as
Variational Message Passing (VMP) \cite{vmp}
%of exponential-conjugate Bayesian networks because it
%can be expressed as a 
is a deterministic graph-based message passing algorithm, 
which can be easily adapted to a distributed graph computation model such as
GraphX \cite{graphX}. 
InferSpark currently supports VMP.
Support of other techniques (e.g., MCMC) is included in InferSpark's open-source agenda.

%$z_{ij}$ is an indicator random variable 
%where if the $j$-th coin is chosen
%in the $i$-th experiment,   then $z_{ij} = 1$, else $z_{ij} = 0$ otherwise.
%\ERIC{am I correct here, ZY?}

%
%Let $k \in \{1, 2\}$ be the indices of the coins. Coin $k$'s probability of turning head is $\phi_k$. 
%We repeat the experiment for $N=2$ times. 
%In the $i^{th}$ experiment, we first choose
%one of the coin with probabilities $\pi_1$ and $\pi_2 = 1 - \pi_1$. 
%Let the index of the chosen coin be $z_i$. 
%Then we flip the coin $z_i$ once and get a
%head or a tail, denoted as $x_i$. For any of the variables $v$ above, we use
%$v_j, j \in \dom(v)$ as an indicator variable for $v = j$. For example, $z_{i1}
%= 1$ if $z_i = 1$ and $z_{i1} = 0$ otherwise.

%The VMP algorithm approximates the posterior distribution with a fully
%factorized distribution $Q$.  The algorithm iteratively passes messages along
%the edges and updates the parameters of each vertex to minimize the KL
%divergence between $Q$ and the posterior.  Because the true posterior is
%unknown, VMP algorithm maximizes the evidence lower bound (ELBO), which is
%equivalent to minimizing the KL divergence~\cite{vmp}. However, ELBO involves
%only the expectation of the log likelihoods of the approximated distribution
%$Q$ and is thus straightforward to compute. VMP algorithm maximizes the ELBO
%iteratively by pulling and aggregating messages from neighbor vertices in the
%graph.


%all the random variables in the two-coin model with messages of the 
%algorithm annotated to the edges.  There are 4 types of vertices and 6 types
%of messages in the VMP algorithm for two-coin model, for each of which we only
%show one instance. 

%Implementing the VMP inference code for a statistical model 
%(Bayesian network) $M$
%requires (i) deriving all the messages mathematically  (e.g., deriving $m_{x_1 \rightarrow z_1}$)
%and (ii) coding the message passing mechanism specifically for $M$.
%%Implementing the VMP algorithm for an arbitrary exponential-conjugate Bayesian
%%network involves deriving all the messages and updates and tranlating the
%%mathematics into a real program. 
%The program tends to contain a lot of boiler-plate code 
%because there are many types of messages and vertices.
%For the two-coin model, 
%$x$ would be coded as a vector of integers 
%whereas $z$ would be coded as a vector of probabilities (float), etc.
%Therefore, even a slight alteration to the model, say, from two-coin to 
%two-coin-and-one-dice, all the messages have to be re-derived 
%and the message passing code has to be re-implemented, which is tedious
%to hand code and hard to maintain.


%They
%can be vectors or scalars, floating point numbers or integers. For each
%individual case, specific code needs to be written. A few lines of
%mathematical derivations could easily grow into tens or even hundreds lines of
%code.
%

%To implement the VMP algorithm for an arbitrary exponential-conjugate model,
%the user first need to derive the ELBO $\mathcal{L}$  for the model. For each
%hidden random variable, separating out the terms int $\mathcal{L}$ relating to
%it to calculate the messages to pull from the neighbours.  Secondly,
%implementing the algorithm involves writing a lot of boiler-plate code. For
%each random variable, the user needs to write code to pull messages from all
%its neighbours. Several lines of mathematical expressions could expand into
%tens or even hundreds of lines of code of function definitions, swith-cases
%and loops. For the two-coins model, first step is to write down the ELBO
%(\eqnref{eqn:ELBO}) and derive the messages in \figref{fig:two_coins_mpg} for each
%edge according to (\eqnref{eqn:ELBO_one_term}). To translate the mathematical
%expressions into program, functions and data structures are declared. The user
%also has to write code to initialize each random variables. Finally, write a
%long loop with switch-cases to implement the iterative updates.



%\subsection{Inference on Apache Spark}
%When a domain user has crafted a new model $M$
%and intends to program 
%the corresponding VMP inference code on Apache Spark,
%one natural choice is to do that through GraphX, the distributed graph processing framework on top of Spark.
%Nevertheless, the user still has to go through a number of programming and system concerns,
%which we believe, should better be handled by a framework like InferSpark instead.
%
%First, %VMP-based inference code may require a vertex sends messages to its neighbors \emph{selectively}.  
%the Pregel programming abstraction of GraphX
%restricts that only updated vertices in the last iteration can send message in
%the current iteration.
%So for VMP,  
%% but we can not always have all vertices that need to
%%send messages active. For example, 
%when $\phi_1$ and $\phi_2$ (\figref{fig:two_coins_mpg})
%are selected to be updated
%in the last iteration (multiple $\phi$'s can be updated in the same iteration when parallelizing VMP), 
%$x_1$ and $x_2$ 
%cannot be updated in the current iteration unfortunately 
%because they require messages from $z_1$ and $z_2$, which were not selected and updated in the last iteration.
%Working around this 
%
%if we have updated only 
%iteration, we can not update any of the other random variables because all of
%them have some neighbours that wasn't updated in the last iteration ($x$ and
%$\pi$ for $z$, $z$ for $\pi$ and $x$).  
%The only choice is to restart the
%Pregel operator whenever this happens.
%through the use of primitive  \texttt{aggregateMessages} and \texttt{outerJoinVertices} API
% would not make life easier.
% Specifically, the user would have to 
% handle some low level details such as determining which intermediate RDDs 
% to insert to or evict from the cache.
 
% code
%a lot of branches in order to filter out vertices that should not receive the
%messages.

%
%
%
%
%second, 
%natural choice is Pregel API of GraphX.
%a vertex send messages to all its neighbor.
%describe above, there is a sequence.
%so,needs to add to do post-filtering.
%inefficient
%%
%to get around, fall back to
%can use aggregateMessages (like map, that parse the whole graphs/table as many aggregateMessage tasks).
%a lot of code to do pattern matching.
%and outjoinvertices (like reduce, that for the same key/vertex, the list of messages)

%Second, unlike some iterative machine learning algorithms, e.g., stochastic
%gradient descent (SGD), 
%that cache \emph{the base data} as RDD for repeated access,
%VMP inference would \emph{update} the values (e.g., the parameter) on the graph \emph{iteratively}
%and each iteration may access the \emph{updated graph of previous iterations} repeatedly.  
%When the user cannot cache all intermediate RDDs,
%she has to manually determine which intermediate RDDs should be insert to and evict from the cache.

%Second, the user has to determine the best timing to do checkpointing so as to
%avoid performance degradation brought by the long lineage created by many iterations.
%
%
%Last but not the least, the user may have to customize a partition strategy for
%each model being evaluated. 
%GraphX built-in partitioning strategies are general and thus do not work well with message passing graphs,
%which usually 
%possess (i) complete bipartite components between the posteriors and the  observed variables
%(e.g., $\phi_1$, $\phi_2$ and $x_1, \ldots, x_N$ in \figref{fig:two_coins_mpg}), and
%(ii) large repetition of edge pairs induced from the plate (e.g.,  $N$ pairs of $\langle z_i, x_i\rangle$ in \figref{fig:two_coins_mpg}).
%GraphX adopts a vertex-cut approach for graph partitioning
%and a vertex would be replicated to multiple partitions if it lies on the cut.
%So, imagine if the partition cuts on $x$'s in \figref{fig:two_coins_mpg}, 
%that would incur large replication overhead as well as shuffling overhead.
%Consequently, that really requires the domain users 
%to have excellent knowledge on GraphX in order to carry out efficient inference on Spark. 
%if a user can carefully co-partition the edges related to $x_i$ and $z_i$ for the same $i$


%
%
%In GraphX vertices are replicated in each edge partition where
%there's at least one edge refers to it so the number of replications depends on
%the partition strategy.  Too many replications degrade the performance and make
%the edge partition easily exceed the 2GB limit. In the two-coin model, the
%number of mixture is fixed at 2 but the number of outcomes $N$ may be
%arbitrarily large. The built-in strategies do not scale well as $N$ increases
%because they scatter the edges related to the $x_i$ and $z_i$ for the same $i$
%to differnt partitions, making the largest part ($x$ and $z$) of the graph
%replicated multiple times. If the edges related to $x_i$ and $z_i$ for the same
%$i$ are co-partitioned, the largest part of the graph will only have one
%replication. Creating such a partition strategy is tedious and error-prone
%because the user needs to write a lot of branches to determine the type of the
%vertex solely from its vertex id.

%Our customized partition strategy that colocates all the edges related to
%$z_i$ and $x_i$ for the same $i$ so both $z_i$ and $x_i$ will have 1
%replication. The largest partition will have roughly $2\frac{N}{p}$ vertices.
%\texttt{RandomVertexCut} distribute the edges uniformly at random to each
%partition. The numbers of replications of $z$ and $x$ tend to the degree $4$
%and $6$. The expected number of vertices in each edge partition is $10\eta$,
%$5$ times of our customized partition strategy.
%\texttt{CanonicalRandomVertexCut} colocates edges in opposite directions so
%the size of partition is simply half of that of \texttt{RandomVertexCut} but
%is still $2.5$ times of the customized one. \texttt{EdgePartition1D} colocates
%all the edges with the same source, resulting in the maximum size of partition
%to be $\frac{N}$ vertices. This is the worst because the size is $O(N)$.
%\texttt{EdgePartition2D} guarantee an $2\sqrt{p}-1$ upper bound of the number
%of replications. In the two-coin case, the expected number of vertices in a
%partition is still $10\eta$.


%RDD1 -> RDD2
%if RDD1 is not cached,
%RDD1 -> RDD3
%then RDD1 is recomputed.

%
%Lastly, the user has to carefully adapt the algorithm to a distributed
%computing framework if the data size is too large to fit in a single machine.
%For example, GraphX is a natural choice of graph computation if the user is to
%implement the algorithm on Spark but a handful of technical difficulties need
%to be overcome.  In a typical GraphX application, the property graph has
%homogeneous vertex properties and edge properties. In the shortest distance
%application, the vertex properties are shortest distance from the origin and
%edges properties are weights. The EM algorithm implementation for LDA in
%Mllib, which only computes the Maximum A Posterior rather than the full
%posterior, uses the vertex properties as topic counts and edge propertices as
%word counts. However, the message passing graph of the VMP algorithm generally
%have heterogeneous vertex and edge properties. The statistics to be stored in
%each type of vertex is different. The structure of edges between the different
%types of vertices are also different. For example, $z$ and $x$ in the two-coin
%model are one-to-one but $x$ and $\phi_j$ are fully connected. The user has to
%write code to handle every case to properly initialize the graph.
%
%The original VMP algorithm updates one vertex in each iteration and updating
%all the vertices in the same time is generally not correct, which implies the
%``pregel'' API of GraphX cannot be used. The user has to carefully find out
%the set of vertices that can be updated concurrently without violating the
%correctness. Even if the vertices are updated sequentially, the implementation
%could still be incorrect because there may be vertices sending messages based
%on stale messages. For example, suppose the update sequence in the two coins
%example is $x$, $z$, $\phi$, $\pi$. The messages sent from $x$ to $\phi$ are based on
%the messages sent from $z$ to $x$ in the previous iteration instead of the
%latest. In this case, the ELBO may decrease after an iteration instead of
%increasing. To overcome this problem, an additional update to $x$ should be
%inserted immediately after update to $z$.
%
%To enforce the sequence of updates, the user has to directly use
%``aggregateMessages'' and ``outerJoinVertices'' APIs instead of the ``Pregel'' API
%because the latter applies sends messages from and updates all the vertices in
%each iteration. Using ``aggregateMessages'' and ``outerJoinVertices', however, leads
%to clumsy code because a lot of branches is needed to filter out the vertices
%that should not send message or be updated. An additional difficulty is that
%the ``aggregateMessages'' API uses a commutative and associative operator
%while certain messages should be retained as a list but prepending an element
%to a list is not commutative or associative. To wrap the operator as a
%commutative and associative operator, the user has to pattern match against
%every possible combinations.
%
%Finally, a straight-forward implementation without proper tuning in GraphX
%will easily fail on large-scale data. Firstly, iteratively apply the updates
%without persisting the intermediate results will incur extremely large amount
%of shuffling but persisting all the intermediate results at the same time is
%also not feasible because of memory limits. The user has to carefully
%persist/unpersist intermediate results and force the graph to be materialized
%in each step. The Spark maintains the lineage of RDDs and too long lineage
%will degrade the performance and even overflow the driver program's heap
%space. It is necessary for long iterative jobs to do periodic checkpointing to
%cut the lineage. But due to the design of GraphX, certain operations (e.g.
%foreachPartition on VertexRDD) are not compatible with checkpointing. We also
%find that using ``aggregateMessage'' and ``outerJoinVertecies' directly on the
%checkpointed graph forces the checkpointed graph to be read from disk in each
%iteration, even if it is persisted in the memory \ZY{not sure about the real
%cause}. Using identity function to transform the vertices and then materialize
%the new graph solves this problem.  Therefore, a GraphX program will easily
%break without a lot of trail and error. The user also needs to carefully
%create a partition strategy that is suitble for the structure of the graph
%since the built-in partition strategies are generally not usable for the VMP
%algorithm.  In out experiement, all but EdgePartition2D fails because the size
%of some partition exceeds the limit. Despite the bounded vertex replications
%in EdgePartition2D, the number of replications still make the shuffle size
%much larger than that of a carefully designed partition strategy.

%\subsection{Existing Bayesian Inference Frameworks}
%
%The inference of topic models on large dataset are too expensive to run on a
%single machine, which may take days or even weeks to complete. Distributed and
%parallel computation helps solve the problem but it is much more difficult to
%implement distributed inference algorithm as the user has to
%deal with additional problems that are not present on a single machine, such
%as load balancing, adapting the algorith, performance
%tuning, and etc. Machine learning libraries like Mllib partly solve the
%problem by providing well-tuned implementation of the most common models such
%as LDA but cannot be applied to customized models.
%
%Infer .NET \cite{InferNET14} is a probabilistic programming framework that
%allows the user to define arbitrary probabilistic models.  Infer .NET is
%capable of automatically apply general inference algorithms to the
%user-defined model.  However, Infer .NET is implemented for a single machine
%and thus cannot easily scale up. It addresses the scalability via two
%approaches: batching and streaming, both of which are not perfect.  In the
%batching approach, user can use shared variables between batches and
%iteratively load a batch that fits in the memory and perform inference.  The
%result will be the same as processing through all the data at the same time
%but the speed will be even slower. In the streaming approach, data are splited
%into chunks and each chunk is processed once, which is fast but less accurate.
%

%\KZ{No subsection on probabilistic programming?}
