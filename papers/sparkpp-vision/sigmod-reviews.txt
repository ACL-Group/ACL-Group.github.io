Masked Reviewer ID:	Assigned_Reviewer_7
Review:	
Question	 
Overall rating	Reject; I have serious concerns about this paper that cannot be addressed with a revision
Briefly summarize your review, and rationale for the chosen rating	A specific form of Bayesian networks (variable relationships) implemented on Spark. Solution of hidden variable LDA model solved with a highly specific machine learning model: latent dirichlet allocation (LDA). Some interesting optimizations on a large input graph (Bayesian net) in Spark. Limited scope of technical contribution within Bayesian statistics. Short and uninteresting performance evaluation.
List at least 3 strong points, numbered S1, S2, S3, ...	S1 a customizable Bayesian machine learning model capturing relationships among variables.
S2 scalable to large n (scale up) and to a large number of processing nodes (scale out).
S3 abstract programming model: a few lines in Scala can implement a fairly complex Bayesian model.
List at least 3 weak points, numbered W1, W2, W3, ...	W1 contribution with limited scope: only LDA for a Bayesian net; there exist many more Bayesian models.
W2 no comparison with R libraries or other systems offering Bayesian statistics.
W3 applicability of LDA seems limited to documents/words (i.e. text processing).
W4 experiments are weak and boring; they hardly qualify as big data.
W5 paper ignores a lot of existing working on Basyesian models from the DB community.
Paper review; use this section to provide authors with your detailed feedback, and suggestions on how to improve the paper. Comment on novelty, depth, presentation quality, and soundness and thoroughness of experimental evaluation.	In my opinion this paper creates an artificial machine learning problem to be programmed in Scala in Spark. Once the paper argues that existing libraries and systems in Spark/Hadoop do not offer the specific kind of variable grapical model (Bayesian nets) then it explains in technical detail how to process the input graph adjacency matrix in parallel in Spark.

The Bayesian graph model, LDA topic models, equations and the coin example give a good idea of the approach and make sense to any person who knows machine learning, but I find it too specific. Graphical models are one of many features an analyst needs to make Bayesian statistical inference. Specifying different probability priors, MCMC methods and many other statistical models (regression, generalized linear models, discriminant analysis, distributions diferent from the Gaussian, and so on) are needed for a person who uses Bayesian statistics. Moreover, it does not make sense to re-implement a lot of things that are already avaiable as R libraries that can work these days on computes with a lot of RAM. On the Spark side it would make more sense to reuse and extend existing functions instead of claiming they are useless.

The core of the contribution is how messages are sent/received, and then partitioning/processing the Bayesian graph adjacency matrix. Several partition strategies are considered and O() analysis is provided. However, the paper does not explain how G connectivity affect O(). What happens if is G sparse or dense? Also, the paper is fuzzy abourt what heppens when G exceeds collective RAM.

A discussion about R is necessary since that is what people use. It is well known R has main memory limitations, but that does not mean it it is useless. These days statistician use Hadoop to pre-process data and then then they get a reasonably sized data set that they can analyze in R.

EXPERIMENTS

All data sets considered in this paper are small enough to fit in RAM on a modern server. Therefore, it is feasible to conduct a more thorough performance evaluation comparing with R. For the case that R crashes due to RAM limitation the authors should compare with other systems.

If the paper was conditionally accepted I would request a comparison with existing R libraries for Dirichlet Allocation. Also, I would ask solving other models besides Dirichlet Allocation, which I find very specific to text processing. The authors would need to show they can either solve a more general problem or they can solve it much faster than R (within RAM limits). However, the amount of programming and experimental work is so much that it is probably not feasible in a minor review.

RELATED WORK

The paper ignores a lot of work from the database systems community to accelerate or implement Bayesian statistics at large scale. The authors should read and compare these papers:

Ole J. Mengshoel, David C. Wilkins, Dan Roth: Initialization and Restart in Stochastic Local Search: Computing a Most Probable Explanation in Bayesian Networks. IEEE Trans. Knowl. Data Eng. (TKDE) 23(2):235-247 (2011)

Zhuhua Cai, Zografoula Vagena, Luis Leopoldo Perez, Subramanian Arumugam, Peter J. Haas, Christopher M. Jermaine:
Simulation of database-valued markov chains using SimSQL. SIGMOD Conference 2013: 637-648

Carlos Ordonez, Carlos Garcia-Alvarado, Veerabhadran Baladandayuthapani:
Bayesian Variable Selection in Linear Regression in One Pass for Large Datasets. TKDD 9(1): 3:1-3:14 (2014)

Lise Getoor, Lilyana Mihalkova: Learning statistical models from relational data. SIGMOD 2011:1195-1198
Masked Reviewer ID:	Assigned_Reviewer_8
Review:	
Question	 
Overall rating	Conditional Accept; I will fight for this paper if the authors address the concerns listed below in a revision
Briefly summarize your review, and rationale for the chosen rating	This paper presents InferSpark, an extension to Spark that allows programmers to succinctly represent a Bayesian network and will automatically generate code for a distributed learning algorithm based on variational message passing.

That, at least is the claim. I am somewhat concerned about the lack of details in most aspects of the presentation and the fact that experiments are run on three models that are slight variations of each other, whereas a general implementation would have supported a larger variety.
List at least 3 strong points, numbered S1, S2, S3, ...	S1. A scalable declarative machine learning system
List at least 3 weak points, numbered W1, W2, W3, ...	W1. The paper has very few details about the approach. Most of it is a high-level anecdotal discussion. There are some vital technical challenges that would have had to be solved but were not mentioned at all.

W2. No mention of related work such as distributed deep learning systems that allow programmers to succinctly specify architectures while letting the framework generate code for learning.
Paper review; use this section to provide authors with your detailed feedback, and suggestions on how to improve the paper. Comment on novelty, depth, presentation quality, and soundness and thoroughness of experimental evaluation.	The experiments and the examples in the paper were all constructed from a limited set of distributions: Dirichlet (beta) and Multinomial. If this is all the system can handle, then the applications are very narrow.

There are many other distributions in existence and the choice of distributions along with how they appear in the Bayesian network (conjugate pairs) determines whether computing the expectation in the messages is standard or requires advanced approaches/approximations. The paper calls this step "straightforward" even though research papers have been written about special cases (for example "Variational Inference in Nonconjugate Models").

The paper does not give details of the language extensions beyond a walk-through of a sample program and does not discuss necessary restrictions on the program in order to compute the expectations in the VMP messages or where these restrictions are enforced.

I would be much more relieved to see a discussion of these issues in the paper and experiments using a variety of models. A good source of models is Bishop's book on machine learning and pattern recognition.
If you recommended a Conditional Accept, describe specific issues you would like to see addressed in a revision.	The revision should include an improved discussion of language extensions, details of how the expectations are computed, what restrictions are imposed to make the computations tractable and where this is enforced.

The experiments should also include models that are not variations of LDA.
Masked Reviewer ID:	Assigned_Reviewer_9
Review:	
Question	 
Overall rating	Reject; I have serious concerns about this paper that cannot be addressed with a revision
Briefly summarize your review, and rationale for the chosen rating	This paper proposes,InferSpark,a probabilistic programming framework on top of Apache Spark.The idea is new and interesting.But InferSpark is less efficient than MLlib.
List at least 3 strong points, numbered S1, S2, S3, ...	s1ï¼šThis paper proposes,InferSpark,a probabilistic programming framework on top of Apache Spark.This framework makes statistical inference on big data possible.
s2:InferSpark is more efficient than Infer.NET.InferSpark can enable statistical inference on both customized and standard models at scale.
s3:InferSpark scales well with the data size.When the data size goes up,total running time of the data grows linearly.

List at least 3 weak points, numbered W1, W2, W3, ...	w1: This paper introduces too much information related with the background.The paper should introduce background briefly.
w2:InferSpark is less efficient than MLlib.At least,InferSpark should be as effective as MLlib.
w3:The experiment is too simple.This paper runs LDA,SLDA and DCMLDA on InferSpark.But these models are all related with LDA.The paper should run other kinds of model on InferSpark.
w4:This paper should explain why InferSpark is less efficient than MLlib.
Paper review; use this section to provide authors with your detailed feedback, and suggestions on how to improve the paper. Comment on novelty, depth, presentation quality, and soundness and thoroughness of experimental evaluation.	This paper proposes,InferSpark,a probabilistic programming framework on top of Apache Spark. This framework makes statistical inference on big data possible.InferSpark can enable statistical inference on both customized and standard models at scale. But this paper introduces too much information related with the background. The paper should introduce background briefly.InferSpark is less efficient than MLlib.At least,InferSpark should be as effective as MLlib.And the experiments run similar model on InferSpark.The experiment is too simple.


