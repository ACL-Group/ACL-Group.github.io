\section{Introduction}
\label{sec:intro}

\subsection{Motivation}
\KZ{Our problem is expoential-conjugate bayesian network and
why it's an important and general domain of problems.}

% wide application
Bayesian inference is an important technique to express hypothesis and reason
about observed data. Bayesian approach assigns priors to the parameters in the
models and captures the uncertainty of the parameters by calculating the
posterior distributions instead of treating parameters as fixed unknown
constants. Bayesian networks are a flexible and power tool to specify the
dependencies of random variables and encode probability distributions.
Bayesian networks have been used in a wide range of scientific research. 

Exponential-conjugate Bayesian networks are those that have nodes in
exponential family and conjugate priors. It's a restricted yet useful subset
of Bayesian networks, which is used in various areas like topic modelling,
sentiment analysis and etc.\cite{Titov2008a, Titov2008b, Jo2011} For example,
the Topic Sentiment Mixture model \cite{Mei2007} extracts topics and
sentiments from blogs. Different from the standard LDA model, it incorporates
latent variables to represent positive/negative setiments in each topic. The
model is useful to generate topic-sentiment summarization of a corpus and help
predict user behaviour.

\begin{figure}[!h]
	\epsfig{file=figs/tsm,scale=0.3}	
	\caption{Topic Sentiment Mixture Model}
	\label{fig:TSM_model}
\end{figure}

\KZ{Give detailed applications and is possible their graphical models in pics.}
\KZ{Why programming language (and specifically PP) is the right 
approach for solve these bayesian network inference problems.}

To perform Bayesian inference, the user could use an off-the-shelf
implementation from an existing machine learning library but his choice is
constrained to the models that the library support, which usually are standard
ones like the latent Dirichlet allocation model. When the application needs a
non-standard model to capture the characteristics in the problem, the user,
however, have no choice but to develop his own code. Developing inference code
requires extensive knowledge in both Bayesian inference and programming
techniques. Moreover, model definition, inference algorithm and unrelated code
are all mixed up in the resulting code, making it hard to debug and resaon
about.

Probabilistic proramming languages are designed to cope with the problem.  User
only needs to specify the model in a concise syntax and the inference is
handled automatically by the compiler and the runtime system for
exponential-conjugate and other certain kinds of Bayesian networks.  The
inference code generated by the compiler could be as efficient as that
carefully optimized by the user.

\KZ{Why there is a need to extend PP to distributed/parallel computing.}
% large data set
\KZ{Stick to the speech/topic examples. You need
a couple of driving examples and keep using them.}

When applying Bayesian inference, the huge data size often overwhelms the
capacity of a single machine. For example, when the TSM model extracts users'
positive/negative opinion from hundreds of millions Amazon reviews, the data
could be too large to fit into the memory of a single machine and even if it
does, training will take days or weeks. To perform the inference task
efficiently, the user has to resort to a distributed and parallel solution, in
which case the training may only takes hours. But it is hard and
time-consuming to write and tune inference code for large scale data even if
the user uses popular distributed computing frameworks \cite{Cai2014}. The
problem applys to other models in various applications.

\KZ{Why existing PP cannot be easily extended to distributed framework
and how we are diff from them.}
%% Existing pp
It is useful to have a probabilistic programming language that supports
flexible user-defined models and that is able to perform distributed and
parallel inference automatically. However, existing probabilistic programming
languages (e.g. Figaro \cite{pfeffer2009figaro}, Infer .NET \cite{InferNET14},
FACTORIE \cite{McCallum2009}) have put more emphasis on the expressibility and
inference algorithms. They are designed for a multi-threaded or
single-threaded machine and hard to be ported to distributed computing
frameworks. For example, the Figaro model objects are mutable and caches
current values while Spark RDD stores immutable objects. It is not possible to
overcome the problem without make significant amount of changes to the Figaro
library.

\subsection{Our solution}

Our solution is to design a probabilistic programming language embedded in
scala and base its inference engine on Spark, a high-performance distributed
data processing framework.

We currently only focus on exponential-conjugate models so that the posterior
has known forms and inference can be handled by efficient algorithms
automatically. Variational message passing and Gibbs sampling are two
algorithms that can be applied to these models without any user derivation.

\subsection{Contributions}

\begin{itemize}
	\item We design a probabilistic programming language embedded in scala
		that can be used to concisely represent bayesian networks.
	\item We implement efficient inference algorithms on Spark to enable
		automatic inference on large-scale data.
\end{itemize}

