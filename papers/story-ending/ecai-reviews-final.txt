SUBMISSION: 263
TITLE: Enhanced Story Representation by ConceptNet for Predicting Story Endings

-------------------------  METAREVIEW  ------------------------ This paper is now rated negatively by all the reviewers, thus the SPC recommends reject.
Reviewer 1 changed their rating from positive to negative after rebuttal; reviewers 2 and 3 were not moved by the rebuttal. All the reviewers found the rebuttal much weaker and superficial than what they were hoping for. 
In particular, reviewer 1 considered that "authors do clarify how stopwords are discarded, but this should have been said in the paper, rather than explaining that words not appearing in ConceptNet are discarded. This means among other things that Algorithm 1 is too shallow and not faithful to what is intended, as pointed out by reviewer 3." Reviewer 1 is also "not convinced by the reply regarding human performance: in addition to clearly stating in the paper what evaluation experiment has been carried out, a tentative explanation for the discrepancy with the 100% ROCStories performance was due." 
SUBMISSION: 263
TITLE: Enhanced Story Representation by ConceptNet for Predicting Story Endings

-------------------------  METAREVIEW  ------------------------ This paper is now rated negatively by all the reviewers, thus the SPC recommends reject.
Reviewer 1 changed their rating from positive to negative after rebuttal; reviewers 2 and 3 were not moved by the rebuttal. All the reviewers found the rebuttal much weaker and superficial than what they were hoping for. 
In particular, reviewer 1 considered that "authors do clarify how stopwords are discarded, but this should have been said in the paper, rather than explaining that words not appearing in ConceptNet are discarded. This means among other things that Algorithm 1 is too shallow and not faithful to what is intended, as pointed out by reviewer 3." Reviewer 1 is also "not convinced by the reply regarding human performance: in addition to clearly stating in the paper what evaluation experiment has been carried out, a tentative explanation for the discrepancy with the 100% ROCStories performance was due." 
In summary, all reviewers considered that the research described in the paper is potentially interesting but that important details have been omitted which should be described in detail.



----------------------- REVIEW 1 ---------------------
SUBMISSION: 263
TITLE: Enhanced Story Representation by ConceptNet for Predicting Story Endings
AUTHORS: Shanshan Huang, Kenny Zhu, Qianzi Liao, Kaijian Li and Kangqi Luo

----------- Relevance -----------
SCORE: 4 (good)
----------- Significance -----------
SCORE: 4 (good)
----------- Novelty -----------
SCORE: 3 (fair)
----------- Technical quality -----------
SCORE: 4 (good)
----------- Quality of the presentation -----------
SCORE: 4 (good)
----------- Overall evaluation -----------
SCORE: -1 (weak reject)
----- TEXT:
Added after rebuttal: 
I'm surprised that what I took as unclear presentation actually is the omission of an important part of the simplification process. This means among other things that Algorithm 1 is too shallow and not faithful to what is intended, as pointed out by reviewer 3. I'm also not convinced by the reply regarding human performance: in addition to clearly stating in the paper what evaluation experiment has been carried out, a tentative explanation for the discrepancy with the 100% ROCStories performance was due. 

-----------------

This paper addresses the task of predicting endings for short narrative stories through the standard Story Close Test (SCT) by 1) "simplifying" the sentences, i.e., keeping only the words and multi-words corresponding to concepts in the very large knowledge graph ConceptNet, thus pruning the sentences from "less-informative" words, and 2) using concept embeddings from Numberbatch (based on ConceptNet) instead of word embeddings, thus leveraging on the commonsense knowledge encoded in the ConceptNet graph. The paper's contribution is also enhanced with a revised training and validation sets to fix the known biases in the ROCStories dataset. The proposed method is experimented building on 3 pre-trained models, SKBC [41] giving the best results. It is compared with a significant number of baselines, giving a 5% increase in accuracy over SKBC alone (second best). The impact of (1) alone (simplifying based on ConceptNet) is also assessed and compared with other simplification bas!
 elines. 
This is an interesting proposal showing how knowledge graphs can improve state-of-the-art approaches to the SCT task. Although ConceptNet has already been used for this task, the method proposed (which exploits ConceptNet for both 1 and 2) improves the results of the best of these previous methods (ISCK) by 7.2% accuracy. The paper is well written and accessible to a large AI audience.

A few critical comments though, should be considered to improve the paper (probably easily): 
First, the simplification algorithm is based on the fact that certain words and multi-words appear in ConceptNet while other do not. However, the discarded words in the two examples provided (Fig.1 and Section 2.1) do all --including pronouns, copula, adverbs-- appear in ConceptNet 5.7 (queried on-line at the time of this review). Perhaps an earlier version of ConceptNet has been used? If this is the resource used, how are some concepts selected while others discarded? The sentence: "If the concept is not in Numberbatch, we approximate its concept vector by averaging the vectors of all its constituent words which can be found in Numberbatch" implies that more than the 2M+ subset exploited in Numberbatch are being used.
Second, I do not understand the human performance figure used to argue for the biases in the ROCStories dataset, especially in "However the ending-only results for SCTv1.5 is still higher than human [47]". In standard publications on the ROCStories dataset, including [31] and [47], human performance is given at 100%, not at 62.4%, and this is also what you use in Table 6 as expressed in the text "Human performance is 100% and can be viewed as an upperbound [31]". Where does the 62.4% figure come from and why does it differ so much from the standard 100% figure? At least a reference is missing, or, if it comes from an experiment of your own, much more is needed to describe this experiment.
Third, I couldn't understand the difference between your new training dataset and on of those proposed in [41] given: "We follow Roemmele et al. [41] to generate negative examples for the 98161 stories in ROC training set by Random and Backward method with 4:2 proportion which is simple but effective for negative ending generation." Where is your contribution here?  
Finally, why not including as baselines the methods proposed in [47]? Although your proposal seems to perform better than EndingReg in [47] on SCTv1.5 (based on the 64.4% accuracy given in [47] and your Table 5), it would be interesting to test it on your new training and validation sets and improve the comparison with state-of-the-art.

Other comments:
- The discussion of Fig.1 makes sense only assuming that there is no analysis of anaphora and other discourse phenomena. "The name Tiffany is often associated with jewelries, thus the introduction of this meaning into the story context does more harm than good": In this context, Tiffany / she is not very relevant because she is the only character involved in the story. But it suffices to change both "Tiffany" and "she" in the two endings by "Bill" and immediately ending 1 becomes the best ending (one could infer that since Tiffany is off work, someone else needs to go). 
- The use of "positive" and "negative" endings on p.4ff is confusing. I suppose positive is the "right" ending and negative the "wrong" one, but I suggest that you keep the original ROCStories terminology, to avoid confusion with syntactically positive or negative sentences. 
- "ISCK [8] incorporates sentiment and commonsense feature between the context and ending into [38]’s text representation, which can get a little improvement." (p. 4)  It is worth saying here that "commonsense" is grounded on ConceptNet just as your approach.
- p.5 "In Table 2, we can find the machine learning with only endings performs worse than human on our reconstructed training dataset" But have you conducted human experiments on the new dataset? Otherwise, you are comparing two different tasks. 
- p.7 Section 4 on Related Work. "For this story cloze task, the semantic language model (SemLM) [35], which serves as the language model at frame level, is able to represent event sequential semantics [19, 7]". I do not see in the paper how you compare your method with the SemLM approach. 

Typos: 
two group of baseline methods
encoder-decoder archicheture
while potentially helps discover more concepts BERT_BASE and BERT_LARGE which implement by us Our BERT_BASE does not preform For reasoning over these knowledge



----------------------- REVIEW 2 ---------------------
SUBMISSION: 263
TITLE: Enhanced Story Representation by ConceptNet for Predicting Story Endings
AUTHORS: Shanshan Huang, Kenny Zhu, Qianzi Liao, Kaijian Li and Kangqi Luo

----------- Relevance -----------
SCORE: 4 (good)
----------- Significance -----------
SCORE: 2 (poor)
----------- Novelty -----------
SCORE: 3 (fair)
----------- Technical quality -----------
SCORE: 2 (poor)
----------- Quality of the presentation -----------
SCORE: 2 (poor)
----------- Overall evaluation -----------
SCORE: -2 (reject)
----- TEXT:
This paper presents a novel sentence representation model which uses commonsense reasoning concepts found in ConceptNet. The main idea is that these concepts can enhance the sentence representation allowing it to more faithfully capture the semantical meaning of the story resulting in a more accurate story ending prediction.

The idea of using conceptual information to reduce the statistical bias in machine learning techniques is good and I believe the authors are on the right track. There are some good points raised, especially regarding the existence of statistical cues in the datasets (Section 3.2).

However, the paper suffers from a number of problems. In terms of presentation, the grammar is poor at times, making quite a few sentences difficult to understand; there are misspelt words; contractions that should be avoided; and several sentences miss the definite article "the". 

In technical terms, some components are not described in sufficient detail or explained properly. For example, Figure 2 does not really depict an "architecture", but rather it summarises a set of steps in a process. Whereas the first step is reasonably well summarised, the depiction of steps 2 and 3 is not really useful. It is not entirely clear how algorithm 1 works. Is C the set of all concepts in ConceptNet? 

More attention needs to be paid to the tables as they are the main conveyor of results. 
In particular, none of the tables are particularly well described or presented. Table 6 should show both accuracies for comparison and Tables 7 and 8 are prooly explained. It would be good to have an overall evaluation summarising the main results, as this is not present in the conclusion either.



----------------------- REVIEW 3 ---------------------
SUBMISSION: 263
TITLE: Enhanced Story Representation by ConceptNet for Predicting Story Endings
AUTHORS: Shanshan Huang, Kenny Zhu, Qianzi Liao, Kaijian Li and Kangqi Luo

----------- Relevance -----------
SCORE: 3 (fair)
----------- Significance -----------
SCORE: 2 (poor)
----------- Novelty -----------
SCORE: 3 (fair)
----------- Technical quality -----------
SCORE: 2 (poor)
----------- Quality of the presentation -----------
SCORE: 3 (fair)
----------- Overall evaluation -----------
SCORE: -1 (weak reject)
----- TEXT:
The paper proposes an enhanced representation of story structures in order to improve the task of story ending prediction. 
The enhanced narrative representation is based on the creation of novel embeddings obtained by merging  ConceptNet Numberbatch Embeddings with Concept Sequence Encoder embeddings. The authors have tested the obtained representation on the prediction of story ending in a novel ROCStory Cloze Test.

I have several concerns about the paper. I will try to briefly summarize the main ones in the following:

- It is not clear what is the actual contribution of the paper since in the evaluation part no comparison has been done with the actual elements forming the extended embeddings (i.e. ConceptNet and Sequence Encoder Embeddings). This is problematic since most of the work seems to be done by ConceptNet and by its “commonsense knowledge”. If this is the case (and currently the paper does not exclude this possibility) the narrative of the paper about the enhanced story “commonsense” story representation is not really convincing.

- The linguistic simplification of the sentences (phase 1, described in Algorithm 1) lacks many technical details in order to fully understand how the “fuzzy match heuristics” actually works. Also, it is not clear at all how the “irrelevant” words are discovered before being discarded. There is no detail at all on this point Is this a manual process? (by reading the paper it seems so, but this obviously a weak point of the overall framework which is supposed to be a framework for “end-to-end ending prediction accuracy”. page 2)

- Another aspect that in my opinion is not clear concerns the comparison of the results on a novel dataset (created by the same authors) with previous works done on another dataset. In particular, it is not clear to me if the previous work has been re-implemented and retrained on the new dataset so that the comparison makes sense. Also, it is not clear how the baseline on "human performances" (62.4%?) can be based on 5 annotations  (a number that is not acceptable for doing any kind of generalization).

- Finally, the related work part is not sufficiently developed since it mainly lists (in part) alternative approaches to the representation of commonsense knowledge without actually a complete critical or empirical comparison between the current proposal and other models used for representing events and stories (like Framester or the commonsense linguistic resource Cover which also builds on ConceptNet and, additionally, on BabelNet/Nasari).



