%%%% ijcai20-multiauthor.tex
\typeout{IJCAI--PRICAI--20 Multiple authors example}
% These are the instructions for authors for IJCAI-20.
\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai20.sty is NOT the same than previous years'
\usepackage{ijcai20}

%%\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.
\usepackage{times}
\renewcommand*\ttdefault{txtt}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\urlstyle{same}
\usepackage{latexsym}

\usepackage{color}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\newcommand{\figref}[1]{Figure \ref{#1}}
\newcommand{\eqnref}[1]{Eq. \ref{#1}}
\newcommand{\tabref}[1]{Table \ref{#1}}
\newcommand{\secref}[1]{Section \ref{#1}}
\newcommand{\algoref}[1]{Algorithm \ref{#1}}
\renewcommand\appendix{\setcounter{secnumdepth}{-2}}
\newcommand{\KZ}[1]{\textcolor{blue}{Kenny: #1}}
\newcommand{\shanshan}[1]{\textcolor{purple}{shanshan: #1}}
\newcommand{\eve}[1]{\textcolor{red}{eve: #1}}
\newcommand{\scite}[1]{\citeauthor{#1}~\shortcite{#1}}

\begin{document}

\title{Story Understanding by Structure Knowledge Guided Pretrained Language Models}

%\author{Shanshan Huang\institute{Shanghai Jiao Tong University, China, email:huangss\_33@sjtu.edu.cn}
%\and Kenny Q. Zhu\institute{Shanghai Jiao Tong University, China, email:kzhu@cs.sjtu.edu.cn}
% \and Qianzi Liao\institute{Shanghai Jiao Tong University, China, email:liaoqz@sjtu.edu.cn}
%   \and Kaijian Li \institute{Shanghai Jiao Tong University, China, email:likaijian@sjtu.edu.cn}
%   \and Kangqi Luo\institute{Data Science Lab, JD.com, China, email:luokangqi@jd.com}}
%\author{Name1 Surname1 \and Name2 Surname2 \and Name3 Surname3 \and Name3 Surname3 \and Name3 Surname3\institute{University,Country, email: somename@university.edu} }
\maketitle

\begin{abstract}
Pre-trained language models, such as BERT, made big progress recently in
story ending prediction by exploiting the statistical bias in the
dataset, instead of ``understanding'' the stories per se.  
%However, previous research have shown that there exits 
%spurious patterns in the standard dataset.
%In this paper, we propose to generate new training data 
%to avoid spurious patterns. 
%And the accuracy of pretrained models significantly drops.
%In addition, we propose to use structure knowledge guide pretrained models in sentence encoding.
In this paper, we improve the representation
of stories by first simplifying the sentences to some key commonsense
concepts and second modeling the structural relationship among these concepts 
in ConceptNet, a large commonsense knowledge base. 
Such enhanced sentence representation, 
when used with pre-trained language models, 
makes substantial gains in prediction accuracy on the popular ROCStory
Cloze Test using the corrected, unbiased training data. 
\end{abstract}

\input{intro}
\input{approach}
\input{experiment}
\input{related}
\input{conclusion}

\bibliographystyle{named}
\bibliography{story}
%\appendix
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
