It appears that R1 isn’t very knowledgeable in this field. For example, R1 asked such novice questions as “what vectorizer do they use?” and “what happens if vectors have different dimensions”, which shows R1 isn’t familiar with neural-based approaches. R1 also complained that we didn't define “transformation mechanism” and “attention mechanism” which are well-known today (and were cited). Besides, R1 didn't read our paper carefully because R1 is hung up on the "parser" being our main contribution which wasn’t in our contribution list (Sec 1). R1 nitpicked the issue of λ, which was not arbitrarily chosen, but a designed parameter.

R2 also mistook our contributions. R2 claimed training dataset creation, dataset bias discovery and structured knowledge generation are our contributions. In fact, our contributions are that we simplified sentences to concepts and incorporated structured knowledge into representation for improvement on a fairer dataset. Besides, R2 said “it would be interesting to see…Chen and Chaturvedi, perform when trained with the new dataset.” But we already compared with these models (ISCK and SeqMANN) in Table 4. 

We agree with R3 that our evaluation would be more complete with the experiment on original SCT 1.0. But our position was since most of previous methods benefit from extraneous cues in SCT 1.0, while our simplification approach may remove some of the cues, it's really not a fair on SCT 1.0 with information leak. So even if we did that experiment, whatever the outcome may be, it wouldn't weaken our contributions.

R4 made some factual errors. 1) R4 claimed that we overlooked the comparison with other methods which also exploit the extra information, especially Chen. In fact, we did already compare with Chen and several other methods in Table 4, e.g., GSMA and FES-LM which also use extra information such as ConceptNet, PropBank and FrameNet. 2) R4 claimed that we didn’t show the results on SCT 1.5. But we did exactly that in Table 2.
