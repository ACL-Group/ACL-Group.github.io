
Paper ID2829
Paper TitleStory Understanding by Structure Knowledge Guided Pretrained Language Models
Track NameIJCAI2020
Reviewer #1
Questions
4. Comments to authors
Overall: The authors propose a method for predicting the end of short stories, using commonsense knowledge from the Web Knowledge Graph ConceptNet (CN) and short stories from Story Clozed Test dataset. More specifically, they consider that three sentences are given as input, and the framework must predict the last one, as output. Their method relies on parsing the three first sentences, then try to find the corresponding entity in the CN, and then by getting the most related entities in CN for the entities annotated in the free text by the NLP parser, they see if any ending sentence in the Story Clozed Test dataset has all/some of these entities in it. Finally, the most related sentence is given as an answer (the ending sentence which has the most corresponding entities from CN in it).

Major Issues:
It is rather strange that the related work is at the end of the paper, given that several other studies are mentioned in the main body of the paper. I had to reach the related work section to understand how these mentioned papers are related to this paper.

Also, given that Introduction and Evaluation sections are lengthy they seem to miss important information that proves crucial in understanding their methodology. I elaborate in the following.

Introduction
It is mentioned that “…what happens next in the story…”. What should that imply? I think a formalization should be helpful to make it easier to understand. Then, they also say they get the corresponding entities in CN for each word annotated by the NLP parser. How? Do they use Web API, do they have some string similarity metric, or something else? Also, no information is given about how the NLP parser is getting the important frames from sentences.

Section 2
Before starting their algorithm, they define some values vaguely and abstractly, such as λ, without explaining how they decided to define it.
The description of the algorithm in natural language is very brief. In order to understand their method, you need to understand the pseudo-code. I believe it is necessary to have a step by step description of the algorithm to make it easier to understand.
The algorithm has some steps that need deeper explanation. For instance:
(a) First, they try to find correspondence between a word (annotated by the parser), and an entity from the CN graph, only with the first component of the CN entity (if it is composed from more than one words). Why? Did they try other methods?
(b) Moreover, if they find correspondence between a word and some CN entity, they consider a temporal variable t with the word that was bind, concatenated with the next i + length(concept words) + λ (i index of word in the sentence, λ integer value arbitrarily defined) from the same sentence only in one direction (from left to right) Why? And why not the other way around?
(c) Also (minor), they define N as the length of the sentence. What happens if i + length(concept words) + λ >N? This could happen because they use this index to define the temporal variable t.

In section 2.2 it is mentioned that “…more informative words…”. What is the definition of an “informative word”?
Other crucial information missing, from this section:
(a) They get “vector representation of words”. How? What vectorizer do they use?
(b) In the same section they define the vector H^{kg} structured knowledge representation. It is not stated what this is supposed to represent. I presume it is the vector representation of the sentence, and they compute it by summing the vector representations of the words. First, what happens if vectors have different dimensions (they do not mention any restriction to the dimensions of the vectors; therefore, this is a plausible question that may rise). Secondly, why sum and not any other function between vectors? Did they try other functions and got worse results in their evaluation?
(c) A word from a sentence that does not have a corresponding CN entity is labeled as UNKNOWN, and a random vector is assigned to it. This seems odd. I would consider not assigning a random vector as a value to an UNKNOWN word, but rather not counting them at all. I believe that giving random values will distort the vector representation.
Crucial information about the probability distribution is missing. Actually, it is not mentioned how they define it.

Evaluation
They mention that they reconstruct a dataset, in order to evaluate their method. This may lead to bias.
Section 3.1 has unexplained values. In the part where they talk about the BERT baseline, they consider some values without explaining why they use the specific values. It should be explained if they tried other values and how they decided the ones they use.
Section 3.2 mentions the term “negative endings” but they do not define previously what this term means.
Section 3.3. The sentence “Transformation mechanism which is attention mechanism” is incomprehensible. What is a “transformation mechanism” and what is an “attention mechanism”? No definitions are given.

Related Work
I think some papers from the field of Node Embeddings should be mentioned, such as:
(1) Finding Interpretable Concept Spaces in Node Embeddings using Knowledge Bases. Idahl et. al.
(2) Matching Node Embedding from graph similarity. Nikolentzos et. al.
Their method seems close to node embeddings, where given a set of nodes in an RDF graph a new node that is semantically related with the previous ones is found. In these methods, commonsense knowledge, path similarity, or property similarity are often used. Therefore, having a number of sentences and trying to predict the ending of a story based on commonsense knowledge seems pretty close.

Conclusion
Toο steep. It does not recap properly the paper. It gives only one future direction. Seems as this research field does not have a future potential.

Minor Issues
Some references in the “Related Work” section are not cited properly and [?] is displayed in the section.
More formal language should be used in expressions like “what happens next”, “Let’s call”.
“Wikipedia” should become “Wikipedia”.
Reviewer #2
Questions
4. Comments to authors
Summary:
The paper focusses on the Story Close Task with two types of contributions. Firstly, the authors simplify the story sentences to only contain the important concepts and also try to model the structure of the knowledge sub-graph that contains these concepts. They then use this simplified representations in several models like BERT, SKBC and DSSM to show that using these simplified representations help in predicting the ending of a given story. Secondly, they mention that the validation dataset, used to train most models, is biased and propose to create a new training set by sampling negative endings for training stories. They explore two different ways of sampling negative endings: Random and Backward. They empirically show that many models trained only on the endings perform from the SCT validation set perform well on the standard test set but their performance reduces to close to random when they are trained on endings from the proposed train set.

Major strengths:
-- Highlights the importance of commonsense knowledge for story understanding with respect to a well-known task.

Major weaknesses:
W1) "Then we randomly choose one from the six (4 from Random and 2 from Backward) alternative endings to ensure the balance of positive and negative data." Since training dataset creation is one of the contributions of this paper, as stated in para 3 of Sec 1, this is an important sentence but it is not clear to me what is happening here. Why 4 and 2? The authors should also include examples to demonstrate that the training examples hence created make sense. Table 1 helps but one needs to read examples to judge the utility of the sampling methods. It would also help to compare the two sampling methods and show why one (or a combination) is better.
W2) "Our approach, when combined with the suitable language model, beats the recent state-of-the-art methods by substantial margins using the corrected, unbiased training data (Section 3.4)." The models used in the main experiment to support this claim (Table 4) are not SOTA. All that Table 4 says is that it is useful to train generic models (like BERT and SKBC) using the proposed representations as far as story understanding is concerned. The authors should either revise their contributions or compare with SOTA methods. For example, Chen et al, 2018 report strong numbers. If the authors choose to claim SOTA results, it would be interesting to see how retraining models that yield good accuracy figures but do not explicitly rely on stylistic patterns, like Chen et al, 2018 and Chaturvedi et al, 2017, perform when trained with the new dataset.
W3) The related work is missing several citations for Story Cloze Test listed below. Even the references that are included in the paper are scattered all over the place. I suggest having a dedicated section on SCT models to give readers a good perspective on what has been done so far.
W4) "In this work, we discover that the SCT dataset suffers from similar statistical bias,..." The authors are overclaiming their contributions here. The bias has already been shown in previous work (e.g. by Roy Schwartz and others mentioned in 3.2) and they should be cited here.
W5) The title and introduction of the paper claim that the method is using "Structured knowledge". However, from Sec 2.2, I understand that H^{kg} is simply sum of representation of all concept-representations. This is misleading because relations and the graph structure do not play any role in this representation! Either the authors should not call this graph representation (in which case, the utility of H^{kg} over H^{seq} should be clarified) or the authors should use a Graph Neural Network to encode the graph. That might also improve performance.
W6) Writing needs to be improved (see presentation suggestions below)

Missing citations:
1. Cai, Z.; Tu, L.; and Gimpel, K. 2017. Pay attention to the ending:strong neural baselines for the roc story cloze task. In ACL.
2. Chaturvedi, S.; Peng, H.; and Roth, D. 2017. Story comprehension for predicting what happens next. In EMNLP.
3. Schwartz, R.; Sap, M.; Konstas, I.; Zilles, L.; Choi, Y.; and Smith, N. A. 2017. Story cloze task: UW NLP system. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics.
4. Radford et al, 2018. Improving Language Understanding by Generative Pre-Training. (this is an arxiv paper).

Suggestions:
-- Page 2 last paragraph. I understand that in this example authors keep both 'come back' and 'come for', but it is not clear what happens to other concepts like 'hope', 'later', etc.
-- Section 2.2, first paragraph. s_c is not defined. Are C_s and s_c being used interchangably? Also, Fig 2 mentions s_{c_1}, s_{c_2} which are not defined but seem to be the same as C_s.
-- What do 1 and 2 represent in the following sentence? "The classification target y \in {1,2}, corresponding to e1 and e2."
-- It would help the reader to define I_s in text, and not just in the algorithm.
-- Sec 3.3: "It is because BERT consists of Transformer unit which is attention mechanism." It would help to clarify what 'it' refer to.
-- The model is being trained on a much bigger set than models which use the validation set. The authors should comment on that.
-- Authors should justify why they choose to have size of ROCS*(V) (about 20K) much bigger than SCT(V) (about 2K)?

The presentation needs to be improved.
-- "Though the performance for some models are very good (e.g. 91.8% for BERT based model [Li et al., 2019])." --> remove 'though'
-- "there may be noises in the data that are not important to story understanding but CAN unintentionally GET picked up by the model".
-- "Pretrain models..." should be "Pretrained models..." (multiple occurrences)
-- "ConceptNet[Speer et al., 2017], which is a large symbolic and structured knowledge BASE to assist the"
-- Sec 3.1 "Pretrained models are more powerful to represent a sequence by considering the context words compared with traditional word embeddings." --> "Pretrained models, which represent a sequence by considering the context words, are more powerful compared with traditional word embeddings."
-- Fig 2 "simplify sentence" --> "simplified sentence"
-- Chen et al., 2018 was published in AAAI 2019. Please cite the conference paper instead of the arxiv version.
-- Missing citation in "For this story cloze task, the semantic language model (SemLM) which serves as the language model at frame level, is able to represent event sequential semantics [?]."
-- "Story Understanding by Structure Knowledge Guided Pretrained Language Models" --> use Structured instead of Structure.
Reviewer #3
Questions
4. Comments to authors
[Summary:]
The problem of Story Understanding under investigation is certainly interesting, and it perfectly fits in an AI conference like IJCAI, especially NLP domain. More specifically, the authors of this paper claim three points. (1) Even though pre-trained language models have make progress in story ending prediction by exploiting the statistical bias in the dataset, it can not understanding the stories or explain the selections. (2) SCT dataset suffers from similar statistical bias, and there exists information leakage from the train data to the test data. (3) Data-driven pretrained models may have noises in the data that are not important to story understanding but unintentionally picked up by the model. Thus, this paper proposes: (1) a concept extraction to simplify the original sentences of the story based on ConceptNet (2) a representation of sentence by concatenating the simplified sentence encoding vector and the concept graph encoding vector incorporating the pre-trained language model (3) a new training and validation dataset generated from automatically augmenting ROCStories corpus with negative endings.

[Strengths:]
(1) The paper have proposed a clear idea to construct structure knowledge for story understanding.
(2) The paper have built a new dataset, which is so called unbiased.
(3) The authors have conducted an extensive experiment to examine the performance of the proposed method.

[Weaknesses:]
(1) Even though this paper have conducted a new setting for story understanding, the baseline of this paper is very preliminary.
To demonstrate the information leakage does exist in SCT dataset, this paper trains five state-of-the-art models only on the ending of each story in both SCT(V) and the new dataset, named ROCS*(Tr). As the result shows, the models trained on SCT(V) dataset achieve much higher accuracy than ROCS*(Tr) dataset. The experimental result provides that SCT(V) dataset introduces statistical biases and the new dataset can reduce the biases. To evaluate the effectiveness of the proposed sentence simplification and concept graph encoding graphs, this paper adds the proposed method into three baselines and compares the performance with the original models. As the result shown, adding concept graph encoding into DSSM model performs worse than the DSSM. According to section 3.4 of the paper, its experiments are not meant to demonstrate the superiority of a particular algorithm but show that the proposed story representation methods work for a variety of models. I wonder why the authors doesn’t apply the proposed methods to more baselines to prove the effectiveness of it, such as the BERT based model which achieves the accuracy of 91.8%. The BERT_BASE(OURS) model in Table 2 and 3 retrains the language model of BERT_BASE model and it performs worse than the original BERT_BASE model according to Table 4.

(2) The definition of the common knowledge and some others in this paper is vague. The proposed method of this paper can be seen as integrated, so called in this paper, which is not novel and does not fit for this conference.
Concept extraction algorithm (Algorithm 1) can be improved. This paper should refer more other methods.
Some problems of the Framework overview in Figure 2: (1) the connection between Step 2 and Step 3 is unclear (2) the vector into the Classifier has no annotation (3) P(S) out from the Classifier isn’t mentioned in equations or other position of the paper.

(3) This paper is not well written. When reading the paper carefully, we can find some other problems.
-In section 3.3, “CE” is mentioned twice. However, this shorthand doesn’t appear above.
-The format of some citation is wrong, for instance, “SIMP, GPT, ISCK, and TransBERT [Srinivasan et al., 2018; Radford et al., 2018; Chen et al., 2018; Li et al., 2019].”
- ......
Reviewer #4
Questions
4. Comments to authors
The authors propose some new methods to improve ending prediction. Specifically, they propose to simply a sentence by using only it key concepts (based on ConceptNet), and/or combination of sequential and structured representation for the key concepts in a sentence as the sentence representation. Experiments shows that these methods improve the accuracy of baseline algorithm on both the dataset designed by the authors and other data set such set SCT(V)1.5. The paper is easy to follow.

My main problems with the report in the paper is as follows. There are some explanations on why the proposed ideas (e.g., simplification of sentences) work. Consider the explanation based on the example in Figure 1. "Most of the words in black ... may distract the data-driven models." It is not clear the claim is speculative or based on data. The evaluation only considers how the proposed ideas improve the baseline algorithms. Since the proposed ideas make use of extra information, it is supposed to improve baseline algorithms. A good evaluation should compare the proposed ideas to other methods which also exploit the extra information (e.g., Chen et al 2018). An exhaustive inclusion of related work using extra knowledge for ending prediction is expected. Or the author may claim that no such work exists. The new data set seems to be novel. However, it would be a different topic and should be analyzed and compared with SCT(V)1.5 (at least in a manner in the SCT(V)1.5 paper). The conclusion also seems to be too general compared with the findings in the experiments. You may also want to explain why an improvement of 3.43% or 4.75% is taken as significant.
Go Back
© 2020 Microsoft Corporation About CMT | Help | Terms of Use | Privacy & Cookies | Request Free CMT Site CMT Supportactually works. Also, it is not clear at all how the “irrelevant” words are discovered before being discarded. There is no detail at all on this point Is this a manual process? (by reading the paper it seems so, but this obviously a weak point of the overall framework which is supposed to be a framework for “end-to-end ending prediction accuracy”. page 2)

- Another aspect that in my opinion is not clear concerns the comparison of the results on a novel dataset (created by the same authors) with previous works done on another dataset. In particular, it is not clear to me if the previous work has been re-implemented and retrained on the new dataset so that the comparison makes sense. Also, it is not clear how the baseline on "human performances" (62.4%?) can be based on 5 annotations (a number that is not acceptable for doing any kind of generalization).

- Finally, the related work part is not sufficiently developed since it mainly lists (in part) alternative approaches to the representation of commonsense knowledge without actually a complete critical or empirical comparison between the current proposal and other models used for representing events and stories (like Framester or the commonsense linguistic resource Cover which also builds on ConceptNet and, additionally, on BabelNet/Nasari).
