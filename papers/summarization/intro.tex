\section{Introduction}
\label{sec:intro}

Document summarization is an increasingly popular natural language processing
task that finds applications in XXX domains. It has been the shared task of
DUC 2003, 2004, ..., and TAC 2007, 2008 etc \cite{website:DUC,website:TAC}. A multiple document 
summarization algorithm takes as input a set of documents about the same topic
and produces one single summary document with limited number of words, by
either extracting existing sentences from the input documents, or by creating
new sentences that capture key content of those input documents.
Evaluation of summarization algorithms are typically done by either
human inspection and grading, or by automatic scoring systems.  In either
cases, for each summarization task, a few human hand-crafted 
``model summaries'' are available to the 
evaluators as the ``gold standard''. Because human grading is expensive, highly
subjective and sometimes downright inconsistent, all existing summarization 
research work relies on automatic evaluation to varying degree.  

The most popular automatic evaluations are ROUGE and its variants \cite{lin2003automatic,lin2004looking,lin2004rouge}. 
ROUGE basically measures the summary's coverage of the n-grams in the 
input documents. While n-grams may be good and succint representation of
basic semantic units in the documents, the current ROUGE scores treat
the n-grams as a set and do not consider the relationships between these 
semantic units. Consequently, in the AESOP task XXX, while ROUGE scores 
perform well on content converage, as indicated by high correlation with the 
Pyramid scores \cite{nenkova2007pyramid}, it doesn't do as well with readability. 
For example, the following system summary has a ROUGE score 0.13333 and a readability
score 1 by human. In the ROUGE system, the `0.13333' is a higher score. But, the range
of the score on readability by human is from 1 to 5. The following system has a higher
ROUGE score and a bad readability.   

\emph{A tanker spilled more than 10,000 tons of crude oil into the Yellow Sea Friday 
after being holed by a barge in what officialssaid was South Korea's worst oil spill.
A tanker spilled more than 10,000 tons of crude oil into theYellow Sea after it was holed 
by a barge Friday in what officialssaid was South Korea's worst ever oil spill.
A Hong Kong-registered oil tanker collided with another vessel inseas off South Korea's 
west coast Friday and leaked about 15,000tons of crude oil, the Maritime and Fisheries Ministry said.
The spill was believed to be South Korea's}

Giannakopoulos et al. attacked this problem by proposing an alternative
data structure called n-gram graph \cite{giannakopoulos2008summarization}. It takes into account the
proximity between n-grams in a document and measures the quality of the 
summary by computing the similarity between the graph representations of
the model summary and the summary to be evaluated. However, this quadratic 
data structure is undirectional, and cannot capture the long-range semantic
and logical flow that exist in a good summary. For example, ...
\KZ{Add an example here where ngram graph gives a high score but the summary
doesn't flow so well. Make sure our score rates this same summary lower.}

% 
%On the topic of automatic multi-document summarisation, lots of research work has already been done on how to create a well-performed summariser. However, what is “well-performed” remains to be defined properly. We discovered in many papers that the ROUGE evaluation method has been widely used to evaluate the quality of automatic generated summaries, but the ROUGE method focuses exclusively on content selection of the systems, ignoring other linguistic qualities. The final objective for automatic summarisation is to generate summaries which is just like written by a real human, not only covering all the contents, but also enjoying a strong readability.
%
%In the Summarisation Task of TAC 2011, it evaluated a summariser according to the following three aspects:content,readability and overall responsiveness. Although content could be evaluated by a pyramid  evaluation, it still needs human judgements. The other two factors also involve the participation of human judges to read the summaries and give a score. Thus, we find developing an automatic evaluation system ***(A NAME FOR OUR SYSTEM) for the summarisers is interesting and meaningful. Our objective is to make the evaluation as close as that made by human. 
%

In our work, we argue that semantic flow plays a very important part in
the overall readability of documents, therefore by measuring the similarity
between the semantic flows in the system summary and the model summary, 
we can produce a score that better reflects the readability of the system 
summary. To that end, we propose a scoring system that implements three 
main ideas. First, we model semantics in the documents not by words 
or n-grams, but by abstract {\em topics}, each of which represented by 
a word distribution. The advantage of this is that in abstractive 
summarizations, different words of the same or similar meanings may be used
and these are better modeled by topics than words or n-grams. Second,
we measure the similarity between semantic flows using time-series-based metric
such as discrete time warping (DTW) and other ordinal association metrics
such as Kendall's tau, Spearman's and Pearson's coefficients. Third, we 
make sure that summaries that cover the most important concepts are rewarded
by boosting the weight of topics that exist in multiple model summaries.
And all of these are accomplished under a straight forward probabilistic 
framework.

This paper makes the following contributions:
\begin{itemize}

\item we propose to model readability in automatic summaries
by their semantic flow induced from topic-word distributions;

\item we show that topic models can be more effective than word embedding
in modeling the semantic flows and that DTW generally performs better than
rank-based correlation metrics for our purpose; 

\item we demonstrate that the proposed scoring framework achieves the
state-of-the-art performance in evaluating readability in the TAC '09 AESOP
task while staying competitive in the other two dimensions, namely Pyramid
and responsiveness. 

\end{itemize}

%trying to capture the order information on a topic level of the system summaries by using LDA to generate word representations and DTW to reflect the distance of the logic flow between model summaries(summaries made by human) and system summaries(automatic generated). We also boost the scores of system summaries according to the importance of the content, in which we imitated the method in Pyramid Evaluation  Function. 
Next, we present the details of the framework in \secref{sec:approach}, 
and comprehensive experimental results in \secref{sec:eval}.
\secref{sec:related} discusses some related work and \secref{sec:conclude}
concludes the paper. 

