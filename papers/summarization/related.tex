\section{Related Work}
\label{sec:related}
In this section, we discuss some previous work about evaluation of automatic document summarisation. In AESOP Task of TAC 2011, the quality of summaries are evaluated in two aspects:content and readability. The intuition behind this is a good summary should not only contain all the important information from the source documents, but also be fluent and easy to be read by humans. Past work about evaluation could be categorised  into three types: (1)content only (2) readability only (3) combine of both.

1. Content only 
BE
BLEU method is precision-based and is first used in machine translation domain. BLEU computes the similarity, which is calculated as the overlap of n-gram, between a candidate translation and corresponding reference translations to measure the closeness. BLEU also adds a brevity penalty to prevent very short translations. However, no bonus was added to BLEU score to boost the important content in candidate which also appears in multiple references.

ROUGE
ROUGE is an extension of BLEU method and is used to compare system summaries with model summaries based on n-gram overlap. In most papers of automatic document summarisation, bigram overlap is used as a typical ROUGE evaluation method. ROUGE is totally automatic and the results correlate well with human judgements, but logical continuity of a summary is not considered in this evaluation method. 

Pyramid
The pyramid method creates a weighted inventory of Summary Content Units(SCUs), in which SCUs are acquired from annotations of summaries, to evaluate content selection in summarisation. This method involves multiple model summaries. After identifying similar sentences from all the model summaries, SCUs from system summary are compared to the sentence sets to find out how many times the SCUs are closely related to the subparts. The times serves as the weight of a corresponding SCU while each model summary will contribute no more than once to a SCU. The final score is a ration of the sum of the weights of the SCUs appeared in a system summary to the sum of weights of an optimal summary with the same number of SCUs. However, this method involves human to annotate SCUs and hasn’t taking linguistic quality into consideration.

2. Readability 
Linguistic Quality
coherence 
In Automatic Evaluation of Text Coherence, Lapata and Barzilay proposed coherence models which could be categorised into two classes:syntactic and semantic.The first class characterises how mentions of the same entity in different syntactic positions are spread across adjacent sentences while the other class quantifies local coherence as the degree of connectivity across text sentences. Several models exhibited a significant agreement with human ratings and a model fused the both syntactic and semantic views improved the result substantially.

In Nenkova’s paper(Automatic Evaluation of Linguistic Quality),  they focused on five aspects of linguistic quality, namely grammaticality, non-redundancy, referential clarity, focus and structure and coherence. Diverse classes of features are introduced to capture vocabulary, sentence fluency and local coherence properties of summaries. Results showed some features like continuity between adjacent sentences, language model and entity coherence features were consistently indicative of the linguistic quality of the summary. However, these features were compared in all the five aspects separately and  no method was proposed to generate one final score for linguistic quality.


3. Both content and readability
Evaluation systems submitted to TAC 2011 for AESOP task tried to improve both content selection and readability. CatolicaSC is a graph-based method that performs sentence matching. C\_S\_IIITH 
identified topics covered in document using Sentence Community Detection Scheme and calculated the strength of each topic by using the local importance of words, which was defined as the page rank score of the directed word graph of sentences. Then information coverage in system summary was calculated. DemokritosGR proposed two models, both based on n-gram graph, which performed well in content and readability. PKUTM combined the ROUGE scores by training regression models on the TAC 2010’s summarisation evaluation data, which could be used to predict the performance of the TAC 2011’S system. However, this model might not work well on other datasets because of its training is based on past TAC data.  
