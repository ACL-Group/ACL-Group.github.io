 \section{Partial Suppression Algorithm}
\label{sec:algo}

\renewcommand{\algorithmicforall}{\textbf{for each}}
%\algnotext{ENDFOR}
%\algnotext{ENDIF}
%\algnotext{ENDWHILE}

The Optimal Suppression Problem defined in Section \ref{sec:prob} is
an NP-hard problem.
%To find the optimal suppressor,
%the naive approach needs to try suppressing all combinations of items,
%with a complexity of $O(2^N)$ where $N$ is the total number of items in $T$.
We therefore present the partial suppression algorithm as a
heuristic solution to the Optimal Suppression Problem.
%By definition of a safe set-valued table (Definition
%\ref{def:safety_table}), the breach probability of each \qid in $Q$ must be
%under the threshold $\rho$ (Definition \ref{def:safety_qid}).
%For
%example, given a \qid $q=\{a, b, \alpha\}$ and all its linked sensitive items
%$\linked(q)=\{\beta\}$, where $\{a, b\}$ are non-sensitive items and
%$\{\alpha, \beta\}$ are sensitive items, if $\csize(q)=5$, $\csize(q \cup
%\{\beta\})=3$ and $\rho=\frac{1}{3}$, then $\breach(q)=\frac{3}{5}>\rho$.
%\begin{definition}[Data Structures]
%We define three key data structures in this framework.
%$B$ is a \qid buffer which is a set of \qids.
%$K$ is a mapping which is essentially a materialized function from \qid $q$ to $\csize(q)$.
%$L$ is a mapping from \qid $q$ to $\linked(q)$.
%\end{definition}
To simplify the discussion of the algorithm, we make the following
definitions.
%
%\textcolor{green}{ Do we need this?
%\begin{definition}[Data Structures]
%We define three key data structures in this framework. $B$ is a \qid buffer
%which stores a set of \qids. $S$ is a \textbf{mapping} which is essentially a
%materialized function from \qid $q$ to $sup(q)$. $L$ is a \textbf{mapping}
%from \qid $q$ to $\linked(q)$.
%\end{definition}
%In the following algorithms, we will also use $sup(\cdot)$ and
%$\linked(\cdot)$ to denote the computations of these two functions, and use
%$S(\cdot)$ and $L(\cdot)$ to denote the access of the elements of the two
%data structures.
%}
%\KZ{This whole thing is not clear, need to rephrase:
\begin{definition}[Number of Suppressions]
\label{minimum}
%Number of suppressions is defined as the number of items that must be
%deleted to disable an unsafe sensitive association rule.
To disable an unsafe rule $q \rightarrow e$, the minimum number of instances of item %s of type
$t \in q\cup\{e\}$ that needs to be suppressed is
\[N_s(t, q\rightarrow e)=
\begin{cases}
sup_T(q\cup \{e\})-sup(q)\rho & t=e  \\
\frac{sup_T(q\cup \{e\})-sup_T(q)\rho}{1-\rho} & t\in q %\\
% \infty & otherwise
\end{cases} \]
\end{definition}
\begin{proof}
If $t = e$,
\begin{eqnarray*}
conf(q \to e)&=&\frac{{{{\sup }_{T'}}(q \cup \{ e\} )}}{{{{\sup }_{T'}}(q)}}\\
&=&\frac{{{{\sup }_T}(q \cup \{ e\} ) - {N_s}(t,q \to e)}}{{{{\sup }_T}(q)}}\\
&=&\frac{{{{\sup }_T}(q \cup \{ e\} ) - ({{\sup }_T}(q \cup \{ e\} ) - {{\sup }_T}(q)\rho )}}{{{{\sup }_T}(q)}}\\
&=&\rho
\end{eqnarray*}
If $t\in q$,
\begin{eqnarray*}
conf(q \to e)&=&\frac{{{{\sup }_{T'}}(q \cup \{ e\} )}}{{{{\sup }_{T'}}(q)}}\\
&=&\frac{{{{\sup }_T}(q \cup \{ e\} ) - {N_s}(t,q \to e)}}{{{{\sup }_T}(q) - {N_s}(t,q \to e)}}\\
&=&\frac{{{{\sup }_T}(q \cup \{ e\} ) - \frac{{{{\sup }_T}(q \cup \{ e\} ) - {{\sup }_T}(q)\rho }}{{1 - \rho }}}}{{{{\sup }_T}(q) - \frac{{{{\sup }_T}(q \cup \{ e\} ) - {{\sup }_T}(q)\rho }}{{1 - \rho }}}}\\
&=& \rho
\end{eqnarray*}
\end{proof}
%Of all item types in $q\cup e$, there exists an item type $t_{min} \in q\cup
%e$ which results in minimum suppressions.
In other words, for each sensitive rule $r$,
we need to delete at least $N_s(t, r)$ instances of item $t$ to make it safe.
In this work, we select these instances randomly for deletion.

\begin{definition}[Leftover Items]
 The leftover of item type $t$ is defined as
 % \vspace{-1mm}
\[ leftover(t)={sup_{T'}(\{t\})}/{sup_T(\{t\})} \]
\end{definition}
$T$ is the original data and $T'$ is the intermediate suppressing result.
The ratio shows the percentage of remaining instances of item $t$
in the intermediate result $T'$.
%More items suppressed indicates a smaller ratio.


%\XJ{move this para to algo sec} \MakeRed{
The key intuition of our algorithm is that although the total number of
``bad'' sensitive association rules
maybe, in the worst case, exponential in the original data, incremental
``invalidation'' of some of the rules through partial suppression of a
small number of affected items can massively reduce the number of these bad
rules, which leads to quick convergence to a solution, that is, a
safe data set.
%}
%%%
%
%In this paper, we adopts three kinds of partial suppression policies.
%The first one is \PartialR, which suppresses only sensitive items
%in the consequents. The second one is \PartialL, which
%suppresses only items in the antecedents.
%The third one is \PartialALL, which suppresses items in
%both the consequents and the antecedents.
%\PartialL and \PartialALL suppress both sensitive
%and non-sensitive items, whereas \PartialR suppresses only
%sensitive items.
Next we present the basic algorithm of this framework.

\subsection{The Basic Algorithm}
\label{sec:basic}

%To ensure a table is safe, we must make sure all \qids in $Q$ are safe.
\PartialSuppression (Algorithm \ref{algo:partialsuppression}) presents the
top-level algorithm. The partial suppressor iterates over the table $T$, and
for each record $T[i]$, the algorithm first generates  \qids from $T[i]$ and
sanitizes the unsafe ones. The suppressor terminates when the whole table is
scanned and there is no unsafe \qid.

% \vspace{-6mm}
\begin{algorithm}[h]
\small
\caption{$\PartialSuppression(T,\bmax)$}
\label{algo:partialsuppression}
\begin{algorithmic}[1]
   % \STATE Initialize $safe\leftarrow\TRUE$, $i\leftarrow 1$;
  %  \STATE Initialize $safe\leftarrow\TRUE$
\STATE \textcolor{red}{ Shuffle $T$ (original table)}
\STATE $T_0 \gets T$ %(original table)
\STATE \textcolor{red}{Initialize $safe$ to $\TRUE$}
    \LOOP
        \STATE \textcolor{red}{Clear buffer $B$}
        %\STATE {Initialize the $sup$ of all \qids to 0}
        \WHILE {$|B|<b_{max}$ \AND $i\leq |T|$} \label{algo:enu_s}
            \STATE \textcolor{red}{$s \gets$ number of \qids generated by $T[i]$}
            \IF {\textcolor{red}{$|B|+s>b_{max}$}}
                \STATE \textcolor{red}{break}
            \ELSE
             \STATE \textcolor{red}{Fill $B$ with distinct \qids generated by $T[i]$ \label{algo:enumerate1}}
             \STATE $i\leftarrow i+1$
            \ENDIF
        \ENDWHILE \label{algo:enu_e}
        \STATE \textcolor{red}{Update $sup$ of all \qids in $B$} \label{algo:enumerate2}
		%\STATE \textcolor{red}{update $sup$, $\linked$, $S$, $L$;}
%        \STATE Calculate $\rho$ of each $qid$ in $|B|$ \label{algo:update}
        \IF {$B$ contains an unsafe \qids}\label{line:containunsafe}
            \STATE $\SanitizeBuffer(T_0, T, B)$\label{line:sanitizebuffer}
            \STATE $safe\leftarrow\FALSE$
        \ENDIF
        \textcolor{red}{\IF {$i \ge |T|$ }%\AND $safe$}
            \IF{\textcolor{red}{$safe$}}
                \STATE \textbf{break}\label{algo:partialbreak}
            \ELSE
                \STATE $i\leftarrow 1$
                \STATE $safe\leftarrow\TRUE$
%                \STATE \textbf{continue}
            \ENDIF
        \ENDIF}
    \ENDLOOP
\end{algorithmic}
\end{algorithm}

% \vspace{-6mm}
A \qid is a combination of different items,
and the number of distinct \qids to be enumerated is exponential.
We therefore introduce a \qid buffer of
capacity $\bmax$ to balance the space consumption with the generation time.
The value of $\bmax$ is significant. Small $\bmax$ values cause
repetitive generation of \qids, while large $\bmax$ values cause useless
generation of \qids which do not exist by the time to process them in the
queue.

\subsection{Buffer Sanitization}
\label{sec:sanitize}
Each time \qid buffer $B$ is ready, \SanitizeBuffer
(Algorithm \ref{algo:sanitize}) is invoked to start processing \qids in $B$
and make all of them safe. $D_S(T)$ denotes the domain of all sensitive
items in $T$.
We first partition \qids in $B$ into two groups, {\em safe} and {\em unsafe}.
%according to Definition \ref{def:probability} and
%\ref{def:safety_qid}.
Then in each iteration (Lines \ref{algo:pick_rs}-\ref{algo:pick_re}),
\SanitizeBuffer
 picks the ``best'' (according to heuristic functions $H$) unsafe
sensitive association rule %whose confidence is above $\rho$
 to sanitize (Lines \ref{algo:heur_dist} and \ref{algo:heur_mine}). 
\SuppressionPolicy in \SanitizeBuffer uses one of the
the following two heuristic functions.

% \vspace{-6mm}
\begin{algorithm}
\caption{$\SanitizeBuffer(T_0, T, B)$}
\label{algo:sanitize}
\begin{algorithmic}[1]
\STATE $\policy \leftarrow \SuppressionPolicy()$ \label{choose_heur}
\REPEAT
\label{algo:pick_rs}
    \STATE pick an unsafe \qid $q$ from $B$
    \STATE $E \gets \{e ~|~ conf(q \rightarrow e) > \rho \land  e \in D_S(T)\}$
    \IF {$\policy = Distribution$}
        %\STATE $(d, q, e) \gets\underset{d\in q\cup E, q, e \in E}{\arg\max}\,H_{dist}(d, q, e, T_0, T)$
        \STATE {\textcolor{red}{$(d, e) \gets\underset{d\in q\cup E, q, e \in E}{\arg\max}\,H_{dist}(d, q, e, T_0, T)$}}
        \label{algo:heur_dist}
    \ELSIF {$\policy = Mine$}
        %\STATE $(d, q, e) \gets\underset{d\in q\cup E, q, e \in E}{\arg\min}\,H_{mine}(d, q, e)$
        \STATE {\textcolor{red}{$(d, e) \gets\underset{d\in q\cup E, q, e \in E}{\arg\min}\,H_{mine}(d, q, e)$}}
        \label{algo:heur_mine}
    \ENDIF
%    \IF {$\policy = Distribution$}
%        \STATE  find $\SA(q,e)$ with maximal $H_{dist}(d)$, where
%        \STATE  $d\in q\cup\{e\}$ \AND $conf(q,e)>\rho$
%        \label{algo:heur_dist}
%    \ELSE
%        \STATE find $\SA(q,e)$ with minimal $H_{mine}(d)$, where
%        \STATE  $d\in q\cup\{e\}$ \AND $conf(q,e)>\rho$
%        \label{algo:heur_mine}
%    \ENDIF
    \STATE $X\leftarrow q\cup\{e\}$
    \STATE $k\leftarrow N_s(d,q \rightarrow e)$\label{line:sanitize-k}
    \STATE \textcolor{red}{$P\leftarrow \emptyset$}
    \WHILE{$k>0$}\label{line:sanitize-whilek}
        \STATE pick a record $R$ from $T$ where
		$R\subseteq \container(X)$ \label{pick_row}
        \STATE \textcolor{red}{Add \qids containing $d$ in $R$ to $P$}
        \STATE $R\leftarrow R-\{d\}$\label{line:sanitize-suppress}
        \label{algo:update_kl}
        \STATE $k \leftarrow k-1$
    \ENDWHILE
    \STATE \textcolor{red}{Update $sup$ of \qids in $P$}
\UNTIL{there is no unsafe \qid in $B$}   \label{algo:pick_re}
\end{algorithmic}
\end{algorithm}

% \vspace{-6mm}
\subsubsection{Preservation of Data Distribution}
%We first present the heuristic function which helps to preserve data
%distribution.
Consider an unsafe sensitive association rule $q \rightarrow e$ where
$conf(q \rightarrow e) > \rho$, and $q \in B$.
%$q$ is thus not a safe by Definition \ref{def:safety_qid}.
To reduce $conf(q,e)$ below $\rho$,
%we must make the confidence
%of the inference not larger than $\rho$, thus
we suppress a number of instances of item $t\in q \cup \{e\}$ from
$\container(q\cup \{e\})$.\footnote{We define $\container(X)=\{T[i]|X\subseteq T[i], 1\le i\le |T|\}$.} We hope to minimize $KL(T ~||~ T_0)$
(see \eqnref{eq:kl}).
%, that is
%the difference in probability distribution between the suppressed table $T$
%and the original table $T_0$.
%$\mathcal{A}(q,e)$ and decide the minimum number of occurrences of $t$ to be
%deleted from  $\mathcal{A}(q,e)$ or just eliminate this inference from $T$.
%Kullback-Leibler divergence is defined as
% \[KL(Q||P)=\sum_{t\in
%D}Q(t)log\frac{Q(t)}{P(t)}\]
%where P(t) is the original distribution of $t$
%and $Q(t)$ is the current
%% (before selecting this removal)
% distribution of $t$, which is often used to characterize the distribution
% distance.
From \eqnref{eq:kl}, we observe that by suppressing some instances of item $t$
where $T(t)>T_0(t)$,\footnote{We denote the probability of item $t$ in $T$
as $T(t)$, which is computed by $\frac{sup_T(t)}{|T|}$.}
the KL divergence tends to decrease, thus we define
the following heuristic function
\begin{equation}\label{eq:hdist}
H_{dist}(t, q, e, T_0, T) =
	\frac{T(t)log\frac{T(t)}{T_0(t)}}{N_s(t, q\rightarrow e)}.
\end{equation}
%The numerator in \eqnref{eq:hdist} indicates the divergence of
%the data distribution on item $t$ only.
%The larger the absolute value is, the larger distribution difference of
%$t$ now is compared with the original
%situation.
%However, if the value is less than 0, i.e. $T(t)< T_0(t)$, we'd
%better not suppress $t$, since it may further decreases $Q(t)$ and
%deteriorates the data distribution. Therefore the larger the numerator is,
%the item has more priorities to be chosen to suppress.
%The denominator indicates the minimum number of items $t$ that needs
%to be suppressed.
%The smaller the number is, the fewer items are suppressed.
Maximizing this function aims at
suppressing item $t$ which maximally recovers the original
data distribution and minimizes the number of deletions.
%Each time we choose a sensitive association rule and a corresponding item $t$
%with the highest $H_{dist}$ value to sanitize (Line
%\ref{algo:heur_dist} in Algorithm \ref{algo:sanitize}).

%
% By iterating over all unsafe \qids, we can all sensitive
%inferences whose confidence larger than $\rho$ from those {\em unsafe} \qids.
%Then in each iteration, the algorithm fetches one sensitive association
%$\SA(q,e)$ and fix it.

\subsubsection{Preservation of Useful Rules}
%\subsubsection{\textcolor{red}{Reduction of Spurious Rules}}
%Then we introduce our second heuristic function which trys to retain minable
%rules with fewer spurious rules invented.
%As we mentioned before, to learn from the characteristic of global
%suppression (no spurious rules introduced), we devise a heuristic which takes
%deletions towards global suppression while using partial suppression.

A spurious rule ($q \rightarrow e$) is introduced when
the denominator of $conf(q \rightarrow e)$, $sup(q)$,
is sufficiently small so that the confidence appears large enough.
\textcolor{red}{
%In order to reduce the introduction of spurious rules,
%our objective to suppress the item whose suppression will result in the least number of spurious rules.
However, if $sup(q)$ is too small, the rule would not have enough
support and can be ignored.
%\PC{Since we do not publish certain data mining results, we can not
%eliminate spurious rules after anoymization.}
Therefore, our objective is to continue suppressing
those items that have been suppressed before,
to minimize the support of the potential spurious rules.
Therefore, we seek to minimize
\begin{equation}\label{eq:hmine}
H_{mine}(t, q, e)=leftover(t)\cdot N_s(t, q\rightarrow e)
\end{equation}
Here are some reasons why we choose to use this formula to achieve our goals. The most apparent way to prevent adding spurious rule 
is to keep deleting the same item we choose before. In this way we can touch as few records as possible, since if one item $t$ is chosen and deleted 
$leftover(t)$ will become smaller.
}
%\begin{equation}\label{eq:hmine}
%H_{mine}(t, q, e)=N_r(t, q\rightarrow e)\cdot N_s(t, q\rightarrow e)
%\end{equation}
%, where $N_r(t, q\rightarrow e)$ denotes the number of spurious rules the suppression of item $t$ will introduce.
%}
%as a heuristic function which is to be minimized
%in Line \ref{algo:heur_mine} of Algorithm \ref{algo:sanitize}.
%
%$leftover(t)$ represents remaining content of item $t$ and
%$N_s(t, q\rightarrow e)$ represents the minimum number of item
%$t$ that needs to be suppressed to satisfy the privacy model.
%%
%The product expresses
%the strategy that we want to introduce fewer spurious rules by imitating the
%effect of global suppression while suppressing as few items as possible.
%%
%As we can see, each time we choose the sensitive association rule with lowest
%$H_{mine}$ to perform sanitization (Line \ref{algo:heur_mine} in Algorithm \ref{algo:sanitize}).

%However, $H_{mine}$ goes the opposite side of preserving the original data
%distribution. Therefore, we introduce the following heuristic function
%separately to preserve the original data distribution and minimizes the
%deletion in local optimal.
%
%
%\subsubsection{Rest of the Algorithm}
%% Next we will show the two heuristic
%% functions that help to find the `best` association to fix.

\subsubsection{Regression}
After suppressing the item $d$, unsafe \qids may become safe, while safe ones
may become unsafe again, an undesirable situation known as {\em regression}.
Line \ref{algo:update_kl} of
Algorithm \SanitizeBuffer determines the set of
\qids that would be affected by regression. This step is like the $qid$
generation step in Algorithm \PartialSuppression Line \ref{algo:enumerate1}
and \ref{algo:enumerate2}. There are also different ways to pick a
record from $\container(q\cup \{e\})$ to suppress item $d$ (Line
\ref{pick_row}), and currently we pick a random record from $\container(X)$
for some reasons.
\textcolor{red}{Deleting one exact item $d$ in a record $c$ from $\container(q\cup \{e\})$ may introduce other new rules.
 This may happen when some record $c$ is sanitized for apparently higher times than other records in $\container(q\cup \{e\})-c$.
 Such process will result in enlarge the confidence of some association rules which $c$ can generate. One cheap method to deal with this problem is
 to randomly choose one record instead of operate one record for many times. In this way we can avoid introducing spurious rules and keep the mining data
 useful.
}
\cut{%%%%%%%%%%%%%%%%%%%%%% begin of cut %%%%%%%%%%%%%
\begin{table*}[th]
\caption{A Running Example}
\centering
%\subtable[The Original Dataset]{
%\begin{tabular}{|c|l|}
%\hline
%% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%{\bf TID} & {\bf Transaction} \\ \hline
%1 & bread, milk, {\em condom} \\ \hline
%2 & bread, milk  \\ \hline
%3& flour, fruits  \\ \hline
%4& flour, {\em condom}\\ \hline
%5& bread, fruits  \\ \hline
%6& fruits, {\em condom}  \\ \hline
%\end{tabular}
%\label{tab:sample1}
%}
\subtable[Step 1 for $H_{mine}$]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, {\em condom} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, {\em condom}  \\ \hline
4& flour, fruits  \\ \hline
5& flour, \sout{{\em condom}}\\ \hline
6& bread, fruits  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sampledata2}
}
\subtable[Step 2 by $H_{mine}$]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, \sout{{\em condom}} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, {\em condom}  \\ \hline
4&flour, fruits  \\ \hline
5& flour, \sout{{\em condom}}\\ \hline
6& bread, fruits  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sampledata3}
}
\subtable[Step 1 by $H_{dist}$]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, {\em condom} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, {\em condom}  \\ \hline
4& flour, fruits  \\ \hline
5& \sout{flour},{\em condom}\\ \hline
6& bread, fruits  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sampledata4}
}
\subtable[Step 2 by $H_{dist}$]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf TID} & {\bf Transaction} \\ \hline
1 & bread, milk, \sout{{\em condom}} \\ \hline
2 & bread, milk  \\ \hline
3 & milk, {\em condom}  \\ \hline
4&flour, fruits  \\ \hline
5& \sout{flour}, {\em condom} \\ \hline
6& bread, fruits  \\ \hline
7& fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sampledata5}
}
\end{table*}
}%%%%%%%%%%%%%%% end of cut %%%%%%%%%%%
%\subsubsection{Data Distribution or Rule Mining?}
%The two heuristic functions aim at different downstream applications.
%%We argue that the user can choose the corresponding heuristic with regard
%%to their specific demands.
%$H_{dist}$ typically causes suppressions of different types of items whereas
%$H_{mine}$ tends to focus on deleting one type of items. This is evident
%from the results shown in Table \ref{tab:sample3} and Table \ref{tab:sample4}.
%%one type of item in the whole dataset in that the latter one will remove all those $bad~rules$
%%containing this item.
%Furthermore, to preserve data distribution, usually more items need to be
%suppressed than to preserve useful association rules.
%%Furthermore, less items will normally represent less association rules.
%This type of trade-off between data distribution and rule mining inspires
%our design of the two heuristic functions.
%

\cut{%%%%%%%%%%%%%%% begin of cut %%%%%%%%%%%%
\subsubsection{A Running Example}
\label{sec:appendix}

\begin{table}
\caption{Sensitive Rules in the Running Example}
\centering
\subtable[Sensitive Rules Table 1]{
\begin{tabular}{|c|c|c|c|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
TID & \qid &Sensitive Item & $\rho$ \\ \hline
1   &  bread, milk& {\em condom}&  1/2\\ \hline
2& bread &{\em condom}& 1/3\\ \hline
3& milk& {\em condom}& 2/3 \\ \hline
4&flour& {\em condom} &1/2 \\ \hline
5&fruits&  {\em condom} &1/3 \\ \hline
\end{tabular}
\label{tab:sampleRule1}
}
\subtable[Sensitive Rules Table2 ]{
\begin{tabular}{|c|c|c|c|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
TID & \qid &Sensitive Item & $\rho$ \\ \hline
1   &  bread, milk& {\em condom}&  1/2\\ \hline
2& bread &{\em condom}& 1/3\\ \hline
3& milk& {\em condom}& 2/3 \\ \hline
4&fruits&  {\em condom} &1/3 \\ \hline
\end{tabular}
\label{tab:sampleRule2}
}
\end{table}


In this section, we demonstrate how our algorithm works step-by-step
using the example data set in Table \ref{tab:sample}.
We assume $\rho=1/3$ in this privacy model and ``condom'' is a sensitive item,
while all other items are non-sensitive.

The original dataset is given in Table \ref{tab:orig-sample}.
Assume that all the \qids could be stored in the buffer and we show each step in the buffer sanitization process.
The original sensitive rules are shown in table \ref{tab:sampleRule1}.

We first show how to suppress the table for rule mining using $H_{mine}$.
Rule 1, rule 3 and rule 4 all have the smallest $H_{mine}$ with $N_s(condom, q\rightarrow condom)$ =1 and $leftover(condom)$=1 in table \ref{tab:sampleRule1}. Then we randomly pick one rule from these three rules. We pick rule 4
in table \ref{tab:sampleRule1} as our candidate and
sanitize this rule by deleting ``flour'' or ``condom'' once to satisfy our privacy model. We choose to delete ``condom'' in this example, but the program may choose ``flour''
since the information loss is the same.
After this step, Table \ref{tab:orig-sample} turns into
Table \ref{tab:sampledata2} and the sensitive rules are updated in
Table \ref{tab:sampleRule2}. In Table \ref{tab:sampledata2},
``condom'' in rule 1 and rule 3 have the least $H_{mine}$ with
the value $\frac{3}{4}$ in that $leftover(condom)$=$\frac{3}{4}$ and
$N_s(condom, q\rightarrow condom)=1$. Therefore, we choose to
sanitize rule 1 by eliminating ``condom''.
Eventually, we get a safe table in Table \ref{tab:sampledata3}.

Next we choose $H_{dist}$ as our heuristic function.
Notice that all sensitive rules in Table  \ref{tab:sampleRule1}
have the same $H_{dist}$ with the value 0 since the
distribution of the data is not altered before suppression.
Therefore, we pick the rule with the smallest
 $N_s(t, q\rightarrow t)$. Suppose we choose rule 4 as our candidate and
delete ``flour'', then the original table turns into
Table \ref{tab:sampledata4} and sensitive rules
table also becomes Table \ref{tab:sampleRule2}.
In Table \ref{tab:sampledata4}, ``condom'' in rule 1 and rule 3 has
 the highest $H_{dist}$ 0.006 (see table \ref{tab:sampleRule2}). Then we delete ``condom'' in transaction 1 and get a safe table \ref{tab:sampledata5}.
}%%%%%%%%%%%%%%%%%%%%%%end of cut %%%%%%%%%%%
%
%\subsection{\textcolor{red}{Speed-up of $sup$ Update}}
%\textcolor{red}{The immense scale of transaction data calls for a well-designed method to store and update the $sup$ of \qids.
%If upon every update of the $sup$, we go through the whole transaction table to count the presence of a specific \qid,
%the process will be time-consuming.
%In this work, we adopt a $trie$ structure to store the $sup$s.}
%
%\textcolor{red}{Every node on the $trie$ represents an item type.
%The items contained in all transactions are pre-sorted in a special order.
%For simplicity, here we assume that the item types are denoted by English letters $A,B,C...$
%and the items in a transaction are ordered alphabetically.
%An attribute is attached to every node
%which records the number of transactions whose first several items are exactly the ones on the path from the root to the very node.
%For example, as shown in Fig.\ref{fig:trie}, there is a letter and a number at each node.
%The letters are item types and the numbers are the values of the attributes.
%The value of the attribute of the root node $A$ is 11.
%It means that there are 11 transactions that begin with item type $A$.
%}
%
%\begin{figure}[tb]
%\centering
%%\subfigure[Example of the Trie Structure]{
%\hspace{-1cm}
%\begin{minipage}[c]{0.45\textwidth}
%\centering
%  \includegraphics[width=7cm]{trie.eps}
%\end{minipage}%
%%}
%\caption{Example of the $Trie$ Structure}\label{fig:trie}
%\end{figure}
%
%\textcolor{red}{
%When the $sup$ of a \qid is queried, the program will quickly find the related \qids on the $trie$
%and calculate the $sup$. For example, if the $sup$ of \qid $\{F\}$ is queried,
%since \qid $\{F\}$ appears in the transactions that begin with $\{A,B,F\}$, $\{A,C,F\}$ and $\{A,F\}$,
%the result of $2+1+2$ will be returned.}
%

\subsection{Optimization with Divide-and-Conquer}
\label{algo:impmentation}

When data is very large we can speed up by
a divide-and-conquer (DnC) framework that
partitions the input data dynamically,
runs \PartialSuppression
on them individually and combines the results in the end. This approach is
correct in the sense that if each suppressed partition is safe, so is the
combined data. This approach also gives rise to the parallel execution
on multi-core or distributed environments which provides further speed-up
(this will be shown in Section \ref{sec:eval}).

\begin{algorithm}
%\caption{Top level partitioning controller}
\caption{$\SplitData(T,\tmax)$} \label{algo:splitdata}
\begin{algorithmic}[1]
    \IF { $Cost(T) > \tmax$ }
        \STATE Split $T$ equally into $T_1$ , $T_2$
        \STATE $\SplitData(T_1,  \tmax)$
        \STATE $\SplitData(T_2, \tmax)$
    \ELSE
        \STATE $\PartialSuppression(T,  \bmax)$
    \ENDIF
\end{algorithmic}
\end{algorithm}

Algorithm \ref{algo:splitdata}
splits the input table whenever the estimated cost of
suppressing that table is greater than $\tmax$.
%For simplicity we randomly partition the original data in two
%\footnote{Recall that the input table is a set of records with
%no predefined order} when the estimated time is above $\tmax$.
Cost is estimated as:
\begin{equation}\label{eq:costfunc}
Cost(T)=\frac{|T|\cdot 2^{\frac{N}{|T|}}}{|D(T)|}
\end{equation}
%where $\mathcal{AVG}$ is the average transaction size, i.e. $\frac{N}{|T|}$,
where $N$ is the total number of items in $T$.
In order to guarantee the consistency in data distribution between the original problem and the sub-problems, when splitting $T$ equally into $T_1$ and $T_2$, we randomly assign half of the items in $T$ to $T_1$ and the remaining items to $T_2$ .
%The function estimates the
%average number of \qids per item type. The larger this value is, the more
%sensitive association rules we should handle. The cost function is used when
%data size is large enough. We argue that when $|T|$ is relatively
%small there is no need to apply DnC.

\subsection{Analysis of the Algorithm}
\label{sec:analysis}

% Analyze the main algo: time complexity, space complexity.
% Some properties to consider:
%
% \begin{itemize}
% \item give a bound on the total number of items suppressed;
% \item give a bound on the deviation in distribution from the original data;
% \item give a bound on the number of association rules that we eliminate;
% \item and what else??
% \end{itemize}
In this section, we present some theoretical results along with their proofs
in order to provide a comprehensive view of the problem and our algorithm.
\begin{lemma}%[Correctness of partitioning]
\label{CorrectnessOfPartitioning}
  If $q$ is safe in both $T_1$ and $T_2$, then $q$ is safe in $T = T_1 \cup T_2$.
\end{lemma}
\begin{proof}
For any item $a$,
  \begin{align*}
   q~\text{is safe in}~T_1 &\Rightarrow sup_{T_1}(q\cup\{a\}) \le \rho\cdot sup_{T_1}(q) \\
   q~\text{is safe in}~T_2 &\Rightarrow sup_{T_2}(q\cup\{a\}) \le \rho\cdot sup_{T_2}(q)
  \end{align*}
  So \begin{align*}
   sup_{T_1}(q\cup\{a\}) + sup_{T_2}(q\cup\{a\}) &\le \rho\cdot sup_{T_1}(q) + \rho\cdot sup_{T_2}(q)
  \end{align*}
  And \begin{align*}
    sup_T(q\cup\{a\}) &= sup_{T_1}(q\cup\{a\}) + sup_{T_2}(q\cup\{a\}) \\
    sup_T(q) &= \csize_{T_1}(q) + sup_{T_2}(q)
  \end{align*}
  Therefore, $$ \frac{sup_T(q\cup\{a\})}{sup_T(q)} \le \rho~\Rightarrow q~\text{is safe in}~T .$$
\end{proof}
\begin{theorem}
\label{CorrectnessOfPartialSuppression}
  \PartialSuppression always terminates with a correct solution.
\end{theorem}
\begin{proof}
We first prove that if the algorithm terminates, the suppressed table is safe.
Note that the algorithm can only terminate on Line \ref{algo:partialbreak}
  in Algorithm \ref{algo:partialsuppression}.
  Therefore, two conditions must be satisfied. First, the record cursor
$i$ should exceed the table size $|T|$. Second, the value $Safe$ must be \TRUE.
$Safe$ is true if and only if there is no unsafe \qids in the table, otherwise  Line \ref{algo:sanitize}
 will assign $Safe$ to \FALSE. If $i$ exceeds $|T|$ and
$Safe$ is \TRUE, the algorithm
must have scanned the table at least once and
hasn't found any unsafe \qids. Hence,
the suppressed table is safe.

Then we prove that \PartialSuppression always terminates by measuring the
  number of items left (denoted $l$) in the table after each step of suppression.
Initially, \[l=l_0=\sum_{i=1}^{|T|} |T[i]|\le |D(T)| |T|.\]
We state that for every invocation of \SanitizeBuffer, Line \ref{line:sanitize-suppress}
  in Algorithm \ref{algo:sanitize} is always executed at least once.
So the value $l$ strictly decreases by a positive integer
when \SanitizeBuffer is invoked.
And before the table becomes safe, \SanitizeBuffer will be invoked for
  every iteration of the loop in Algorithm \ref{algo:partialsuppression}.
So $l$ strictly decreases for each loop iteration in \PartialSuppressor.
Because $l$ starts from a finite number which is at most
\[l_0=\sum_{i=1}^{|T|} |T[i]|,\]
\PartialSuppressor must terminate.
%Otherwise there will be an infinite descending chain of all the $l$ values.

Now we prove that Line \ref{line:sanitize-suppress} in Algorithm \ref{line:sanitizebuffer}
  is always executed once \SanitizeBuffer is invoked.
Whenever \SanitizeBuffer is invoked, it is guaranteed that there exists
  an unsafe \qid $q\in B$ (see Line \ref{line:containunsafe}  in Algorithm \ref{algo:partialsuppression}).
$q$ is unsafe so that there always exists an item $e\in\linked(q)$ such that $conf(q,e)>\rho$,
  i.e. \[ \frac{sup(q\cup\{e\})}{sup(q)}>\rho \Rightarrow
   sup(q\cup\{e\})-\rho\cdot sup(q)>0 .\]
For $k$ on Line \ref{line:sanitize-k} in Algorithm \ref{algo:sanitize},
  \[ k = N_s(t, q\rightarrow e)\]
  and
  \[N_s(t, q\rightarrow e) \geq sup(q\cup\{e\})-\lfloor\rho\cdot sup(q)\rfloor \ge 1\]
  as is shown in Definition \ref{minimum}. Therefore,
  it is guaranteed that the number of deletions is at least 1
  because the rule $q\rightarrow e$ is unsafe and there must be some deletions to make it safe.
So $k\ge 1$ on Line \ref{line:sanitize-whilek} for the first time.
Thus the condition is satisfied and Line \ref{line:sanitize-suppress} is executed.
\end{proof}
\begin{corollary}
The divide-and-conquer optimization \SplitData is correct.
\end{corollary}
\begin{proof}
It follows directly from Lemma \ref{CorrectnessOfPartitioning} and
Theorem \ref{CorrectnessOfPartialSuppressor}.
\end{proof}

\subsection{\textcolor{red}{Detailed implementation and Analysis of the complexity}}
\textcolor{red}{During this section, we will discuss the implementation of those algorithms and how the process cause the time complexity.}

\textcolor{red}{Before the process of \PartialSuppressor Algorithm, we need to split the whole data into smaller ones according to \SplitData.
 It does not cost too much so we will skip this part.}

\textcolor{red}{Then we need to collect enough $qid$s. In this part we can generate at most $b_{max}$ $qid$s, which means the buffer is filled up.
 Because it is quite time-consuming to get the $sup$ of $qid$s, we adopt a $hashmap$ to include all the $qid$s we have encountered and put their $sup$ into this $hashmap$. Since each $qid$ is generated by a row of the records, it will be compared with the record it comes from, to make sure the sensitive items it links with (means the $qid$ and the sensitive items show in one record $T[i]$) and create such $linkedmap$ for every $qid$.  Every $qid$ may experience such comparison $|T|$ times. When generating $qid$ which already exists, we choose to update the $linkedmap$ mentioned above.}

\textcolor{red}{We need to tell whether each $qid$ is safe or not. Then we need to scan the whole $qid$s generated, to calculate $conf(q,e)$, such $q$ refers
 to each $qid$ and $e$ refers to the sensitive items in $q$'s $linkedmap$. Here consumed at most $|linkedmap|$ times for each $qid$.}

\textcolor{red}{In our heuristic functions, we also need to scan the unsafe $qid$s, whose size is at most $bmax$. Then we should calculate the function for
 each $(q, e)$ pair and choose the best choice for suppressing. Then we have to delete the selected items randomly from the records it has showed up. Since we
 have the $linkedmap$ above it can save a lot of time in finding the records containing both $q$ and $d$, in \SanitizeBuffer Algorithm. However, the maximum time consumed is also related with $|T|$. In the process of deleting items we need to check $|T[i]|$ (the size of each record) times for some operations, such as finding the items exactly equal to our goal item $d$ in \SanitizeBuffer Algorithm.
}

\textcolor{red}{In all, we can calculated the total complexity of the major part in one iteration. The whole complexity is O().}

\textcolor{red}{Notice that this is the maximum. In fact we have shorter time for searching $qid$s and items since items are distributed in only some of the records.}
