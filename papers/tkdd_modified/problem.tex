\section{Problem Definition}
\label{sec:prob}

This section
%defines a few preliminary concepts and then
introduces the privacy model and data utility before formally describing
the problem of partial suppression.

\cut{%%%%%%%%%%%%%%% BEGIN CUT %%%%%%%%%%%%%%%%%
\subsection{Preliminaries}
%A {\em multiset} $S$ is a set which allows repetitive elements, while the
%{\em power multiset} $\mathbb{N}^S$ is the set of all subsets of the multiset
%$S$.
%Examples or formal definitions of these are given below.
%\begin{description}
%  \item[Multiset] $[a,a,b]$ is the same as $(\{a,b\},\{(a,2),(b,1)\})$
%  \item[Power set] $2^S$ is the power set of the set $S$
%  \item[Power multiset] $\mathbb{N}^S$ is the power multiset of the set $S$
%\end{description}

%A {\em multiset} is a set which allows repetitive elements, while a {\em
%power set} is the set of all subsets of a set. Examples or formal definitions
%of these are given below.
%\begin{description}
%  \item[Multiset] \hspace{2em} $[a,a,b]$ is the same as
%      $(\{a,b\},\{(a,2),(b,1)\})$
%  \item[Power set]  \hspace{2em}  $2^S$ is the power set of the set $S$
%  \item[Power multiset] \hspace{4em} $\mathbb{N}^S$ is the power multiset
%      of the set $S$
%\end{description}

\begin{table}[th]
\centering
\caption{Notations}
\label{table:problem_notations}
\begin{tabular}{m{0.28\columnwidth}|m{0.6\columnwidth}}
  \hline
  \textbf{Symbol} & \textbf{Definition} \\
  \hline
$T$ %\in\natnum^{2^D}$
	& a set-valued table, which is a multiset of $m$ transaction records \\ \hline
  $D(T)$ & the domain of all items in $T$ \\ \hline
  $D_S(T)$ & the domain of sensitive items \\ \hline
  $D_N(T)$ & the domain of non-sensitive items \\ \hline
   % $T[i]\in T$ & the $i$-th record of $T$ \\ \hline
  $R$ & a transaction record in $T$ \\ \hline
 % $|R|$ & the number of items contained in $R$ \\ \hline
  $I$ & an itemset, which is a subset of $D(T)$\\ \hline
  $e$ & an item in $T$ and $e \in D(T)$ \\ \hline
  $q$ & a \emph{quasi-identifier} (also \qid), which is any itemset
(\textbf{including sensitive items}) drawn from any record in table $T$ \\ \hline
%  $e\in D$ & \KZ{a data item} \\ \hline
  $ X \rightarrow Y $ & an association rule where $X \subset R$, $Y \subset R$
and $X \cap Y = \emptyset$\\ \hline
  $nr(T)$ & set of all non-sensitive associations rules mineable from $T$
with sufficient support and confidence \\ \hline
  $\enum(R) = 2^R - \{\emptyset\}$ & the \qid enumeration of $R$, which is the power set of $R$ except the $\emptyset$ \\ \hline
  $\displaystyle Q(T)=\bigcup_{R\in T} \enum(R)$ & the set of all \qids in table $T$ \\ \hline
%  $\rho$ & the strong association rule threshold \\ \hline
  %$\mathcal{A}(q,e)$ & an inference/association rule $q\rightarrow e$\\  \hline
%  $\SA(q,e)$ & a sensitive association rule $\mathcal{A}(q,e)$  if \KZ{$e$ contains at least one sensitive item, or is a sensitive item?}\\ \hline
  $sup_{T}(I)$ & the support of $I$ is the number of transactions $t\in T$ such that $I\subset t$\\ \hline
  $conf_{T}(X \rightarrow Y)$ & the confidence of
$X \rightarrow Y$ from table $T$, given by $sup_T(X \cup Y)/sup_T(X)$\\ \hline
\end{tabular}
\end{table}

%A set-valued table $T$ is a multiset of transaction records,
%  each record $R \in T$ is a set of items drawn from domain $D$.
%  $D$ is the union of two non-intersecting set, sensitive domain $D_S$ and non-sensitive domain $D_N$.
%We follow the step of \cite{Sweeney2002:k-anonymity} and
%  extend the definition \emph{quasi-identifier} ($qid$) in relational database for set-valued data.
%Then we give a series of other definitions related with $qid$.
%As simple as you can imagine, a $qid$ is just a set of items taken from $D$.
%$Q$ is a set of $qid$s, $\Omega(R_i)$ is the $qid$ enumeration of $R$ which is the power set of $R$ except the $\emptyset$.
%The column count $cc$ of row $R$ is the number of items contained in $R$.
Table \ref{table:problem_notations} lists some notations used in the
rest of this paper. We define {\em container} and {\em linked items}
as follows.

\begin{definition}[Container]
The \emph{container} of an itemset $I$ in table $T$ is defined as
\begin{equation}
\container_T(I) = \{ i ~|~ I \subseteq T[i],~ 1 \leq i \leq |T| \}
.
\end{equation}
\end{definition}

\begin{definition}[Linked Items]
The set of all sensitive items linked by a \qid $q$ in table $T$ is defined as
\begin{equation}
\linked_T(q) = \{ e ~|~ e \in D_S(T) ~\backslash~ q,~ sup_{T}(q\cup\{e\}) > 0 \}.
\end{equation}
\end{definition}

According to the definition, $sup_{T}(I)$=$|\container_T(I)|$.
Also we will use $\container(I)$, $\linked(q)$, $sup(I)$,
$conf(X \rightarrow Y)$ to represent $\container_T(I)$,
$\linked_T(q)$, $sup_{T}(I)$ and $conf_{T}(X \rightarrow Y)$ respectively
when $T$ is the only table within discussion.
}%%%%%%%%%%%%%%%%%% END OF CUT %%%%%%%%%%%%%%%%%%%%

\subsection{Privacy Model}
$X \rightarrow Y$ is a {\em sensitive association rule} iff $Y$ contains
at least one sensitive item.
The privacy model of this paper
stipulates that, a dataset $T$ is {\em safe} iff no sensitive rules can be inferred from it
with a confidence higher than $\rho$ \cite{Cao:2010:rho}.
It is clear that,
if all sensitive association rules with a consequent of exactly one item can't be inferred from $T$ 
with a confidence higher than $\rho$, then all sensitive association rules 
can't be inferred with a confidence higher than $\rho$, and hence $T$ is safe.

Formally, we define {quasi-identifier} (a.k.a. \qid) to be any itemset
(including sensitive items) drawn from any record in table $T$.
A \qid $q$ is safe \wrt~$\rho$ iff $conf(q \rightarrow e)\leq\rho$,
for any sensitive item $e$ in $T$.
We say $T$ is safe \wrt~$\rho$ iff $q$ is safe \wrt~$\rho$ for any
\qid $q$ in $T$.
A \emph{suppressor} is a function
$S : T \mapsto T'$ where $T'$ is a suppressed table which is
safe \wrt~ $\rho$.
There are many different ways to suppress a table. The goal
is to find a suppressor that maximizes the {\em utility} of the suppressed
table.

\subsection{Data Utility}
\label{sec:du}
In this paper, we identify two major uses of an anonymized table:
{\em statistical analysis} and {\em association rule mining}.
In the first case, we want the anonymized table to have a distribution
as close to the original table as possible;
in the second case,
we would like the anonymized data to
retain all non-sensitive association rules while introducing few
or no spurious rules.
In both scenarios, the common goal is
to minimize the {\em information loss}, i.e.,
the total number of items suppressed.

Notice that the two scenarios are orthogonal. That is, 
when we prefer to preserve the useful association rules in the transaction,
we are inclined to delete more of the same type of items instead of 
different types since the latter may affect more rules. However, such strategy will
change the distribution of the deleted items. 
%So we have to define different objective functions for them.
With these two potentially conflicting scenarios in mind, 
we define two variants of an objective function $f(T, T')$ as:
\begin{equation}
f(T, T') =
\begin{cases}
NS(T, T')\cdot KL(T'~||~T) & \rm{(data~ distribution)} \\
& \\
\frac{NS(T, T')}{J(R(T), R(T'))} & \rm{(rule~ mining)}
\end{cases}
\end{equation}
where
\begin{eqnarray}
NS(T,T') &=& \frac{\sum_{e\in D(T)}(sup_T(e) - sup_{T'}(e))}{\sum_{e\in D(T)}sup_{T}(e)} \\
KL(P~||~Q)&=&\sum_{i}P(i)log\frac{P(i)}{Q(i)} \label{eq:kl}\\
J(A, B) &=& \frac{|A \cap B|}{|A \cup B|}\\
R(T)&=& \text{ set of rules minable from dataset } T
\label{eq:kl-dis}
\end{eqnarray}
Here $D(T)$ denotes the domain of items in $T$, $sup_T(e)$ denotes the
support of item $e$ in $T$, while the support of item $e$ means the number of instances of item $e$ in a certain set.
$P(i)$ and $Q(i)$ denote the probability of item $i$ in distribution $P$ and $Q$, respectively.
The functions $NS$, $KL$ and $J$ represent
total number of suppressions (normalized to 1),
K-L divergence\cite{kl-divergence} and
Jaccard similarity\cite{jaccard-sim}, respectively. K-L
divergence measures the distance between two probability distributions.
\cite{Kifer:l-diversity} first used K-L divergence in the privacy research.
Jaccard similarity measures the similarity between two sets.
$R(T)$ is the set of rules mined from dataset $T$ with a given confidence. In this paper, we set the this confidence to be
$\rho$, same as the parameter of our privacy model.
In our model, $\rho$ denotes that an attacker is only
interested in mining sensitive rules with at least a
confidence of $\rho$.
%
%The exact definition of $f$ depends on specific downstream utility of
%the data \cite{Xu:2008:ATD}. In this paper, we assume there are two variants of
%$f$, namely $f_{mine}$ which focuses on preserving the association rules
%mineable from the data, and $f_{dist}$ which focuses on preserving the
%data distributions. These two variants can be defined as,
%
%where $J$ is the Jaccard similarity function:
%\[J(A, B) = \frac{|A \cap B|}{|A \cup B|}\]
%and $KL$ is the Kullback-Leibler divergence which measure the
%similarity between two distributions:
%\[KL(P||Q)=\sum_{i}Q(i)log\frac{Q(i)}{P(i)}.\]
%
%is the \emph{symmetric relative entropy} which measures the change in
%data distribution. The third is the \emph{number of rules mined}
%(including original and spurious rules) from the anonymized data. The first
%measure is more general and used by other work. The last two metrics target
%the utility of the anonymized data for statistical analysis and rule mining,
%and they will be introduced in Section \ref{sec:eval}.

%Information loss of $T$ is essentially the number of items deleted in $T$
%divided by total number of items in $T$.
%Our goal is to find a suppressor
%defined in Definition \ref{def:suppressor} which reduces information loss as
%much as possible.


%\textcolor{red}{ We'll consider these three metrics in our
%heuristic solution later. }
%\begin{definition}[Optimal Suppression Problem]
%\label{def:osp}
%The optimal suppression problem is to find an optimal suppressor $S_\text{OPT}$ for a given table $T$ such that
%\[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor $S$.
%\end{definition}
%\begin{definition}[Minimum suppression]
%Minimum occurrence suppression of item type $t$, to make confidence of
%inference $\mathcal{A}(q,e)$ below $\rho$:
% \hspace{4mm}
%\[MS(t,\mathcal{A}(q,e))=
%\begin{cases}
%sup(q\cap \{e\})-sup(q)\rho & t=e  \\
%\frac{sup(q\cap \{e\})-sup(q)\rho}{1-\rho} & t\in q \\
% \infty & otherwise
%\end{cases} \]
%\end{definition}
%\PC {
%\begin{definition}[Remaining probability of item $i$]
%The Remaining probability of item $i$ is defined as
%\[ remain(i)=\frac{\kappa_{T^\prime}(\{i\})}{\kappa_T(\{i\})} \]
%where $T$ is the original
%dataset and $T^\prime$ is the current dataset processed by suppression but
%not finished
%\end{definition}
%}

\subsection{Optimal Partial Suppression Problem}
The optimal partial supression problem is to find a {\em Partial Suppressor} $S$
which anonymizes an input set-valued table $T$ to minimize
the objective function:
\[\min_S f(T, S(T))\]
such that $S(T)$ is {\em safe} w.r.t. to our privacy model.
In the following section, we will present the algorithm which 
includes two important heuristic 
methods for minimizing the objective function.

%In effect, this is an optimization problem that attempts to minimize the
%number of supressions while maximizing the similarity between the set of mineable
%non-sensitive rules before and after the suppression, or minimizing the distributional distance
%before and after the suppression.
%preserves the original data distribution or retains mineable useful
%association rules with limited spurious rules invented, and also minimizes
%item deletions.
%\begin{definition}[Optimal Partial Suppressor]
%The \emph{optimal partial suppressor} $S_\text{OPT}$ is the suppressor such that
%\[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor $S$.
%\end{definition}

%\PC {We have to mention that actually this metric of information loss makes sense, since the value of it is exactly the value
%calculated by the avgloss\cite{Cao:2010:rho} under the condition that only suppression is executed}
%Because most of the existing metrics of information loss are proposed
%for anonymization using generalization,
% \begin{definition}[Optimal Partial Suppressor for Distribution]
% \label{def:distribution}
% \[ Dist_{distance}(S_\text{OPT}(T), T) \leq  Dist_{distance}(S(T), T)\]
% while
% \[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor.
% \end{definition}
% \begin{definition}[Optimal Partial Suppressor for Mining]
% \[ Spurious(S_\text{OPT}(T)) \leq  Spurious(S(T)) \] and
% \[OriginalRule(S_\text{OPT}(T)) \geq OriginalRule(S(T))\]
% while
% \[ IL(T,S_\text{OPT}(T))\leq IL(T,S(T)) \] for any suppressor.
% \end{definition}
