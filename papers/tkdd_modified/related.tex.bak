\section{Related Work}
\label{sec:related}
This paper is an extended version of the best
paper award winning work published at DASFAA 2014 \cite{JiaPXZL14}.
In this paper, we formalize the problem more rigorously,
give more details of the algorithm and prove a few of its
interesting properties. Furthermore, additional
experiments as well as in-depths analysis of the experimental results
are added to this extended version.
In this section, we will discuss
a range of related work. We first present a number
of well-known privacy models, then compare and contrast several general
anonymization techniques, and finally show how our anonymizaton method
can protect several kinds of unsual attacks.

\subsection{Privacy Models}

Privacy-preserving data publishing of relational tables has been well
studied in the past decade since the original proposal of $k$-anonymity by
Sweeney \etal \cite{Sweeney2002:k-anonymity}.
Recently, privacy protection of set-valued data has received increasing
interest. The original set-valued data privacy problem was defined in the
context of association rule hiding
\cite{atallah99:disclosure,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding},
in which the data publisher
wishes to ``sanitize'' the set-valued data (or {\em micro-data}) so that all
sensitive or ``bad'' associate rules cannot be discovered while all (or most)
``good'' rules remain in the published data.
Subsequently, a number of privacy models
including $(h,k,p)$-coherence \cite{Xu:2008:ATD},
$k^m$-anonymity \cite{Terrovitis:2008:PAS},
$k$-anonymity \cite{He:2009:ASD} and
$\rho$-uncertainty \cite{Cao:2010:rho} have been proposed.
$k^m$-anonymity and $k$-anonymity are carried over directly from
relational data privacy,
while $(h,k,p)$-coherence and $\rho$-uncertainty protect the
privacy by bounding the confidence and the support of
any sensitive association rule inferrable from the data. This is
also the privacy model this paper adopts.

\subsubsection{The $k$-anonymity Model}
Many datasets are published simply with key identifiers (e.g. name and
social-security number) removed so the records are not related to specific
people.  However, some pseudo-identifiers (e.g. age and zip-code) can be
combined to narrow down to or even identify a small number of individuals.
In order to prevent identification, the $k$-anonymity model requires that
every such combination in the dataset occurs at least $k$
times so that
every record is indistinguishable from at least $k-1$ other records.

\subsubsection{The $l$-diversity Model}
Kifer \etal \cite{Kifer:l-diversity} showed using two simple attacks that a
$k$-anonymized dataset has some subtle but severe privacy problems, and
proposed a novel and powerful privacy criterion called $l$-diversity that
can defend against such attacks.

While $k$-anonymity is effective in preventing identification of a record,
$l$-diversity focuses on maintaining the diversity of the sensitive attributes \cite{aggarwal2008general}.
Therefore, the $l$-diversity model is defined as follows:

\begin{definition}
  Let a $q^*$-block be a set of tuples such that its non-sensitive values
  generalize to $q^*$.  A $q^*$-block is $l$-diverse if it contains $l$
  ``well-represented'' values for the sensitive attribute $S$.  A table
  is $l$-diverse, if every $q^*$-block in it is $l$-diverse.
\end{definition}

A number of different instantiations for this definition are discussed
in \cite{Kifer:l-diversity}, where the term
``well-represented'' is attached with different meanings.

\subsubsection{The $(h,k,p)$-coherence Model}
The $(h,k,p)$-coherence model by Xu \etal \cite{Xu:2008:ATD}
requires that the attacker's prior knowledge to be no more than $p$ public
(non-sensitive) items, and any inferrable rule must be supported by at least
$k$ records while the confidence of such rules is at most $h$\%. They believe
private items are essential for research and therefore only remove public
items to satisfy the privacy model. They developed an efficient greedy
algorithm using global suppression. In this paper, we do not restrict the
size or the type of the background knowledge, and we use a partial
suppression technique to achieve less information loss and also better retain
the original data distribution.

\subsubsection{The $\rho$-uncertainty Model}

Cao \etal \cite{Cao:2010:rho} proposed a similar $\rho$-uncertainty model
which is used in this paper.
They developed a global suppression method and a top-down
generalization-driven global suppression method (known as TDControl)
to eliminate all sensitive inferences with confidence above
a threshold $\rho$.
Their methods suffer from same woes discussed earlier for generalization and
global suppression.
Furthermore, TDControl
assumes that data exhibits some monotonic property under a generalization
hierarchy. This assumption is questionable.
Experiments show that our algorithm significantly outperforms the
two methods in preserving data distribution and useful
inference rules, and in minimizing information losses.

\subsubsection{Differential Privacy}

Differential privacy \cite{Dwork08:diff:survey}
is targeted at a statistical database. A statistic is a quantity computed from a sample.
The goal of differential privacy is to release statistical information without
compromising the privacy of the individual respondents.
It ensures that the removal or addition of a single database item does not
(substantially) affect the outcome of any analysis.
It follows that no risk is incurred by joining the database,
providing a mathematically rigorous means of coping with
the fact that distributional information may be divulged.
However, in the scenario of a statistical database,
users do not have exclusive access to all of the data set, but can only gain some statistics of the required data.
Such mechanism is then less efficient and harder to
deploy in real world than where the users can load the
whole data set into memory and do arbitrary computations.
Anonymization techniques of differential privacy add appropriately chosen random noise to produce response to the queries,
which will lead to spurious rules when doing data mining.

\subsection{Anonymization Techniques}

A number of anonymization techniques
were developed for these models.
These generally fall in four categories\cite{Machanavajjhala12}: {\em
global/local generalization}
\cite{Terrovitis:2008:PAS,He:2009:ASD,Cao:2010:rho}, {\em global suppression} \cite{Xu:2008:ATD,Cao:2010:rho},
{\em permutation} \cite{2011:TKDE:Anonymous} and {\em perturbation}
\cite{Zhang:2007:agg,ChenMFDX11:Diff,Javier2012,WangW05}. Next we briefly discuss the pros and
cons of these anonymization techniques.

\subsubsection{Generalization}

Generalization replaces a specific value by
a generalized value, e.g., ``beer'' by ``drink'',
according to a generalization hierarchy \cite{FungWCY10:Survey}.
Table \ref{tab:samesample} illustrates generalization by reusing the
same dataset in Table \ref{tab:sample}, where both ``beer'' and ``coffee''
are generalized to ``drink'', according to some generalization hierarchy.
While generalization preserves the correctness of the data,
it compromises accuracy
and preciseness. Worse still, association rule mining is impossible
unless the data users have access to the same generalization taxonomy
and they agree to the target level of generalization. For instance, if
the users don't intend to mine rules involving ``drink'', then
all generalizations to ``drink'' are useless.

\begin{table*}[thb]
\caption{The Same Dataset in Table \ref{tab:sample} and Generalization Anonymization Result
\label{tab:samesample}}
%\small
\centering
\subtable[Original Dataset]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, {\bf beer}, {\em condom} \\ \hline
2 & {\bf coffee}, fruits  \\ \hline
3 & {\bf beer}, {\em condom}  \\ \hline
4 & {\bf coffee}, fruits  \\ \hline
5 & flour, {\em condom}\\ \hline
6 & bread, {\bf coffee}  \\ \hline
7 & fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:orig-sample-same}
}
\subtable[Generalization Result]{
\begin{tabular}{|c|l|}
\hline
% after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
{\bf ID} & {\bf Transaction} \\ \hline
1 & bread, {\bf drink}, {\em condom} \\ \hline
2 & {\bf drink}, fruits  \\ \hline
3 & {\bf drink}, {\em condom}  \\ \hline
4 & {\bf drink}, fruits  \\ \hline
5 & flour, {\em condom}\\ \hline
6 & bread, {\bf drink}  \\ \hline
7 & fruits, {\em condom}  \\ \hline
\end{tabular}
\label{tab:sample-generalization}
}

\end{table*}

\subsubsection{Global Suppression}

Global suppression is a technique that deletes all instances of some items
so that the resulting dataset is safe.
The advantage is that it preserves the support of
existing rules that don't involve deleted items and hence retains these rules
\cite{Xu:2008:ATD}, and also it doesn't introduce
additional/spurious association rules.
The obvious disadvantage is that it can cause unnecessary
information loss. In the past, partial suppression
has not been attempted mainly due to its perceived side effects of
changing the support of inference rules in the original data
\cite{Xu:2008:ATD,Cao:2010:rho,tkde:VerykiosEBSD04:ARH,tkde:WuCC07:hiding}.
But our work shows that partial suppression introduces limited
amount of new rules while preserving many more original ones than
global suppression. Furthermore,
it preserves the data distribution much better than
other competing methods.

\subsubsection{Permutation}

Permutation was introduced by Xiao \etal \cite{Xiao:2006:Anatomy} for
relational data and was extended by
%. With generalization technique severely compromising the
%accuracy of data aggregation analysis, Xiao \etal propose the
%\textit{Anatomy} which releases quasi-identifier and sensitive values in two
%separate tables. Specifically quasi-identifier values are not changed and
%organized into groups, and for every such group the corresponding sensitive
%values are aggregated. After that,
Ghinita \etal \cite{2011:TKDE:Anonymous}
for transactional data.
Ghinita \etal propose two novel anonymization techniques for sparse
high-dimensional data by introducing two representations for transactional
data. However the limitation is that the quasi-identifier is restricted to
contain only {\em non-sensitive items}, which means they only
consider associations between quasi-identifier
and sensitive items, and not {\em among} sensitive items.
Manolis \etal \cite{terrovitis:privacy} introduced ``disassociation''
which also severs the links between values attributed to the
same entity but does not
set a clear distinction between sensitive and non-sensitive attributes.
%They set those frequent itemsets into a cluster and partition the table into
%several parts, which eliminates the rules with
%a high confidence and a certain support.
In this paper, we consider all kinds of associations and try best to
retain them.
%While in our paper, we consider all kinds of associations and try best to
%retain the accuracy of those associations
%
%\PC{Moreover, the model introduced in their method is not strong. The attack
%effects in the following steps. Step 1:The attacker gains some background
%knowledge such that the number of people buying creams and pregnancy tests
%are around five times more than people buying butter and pregnancy tests. .
%Step 2: The attacker downloads the result processed by permutation and one of
%the group in the result has such form that only contains people buying cream
%or butter with probability of sensitive item pregnancy test $\frac{1}{3}$.
%Step 3: with a simple equation $\frac{1}{3}(x+y)=5Px+Py$, where x represents
%people buying butter and y represents people buying cream, the attacker can
%get the exact probability of people who buy cream buy the pregnancy
%test($5P$) which is likely to be a high value with different x and y.
% }

\subsubsection{Perturbation}

Perturbation is developed for statistical disclosure control
\cite{FungWCY10:Survey}. Common perturbation methods include {\em additive
noise}, {\em data swapping}, and {\em synthetic data generation}. Their
common criticism is that they damage the data integrity by adding noises and
spurious values, which makes the results of downstream analysis unreliable.
Perturbation, however, is useful in non-deterministic privacy model such as
differential privacy \cite{Dwork08:diff:survey}, as
attempted by Chen \etal~ \cite{ChenMFDX11:Diff} in a probabilistic top-down
partitioning algorithm based on a context-free taxonomy.
 %Interestingly, the
%algorithm proposed in this paper is probabilistic in nature as well.
%Considering the fact that the noise introduced by randomization leads to
%severe data utility, some work related with differential privacy focuses on
%releasing certain data mining results
%\cite{Barak:2007:PAC:1265530.1265569,Bhaskar:2010:DFP:1835804.1835869,Friedman:2010:DMD:1835804.1835868,Korolova:2009:RSQ:1526709.1526733}.
%However, the usability of the published data is constrained by the pattern
%the owner decide to release and moreover the assumption that the data owner
%is able to perform data mining tasks is also weak. In addition, Leoni \etal
%\cite{DBLP:journals/corr/abs-1205-2726} also indicates the weakness of
%differential privacy model itself.
%
%\textbf{
%Recently, a new method called slicing was firstly proposed in \cite{10.1109/TKDE.2010.236} and was
%further developed in \cite{terrovitis:privacy}. Slicing, also called disassociation, aims to protect
%identity or attribute disclosure using identify combinations. However, the privacy model they introduce is
%different from ours and the type of targeted
%data utility is also completely different from ours.
%(I am in a dilemma. Since their methods are totally different
%from ours, I can't make comparison with ours.
%Their methods are somewhat useful in association rule mining and distribution remaining.
%The only disadvantage is that they changed the original structure of the table, but
%according to their data utility such change is acceptable.  )}

\subsection{Adversarial Attacks}
It is straight-forward to see that the anonymized data under our algorithm
is immune from the {\em record linking attack} \cite{FungWCY10:Survey}.
Furthermore, our technique can protect transactional data from {\em minimality
attacks} \cite{Wong:2007:Minimality} and {\em composition attacks} \cite{Ganta:2008:Composition}.

The minimality attack \cite{Wong:2007:Minimality}
is proposed for relational data. Assume an adversary knows the whole original
quasi-identifier values as external data, also knows the privacy model and
anonymization technique, by comparing the generalized version of
quasi-identifier values with the original quasi-identifier values, the
adversary can successfully predict some privacy. The minimality attack relies
on the generalization anonymization technique, while our method uses
suppression technique. Also for set-valued data it doesn't have fixed
combination of items as quasi-identifiers, so it's unrealistic for an
adversary to obtain the satisfactory external data.

The composition attack
\cite{Ganta:2008:Composition} is proposed for relational data by using the
overlap population of multiple organizations' independent release of
anonymized data, through intersection the privacy can still be breached in
relational data. For example $l$-diversity \cite{Ganta:2008:Composition}
model can be violated by composition attack. The reason why composition
attack can succeed is that quasi-identifier attribute values are generalized
and sensitive attribute values are retained, when performing intersection the
probability of a correlation between quasi-identifier and sensitive values
will definitely increase. On the contrary, our partial suppression algorithm
anonymizes set-valued data by randomly suppressing some sensitive items. In
the same way to perform intersection,
 the probability that a sensitive item correlated with quasi-identifier items can
both be higher or lower than before, which makes the
composition attack not plausible. So in a summary our partial suppression technique is ideal to
avoid composition attack depending on the randomized characteristic of
suppression.

\subsection{\textcolor{red}{Improvements of the previous version}}
\textcolor{red}{
This paper is a improved version of our previous one\cite{JiaPXZL14}.
In the last version, we stated our fundamental research purpose and the basic algorithms. However, there are some problems in that version, such as some vague definitions and inadequate experiments. In this version, we have proposed some specific explanations of the methods we use.
}

\textcolor{red}{
Firstly, we have proved some formulas from the definition in Section 3. Those proofs can show the validity of our algorithms. The significance of Regression is also stated to guarantee the correctness of the suppression. Particularly, we have added plenty of analysis of our algorithms in Section 3.4 in order to make our algorithms more pellucid. This part clearly presents why \PartialSuppressor can successfully terminate with a correct solution, which outputs safe transaction records. Also it shows the reason why we use divide-and-conquer to speed up the process.
}

\textcolor{red}{
Secondly, we have clearly defined the evaluation, such as \emph{info loss}, to make the experiments easier to understand. In section 4.1, Figure 1 is introduced to show the difference of data sets and the distribution of each one and we introduced some analysis to help better explain the results of different data sets. We reorganized the statement of experiments on variation of parameters such as $\rho$, \emph{t$_{max}$} and \emph{b$_{max}$} in following sections. This change makes it better to understand the functions of different variables.
}

\textcolor{red}{
Finally, we have discussed some privacy models this paper adopts. In this part, previous models, including their advantages and disadvantages are introduced. We also include the explanation of some anonymization techniques in this part to analysis the pros and cons of them. At last, some basic adversarial attacks are presented, in order to show the success of our partial suppression technique.
}
