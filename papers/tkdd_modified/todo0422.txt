First, the novelty of this paper is not high. It did not make new insight into adversarial attack models or privacy models, and it was not the first that proposed the partial suppression technique or applied it to anonymize set-valued data although it claimed that. Please refer to the work by (Atallah et al 1999) and (Verykios et al 2004) and the discussion in (Cao et al 2010).

As to the algorithmic aspect, I would suggest an algorithm that could be instantiated with any metric in a plugin-and-play mode (i.e., works with many utility metric without hard-coded). 

I did not agree to your discussion about differential privacy in Section 5.1.5. 

One claim by this paper, "we are the first to propose an effective partial suppression framework for anonymizing set-valued data" (your first contribution stated at line 44 of page 2), is not factual. 
In fact, (Atallah et al 1999) and (Verykios et al 2004) employed partial suppression in limiting the disclosure of sensitive rules embedded in set-valued data. Concretely, one of their approaches is to decrease the supports of itemsets and confidences of rules by removing items partially, which is exactly partial suppression. Notice that Cao et al 2010 also pointed out this. 

Some concepts were not accurately defined, which may confuse readers. In particular, the paper did not make it clear whether the confidence threshold (rho) for sensitive rules is same as the confidence threshold for mining non-sensitive rules in the anonymized data. The questions are: if they were same, what was the rationale; if they were different, how they were set in the experiments. There was no answer to such questions in your definitions (Section 2.2 Data Utility) and in your experimental evaluations (Section 4.2 Data utility). 

First of all, as one of your scenarios anonymizes data for data mining, you may explain why your scenario is necessary in the first place. You may compare your scenario with the following scenarios releasing synthetic data or releasing mining results instead of data. 
Y. Wang, X. Wu. 2005. Approximate inverse frequent itemset mining: Privacy, complexity, approximation. In ICDM, 2005 (this work proposed generating a synthetic database from frequent itemsets discovered in the original database). 
M. Atzori, et al. 2008. Anonymity preserving pattern discovery. VLDB Journal, July 2008, 17(4):703-727 (this paper considered privacy violations embedded in data mining results and suggested ways eliminating such violations by means of pattern distortion).

introduce the advantage of global suppression, in Regression part

TDControl generates generalized items. Thus, generalized association rules could be mined from the output data of TDControl. For a fair comparison, the authors need to generalize the rules, which are mined from the anonymized data by their algorithms, and then compare these rules with those mined from the anonymized data by TDControl.

permutation method, jaccard similarity evaluation

cleared problems:
1. complexity
2. clarified the choice of tmax, bmax, cutoff, and why we need those parameters.
2. Jaccard similarity. Have already changed the formula.
4. information loss function.
5. clarified the orthogonality of the two functions
6. typos
7. how to split the dataset
8. dont care about plural items

----------------------------------

When discussing generalization in Section 5.2.1, the following should be cited and discussed: 
P. Samarati and L. Sweeney. 1998. Generalizing data to provide anonymity when disclosing information. In PODS, 1998, pp.188 (this is the first work about generalization) V. Iyengar. 2002. Transforming data to satisfy privacy constraints. In KDD 2002, 279-288 (this I s the first work about global generalization, a.k.a. full subtree generalization) K. LeFevre, et al. 2006. Mondrian multidimensional k-anonymity. In ICDE 2006 (this is the first work about local generalization, a.k.a. local generalization). 

When discussing suppression in Section 5.2.2, the following should be included: 
M. Atallah , et al. 1999. Disclosure limitation of sensitive rules. Proc. 1999 Workshop on Knowledge and Data Engineering Exchange, 1999 (this work employs partial suppression in sanitizing sensitive rules in set-valued data. So does Verykios et al 2004). 
J. Liu and K. Wang. 2010. Anonymizing transaction data by integrating suppression and generalization. In PAKDD, 2010 (this is the first work that integrates suppression and generalization in anonymizing set-valued data. So does Cao et al 2010 ). 

When discussing perturbation in Section 5.2.4, please discuss the following: 
A. Evfimievski, et al. 2002. Privacy preserving mining of association rules. In KDD 2002, pp.217-228. 
A. Evfimievski, et al. 2003. Limiting privacy breaches in privacy preserving data mining. PODS 2003 (Both papers prevent privacy breach when releasing set-valued data for data mining).

As to data utility metrics, you may cite the following: 
D. Kifer and J. Gehrke. 2006b. Injecting utility into anonymized datasets. In SIGMOD, pp217â€“228, 2006 (this is the first work using K-L divergence in privacy research).
Moreover, you may cite Iyengar 2002 (listed above) which formally defined the information loss metric, you may use this metric instead of your "number of suppressed items", as it can measure data utility both for generalization and for suppression. By the way, you did not explain how you measured "information loss" for TDControl and Global in your experiments.