\section{Related Work}
\label{sec:related}

In the following, we first present the state of the affairs in WSD research,
and then put our work into perspective by discussing
various approaches in wikification before briefly introducing several
additional pieces of work related to wikification.

\subsection{Traditional WSD}
%After discussing our own wikification method, we would like to give a brief introduction of
%the previous work. As a famous open problem, much work has been done on WSD.
There are two types of WSD tasks: single-word WSD and
all-word WSD . Single word WSD aims to disambiguate a given word
in a sentence, a paragraph or a document, whereas all-word WSD demands
the disambiguation of all words within a syntactic unit.
Since all-word disambiguation can be broken down
into many single-word disambiguation problems, most work on WSD
focuses on a single word.
Traditional methods for WSD are either dictionary-based
or machine learning methods. Co-occurrence information
between words is also used in some WSD work \cite{GuthrieGWA91,Li1998:wcd,ChungKML01,Stokoe2003:WSD,Fernandez-AmorosGSS10,Veronis04:Hyperlex}.
Among them, Guthrie \cite{GuthrieGWA91},
Fernandez-Amoros \cite{Fernandez-AmorosGSS10} and
V\'{e}ronis \cite{Veronis04:Hyperlex} introduced unsupervised methods.
Guthrie \cite{GuthrieGWA91} proposed a two-level WSD on subject (domain) level
and sense level (within a subject). In each
disambiguation level, they chose the subject/sense with the highest similarity
between the context and the description of the subject/sense in a dictionary.
Fernandez-Amoros \cite{Fernandez-AmorosGSS10} used co-occurrence to compute a
relative matrix using mutual information (MI) measure.
They used the MI between the target word and words of its context as the
weight of each context words, and select the sense whose WordNet
definition is most
similar to the context by the bag-of-words model.
V\'{e}ronis \cite{Veronis04:Hyperlex}
clustered words into hubs on a co-occurrence graph mined from a large corpus.
The hubs that contain a target word define its different senses.
The disambiguation of a word is done by computing the similarity between the context
of this word and its various hubs by bag-of-words again.
%Hubs are the most related words to the target word and
%have low relatedness with other hubs. The process of their system is a kind of clustering
%on related words. The senses of a word are represented by a hub and the corresponding related word clusters.
%They finally used bag-of-words similarity between the context of the target word and the word cluster for each sense to pick up
%the right sense.
All of the above methods disambiguate {\em words} only whereas
in this paper, we proposed a method for the more general problem of
{\em phrase} sense disambiguation,
and instead of relying on bags-of-words approach,
our method takes advantage of the link co-occurrences in Wikipedia articles.
%making use of the co-occurrence information between Wikipedia concepts
%to help distinguish the different concepts of a term, given different contexts.

\subsection{Wikification Algorithms}
%\textcolor{blue}{(Kaiqi: divide related works by three categories: local, global, hybrid)}
Phrase sense disambiguation problem has thus far been attempted with
the help of Wikipedia due to its comprehensive coverage of nominal
concepts. When disambiguating words
and phrases of a document into the concepts of Wikipedia,
the process is known as {\em wikification}. Wikification can be thought of
as a generalization of the WSD task, since it seeks to label as many
terms in an article as possible. The earliest work on wikification can be
attributed to Mihalcea and Csomai\cite{MihalceaC07}.
%They were the first to address the task of automatically linking terms in a document
%to Wikipedia concepts and defined this process as wikification.
%They divided the process into two steps: detecting proper terms that need to be linked and
%determining the correct concept for a term via a classifier trained
%with Wikipedia corpus context.
%Some other researchers contributed
%to Wikification task after Mihilcea's work.
There are generally two broad approaches to the wikification problem
\cite{RatinovRDA11}:
{\em local algorithms} which labels the terms in a document one by one
using the local context of each term only;
%{\em global algorithms} which
%labels some or all terms together by exploiting the global semantic relatedness
%or constraints between Wikipedia concepts;
and the {\em global algorithms} which use global information of
the sense configuration in the whole sentence
to improve the previous local methods.
Next we discuss these two approaches separately.

%\subsection{Local Algorithms}
Most local algorithms use bag-of-words similarity between
the context of the target term and the context of each candidate sense
to identify the correct sense.
%We use co-occurrence between two concepts to measure the relatedness between them in our Wikification
%method, which is different from that.
Cucerzan \cite{cucerzan2007large} proposed a method of linking
{\em named entities} to Wikipedia concepts based on a vector space model.
%Strictly, Cucerzan's work is a special case of wikification
%since it doesn't link any terms other than named entities.
For each Wikipedia concept, the method picks up some features to form
a vector. Given a document, it compared the vector of each
Wikipedia concept in a term's Wikipedia concept candidate list
with the vector of the document. The concept with the most similar vector
is chosen. To get the document vector, the method
merges the vectors of all the candidate Wikipedia concepts of all terms
in the document which may lead to dilution and weakening of important signals.
Furthermore, this method works only for disambiguation of named entities.
Ferragina and Scaiella targeted their wikification work on
short text \cite{ferragina2010tagme}.
In that work, they also defined a scoring function to
calculate the relatedness between two Wikipedia concepts.
For each candidate concept of a term, they calculated a ranking score
by summing up the relatedness score between that concept and
all candidate Wikipedia concepts of all
other terms in the text. Then they chose the concept with the
highest score for that term.
Since this disambiguation process is very time-consuming for an online system,
they restricted the input documents to be short texts.
Milne and Witten applied a machine learning method on wikification
\cite{milne2008learning}.
Using part of the Wikipedia corpus as the training set, they combined three
features together, which were commonness, relatedness and quality of text,
to train a classifier that
can distinguish correct Wikipedia concept from irrelevant ones for a given term.
Several well-known machine learning methods were used like
Na\"{i}ve Bayes, C4.5 and SVM.
Fernando and Stevenson limited their wikification work on cultural heritage
\cite{fernando2012adapting}.
Their method, which is based on Milne and Witten's work,
only adds links of Wikipedia concepts related to cultural heritage.
They used category information in Wikipedia to filter
both the training set and the result.
%The Wikipedia concepts do not belong to category of culture,
%arts or humanities were removed.
They also used the link structure in Wikipedia to help find more proper
links.
Skaggs \cite{skaggs2011topic} and Boston \cite{Boston:2012} proposed topic
models for wikification. Skaggs used an LDA-based topic model for
wikification. Boston applied a simple generative model which selects the sense
with the highest conditional probability given the context of the term.
%He also built up a web based service to aid Wikipedia editors.

%\subsection{Global Algorithms}


%Comparing the disambiguation methods used in Cucerzan\cite{cucerzan2007large}
%and Ferragina\cite{ferragina2010tagme}'s system,
%we can see that they share a common idea. No matter
%what scoring function they used, they both tried to use all other terms' candidate Wikipedia concepts
%to help finding the correct concept of the term being disambiguated. However, candidate Wikipedia
%concepts of a term may be very different from each other, taking use of all the concepts may bring
%noise to the result.

%\subsection{Hybrid Algorithms}
Several recent attempts were made to combine both local and global features
in Wikification.
Global information is the relation or constraints between Wikipedia
concepts in the whole sentence, it usually comes from the
context similarity between two Wikipedia articles.
The context can either be the content of the article or the
surrounding words of links which point to the article.
Kulkarni et al. \cite{kulkarni2009collective} proposed a method
that considers two factors in disambiguating a term:
compatibility between the term and a candidate concept,
and relatedness among the concepts.
Compatibility is measured based on similarity features between
the term's context and the concept's context. The concept's context
is captured in four ways: the first paragraph of the Wikipedia article
describing the concept, full text of the article, anchor texts for the
occurrences of that concept in the corpus and its surround words.
Combined with three different similarity metrics, they obtain
a total of 12 feature vectors whose weights are determined from
training samples.
%compute similarity between term and concept on each field.
%In total they produced 12 similarity features to estimate the
%compatibility between term and concept.
The relatedness between two concepts is computed by the category structure
in Wikipedia and the cross references between the two articles.
The above two factors are combined to form an optimization problem,
which is approximately solved by greedy hill-climbing.
Tonelli \cite{Tonelli2012} developed a WSD system called Wiki Machine.
Wiki Machine is based on an SVM model by combining local and
global kernels. Local kernels are built from non-contiguous n-grams and
part-of-speech tags while global kernels use bag-of-words and latent
semantic to capture the topical information.
%They learnt the SVM model from a training set obtained
%from Wikipedia links and its context.
Ratinov \cite{RatinovRDA11} did a similar work to Tonelli, using both local
and global information to train an SVM model. Different from Tonelli's work
the local information they used is context similarity, while the
global information is the Wikipedia article/concept similarity.
They also trained a linear SVM classifier to decide
whether a term should be tagged or not.
This classifier is trained based on the link distribution of Wikipedia corpus.
%The more a term is likely to be linked in Wikipedia,
%the higher probability that term will be linked.
The advantage of this system is that it makes an attempt to avoid labeling
general terms which are usually not labeled by people in Wikipedia.
%But as always, manual labeling in these machine learning approaches
%is always tedious and inflexible.

%Li et al. presented a method for word disambiguation using co-occurrence data.
%\cite{Li1998:wcd}.
%\textcolor{blue}{(Kaiqi: summary)}
Strictly speaking, the link co-occurrence approach adopted in this paper
can be categorized as a global approach because the link co-occurrence
matrix that we obtain from the iterative algorithm is indeed global
information among the concepts, while at the same time during wikification
time we disambiguate several neighboring terms together in a sliding
window which forms a local context. Our method is different from all
existing methods because 1) it doesn't use a bag-of-words
(or bag-of-terms) model at all; 2) it relies on co-occurrences between
Wikipedia links or concepts rather than words or surface terms; and
3) wikification algorithms which depends on the Wikipedia link structures
or the link distribution face the problem of link sparsity on Wikipedia,
our iterative link enrichment process solves this problem and
can be used as a complementary preprocessing step for many of these algorithms.

%From the discussion above, we can see that most wikification methods
%are based on the words similarity between context of the
%target term and the context of the senses/concepts. We directly
%use the sense-level co-occurrence which is a kind of special global information
%with local characteristics. The co-occurrence matrix is the global relation
%between senses, and when wikifying, the senses of the surrounding terms
%provides local co-occurrence information. The co-occurrence matrix is enriched
%by an iterative bootstrapping process to ensure the completeness of the links.
%In addition, in wikification process, instead of merging other terms'
%candidate concepts to determine concept of one term once a time,
%we try to find the best combination of concepts in a window all at once.

\subsection{Other Work Related to Wikification}
There are some other work related to Wikification.
Strube and Ponzetto \cite{StrubeP06} proposed a method to measure
the semantic relatedness between two Wikipedia articles/concepts.
Three kinds of measures are used in their method: path based,
information content based and text overlap based measure.
These relatedness features can be used to train some
statistical model\cite{kulkarni2009collective,RatinovRDA11}.
Different from them, our approach is based on co-occurrence,
which is a more natural relation between concepts.
%Bartunov et al. \cite{bartunov2011wikifyme}
%built a gold standard corpus to evaluate different
%wikification methods. They have deployed their system so that
%contributors can edit the test data via the online interface.
%However, Bartunov's testbed is too limited for the evaluation
%of wikification systems.
He et al. \cite{HeRSOQ11} tried to
generate links from narrative radiology reports to Wikipedia automatically,
which can help users better understand the complex medical terminology.
Their work mainly targets on resolving the mapping from some medical terminology
to Wikipedia anchor text like article title and redirect page title.
He's work didn't share the same goal with ours since their main concern is
to resolve the ambiguity of anchor terms other than semantic ambiguity.
Lui et al. \cite{lui2011generation} applied wikification on
the generation of hypertext for web based learning.
%They divided the process of generating hypertext into two steps.
Given a set of documents,
they first linked these documents to Wikipedia articles.
Then they tried to replace the destination of the link with
a document from their set which is semantically
related to the original destination(Wikipedia article).
Jadidinejad et al. \cite{jadidinejad2009query} applied wikification
on structured query generation, which implemented the approach by
Milne et al. \cite{milne2008learning}.
Miao and Li proposed sentence wikification for query
oriented summarization, but they only applied an exact match strategy in
their system. The proposal in this paper complements much of the above
work as it improves the accuracy of wikification, a key component in
these systems.

%Then they try to replace some of
%Wikipedia links of links that link to documents in the set, which are semantically related to
%articles the original Wikipedia links link to.
%However, their result showed that few Wikipedia links
%could be replaced and they left the improvement as part of their future work.


%Their experiment showed that the previous
%wikification methods applying machine learning on Wikipedia corpus
%could not achieve good enough result.
%So instead of using Wikipedia corpus, they used a set of narrative
%radiology reports they collected as the training set to run machine learning.
%To generate candidate Wikipedia concepts, they split a term into sub-sequences.
%For each sub-sequence, they got a candidate
%list of Wikipedia concepts and then merged them together.
%Rather than developing a new wikification method,

