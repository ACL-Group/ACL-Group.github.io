\chapter{Scene recognition}
In this chapter, we will discuss how to combine the previous two part to do scene recognition.

\section{Problem Description}
Given the relation of event and scene, and models for each event, recognize the most probable scene context of a testing sound sample. Formally, given $p(scene|event)$, and GMM models $p(\mathbf{x}|event)$, we need to calculate a score $Score(scene)$ for each scene.

\section{Scene inferring}
\subsection{Data preprocessing}
\label{sec:datapre}
Given an audio, to recognize the scene context, we firstly need to find out what events are happened in the audio. 

Usually, we can regard an audio as a combination of events and background sound. From our experiment, we find that the short time energy of sound events is much higher than background sound. According to this knowledge, we cut the audio by energy, and only keep the high energy part. After this, our audio is split into several non-overlap segments. Then we discard some very short segments, since we believe an audio event should last for a while. Next, we cut each segment into smaller fixed length pieces. However, the length of the segment may shorter than the fixed length. If so, we will not cut it. Detailed implementation will be discussed in chapter \ref{cha:sys}.

\subsection{Score calculation}
After cutting data, we can calculate the probabilities of each piece $piece_i$ belonging to each event by equation \ref{pp}.

To test a sample $\mathbf{O}=(\mathbf{piece_0}, \mathbf{piece_1}, \cdots, \mathbf{piece_{k-1}}$, we can combine these $k$ results $p(\mathbf{piece_i}|event)$, \ie
\begin{equation}
\label{sc}
Score(scene) = \sum_{i=0}^{k-1} p(\mathbf{piece_i}|event)*L(piece_i)*p(scene|event),
\end{equation}
where $L(piece_i)$ is the length (number of frames) of the $i^{th}$ piece.



