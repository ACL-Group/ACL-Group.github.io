\section{Experiments}

In our experiments, we first evaluate the quality on selected schemas on
both clean and noisy input entity pairs, and compare the result of selected 
schemas compared with a baseline that do not contains DFS searching.
Finally, we list some sample relations with schemas learnt from our system.

\subsection{Experimental Setup}
%1. NELL ontology, FB version
In the experiments, we use NELL and Freebase as our background knowledge base.
More specifically, we use the 196th iteration of NELL ontology\footnote{We need url here}, 
which is the available version closeset to experiments of Lao et al. 
\cite{lao2011random}, and we use the July 2015 dump of Freebase 
\footnote{add url here} int our experiments.
We set $k = 3$ in BFS searching step as the maximum length of simple schemas.
For the remaining parameters, we set \KQ{bla = bla, bla = bla} based on the trade-off
between executing time and pruning size.

\subsection{Paraphrasing on Clean Entity Pairs}
In this part, we follow the setting used in Lao et al.'s work. We first obtain the labeled
entity pairs $R(e_1, e_2)$ for 48 different NELL relations, which is a clean data.
For each relation $R$, we create 2 symmetric queries: based on training entity pairs, 
learn a ranked list of schemas, predicting all $e_2$ given $e_1$, and predicting all 
$e_1$ given $e_2$, yielding 96 query tasks in total. 

For each task, we split labeled entity pairs into 5 parts, and make sure that 
the same $e_1$ (or $e_2$) in the given side won't occur in different parts \KQ{no information leak}.
We perform 5-fold cross validation (4 parts as input pairs, the remaining part as gold answer),
and calculate the average Mean Reciprocal Rank (MRR) score over different relations, that is, 
the inverse rank of the first correct $R(e_1, e_2)$ occurred in the gold answer.

We compare the MRR result with results in Lao's work, along with a untrained and trained
\textit{Random Walk with Restart} (PWR) model \cite{Haveliwala2002topic} as baselines (PWR\_ut, PWR).
Also we add an ablation baseline (w/oD) that we do not generate any specific schema in
DFS search, only picking schemas from predicate paths.

\tabref{tab:mrr} lists the average MRR score on our approach and other 4 baselines.
As the comparison result shows, our approach improves MRR score by 16.3\%. Besides,
the w/oD baseline reaches a score close to Lao's approach, bringing us the evidence
that searching specific schemas is a crucial part in paraphrasing tasks.

\begin{table}[ht]
%\small
	\centering
	\caption{MRR Result compared with baselines}
	\begin{tabular}{|c|c|c|c|c|c|}
		%\toprule
        \hline
				 & PWR\_ut & PWR & Lao & w/oD & Ours \\
        \hline
        MRR		 & 0.456 & 0.471 & 0.516 & 0.528 & 0.603 \\
        \hline
	\end{tabular}%
	\label{tab:mrr}%
\end{table}

\tabref{tab:emnlp-relation-specific} shows the MRR results per relation. We only compare the
result of our approach with the ablation baseline, due to no results reported in previous papers.
Since the size of relation instances $R(e_1, e_2)$ varies among different relations, 
the MRR scores on these relations also differs a lot.

\begin{table}[ht]
	\centering
	\caption{Relation-Specific MRR score}
	\label{tab:emnlp-relation-specific}
	\begin{tabular}{|l|c|c|}
		\hline
			Relation & w/oD & Ours \\
		\hline
			stateLocatedInCountry & 0.930 & 0.856 \\
		\hline
			competesWith & 0.750 & 0.707 \\
		\hline
			stadiumLocatedInCity & 0.318 & 0.750 \\
		\hline
			coachesTeam & 0.361 & 0.420 \\
		\hline
			cityCapitalOfCountry & 0.648 & 0.808 \\
		\hline
	\end{tabular}
\end{table}

%In order to measure in MRR, our system need to rank all $e_2$ (or $e_1$) given output schemas
%and the known $e_1$ (or $e_2$). 
%Write this at the end part of ILP, or candidate search.


\subsection{Paraphrasing on Noisy Entity Pairs}
The previous part makes the assumption that entity pairs for the input relation
is completely correct. However, in the real scenario, the relation instances extracted
by Open IE systems are usually made up of pure strings, and we need to perform
entity linking beforehead, coverting each argument string to an entity in the knowledge base. 
Therefore, noisy data can be brought from two aspects: wrong relation instances extracted,
and and error entities linked. 

In this part, we use Freebase as the knowledge base, and conduct paraphrasing tasks on 
ReVerb relations. We picked \KQ{20} popular relations from ReVerb which has at least
1000 relation instances.




1. alternative of MDL, check difference on principles
2. ablation test, DFS/ without DFS
3. state-of-the-art, find new papers
4. VELVET 2012 implementation
5. entity linking (self, others)
6. make bfs faster. robustness even in noisy situation
(optional) 6. down-streaming application (QA?)
