
\section{Experimental Details}
\label{sec:detail}
\subsection{Data Cleaning}
The following steps are enforced to remove noises as well as remove unhelpful parts for the CQGen task in the original data:

\paragraph{Fixing Unescaped HTML characters} We noticed that there are unescaped HTML special characters in both context and the question. (e.g. ``does it slice like zucchini \textbf{\& amp ;} cucumbers?'' is changed to ``does it slice like zucchini \textbf{\&} cucumbers?'')

\paragraph{Remove non-question parts} Sometimes there are declarative sentences following the question, which is not the focus of our task. We thus removed them. (e.g. where is this product made ? \sout{i contacted customer service and the representative was uninformed and could not offer any information .})

\paragraph{Remove noise questions} Some questions contain the comparison between 2 specific entities, which is unlikely to be tackled by our model, so we dropped them. And some questions are too universal (``Does it ship to Canada?''). We consider them as noise and also dropped them.

Note that the data cleaning was only imposed on the training set and the validation set. We preserve exactly the same test set as \citet{rao2019answer} for fair comparison.

\subsection{Hyperparameters and other settings}
For all models, we set the max length of context to be 100, question to be 20. For all variants of KPCNet, we use 2-layer GRU \citep{cho2014learning} with 100 hidden units for both the encoder and decoder. We use a learning rate of 0.0003 to train at most 60 epochs. For MLE, the model structure and parameters are identical to KPCNet, and we follow the setting of \citet{rao2019answer}, using dropout=0.5, learning rate=0.0001 to train 100 epochs. To improve the generation quality, we block bigrams from appearing more than once, and also forbid 2 same words to appear within 3 steps. For sampling-based keyword selection, we sampled 3 keywords from top-$K$ top-$p$ filtered keywords distribution with $K=6, p=0.9$ for 2 times. For clustering-based keyword selection, we produce 2 clusters from the top 6 predicted keywords. For hMup, we use the implementation in fairseq\footnote{\url{https://github.com/pytorch/fairseq/blob/master/examples/translation_moe}}. The architecture is set to 2-layer LSTM \citep{hochreiter1997long} with 100 hidden units, and other settings are identical to KPCNet for fair comparison. The threshold $\alpha$ for the default keyword selection method of KPCNet is manually tuned within range [0.05, 0.1]. The dropout strength is shared among all components of KPCNet and is manually tuned within range [0.2, 0.5]. MLE and KPCNet is implemented in PyTorch. For all manually tuned hyperparameters, we fix all other hyperparameters and random search for value within given range that can achieve the best BLEU on our validation set. The models are trained on a Ubuntu 18.04.4 LTS server with one NVIDIA GeForce RTX 2080 Ti. 

For \texttt{Home \& Kitchen} dataset, all models are operated on 200D word embeddings borrowed from \citet{rao2019answer}, which are pretrained from in-domain data with Glove \citep{pennington2014glove} and are frozen during training, except for hMup, which uses unique embedding to distinguish between experts and thus the embeddings are trained from scratch. The selected threshold $\alpha$ is 0.07, after 3 trials, and the selected dropout is 0.3 after 4 trials. 


For \texttt{Office} dataset, all models are operated on 200D word embeddings that we pretrained from in-domain data with Word2vec\citep{mikolov2013distributed} in gensim\footnote{\url{https://radimrehurek.com/gensim/models/word2vec.html}}, except for hMup. The selected threshold $\alpha$ is 0.07, after 3 trials, and the dropout is initially selected as 0.3 based on the result of \texttt{Home \& Kitchen}.

For hypothesis test in Table 4, we use \texttt{proportions\_ztest} of scipy for the first 3 columns whose range is binary, and \texttt{ttest\_rel} for the other 3 columns. The procedure we assign the underline are: First, we underline the best number at each column. Then we run hypothesis test against every other number. If the difference is not significant, we also underline it, otherwise we don't underline it. 

\section{Human Judgement Details}
\subsection{Metric Descriptions}
For human evaluation, we show each annotator a detailed annotation guideline with definitions and examples. Here we provide some brief explanations:
\begin{itemize}
  \item Grammaticality=0, if there is syntax error, or the generation result is not a question
  \item Relevance=0, if the problem is not related to the context
  \item Logicality=0, if there is clear nonsense within the question it self (does the lid have a lid ?), or the question is not suitable for the context (asking "how many bottles does it hold ?" for a bottle).
  \item Seeking New Information=0, if the question is asking for information already contained in the context, like asking color for a product titled "blue chair".
\end{itemize}

For Specificity, we ask “How specific is the question?”
and let annotators choose from:
\begin{itemize}
  \item 4: Specific pretty much only to this product (or same product from different manufacturer)
  \item 3: Specific to this and other very similar products
  \item 2: Generic enough to be applicable to many other products of this type
  \item 1: Generic enough to be applicable to any product under this category (H\&K or Office)
  \item 0: N/A (Not applicable) i.e. Question is ungrammatically, irrelevant or illogical
\end{itemize}

\subsection{Inter-annotator Agreement}
We report the inter-annotator agreement measured by Randolph's $\kappa$ \citep{randolph2005free} in Table \ref{tab:inter-annotator}. It can be seen that \textit{Grammatical} and \textit{Relevant} have high agreement as they are easy to judge. \textit{New Info} has lower agreement possibly because it is harder to decide. For the example in Table \ref{table:quality}, the question ``what is the color of the chair ?'' may have not been annotated as repetitive as the word ``color'' doesn't appear in the context, though it is actually covered by the specific value ``blue grey''. \textit{Logical} and \textit{Specific} have the lowest degree of agreement as they are more subjective criteria. According to the table suggested by \citet{landis1977measurement}, all the criteria achieved at least moderate agreement.


\begin{table}[htbp]
  \centering
  \begin{tabular}{l|ccccc}
  \hline
  Criteria & Agreement \\
  \hline
  Grammatical\tiny{[0-1]}  &  0.933 \\
  Relevant\tiny{[0-1]} &  0.853 \\
  Logical\tiny{[0-1]} &  0.659  \\
  New Info\tiny{[0-1]} &  0.701  \\
  Specific\tiny{[0-4]} &  0.546  \\
  \hline
  \end{tabular}
  \caption{\label{tab:inter-annotator} Inter-annotator Agreement measured by Randolph's $\kappa$ \citep{randolph2005free}}
\end{table}

\section{Ablation Test}
\label{sec:ablation}

Below we describe the ablation test to check the influence of the components and hyperparameters of the model. These tests are all conducted on the \texttt{Home \& Kitchen} dataset.

\subsection{Additional Metrics}


To evaluate the quality of our keyword predictor and keyword bridge, we propose these additional automatic metrics:
\paragraph{P@5} Since the number of keywords in ground truth questions are different across each sample. We take the top 5 keywords with the highest predicted probability as selected keyword set $\mathbf{z}^s$, and calculates precision@5 by:
\begin{equation}
  P@5 = \frac{|\mathbf{z}^s \cap \mathbf{z}^T|}{5}
\end{equation}
where $\mathbf{z}^T$ is the union of keywords extracted from all ground truth questions of a sample.

\paragraph{Response Rate} which is the proportion of conditioned keywords that appears in the corresponding generation, and we report the macro average on all the records. We use this to evaluate the controllability of the keyword conditions.

We also report the average generation length(\textbf{Length}) as it is related to almost all metrics proposed above, but neither long or short generation should be considered an indicator of good performance.


\begin{table}[htbp]
  \centering
\begin{tabular}{l|ccccc}
\hline
{} & Distinct-3 & BLEU & P@5 & Response & Length \\
\hline
KPCNet(C, S) & 15.30 & 17.77 & 0.47 & 0.40 & 7.26 \\
-C & 16.51 & 15.88 & 0.51 & 0.35 & 7.52 \\
-S, +E & 12.00 & 9.04 & 0.22 & 0.50 & 7.66 \\
+H & 29.97 & 12.85 & 0.47 & 0.66 & 9.17 \\
\hline
\end{tabular}
\caption{\label{table:ablation1} Ablation test results on \texttt{Home \& Kitchen} for data and keyword predictor at individual-level. The first line is final adopted setting.}
\end{table}


\begin{table}[htbp]
  \centering
  \begin{tabular}{l|ccccc}
  \hline
  {} & Distinct-3 & BLEU & Response & length \\
  \hline
  Dropout = 0.2 & 17.29 & 17.11 & 0.45 & 7.16 \\ 
  Dropout = 0.3 & 15.30 & 17.77 & 0.40 & 7.26 \\
  Dropout = 0.4 & 13.02 & 18.33 & 0.35 & 6.95 \\
  Dropout = 0.4, NE & 15.04 & 18.19 & 0.34 & 6.78 \\
  Dropout = 0.4, ND & 12.19 & 17.47 & 0.32 & 6.53 \\
  Dropout = 0.5 & 11.77 & 18.53 & 0.32 & 6.66 \\
  \hline
  \end{tabular}
  \caption{\label{table:ablation2} Ablation test results for keyword bridge at individual-level on \texttt{Home \& Kitchen}.}
  \end{table}

  

  \begin{table}[htbp]
    \centering
    \begin{tabular}{l|ccccc}
    \hline
    {} & G & R & L & N & S \\
    \hline
    KPCNet &        \textbf{0.99} &     \textbf{0.99} &    \textbf{0.95} &     0.80 &     1.81 \\
    KPCNet(filter) &        \textbf{0.99} &     \textbf{0.99} &    0.94 &     0.85 &     \textbf{1.84} \\
    \hline
    KPCNet &        0.98 &     0.97 &    0.88 &     0.84 &     1.77 \\
    KPCNet(filter) &        0.98 &     0.97 &    0.89 &     \textbf{0.88} &     1.80 \\
    \hline
    \end{tabular}
    \caption{\label{tab:ind-human-eval-2} Comparison between KPCNet with Dropout=0.3 (upper half) and Dropout=0.2 (lower half) with individual-level human judgements on 100 sample products from \texttt{Home \& Kitchen}. G/R/L/N/S stand for Grammaticality, Relevance, Logicality, New Info and Specificity respectively.}
    \end{table} 

\subsection{Ablation Factors}

These are many important factors and parameters in our model. So we divide the ablation test into 2 logical parts: one for keyword predictor (and the effect of data cleaning on it), and another for keyword bridge. 

The ablation factors for keyword predictor are as follows (abbreviated for readability):
\begin{itemize}
  \item \textbf{E}: End2end training of keyword predictor with other component. The training objective is a weighted sum of the 2 objectives (Equation 2 \& 3).
  \item \textbf{S}: Separate training, first train predictor, and then freeze its parameters to train other parts. 
  \item \textbf{H}: Hard label fed to bridge instead of masked soft logits. The label can be provided from ground truth in training and is decided with threshold filtering in inference. If this setting works well, we can then completely separate the parameters of predictor from other parts.
  \item \textbf{C}: Cleaned dataset.
\end{itemize}


The ablation factors for keyword bridge are:
\begin{itemize}
  \item \textbf{NE}: No encoder feature fed back to encoder
  \item \textbf{ND}: No decoder feature fed to decoder
  \item \textbf{Dropout}: We add a dropout layer for the unmasked keywords logits before it passes the latter transformation. Due to the nature of dropout, this part may help ease the noise introduced by the error of keyword predictor. And we study the effect of the strength of this layer.
\end{itemize}

\subsection{Results}

The ablation test result for data and keyword predictor at individual-level is shown in Table \ref{table:ablation1}. The setting for keyword bridge is fixed: dropout=0.3, both encoder and decoder feature are used. After data cleaning(C), \textit{P@5} dropped because of the reduction of the number of ground-truth keywords. The decreasing of \textit{Distinct-3} and \textit{Length} shows the effect of irrelevant part removing. The improvement on \textit{BLEU} and \textit{Response} indicates the overall benefits brought by the cleaning. End2end training(-S, +E) leads to significant performance degradation on all metrics except slight increase on \textit{Response}. The possible reason is that keyword prediction skews highly towards frequent keywords under this condition. Finally, feeding hard label instead of logits also produce worse result. We can see from the extremely high \textit{Response} and \textit{Length} that this setting suffers severely from over-generation of keywords: model generates illogical long questions to contain as much keywords as possible. We hypothesize that the soft logits can reflect subtle difference on the importance of each conditioned keyword and thus can lead to more robust performance. Moreover, we can achieve a \textit{P@5} of 0.628 with one group of group truth keywords, as compared to 0.472 of the current model, which shows a huge room for improvement of the keyword predictor.


  The ablation test result for keyword bridge at individual-level is shown in Table \ref{table:ablation2}. The setting for keyword predictor is fixed as KPCNet(C, S). We can clearly witness the trend that the higher dropout, the higher controllability keywords will have over generation (Response). As a result, the behavior of KPCNet will be more and more like MLE when dropout grows, with lower generation length, lower keyword response and higher BLEU. We speculate that the dropout imposed on the keywords logits to be masked forces the model to make prediction with incomplete keyword set. Therefore, proper level of dropout can make the model robust to the noise introduced by keyword predictor. Furthermore, the ablation of either encoder bridge or decoder bridge would harm BLEU, response and length, which proved the effect of KPCNet's double-bridge design to guide the generation via attention between the two sides. 


We also conducted human evaluation for different value of dropout (Table \ref{tab:ind-human-eval-2}), and found that lower dropout trades logicality for new information. We selected Dropout=0.3 as the final setting for its good balance of all metrics.
 