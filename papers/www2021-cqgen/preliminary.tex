\section{Preliminaries}

\subsection{Keyword-based Diverse CQGen}

Given a textual \textit{context} $\mathbf{x} = (x_1, x_2, ..., x_{T_1})$, our aim is to generate a \textit{clarification question} $\mathbf{y} = (y_1, y_2, ..., y_{T_2})$, so that $y$ asks for relevant but not repetitive information to $\mathbf{x}$. In the setting of \textit{Diverse CQGen}, we should generate a group of CQs for the same context such that they are semantically different from each other. In this work, we additionally consider \textit{keywords} $\mathbf{z} = (z_1, z_2, ..., z_k)$ that are expected to capture the main semantic of $\mathbf{y}$. The definition of keywords may vary across domains, and here for e-commerce, we empirically define keywords as lemmatized, non-stopping nouns, verbs and adjectives appearing in \textit{questions}, according to our observations on specificity (\S \ref{sec:specific}). Note that keywords are different from \textit{answers}, and we don't assume the 
existence of an answer in our approach. We extract ground truth keywords and a keyword dictionary 
$Z$ of size $C$ from the CQs in the training set using this definition. 

With keywords introduced, the marginal likelihood of a question are 
decomposed as:

\begin{equation}
  \begin{split}
    p(\mathbf{y}|\mathbf{x}) &= \sum_{\mathbf{z} \subseteq Z}p(\mathbf{y},\mathbf{z}|\mathbf{x}) \\ 
    &= \sum_{\mathbf{z} \subseteq Z}p(\mathbf{y}|\mathbf{x},\mathbf{z})p(\mathbf{z}|\mathbf{x})
  \end{split}
  \label{equ:decompose}
\end{equation}
where $p(\mathbf{z}|\mathbf{x})$ corresponds to the keyword prediction part, and $p(\mathbf{y}|\mathbf{x},\mathbf{z})$ refers to the keyword conditioned generation. The range of $\mathbf{z} \subseteq Z$ is very large, so in practice, we sample portions of them to get an approximation, as will be discussed later (\S \ref{sec:selection}).

\subsection{Specificity}
\label{sec:specific}
In this work, the specificity of a question is determined by the size of 
its applicable range. Question that can only be raised against one particular context is 
considered more specific than universal questions. 
First, \textit{relevance} of the question is the basic
requirement of specificity.  Traditional MLE training may generate generic but not 
relevant question for higher likelihood. We conjecture that the 
additional task of keyword prediction will help focus on relevant aspects. 
Moreover, by observation, we discover that \textit{specificity} of e-commerce questions can be further promoted by: 
\begin{enumerate}
  \item Focusing on certain aspects, like the type, brand and attributes.
  \item Mentioning components of a product, e.g. blade of a food grinder.
  \item Describing a using experience specific to the product, such as cooking rice in a cooker.
\end{enumerate}

We hypothesize that many of them can be captured by keywords, with nouns and adjectives covering aspects and components, and verbs constituting the using experience.

\subsection{Diversity}
\label{sec:diversity}

Diverse CQGen requires a group of questions to be generated about the same context, to cover various information needs as well as improve the robustness to problematic generations. This setting differs from some previous 
literature~\citep{wang2018learning, rao2019answer}, where they generate 
only one response at a time, and \textit{Diversity} is used to measure 
the expected variety among 
\textit{all generated response}. We call it \textit{global diversity}. 
Our setting is referred to as \textit{local diversity}, measuring 
the diversity \textit{within one usage}. This is also adopted by another 
line of literatures \citep{vijayakumar2018diverse, shen2019mixture}. 
If not specified, we mean \textit{local diversity} by using \textit{diversity}. 
Global diversity is also desired, as it increases the likelihood of the questions to be specific to various contexts. 

To meet the diversity requirement as well as to promote specificity, we propose KPCNet below.

