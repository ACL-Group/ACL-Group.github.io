# Response

R1:

Thank you for your detailed review with valuable questions and constructive suggestions.

Discussion on the questions raised at "Relevance" section:
We strongly agree with you that if the generated questions can only tackle 
the aspects already covered in 'product details', then they are useless. 
The only way CQs can add values is to consider aspects that are not considered
by the seller. Though it might sound like a rare occasion, we observed
that such questions are raised fairly frequently at Customer Q&A section 
on major e-commerce platforms. These questions can be quite specific and 
diverse as each user has particular information needs. One example is "I called oreck and I need part # 79031-01, will this fit it or you have it ?", 
which could hardly come to the seller's mind before hand. 
The 110V/220V example, though relatively easy to solve, is also a real 
and common case in the Amazon H&K dataset. This may further validate that 
it's hard for sellers to cover all kinds of information needs, 
and they may benefit from CQGen services provided by the platform, 
which has a much boarder knowledge on products and user needs. 
To conclude, We aim to simulate the features of real customer questions with 
our algorithms emphasizing specificity and diversity. We hope to draw 
more attention to this valuable diverse CQGen task with our first attempt 
in this paper. Our method is compatible with, and may be further enhanced 
by KBs (Sec 3.4.2), and we are also looking forward to future progress 
on this task with richer knowledge.

Explanation of the questions raised at "execution" section. 
To extract keywords dictionary, we take the lemmatized form of content nouns
(i.e., not stop words), verbs and adjectives from questions in the training 
set (thus ground truth keywords), according to our definition of keywords. 
The result is a set of keywords, which can be regarded as a minimum knowledge 
source of the main semantics that we guide the model to focus on. 
More specifications on 'Local Diversity': Suppose we are generating 
K questions for N products, then the semantic diversity within each group 
of K questions is considered local, which is measured by Pairwise-BLEU. 
While the diversity among all K*N questions is regarded as global, 
as is measured by Distinct-3. Previous works only generate 1 question per 
product, which is a special case of our work, in which K=1.
Clarification on our research questions:
Q1: We compare specificity with baseline CQGen methods.
Q2: We operate on the conditioning keywords to control the generation, 
either by keyword selection (Sec 3.3) and keyword filtering (Sec 3.4).
Q3: Our diversity measure is Pairwise-BLEU in automatic evaluation and #Useful in human judgements.

Questions on Reproducibility:
We didn't include the links to code and data for blind review purpose, 
we'll make them public. For the annotation setting, 4 annotators are recruited to annotate on 100 sample products for 8 systems with each group having 
3 questions, which is already mentioned at the last paragraph of Sec 4.1. 
The inter-annotator agreement is reported at Appendix B.
We have a detailed annotation guideline for human evaluation, with definitions 
and examples. We didn't include them for length consideration and 
we'll release them along with code, and we give some explanation here:
Logicality=0, if there is clear nonsense within the question itself (does the lid have a lid?), or the question is not suitable under the context 
(asking "how many bottles does it hold ?" for a bottle).
Grammaticality=0, if there is syntax error, or the generation result is 
not a question. Seeking New Information=0, if the question is asking 
for information already contained in the context, like asking color for 
a product titled "blue chair".
We agree with you that this is a precondition of CQGen. However, 
existing approaches can't guarantee to achieve this, so we also evaluate it. 
We also proposed a specific solution, keyword filtering to alleviate 
this problem.



R3:
Thank you for your reviews. 
For your consideration of the limited application scope of our model:
The proposed solution is generally applicable to any scenario with (context, question) pairs, as long as we can give reasonable definition of keywords. Therefore, we believe our method can be applied to other domains like community QA and dialog systems, perhaps with slight adjustments on the keyword extraction method.
