R1:
Thank you for your valuable comments.

1. We agree with you that the overall extractor-abstractor (ext-abs) framework 
adopted in this paper is not new. However, some of the key components in 
our ext-abs framework are distinct from previous works.

Extractor: We propose a keyword-based extractor 
with a keyword-guided dual encoder and an aligned pointer decoder to 
extracts sets of sentences from source document. 
The extractor is trained on proposed combinational loss.

Abstractor: We design a special loss to train the abstractor,
which considers all of the sets in a complete summary.

Reinforcement Learning: We present comprehensive reinforcement learning 
(CRL) to bridge the extractor and abstractor, consisting of 
sentence-level reward, set-level reward and summary-level reward.
The rewards of CRL consider the output of the extractor and abstractor 
at the same time.

2. We did not include UniLM as a choice of our abstractor
because Lewis et al.[1] has showed that Bart is stronger than UniLM
in summarization tasks.

R2:
Thank you for your questions.

1. The reasons for us to use two human annotators were as follows:
First, for a sample, we show the original article, the ground truth summary as well as generated summaries 
to each annotator.
Although the two annotators do not know what model produces a given summary or 
which dataset a sample belongs to, the inter-annotator agreements (Cohenâ€™s Kappa coefficient) 
between them on all human evaluation metrics are greater than 0.6 (substantial agreement).
This indicates that their evaluation results are consistent and reliable (see Page 6 Line 694~695).
Second, in the literature, we have seen papers using either 2, 3 or 5 annotators for human evaluation
of text generation, e.g., [2],[3] and [4] all use 2 annotators. 
However, with your permission, we are willing to add another annotator in this part of the evaluation
in the final version.

2. Readability is one of the human evaluation metrics in our paper, which focuses on the
summary's logical consistency with the source document and the summary's informativeness, 
i.e., the usefulness and/or relevance of the summary (please see Page 7 Line 730~735).

R3:
Thank you for your suggestion and question.

1. As our models are two-stage models (extractor and abstractor) trained 
on set-level pseudo summaries,
sentence-level or summary-level rewards alone are not optimal.
The sentence-level reward focuses on each extracted sentence,
which ignores the consistency between generated summaries and reference summaries.
The summary-level reward focuses on differences between generated summaries 
and reference summaries as a whole, 
but it does not consider the extracted sentences and set-level intermediate summaries.
Our proposed CRL, instead, combines sentence-level, set-level and summary-level rewards,
which better guides the extractor to select correct sentences and 
helps abstractor to generate more accurate summaries (please see Page 9 Line 976~1035).

2. We will improve the presentation of Section 2 in the final version.

R4:
Thank you for your suggestions. 

Because PEGASUS was not published at ICML and there was no stable version 
at the time we prepared this paper, it was not discussed in the submission.
With your permission, we can add the following discussion into the final version:

PEGASUS is a new, pretrained model for text summarization, 
which uses self-supervised objective Gap Sentences Generation (GSG) to train 
a transformer seq2seq model. Different from previous pretrained models, 
it masks sentences rather than smaller continuous text spans.
Since PEGASUS is an encoder-decoder model and can be used in the same context as BART, 
it has the potential to substitute BART in our ext-abs framework
and enjoy similar boost in accuracy and speed.

References

[1] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer.
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. ACL 2020.

[2] Minh-Tien Nguyen, Chien-Xuan Tran, Duc-Vu Tran, Minh-Le Nguyen.
SoLSCSum: A Linked Sentence-Comment Dataset for Social Context Summarization. CIKM 2016.

[3] Maxime Peyrard, Iryna Gurevych.
Objective Function Learning to Match Human Judgements for Optimization-Based Summarization. NAACL-HLT (2) 2018.

[4] Minh-Tien Nguyen, Duc-Vu Tran, Le-Minh Nguyen, Xuan-Hieu Phan.
Exploiting User Posts for Web Document Summarization. ACM Trans. Knowl. Discov. Data 12(4). TKDD 2018.

