\section{Related Work}
\label{sec:related}
%\KZ{Missing preamble}
Related work on extractor-abstractor framework and pretrained models for summarization are introduced as follows.
\subsection{Extractor-Abstractor Framework}
%\KZ{Always put our approach into perspective when u discuss 
%other works. This is not done at all here!}
In this paper, we adopt the extractor-abstractor (ext-abs) framework, which has been
a popular method for abstractive summarization recently.
Unlike the end-to-end models
\cite{NallapatiZSGX16,SeeLM17,LiuLZ18,BART19,ProNet20} 
in abstractive summarization,
the ext-abs framework trains two enc-dec models, extractor and abstractor.
%In this framework,
The extractor captures salient content (pseudo summary) of source document, 
where the pseudo summary can be either sentence-level~\cite{TanWX17, hsu18,FastAbs18}
or summary-level~\cite{summlevel19, SharmaHHW19},
and then abstractor paraphrases the salient content to generate a summary. 
In this paper, we present a set-level matching heuristics to construct the pseudo summaries, better aligned to reference summaries.

%Both extractor and abstractor are encoder-decoder models.
%The salient content (pseudo summaries) 
%as the ground truth of extractor's outputs and abstractor's inputs
%should be first identified, usually at sentence-level or summary-level.
%The approaches at sentence-level~\cite{TanWX17, hsu18, KeyGuid18,FastAbs18} 
%employ greedy search to select the sentence from source that maximaze the
%ROUGE scores with each sentence in reference symmary.
%As for summary-level methods~\cite{summlevel19, SharmaHHW19}, 
%it is to find an optimal
%combination of sentences, which achieves the best ROUGE scores with reference summary.

Extractive models adopt hierarchical neural network as encoder
and pointer network as decoder~\cite{Cheng16,NallapatiAAAI17}.
It is extended with variant models, 
such as reinforcement learning~\cite{NarayanCL18} and
joint scoring~\cite{score18}. 
As transformer preforms excellent on language model, 
Liu et al.~\shortcite{LiuML19} and Zhang et al.~\shortcite{HiBert19} 
apply pretrained transformers to extractive summarization.
Zhou~\cite{ZhouYWZ17} and Li et al.~\shortcite{KeyGuid18} have shown that keywords play an important role in summarization.
So we enhanced the extractive models with an additional keyword encoder to get better alignments between pseudo summaries and reference summaries.
%Zhou~\cite{ZhouYWZ17} uses a selective gate network to retain 
%key information.
%Li et al.~\shortcite{KeyGuid18} 
%utilize keywords
%to guide the summarization generation.


Abstractive models are based on sequence-to-sequence learning~\cite{SutskeverVL14,BahdanauCB14}.
The pointer-generator networks~\cite{SeeLM17} consisting of
copy mechanism and coverage model are the most popular 
baseline in abstractive summarization. 
The pretrained transformer language models have success in 
natural language processing.
%natural language understanding (NLU) and natural language generation (NLG). 
Through fine-tuning the pretrained models on summarization task,
the quality of generated summaries are improved~\cite{HiBert19,ZhongLWQH19}.

The reinforcement learning (RL) is always used to connect the extractor and abstractor together,
which makes an end-to-end trainable model.
Chen~\shortcite{FastAbs18} and Bae et al.~\shortcite{summlevel19} encourage extractor
to select sentences with high ROUGE scores by RL. 
Sharma et al.~\shortcite{SharmaHHW19} propose an entity-driven encoder and
utilize RL with coherent rewards to make abstractor generate readable summaries. Different from previous end-to-end evaluation rewards, 
we propose a comprehensive reward, taking the intermediate extracted pseudo summary and set-level abstactive summaries into consideration.

\subsection{Pretrained Models for Summarization}
The pretrained transformer language models have success in natural language understanding (NLU)
and natural language generation (NLG).
NLU models are pretrained on unidirectional and bidirectional prediction.
GPT~\cite{GPT18} employs a unidirectional transformer~\cite{attn17} to predict the sequence. 
ELMo~\cite{ELMo18} learns two unidirectional language models of forward and backward.
BERT~\cite{Bert19} uses a bidirectional transformer encoder 
to predict the masked words.
NLG models pretrain on sequence-to-sequence (seq2seq) models. 
UniLM~\cite{UniLM19} is a multi-layer transformer network,
including unidirctional, bidirectional and seq2seq language model. 
BART~\cite{BART19} takes combines bidirectional transformer encoder 
and auto-regressive transformer decoder.
ProphetNet~\cite{ProNet20} trains on the transformer seq2seq model and 
takes future n-gram prediction as self-supervised.
Through fine-tuning the pretrained models or representations on summarization task,
the quality of generated summaries can be improved~\cite{HiBert19,ZhongLWQH19,MatchSum}.
PEGASUS~\cite{PEGASUS20} is a new, pretrained model for text summarization, 
which uses self-supervised objective Gap Sentences Generation to train 
a transformer seq2seq model. Different from previous pretrained models, 
it masks sentences rather than smaller continuous text spans.
We choose BART which has achieve the SOTA results on summarization tasks as a basic component.
Since these pretrained model are encoder-decoder models and can be used in the same context, %as BART, 
it has the potential to substitute BART in our ext-abs framework
and enjoy similar boost in accuracy and speed.

%In order to enhancing the alignment bewteen source documents and
%summaries in neural abstractive summarization,


%%and cover all keywords of the reference summaries.
%%Based on Extractor-Abstractor framework, 
%We propose a keyword-based extractor. Both extractor and abstractor are
%%Moreover, we fine-tune the extractor and abstractor 
%fine-tuned on pretrained models accompanied with comprehensive RL considering ROUGE scores and keywords.
 
