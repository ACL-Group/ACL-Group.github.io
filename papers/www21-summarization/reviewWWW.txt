----------------------- REVIEW 1 ---------------------
SUBMISSION: 742
TITLE: Keyword-aware Abstractive Summarization by Extracting Set-level Intermediate Summaries
AUTHORS: Yizhu Liu, Qi Jia and Kenny Zhu

----------- Relevance to the Web Conference -----------
SCORE: 1 (Relevant)
----------- Overall evaluation -----------
SCORE: -1 (Weak reject)
----------- Summary -----------
This paper presents an abstractive summarization approach that extracts set-level intermediate summaries based on overlapping keywords between source and target to generate final summary. Reinforcement leaning is used to guide both the extractor and abstractor. The proposed approach is evaluated using multiple public datasets showing improvement in standard evaluation metrics.
----------- Originality -----------
SCORE: 2 (Conventional: Rather straightforward, a number of people could have come up with this)
----------- Potential impact -----------
SCORE: 2 (Limited: Impact limited to improving the state-of-the-art for the problem being tackled)
----------- Reproducibility -----------
SCORE: 2 (Some but not all code, data or the details of the experimental setup are made available)
----------- Quality of execution -----------
SCORE: 2 (Poor: Potentially reasonable approach, but certain core claims lack justification)
----------- Quality of presentation -----------
SCORE: 3 (Reasonable: Understandable to a large extent, but parts of the paper need more work)
----------- Adequacy of citations -----------
SCORE: 1 (Inadequate: Literature review misses many important past papers)
----------- Ethics -----------
SCORE: 1 (No)
----- TEXT:
I do not have  ethical concerns for this study.
----------- Strengths -----------
* Abstractive summarization is an important problem having many real applications.
* Inference time of the proposed approach is compared with the baselines. Although summarization may not be a time sensitive task, this comparison is interesting to have.  
* The paper is well written. A running example helps understanding of the proposed approach as well as the difference with baselines.
----------- Weaknesses -----------
* The technical contribution is limited. The model is mostly a combination of existing techniques. The main contribution is mostly the set-level intermediate summaries that cluster input sentences into disjoint clusters based on keyword overlap.  
* There are some strong baselines missing. For instance, [1] presents a pre-trained language model that is much simpler than the proposed approach and the reported Rouge scores on CNNDM is even higher.

[1] Unified Language Model Pre-training for Natural Language Understanding and Generation.


----------------------- REVIEW 2 ---------------------
SUBMISSION: 742
TITLE: Keyword-aware Abstractive Summarization by Extracting Set-level Intermediate Summaries
AUTHORS: Yizhu Liu, Qi Jia and Kenny Zhu

----------- Relevance to the Web Conference -----------
SCORE: 1 (Relevant)
----------- Overall evaluation -----------
SCORE: 2 (Accept)
----------- Summary -----------
The authors explore the sub-field of abstract summarisation and, more specifically, address challenges with existing encoder-decoder summarisation models. They point out that the major drawback with existing models is tied to the latent alignment between source documents and summaries.

The authors propose a novel extractor-abstractor framework, which selects sets of sentences more aligned with the summary using three core components: a keyword-based extractor, an abstractor and a rewarded RL. The proposed framework, arguably, results in higher quality summaries and, additionally, efficient training speed and memory usage.
----------- Originality -----------
SCORE: 2 (Conventional: Rather straightforward, a number of people could have come up with this)
----------- Potential impact -----------
SCORE: 3 (Broad: Could help ongoing research in a broader research community)
----------- Reproducibility -----------
SCORE: 2 (Some but not all code, data or the details of the experimental setup are made available)
----- TEXT:
* All datasets used in the studies are clearly described
* The implementation and execution of the experiments are clearly outlined
----------- Quality of execution -----------
SCORE: 3 (Reasonable: Generally solid work, but certain claims could be justified better)
----------- Quality of presentation -----------
SCORE: 4 (Lucid: Very well written in every aspect, a pleasure to read, easy to follow)
----- TEXT:
1. Table captions are meant to be placed above, not below the tables
    * Please make sure that all table captions are above the tables

2. This submission is missing ACM CCS concepts, a standard aspect of the ACM sigconf template. The authors will need to include this important information.
    * The authors will need to ensure the submission conforms to the prescribed format and template
----------- Adequacy of citations -----------
SCORE: 3 (Comprehensive: Can't think of any important paper that is missed)
----------- Ethics -----------
SCORE: 1 (No)
----- TEXT:
No ethical concerns linked to this submission
----------- Strengths -----------
1. The use of a pre-trained language model that results in a framework that performs better than the state-of-the-art models

2. The use of five different “summarisation” datasets for experimentation results in comprehensive and rigorous experimental results

3. The inclusion of the “Keyword Coverage” aspect in the “Human Evaluation” experiments
----------- Weaknesses -----------
1. The “Human Evaluation” manual alignment accuracy experiment seems flawed as it is not clearly stated how the two annotators are best suited to provide the rating, especially the five different types of datasets are used

2. There is no mention of the usefulness and/or relevance associated with the resulting summaries
----------- Reasons to accept -----------
1. Very well-thought-through and designed experiments
2. Utilisation of five different datasets reinforces the effectiveness of the proposed approach
----------- Rebuttal -----------
1. How do you justify using two annotators in the “Human Evaluation” experiments, especially that five different types of datasets are used?

2. Why was there no focus on the usefulness and/or relevance of the resulting summaries?


----------------------- REVIEW 3 ---------------------
SUBMISSION: 742
TITLE: Keyword-aware Abstractive Summarization by Extracting Set-level Intermediate Summaries
AUTHORS: Yizhu Liu, Qi Jia and Kenny Zhu

----------- Relevance to the Web Conference -----------
SCORE: 1 (Relevant)
----------- Overall evaluation -----------
SCORE: 2 (Accept)
----------- Summary -----------
This paper tackles the abstractive summarization problem. It takes the technical route of extractor-abstractor pipeline, where the model first extracts salient sentences, then summarizes these intermediate results as the final summarization. One key problem of these models is how to generate pseudo labels for intermediate salient sentences.

The main contributions of this work are: (1) Proposing a (keyword-based) set-level salient sentence generation method. (2) Using CRL to connect the two parts and make the model end-to-end. The authors carry out comprehensive experiments to not only show SOTA results of proposed models, but also verify the effectiveness of each proposed component.
----------- Originality -----------
SCORE: 2 (Conventional: Rather straightforward, a number of people could have come up with this)
----- TEXT:
I think the biggest innovation of the paper is to propose a set-level salient sentence extractor/pseudo label generate. Compared to previous pseudo labels at sentence-level or summary-level, set-level pseudo label is both effective and easy to train in parallel.
----------- Potential impact -----------
SCORE: 2 (Limited: Impact limited to improving the state-of-the-art for the problem being tackled)
----------- Reproducibility -----------
SCORE: 1 (None of the code, data, and the details of the experimental setup are made available)
----------- Quality of execution -----------
SCORE: 4 (Thorough: The approach is well-justified and all the claims are convincingly supported)
----- TEXT:
The authors carry out comprehensive experiments to verify their ideas and each component of their models.
----------- Quality of presentation -----------
SCORE: 3 (Reasonable: Understandable to a large extent, but parts of the paper need more work)
----------- Adequacy of citations -----------
SCORE: 3 (Comprehensive: Can't think of any important paper that is missed)
----------- Ethics -----------
SCORE: 1 (No)
----- TEXT:
I don't have ethical concerns for this study.
----------- Strengths -----------
(1) The proposed set-level pseudo labels are intuitive and are better in effectiveness and efficiency (since it can be trained in parallel) than previous sentence-level and summary-level methods. Both effectiveness and efficiency are also verified by experiments.

(2) The two-stage model, connected by CRL, achieves state-of-the-art performance on several summarization benchmarks. The authors also justify the need for CRL by ablation studies.
----------- Weaknesses -----------
The presentation for Section 2 (Approach) could be improved. The current version is too dense and hard to follow.
----------- Reasons to accept -----------
As written in the strengths part, the proposed set-level pseudo summaries mitigate the problems with previous sentence-level or summary-level pseudo labels. The authors also successfully use CRL to connect the two-stage model and train it in an end-to-end manner. The effectiveness of all the components is verified by comprehensive experiments. Thus I agree with the acceptance of the paper.
----------- Rebuttal -----------
Do you have any insight about why other RL methods failed in Table 7?


----------------------- REVIEW 4 ---------------------
SUBMISSION: 742
TITLE: Keyword-aware Abstractive Summarization by Extracting Set-level Intermediate Summaries
AUTHORS: Yizhu Liu, Qi Jia and Kenny Zhu

----------- Relevance to the Web Conference -----------
SCORE: 1 (Relevant)
----- TEXT:
Automatic text summarization is certainly important for consuming web technology, although these works have more traditionally been disseminated through more NLP or ML specific venues. However, it is completely appropriate.
----------- Overall evaluation -----------
SCORE: 1 (Weak accept)
----------- Summary -----------
The key conceptual contribution of this paper is to take the ‘extractor-abstractor’ (ext-abs) framework for abstractive summarization and: (1) group the reference summary into a few (disjoint) sentence clusters where each represents a topic and (2) match a set of sentences in the source to these clusters (i.e., the alignment of ext-abs is many-to-many). To bias the alignment between the (intermediate) pseudo-summary and the reference summary, a keyword-centric loss is introduced. Building on previous RL/ROUGE-loss based methods, a multi-granular (sentence/set/summary) ROUGE loss is used (and trained in a similar way). Several design-choice alternatives are explored (e.g., BiLSTM vs. HiBERT) and a large set of experiments are conducted over five widely-used datasets and several competing methods using automatic and manual evaluation metrics on the pseudo-summaries and entire summaries (obviously different comparisons for different attributes). Additionally, computation co!
 ncerns are addressed (showing some computation-time improvements).
----------- Originality -----------
SCORE: 2 (Conventional: Rather straightforward, a number of people could have come up with this)
----- TEXT:
Conceptully, all of the ideas have been inspired by existing works (e.g., ext-abs, RL/ROUGE training, sentence clustering, keyword extraction, keyword-focused summarization), so the core insight is really the many-to-many mapping in this setting. That being said, I think it did require some insight to modify the various components to support this perspective, get it to work, and focus on comparisons with respect to each component to demonstrate effectiveness. I believe there is more actually more creativity here than may appear at first glance as the paper has more of a technical and understated presentation. That being said, there is nothing particularly methodologically innovative beyond getting the core idea to work — but a non-negligible innovation for a subfield that is pretty mature.
----------- Potential impact -----------
SCORE: 3 (Broad: Could help ongoing research in a broader research community)
----- TEXT:
There are two primary contributions for this paper (in my opinion). First, the many-to-many mapping in the context of summarization and associated evaluation. The second is a more general approach to hierarchical string transduction that essentially clusters and ‘translates’ with a keyword focus. Most of the field is interested in more directly hierarchical encoder-decoders, but I actually think there is something interesting here for entity/keyword-centric approaches that is efficient and might apply to other string transduction problems over longer texts.
----------- Reproducibility -----------
SCORE: 3 (All code, data, and the details of the experimental setup are made available)
----- TEXT:
There are a lot of details here; they promise code and I will take them at their word (and think that given the number of experiments that the code is probably actually pretty useful).
----------- Quality of execution -----------
SCORE: 3 (Reasonable: Generally solid work, but certain claims could be justified better)
----- TEXT:
From the perspective of validating the methodological approach, I believe the authors have done a fairly convincing job empirically — Table 4 shows the impact of using a set-based pseudo-summary, there is ablation around RL, encoder choices, keywords, etc. While the number of results could use some better discussion, the evaluation is thorough. In terms of evaluating relative to competing methods, again, there are a large number of evaluations against many baselines on many datasets. In looking for other recent papers, [Zhang, et al., PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, ICML20] seems to have gotten attention, has different datasets, and different numbers. They are all within the same range, but their code is available, so this could be contrasted. Otherwise, I think this is a detailed analysis in general.
----------- Quality of presentation -----------
SCORE: 3 (Reasonable: Understandable to a large extent, but parts of the paper need more work)
----- TEXT:
The approach is well-motivated, details are mostly described, and many empirical results are presented in a reasonable order. The discussion of results could be a bit stronger in terms of described ‘why/when’ different models perform better or at least what to look for as it sometimes reads as a ‘wall-of-results’ (but I understand there are space limitations). Overall, I was able to understand what they were doing relatively easily.
----------- Adequacy of citations -----------
SCORE: 2 (Reasonable: Coverage of past work is acceptable, but a few papers are missing)
----- TEXT:
This is a fast-moving field and thus, it is difficult to really be exhaustive. In addition to [Zhang, et al., PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, ICML20], [Li, et al., Keywords-Guided Abstractive Sentence Summarization, AAAI20] is relevant (albeit clearly different as it is sentence-focused).
----------- Ethics -----------
SCORE: 1 (No)
----- TEXT:
This uses well-established datasets for an established problem — I have no concerns.
----------- Strengths -----------
1. This is a solid advance on the ‘extractor-abstractor’ (ext-abs) vein of abstractive summarization that takes a simple idea (the many-to-many mapping in extraction) and works through it to get competitive results (ostensibly SoTA in several cases) while taking into account computation considerations.
2. I can certainly see some of the ideas in this work making its way into other longer-from string transduction problems.
3. The experiments are thorough and promising.
----------- Weaknesses -----------
1. As this is a maturing sub-field, it is difficult to assess how competitive the overall system is without a GLUE-like leaderboard. [Zhang, et al., PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, ICML20] uses other datasets and provides other numbers for some of the methods. I wasn’t able to easily coordinate and evaluate.
2. I think the discussion of experimental results could better focus the reader regarding what we are looking for. While not entirely a slew of results, there is a lot here to process (which is also a strength of the paper).
----------- Reasons to accept -----------
This paper is a sufficient innovation wrt ext-abs approaches to abstractive summarization, both conceptually and empirically. I think it was non-trivial to get this to work and the numerous ablation studies and intermediate results demonstrate that the conceptual explanations are supported with experiments.
----------- Reasons to reject -----------
A recent, well-regarded competing paper wasn’t considered or discussed ([Zhang, et al., PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, ICML20]). I don’t think the results in that paper diminishes these results, but it should be discussed.
----------- Rebuttal -----------
I am most interested in discussing results wrt [Zhang, et al., PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization, ICML20]. I am hoping this is a straightforward inclusion.
