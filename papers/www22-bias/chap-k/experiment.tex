\section{Experiment and Results}

We use BASIL (Bias Annotation Spans on the
 Informational Level) dataset proposed by \citet{fan-etal-2019-plain} for the sentence-level informational bias detection task.
We experiment with four baselines including the current state-of-the-art model and four variants of MultiCTX in order to fully demonstrate each module's utility. Our results suggest that MultiCTX greatly outperforms the current SOTA and effectively incorporates the contextual information in sentence-level informational bias detection.

\subsection{Data}

 BASIL dataset provides sentence-by-sentence span-level annotation of informational bias for 300 online English news articles grouped in 100 triplets, each discussing on the same event from three news outlets. The articles are selected in order to make a fair coverage in terms of time and ideology: 1) From 2010 to 2019, 10 events are included each year in the dataset; 2) Fox News (FOX), New York Times (NYT) and Huffington Post (HPO), representative of conservative, neutral and liberal respectively in the US journalism, are chosen as three news sources.

As for the sentence-level informational bias detection task, we use the same data formulation in \citet{van-den-berg-markert-2020-context}. In this sentence-wise binary classification task, a sentence is labeled as biased if at least one informational bias span occurs, and seven empty sentences are removed, resulting in a total of 7977 sentences with 1221 annotated bias.

Examples are shown in Table \ref{tab:basil}.


        % 92 & hpo & 1 & WASHINGTON -- Massachusetts Gov. Deval Patrick (D) on Wednesday appointed William Mo Cowan to the Senate seat vacated by newly confirmed Secretary of State John Kerry. & 0\\
        % \hline
        % 92 & hpo & 2 & Cowan will hold the seat in an interim capacity until an election in June. & 0\\
        % \hline
        % 92 & hpo & 3 & Patrick, Cowan and Lt. Governor Tim Murray were all smiles as they walked into a news conference to announce the appointment. & 1\\
        % \hline
        % 51 & fox & 0 & Kathy Griffin admitted Tuesday she "went too far" on her latest photo shoot with controversial photographer Tyler Shields in which she is holding a bloodied mask of President Trump. & 0\\
        % \hline
        % 51 & fox & 1 & The gory photo first published in TMZ from Griffin's photo session with the famed Shields, known for his shocking pictures.& 1\\
        % \hline
        % 51 & fox & 2 & In a Twitter post late Tuesday, Griffin said the image "is too disturbing," and that "it wasn't funny." & 0\\
        % \hline

% People always face a new event around which a cluster of / plenty of reports emerge altogether/in batches.

% the emergence of cluster of news reports is event-driven / reports occur in event wise,, as well as people's perception of the world.

% develop views
% get a bigger picture of a new event.

% 模拟人学习新事件
% 人们学新的事物的时候是不会有可能的标签的

% 人们看文章的时候一篇看完且要多看几篇

% 人们对新事件的认知来自于这些related的新闻
% 时间上来说（news nature） 新的news一般会有很多文章一起出来因此unlabeled
% 人类学习来说 每次去认识一个新事件

% Keeping sentences from same event 
% sentence-wise event-wise

% assign distribute divide partition
% subsets division partitions parts

% splitting event-wise rather than sentence-wise
% 之前关于 BASIL 语料库的工作采用随机划分句子形成训练(training)、验证(validation)和测试集(test)（Fan 等，2019）。 这种类型的划会分将目标句子与同一文章中的其他句子以及涵盖同一事件的其他文章分别划分到不同集合。 以这种方式跨集合划分句子不符合实际生活中人们获取新闻报道的方式：同一事件的新闻报道总是几乎在同一时间出现，而人们也通常会整体地阅读一篇文章而非从中随机抽取几个句子浏览；况且，将同一文章中的句子分布在训练和测试数据中可以被视为一种泄漏，因为知道文章中的某些句子有偏见可能有助于识别同一文章或同一主题的另一篇文章中的相似句子 . 因此，我们采用按照事件进行划分，即同一事件下地三篇文章总是同时出现在同一集合中，这不仅更符合现实，更对模型的泛化能力提出更高要求，即要求能识别从未出现过的新的事件中的信息偏差。


% 人类获取信息的习惯
% 信息泄露
% 更好的泛化能力

% When we 

% Since context information is intuitively essential, and ideal model should be able to generalize to any kind of 



% generalization, leakage,xxxx
% Table 1 describes the BASIL dataset. 




\subsection{Set-up}

We use the same 10-fold cross-validation event-split in \citet{van-den-berg-markert-2020-context} to facilitate the comparison.

Each fold has 80/10/10 non-overlapping events for train/val/test partition, and sentences from the same event never appear simultaneously in two different subsets within one fold. There are on average 6400/780/790 sentences in train/val/test set respectively. We use 5 different seeds for each method and the F1 score, precision and recall ('biased' is positive class) as the evaluation metrics. For each experiment, a mean value and standard deviation across 5 seeds will be reported if applicable.

We use the same hyper-parameters provided in \citep{van-den-berg-markert-2020-context} to reimplement BERT, RoBERTa and WinSSC baselines. However, for EvCIM, We cut the training epochs from 150 to 75 and increase the batch size from 32 to 64 due to the considerable time usage. For MultiCTX, We trained  use a RoBERTa-based contrastive learning following the implementation in \citep{gao2021simcse}. Due to unavoidable non-deterministic atomic operations in implementation of GAT, the result presented below may cannot be exactly reproduced, but we took an average on our experiments to reflect its range. All models are trained and evaluated on a GeForce GTX 1080 Ti GPU with 11G RAM and Intel(R) Xeon(R) CPU E5-2630 with 128G of RAM. Training details will be described in Appendix.

% For MultiCTX, We use epochs=5, max length=256, lr = 4.5e-5, training batch size=16 in RoBERTa-based contrastive learning following the implementation in \citep{gao2021simcse}. We use adjacent sentence limitation = 5, semantic similarity threshold=0.98 in the construction of relational sentence graphs. Due to unavoidable non-deterministic atomic operations in implementation of GAT, the result presented below may cannot be exactly reproduced, but we took an average on our experiments to reflect its range. We use a one-layer SuperGAT with heads=1, negative sample ratio = 0.9, dropout = 0.1, learning rate = 0.1, l2 regularization = 1e-4, attentional loss weight = 1e-3 and then train for 300 epochs to select the best result.All models are trained and evaluated on a GeForce GTX 1080 Ti GPU with 11G RAM and Intel(R) Xeon(R) CPU E5-2630 with 128G of RAM.


\subsection{Baselines}

There are few models in sentence-level informational bias detection. \citet{fan-etal-2019-plain} has proposed BASIL dataset and corresponding BERT and RoBERTa benchmarks. \citet{cohan-etal-2019-pretrained} has proposed several models trying to incorporate context in different ways. We will take two of them, WinSSC and their best and also current SOTA model EvCIM, as our baselines. Few other works used BASIL dataset but with objectives other than sentence-level informational detection. Thus we have four baseline models:

\begin{itemize}
    \item \textbf{BERT} \citep{devlin-etal-2019-bert} and \textbf{RoBERTa} \citep{liu2019roberta}: we finetune the individual sentence informational bias detection task on $\text{BERT}_{base}$ and $\text{RoBERTa}_{base}$.
    \item \textbf{WinSSC} \citep{van-den-berg-markert-2020-context}
        
        WinSSC (windowed Sequential Sentence Classification) is a variant of SSC \citep{cohan-etal-2019-pretrained}. We include it as one of the baselines because SSC implements the very natural idea that comes to us when we think of using context: directly inputing sequences of consecutive sentences to BERT. SSC feeds the concatenation of sentences from a chunk of document to pretrained language models (PLMs),and  then classifies each sentence using the embedding of the separator tokens \texttt{[SEP]} at its end. SSC makes non-overlapping chunks while WinSSC makes chunks by overlapping sentences at both ends, which eretains the contextual information for bookended sentences. 
        
    \item \textbf{EvCIM}\label{para:evcim}  : PLM embeddings + BiLSTM
    
        EvCIM (Event Context-Inclusive Model) proposed by \citet{cohan-etal-2019-pretrained} is the SOTA model on BASIL dataset and it also uses the contextual information.
         It takes the average of the last four layers of fine-tuned $\text{RoBERTa}_{base}$ as the sentence embedding, and then uses BiLSTM to encode each article from the same event as the target sentence. Finally it concatenates three article representations and the target sentence embedding to make the sentence-level prediction. Besides using the hyper-parameters from the original paper, we generate the result from a separate set of reasonable hyper parameters. We present below results both from the original paper and from our experiments.
        %  We reproduce it in our study using other reasonable hyper-parameters different with the original version. and will show both results from the original paper and from our experiments.
    
\end{itemize}



\subsection{Our Models}
\begin{itemize}
    \item \textbf{CSE: Contrastive Sentence Embedding}
    
    Classification by a logistic regression on sentence embeddings directly obtained from contrastive learning.
    
    \item \textbf{EvCIM w/ CSE}: CSE + BiLSTM
    
    Similar to EvCIM described in Section \ref{para:evcim}, we utilize BiLSTM-encoded context as well as the target sentence to perform the sentence-wise classification. However, instead of the average of the last four layers of fine-tuned $\text{RoBERTa}_{base}$ in EvCIM, we use CSE (Contrastive Sentence Embedding) in our study. Moreover, we also add news source embeddings before the final fully connected classification layer on top of BiLSTM-encoded in-event article embeddings. 
    
    In the original paper \citep{cohan-etal-2019-pretrained}, adding news source embeddings hurts EvCIM's performance, but because it is useful for EvCIM w/ CSE according to our experiments, we use this version here. This also indicates that CSE has better captured inherent properties of sentences compared to PLM embeddings. CSE can therefore well incorporate extra news media information rather than be disturbed by it.
    
    \item \textbf{MultiCTX w/o CSE}: PLM embeddings + SSGAT
    
    We use the original sentence embedding in EvCIM, which is the
    the average of the last four layers of fine-tuned $\text{RoBERTa}_{base}$ to build the relational sentence graph. We then apply Self-supervised GAT on the graph (SSGAT, Self-supervised Sentence GAT). In other words, we replace CSE in MultiCTX with EvCIM's sentence embedding.
    
    \item \textbf{MultiCTX}: our full model (CSE + SSGAT)
    
    MultiCTX first performs contrastive learning on carefully composed triplets to obtain CSE. It then builds relational sentence graph according to inter-sentence relationships. Finally, MultiCTX applies Self-supervised GAT above to get the final sentence informational bias prediction.
    
    
\end{itemize}



\begin{table*}[htbp]
  \centering
    \begin{tabular}{l|l|cccc}
    \toprule
     \multicolumn{2}{c}{ \textbf{Model$^{*}$}} & \textbf{Explanation} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
    \midrule
    % BERT in \citet{van-den-berg-markert-2020-context}   & 38.96 \pm 5.55 & 35.79\pm2.15& 37.00\pm 2.11 \\
    % \hline
    % RoBERTa in \citet{van-den-berg-markert-2020-context} & 43.12\pm1.03 & 41.29 \pm 1.37    & 42.16 \pm 0.30 \\
    % \hline
    % WinSSC in \citet{van-den-berg-markert-2020-context} & 42.28 \pm0.99& 36.94\pm 0.88  & 38.67 \pm 0.82 \\
    \multirow{5}{*}{baselines} & $\text{BERT}_{base}$  &  & $40.44 \pm 1.07^{**}$ & $31.65\pm1.11$& $35.49\pm 0.67$ \\ \cline{2-6}
    &$\text{RoBERTa}_{base}$ & & $44.588\pm0.80$ & $40.02 \pm 2.22$    & $42.13 \pm 1.02$ \\  \cline{2-6}
    &WinSSC & &$41.47\pm1.31$& $34.37\pm 0.57$  & $37.58 \pm 0.77$ \\ \cline{2-6}
    &EvCIM (our reproduction)
     &  \multirow{2}{*}{PLM embed. + BiLSTM} & $38.40 \pm 0.64$   & $48.53\pm 1.45$& $42.87 \pm 0.69$ \\
    &EvCIM (original paper)
      &  & $39.72\pm 0.59$ &  $49.60 \pm 1.20$ & $44.10\pm 0.15$ \\ \hline
    \multirow{4}{1cm}{models}&CSE   &  & $47.53^{***}$     & 40.13     & 43.51  \\ \cline{2-6}
    &EvCIM w/ CSE & CSE+BiLSTM &$48.53 \pm 0.73$     & $41.98\pm 0.36$    & $45.01 \pm 0.26$\\ \cline{2-6}
    &MultiCTX w/o CSE & PLM embed.+ SSGAT &   $46.89 \pm 0.71$  &  $42.88\pm 0.67$   & $44.79 \pm 0.63$ \\ \cline{2-6}
    &MultiCTX (full) & CSE+SSGAT &  $47.78 \pm 0.94$   &  $44.50 \pm 0.65$   & $\mathbf{46.08 \pm 0.21}^{****}$ \\
    \bottomrule
    \multicolumn{5}{l}{$^{*}$  All results are implemented or reproduced by ourselves except for the second EvCIM record} &\\
    \multicolumn{5}{l}{$^{**}$  Mean value and standard deviation across 5 seeds are reported if applicable} & \\
    \multicolumn{5}{l}{$^{***}$ CSE uses a linear regression therefore no randomness occurred, so the result is a deterministic value.}&\\
    \multicolumn{6}{l}{$^{****}$  The best result on a single run obtained in our experiments is \textbf{F1=46.74}} \\
    % Due to unavoidable non-deterministic atomic operations in implementation, the result here may cannot be reproduced exactly, but we took an average on our experiments to reflect its range. Also 

    \end{tabular}%
  \caption{ Results}
  \label{tab:res}%
\end{table*}%

We can obtain the following observations and conclusions:

% \paragraph{}
\textit{\textbf{1. Encoding sequential sentences brutally by PLM may fail.}}


Here we use $\text{RoBERTa}_{base}$ as pretrained language model in WinSSC. However, it obtains worse result (F1=37.58) than the original $\text{RoBERTa}_{base}$ (F1=42.13). The result is similar as in \citet{van-den-berg-markert-2020-context}. There are two possible reasons: First, we take sentence chunks instead of individual sentences as input, and doing so may introduce data reduction. Second, BERT-based pretrained language models are not good at processing long text. They simply join neighboring sentences, which may introduce more noise and complexity rather than help integrate the context. Therefore, brute-force concatenation of sequential sentences can rarely make use of the contextual information, and it probably brings in more noise and reduces the data quantity. 
%  and it may suffer from data reduction

% \paragraph{}
\textit{\textbf{2. Contrastive learning helps improve sentence embeddings.}}


The results show that contrasive sentence embeddings (CSE) classified simply with a logistic regression (F1=43.51) beats our reproduction of EvCIM (F1=42.87); moreover, CSE combined with BiLSTM (EvCIM w/ CSE, F1=45.01) outperforms EvCIM even more in comparison with the declared F1=44.10 in its original paper \citep{cohan-etal-2019-pretrained}. 

Note that EvCIM uses the average of the last four layers of fine-tuned $\text{RoBERTa}_{base}$ as the sentence embedding. Therefore, our results prove that contrastive learning produces better sentence representations than BERT-based PLMs. this can be achieved via contextual information incorporation. 

\begin{itemize}
    \item BERT-based PLM tends to encode all sentences into a smaller spatial region, which results in a high similarity score for most of the sentence pairs, even for those that are semantically completely unrelated. Specifically, when the sentence embeddings are computed by averaging the word vectors, they are easily dominated by high-frequency words, making it difficult to reflect their original semantics.
    \item Instead of individual sentences, CSE considers for each target sentence a context built up by all its positive and negative counterparts in related triplets. Among them, negative samples provide an article-level context and positive samples provide an event-level context. With the goal of contrastive learning to "distill essence", it learns from its context and naturally suppresses such shallow high-frequency-words features, thus avoiding similar representations of semantically different sentences. 
\end{itemize}



% Although the BERT-based model achieves good performance on many NLP tasks, its own derived sentence vectors are of unsatisfactory quality. 


% \paragraph{} 
\textit{\textbf{3. Sentence graph can effectively integrate context.}}


We can see that MultiCTX w/o CSE, i.e. PLM embed.+SSGAT (F1=44.79) outperforms EvCIM (F1=44.10 in the original paper and F1=42.87 from our reproduction). The two models both use averaged RoBERTa embedding as sentence embeddings. The former uses graph structure (SSGAT) while the latter use BiLSTMs to carry out classification. 

The results prove that our sentence graph structure is better in encoding contextual information than sequential models such as BiLSTM. We will examine different levels of context, i.e., adjacent sentences, the article and the the event context in the ablation study in the next section.


 
\textit{\textbf{4. Contrastive learning together with sentence graph achieves the best performance.}}

Our full model MultiCTX achieves F1=46.08 in the sentence-level informational bias detection task, significantly outperforms the current State-of-the-Art model EvCIM \citep{cohan-etal-2019-pretrained} (F1=44.10 declared in original paper). Possible reasons are: 1) BiLSTMs are limited to the event context in EvCIM; 2) MultiCTX uses better sentence representations (CSE); 3) MultiCTX incorporates the context in varying degrees explicitly using graph structure and implicitly via contrastive learning.