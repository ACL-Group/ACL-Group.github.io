We highly appreciate every reviewer's constructive feedback and provide explanations to some critical questions below. 
Concerns about the 0.5 threshold (@Reviewer1,3)
Our aim is to examine model performance in general settings, so we would like to minimize the number of hyperparameters to tune, like threshold.  We choose 0.5 as it is a common choice for binary classification (sigmoid(0)=0.5). We trained and validated all models with this threshold, and they achieved balanced Precision and Recall for binary detection (e.g. P=R=F=62.96% for feature-rich on eRisk2017). The unbalanced P/R for baselines in early detection is due to its special streaming setting. The model capable of giving precise prediction given all posts, may give false alarms with the "partial" seen data. It is possible to separately search for another threshold for test-time early detection that can achieve better performance, but it may run the risk of overfitting on the test-set, so we opt not to do so.

Introduction & Comparison with previous works (@Reviewer1,3,4)
Due to the sensitivity of depression detection, most previous work did not release their datasets or code, which makes comparison very difficult. We've tried our best to reimplement and compare with competitive baselines, like BERT(Clus+Abs) is a reimplementation of DepressionNet (Zogan et al., SIGIR21') with BERT on our data. We also observed that many competitive methods [5,11,21] adopted the same backbone model LR and similar features (e.g. TF-IDF, LIWC, LDA). We thus reimplement them and call them feature-rich. We might still miss a few, while the current methods may already constitute a meaningful comparison. 
Specifically for the paper mentioned by Reviewer3, we are aware of this pioneering work studying depressive symptoms. We initially didn't include this work in our comparison because it deals with post-level symptom identification instead of user-level depression detection. But we may supplement the introduction of these papers in our future version for a more appropriate acknowledgement of prior works.

Confusion about Lexical analysis section (@Reviewer1)
The comparison is made on users with depression labels. This part is to test the convergent validity of our method, since we have similar findings as previous works on depression detection. A further finding is that our method is able to select truly risky posts out of the whole posting history (risk estimated with these lexical measures here), which explains the effectiveness of the proposed method.

How to operationalise psychiatric scales to depression templates (@Reviewer2)
Psychiatric scales like BDI-II are already in the form of natural language. They assess depression symptoms in multiple dimensions, and provide descriptions for different levels of severity on each dimension. For example, "I do not feel sad", "I feel sad much of the time" and "I am sad all the time" are regarded as scoring 0,1,2 in Sadness. We pick sentences from those with positive scores and make minimal modifications to constitute our templates (e.g. I feel sad.)

About dataset inconsistency (@Reviewer2)
The datasets we adopted are all well-recognized ones in the research field as they are official ones published by challenges. Although they label "depression" with different methods, they are all valid proxy signals of the same disease (DSM-5 "Depressive Disorders"), covering overlapping but slightly different subsets of patients. The relatively robust performance across these depression subsets of the proposed methods can thus indicate its generalizability.

The basis for ascertaining queue length ‘K’ (@Reviewer3)
We empirically set the initial post number as 16. As can be seen in Figure 5, our models' performance can be enhanced with more posts, but it will affect inference efficiency and early detection latency. We thus stick to 16 finally.

The "as few posts as possible" question (@Reviewer4)
"As few user posts as possible" actually means "as early as possible". We would prefer a system that can precisely detect depression when the user has just posted K posts, before he/she posts more, which will be reflected in metrics like lower ERDE. This is specifically for early detection, where the dataset is not static, and user posts come in streams. 

We also greatly appreciate the other constructive comments, and we will make improvements based on them in the final version.
