\section{Survey of KBQA}

QA systems on knowledge bases are used to answer factoid questions.
Based on how the query structure is represented, we split state-of-the-art QA techniques into 3 categories.

\subsection{Semantic Parsing}

Semantic parsing technique aims at translating questions directly into structural
query graphs through pre-defined grammars.
Combinatory categorial grammar (CCG) is commonly used in semantic parsing~
\cite{kwiatkowski2013scaling,cai2013large}.
In CCG grammar, each word maps to one (or more) lexical entry which contains a syntactic type and a semantic logical form.
The parsing tree is built bottom-up: at each step, two neighbour nodes can be composed into a
new node if the syntactic type is compatible, and the new logical form is formed by function applying between logical forms of these two children nodes.
Finally, the logical form of root node represents the query structure of the whole question.

The advantage of CCG is the strong expressive strength,
However, it could be a burden of system to learn complex logical form for each word,
when given only a small number of QA examples.
Reddy et al.~\shortcite{reddy2016transforming} alleviates the problem by
building semantic parsing tree based on the dependency parsing result.
Partially parse sub-trees are composed through dependency arcs, therefore the system only need to learn logical forms on limited numbers of distinct dependency arcs,
which reduces the complexity.
%(check the paper and find the sentence about comparison between this and CCG)
%(claim: dependency path is useful in well-grammar sentences, like Free917)

The result logical form of CCG is an ``unground'' structure (independent of knowledge base),
and there are also grammars directly generates a KB-specific semantic representation.
$\lambda$-DCS is one such grammars used in state-of-the-art QA systems~
\cite{berant2013semantic,berant2014semantic,berant2015imitation}.
In this grammar, each word can map to entities, type or predicates in the KB, and these elements are composed
under several pre-defined rules (join, intersection and aggregation), which is simpler than CCG grammar.


\subsection{Slot Filling}

Slot filling technique also aims at finding a structural representation.
To distinguish from semantic parsing, slot filling method provides the template of a query structure,
and the task has turned to fill suitable KB predicates into templates.
Bast and Haussmann \shortcite{bast2015more} proposed the QA system which based on 3 different query templates:
query as a single predicate, query as a two predicates with mediator, and query as three predicates with mediator.
The last template is most complex and it can represent a ternary relation in the question.

Yih et al. \shortcite{yih2015semantic} generated query structure via stages: first generate path of predicates
from focus entity to answer entity,
and then search for additional constraints applied to the nodes on the predicate path.
Typical features contain the overall similarity between question surface and predicate path (learnt from a CNN model),
as well as association confidence between constraints and certain indicating words in the question.
Further more, Xu et al. \shortcite{xu2016enhancing} followed Yih's idea: use deep neural network to predict the most suitable
path of predicates, given the question surface and its dependency parsing results.

In general, templates are less expressive than semantic parsing, which sacrifices the performance of complex questions, but error propagating in the generating process can be reduced, since bottom-up construction is avoided.
Since no annotated query structures are provided, training steps in both semantic parsing and slot filling techniques are \textit{distant supervised}.


%SP & SF: q --> structure --> SPARQL


\subsection{Answer Based}

Answer based techniques actually don't focus on the query structure of the question,
instead it search for answer entities directly and seek syntactic-semantic features
from the local graph of the candidate answer entity.
Generally speaking, the ``local graph'' consists of (at least one) predicate path
from focus entity to the candidate answer, as well as neighbour entities to the candidate in KB.
In this approach, QA task is a \textit{direct supervised} problem
rather than \textit{distant supervision}, 
thus QA examples won't be used to generate ``gold'' query structures.
Yao~\shortcite{yao2015lean} built a logistic regression model for QA task, where each feature
capture the assocation between unigrams (or bigrams) in the question and relations in the predicate paths.
Meanwhile, Bordes et al.~\shortcite{bordes2014question} learned a embedding model to map questions and answers
into a uniform vector space, where one candidate answer is encoded by its types, predicates and neighbour entities.


\subsection{Benchmarks and Evaluation Results}

Much recent work on natural-language queries on knowledge bases has focused on two recent benchmarks, both based on Freebase: Free917 and WebQuestions.
Free917 consists of 917 questions along with the correct knowledge base query.
All queries have exactly one (possibly k-ary) relation.
Webquestions are constructed via crowdsourcing, which is much larger (5,810 questions)
than Free917, but only provides the result set for each question, not the
knowledge-base query. 
\tabref{tab:benchmark} shows the results on Free917 and Webquestions for all the systems introduced above.
\begin{table}[ht]
	\centering
	\caption{Results on Free917 and Webquestions}
	\begin{tabular}{l|cc}
		%\toprule
		\hline
		Method				& \tabincell{c}{Free917 \\ Accuracy} & \tabincell{c}{Webquestions \\ Average F1} 	\\
        \hline
        Cai and Yates \shortcite{cai2013large}                  & 59.0  & -     \\
%        \hline
		Berant et al. \shortcite{berant2013semantic}     	    & 62.0	& 35.7	\\
%       \hline
        Kwiatkowski et al. \shortcite{kwiatkowski2013scaling}   & 68.0  & -     \\
%        \hline
        Bordes et al. \shortcite{bordes2014question}            & -     & 39.2  \\
%        \hline
        Berant and Liang \shortcite{berant2014semantic}         & 68.5  & 39.9  \\
%        \hline
        Yao \shortcite{yao2015lean}                             & -     & 44.3  \\
%        \hline
        Bast and Haussmann \shortcite{bast2015more}             & 76.4  & 49.4  \\
%        \hline
        Berant and Liang \shortcite{berant2015imitation}        & -     & 49.7  \\
%        \hline
        Reddy et al. \shortcite{reddy2016transforming}          & 78.0  & 50.3  \\
%        \hline
        Yih et al. \shortcite{yih2015semantic}                  & -     & 52.5  \\
%		\hline
        Xu et al. \shortcite{xu2016enhancing}                   & -     & 53.3  \\
        \hline
	\end{tabular}%
	\label{tab:benchmark}%
\end{table}

